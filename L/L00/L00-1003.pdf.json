{"sections":[{"title":"Enhancing the TDT Tracking Evaluation Amit Bagga","paragraphs":["GE Corporate Research and Development","1 Research Circle","Niskayuna, NY 12309. USA","bagga@crd.ge.com","Abstract Topic Detection and Tracking (TDT) is a DARPA-sponsored initiative concerned with finding groups of stories on the same topic (tdt, 1998). The goal is to build systems that can segment, detect, and track incoming news stories (possibly from multiple continuous feeds) with respect to pre-defined topics. While the detection task detects the first story on a particular topic, the tracking task determines, for each story, which topic it is relevant to. This paper will discuss the algorithm currently used for evaluating systems for the tracking task, present some of its limitation, and propose a new algorithm that enhances the current evaluation."]},{"title":"1. Introduction","paragraphs":["Topic Detection and Tracking (TDT) is a DARPA-sponsored initiative concerned with finding groups of stories on the same topic (tdt, 1998). The goal is to build systems that can segment, detect, and track incoming news stories (possibly from multiple continuous feeds) with respect to pre-defined topics. While the detection task detects the first story on a particular topic, the tracking task determines, for each story, which topic it is relevant to. In more precise terms, the tracking task is specified as follows: given N","t training stories on a topic, the system must find all subsequent stories on the same topic in all tracked news sources. These sources include radio and television news broadcasts, as well as newswire feeds. The initial set of training stories (usually 1, 2, or 4) is the only information about the topic available to the tracking system. This paper will discuss the algorithm currently used for evaluating systems for the tracking task, present some of its limitation, and propose a new algorithm that enhances the current evaluation."]},{"title":"2. Description of the Current Tracking Evaluation Algorithm","paragraphs":["The current algorithm measures the performance of systems in terms of miss and false alarm (FA) rates. A “miss” occurs when, for a particular topic, the system identifies a story to be non-relevant when it actually is relevant. A “false alarm”, on the other hand, occurs when, for a particular topic, the system identifies a story as relevant when it actually is not. The computation of the miss and false alarm rates can be done in two ways:"," By computing the miss and false alarm rates without any regard to the topic – the unweighted or Pooled method."," By computing the miss and false alarm rates such that all topics have equal weight – the Weighted method. While the pooled method minimizes the variance caused by individual story decisions, the weighted method has the advantage of minimizing the variance of the estimates caused by topic differences. Because of the small number of topics in TDT-2, and because of topic inhomogeneity, the weighted method was actually used for that evaluation. We describe, in detail, each of these methods. 2.1. The Unweighted or Pooled Method The unweighted tracking evaluation algorithm mea-","sures the miss and the false alarm rates as shown in the","equations in Figure 1. In these equations, the summation","is over all topics, where Storiest",", for each topic t",", is the","number of non-training stories to be tracked, and where","","sys t s","","is as shown in Figure 2. Similar to","sys","t","s","",",","","ref t s","","is 1 if the answer key (or, the truth) deemed that","topic t was discussed in story s, and 0 otherwise. A more","detailed explanation of the equations in Figure 1 follows. As mentioned earlier, P","Miss estimates the percentage","of times the system identifies a relevant story as being non-","relevant. Therefore, the denominator in the P","Miss equation","counts, for each topic, the number of stories that are actu-","ally relevant to that topic since","ref","t","s","","is 1 iff topic t","is","mentioned in story s",". Computing the sum over all topics","yields the total number of stories, in the collection, that are","relevant to at least one topic. It should be noted that the","TDT evaluation enforces that a story is relevant to only one","topic. In the numerator of the equation for P","Miss, the","","","sys t s","","","ref","t","s","","product is 1 iff","sys","t","s","","is 0 and","","ref t s","","is 1. In other words, the product is 1 iff the sys-","tem determines that t","is not mentioned in s","when the truth","determines that it is. Therefore, computing the sum of this","product over all stories and over all topics yields the total","number of occurrences of a missed story. Similarly, the denominator in the P","FA equation com-","putes, for each topic, the number of stories that are not rel-","evant to that topic. However, since a story is relevant to","only one topic, when the sum is computed over all topics,","each story is counted Topics","","(where Topics is the to-","tal number of topics) times as it is not relevant to each of","these Topics","","topics. Therefore, this sum is significantly","greater than the total number of stories, Storiest",", in the col-","lection since, for each topic, a large number of stories are","not relevant to it. P Miss X t  Topics    X s  Storiest n","","","","sys t s ","","ref","t","s"," o     X t  Topics    X s  Storiest","","ref t s","    P FA X t  Topics    X s  Storiest n","","sys","t","s   ","","","ref","t","s"," o     X t  Topics    X s  Storiest","","  ref","t","s","    Figure 1: Equations for the Current Evaluation Algorithm","","sys t s","   if the system deemed that topic t was discussed in story s  otherwise","Figure 2: Definition of","sys t s","","The","sys","t","s","",""," ","","ref","t","s","","product in the numerator","of the P","FA equation is 1 iff","sys","t","s","","is 1, and","ref","t","s","","is","0. In other words, the product is 1 iff the system determines","that t","is mentioned in s","when the truth determines that it ac-","tually is not. Therefore, computing the sum over all stories","and over all topics yields the total number of occurrences","of a false alarm.","The Weighted method is discussed later in the paper in","Section 5."]},{"title":"3. Limitation of the Current Algorithm","paragraphs":["While the evaluation algorithm has been successful in measuring the performance of systems, it has three main limitations.","1. It is possible for a system to get low miss and false","alarm rates in cases where the system actually makes","a lot of errors. Consider the following example. Sup-","pose we are tracking only one topic, and the num-","ber of stories that are going to be tracked is 1000","(S","","S","","","","S","","). Also suppose that the truth iden-","tifies that S  S","","S","","are the stories relevant to the","topic while the others are not relevant. Now if a system","responds by tracking S","","S","","S","","S","","","S","","as relevant to the topic while identifying the rest to","be non-relevant, then P Miss  ","  ","since it only misses one story, S ",". In addition, P FA  ","  ","since the system identifies 91 (S  S","",")ofthe","990 non-relevant stories as actually being relevant.","But, the precision in this case is  ","","","","","i.e.","the system correctly tracked only 9 out of the 100 arti-","cles. The poor performance of the system in this case","is not accurately reflected by the low miss and the false","alarm rates which actually indicate good performance.","2. The algorithm penalizes all errors equally. Consider","the following example. Suppose we have four top-","ics t","","t","","t  , and that, for each topic, Storiest",",","the total number of stories to be tracked for that","topic, is 500. Also suppose that the truth deems that","S","","","S","","are relevant to topic t","",", S","","","S","","are relevant to t  , S","","S","","are relevant to t","",",","and S","","S ","are relevant to t","",". Now if a re-","sponse, R","",", has a system identifying that S","","","S","","are relevant to t  , S","","S","","are relevant to t","",",","and S","","","S ","are relevant to t","",", then, P Miss              ","  ","since the only stories missed are the 50","(S","   S","",") relevant to t","",". In addition, P FA                ","  ","since the system identifies, for topic t","",", 50 stories","(S","   S","",") as being relevant when they actually","are not.","But, if a response, R ",", has the system identifying that","S","","S","","","","S"," are relevant to t","",", S","","S","","are","relevant to t","",", and S","","S","","are relevant to t","",",","then, P Miss              ","  ","and, P FA               ","  ","Therefore, the current algorithm assigns equal miss","and false alarm scores for both the responses in the","example above when in fact R","","merged all the arti-","cles corresponding to a large topic (in the number of","documents relevant), t","",", and a small topic, t","",", together","while R  merged all documents corresponding to the","two small topics, t","",", and t","",", together. We feel that R","","should have been penalized more heavily than R","","be-","cause R  implicitly creates a larger number of errors","by linking the 200 document topic with the 50 docu-","ment topic.","3. The False Alarm measure deflates the actual error rate because for each story it credits the system for not making a mistake multiple times. This happens because the algorithm considers all non-relevant stories when computing the false-alarm rate. Since a story is relevant to only one topic (i.e. it is not relevant to the remaining topics), the system is credited for not making a mistake for each of the t","","","topics (if t","is the total number of topics). The deflation actually occurs because of the denominator of the equation that calculates P","FA . As mentioned earlier, the value of the denominator is usually significantly greater than the total number of stories in the collection since, for each topic, a large number of stories are not relevant to it. Therefore, computing this sum over all topics yields a rather large number compared to the number of stories. For instance, in the example described above, the denominator for P","FA is 1500, or three times the total number of stories."]},{"title":"4. The B-CUBED Algorithm","paragraphs":["The B-CUBED algorithm is based upon an algorithm developed to score coreference chains(Bagga and Baldwin, 1998). It was designed specifically to identify and penalize both explicit as well as implicit errors. The algorithm measures performance in terms of precision and recall. These are defined as shown in Figure 3. In the equations shown in the figure, ST is the set of stories being tracked, Topics is the set of topics in the response, and","ref, and","sys are as defined earlier for the current algorithm.","Alternatively, for each story, s",", Precision","s",", and Recall","s can be explained, in words, as in Figure 4. The final precision and recall numbers are computed by taking an average of P","s",", and R","s",", for all stories s","in ST.","The B-CUBED algorithm computes precision and recall with respect to the stories tracked. As shown in Figure 4, the algorithm computes the numerator of the precision equation, for each story s",", by analyzing the topic that s is actually relevant to (as deemed by the truth), and computing the number of stories in common with the topic in the response containing s",". The denominator of the equation is computed by counting the total number of stories relevant to the topic in the response that s","is relevant to. While the numerator of the recall equation is the same as that of the precision equation, the denominator is computed by counting the number of stories relevant to the topic in the truth that s","is relevant to.","The equations in Figure 4 are almost equivalent to the","formulae in Figure 3 the only difference being that the for-","mulae in the latter figure compute final precision and recall.","In the formula for precision,","sys","t","s","","has value 1 only","when t","is instantiated to the topic that the system deems","s","relevant to. For all other topics it has value 0. Suppose","the topic that the system determines that s","is relevant to is","topic x",", then the denominator of the fraction computes the","total number of stories that are relevant to topic x",", as deter-","mined by the response. The numerator of the fraction, on","the other hand, is the sum of the products of three expres-","sions:","ref","t","","s","",",","ref","t  s","","",", and","sys","t","s","","",". Therefore,","the numerator counts the number of times each of these","three expressions simultaneously have value 1.","ref","t","","s","","has a value 1 when t","","is the topic that s","actually is relevant","to, say topic y","(note: if the response is correct, then y","is the","same as x","). Therefore, ref","t","","s","","","is 1 iff s","","is one of the","stories relevant to topic y in the truth. Finally,","sys","t","s","","","is","1 iff the story that is relevant to topic y","in the truth is also","relevant to topic x","in the response. Thus, the numerator","of the fraction counts, for each story s",", the number of sto-","ries in common between the truth and the response topics","containing s",".","For recall, the numerator of the fraction is the same as","the numerator of the fraction in the formula for precision.","However, the denominator is the sum of the products of","two expressions:","ref","t  s","",", and","ref","t","","s","","",". Therefore,","the denominator counts the number of times both these ex-","pressions simultaneously have value 1.","ref","t","","s","","is 1 iff t","","is the topic that s","is actually relevant to (topic y","). Moreover,","","ref","t","","s","","","is 1 iff s","","is a story that is actually relevant to","topic y",". Therefore, the product counts the total number of","stories that are relevant to the topic that s","actually is rele-","vant to.","Consider, for example, the response R","","presented ear-","lier while describing the second limitation of the current","algorithm. The precision for this response would be calcu-","lated in the following way. P","","","P","","","","","P","","","","","since for each of these stories, s",", the system identifies 200","stories as relevant to the topic that s","is actually relevant","to (t","","). However, P"," ","","","P","","","","","since for","each of these 200 stories, s",", only 200 out of the 250 sto-","ries are actually stories in the topic that s","is actually rele-","vant to (t","","). Similarly, P ","","","","P","","","","","since","for each of these stories only 50 stories are relevant to the","topic that the story itself is actually relevant to (t","","). Finally,","P","","","","","P","","  ","since for each of these stories,","s",", the system identifies the 50 stories in the topic that s","is","actually relevant to (t"," ). Therefore, the final precision, the","average of the precisions for the 500 stories, equals   ","     ","    ","    ","                ","  ","","In comparison, the precision for R  would equal   ","      ","    ","    ","   ","P   ST   X s  ST X t  Topics","","sys t s","","   n P s   ST P","t","","","Topics","ref","t","","s","",""," ref t","","s","","","","","sys","t","s",""," o P","s","","","ST sys","t","s","","  A  ","R   ST   X s  ST X t  Topics","","sys t s","","   n P s   ST P","t","","","Topics","ref","t","","s","",""," ref t","","s","","","","","sys","t","s",""," o P s   ST P","t","","","Topics","ref","t","","s  ","","ref","t","","s","","  A   Figure 3: Equations for the B-CUBED Algorithm P s ","","of","stories","in","c","ommon","b","etwe","en","topic","t","hat","s","ystem","d","e","e","m","ssr","el","to","and","t","opic","that","s","i","s","actually","r","el","to","total","numb","er","of","stories","de","eme","d","b","y","s","ystem","as","r elevant t o t opic","that","it","de","ems","s","i","s","r","elevant","to R s ","","of","stories","in","c","ommon","b","etwe","en","topic","that","system","de ems s r","el","to","and","t","opic","that","s","i","s","actually","r","el","to","total","numb","er","of","stories","d","e","eme","db","y","truth","as","r elevant t o t opic","that","s","i","s","actually","r","elevant","t","o","Figure 4: Descriptions of Precision s , and Recall","s","for the B-CUBED Algorithm             ","   ","In addition, if there was a third response, R","","which merged","the two large topics, t  and t","",", together, then the precision","for R","","would equal   ","      ","    ","    ","                ","   ","On the other hand, the recall for all the three responses","would equal 1 as the system, for each of the responses,","groups all the articles belonging to the same topic together.","In other words, for each of the topics, all the stories cor-","responding to that topic are grouped together although the","grouped stories may actually have been placed in a wrong","topic. For example, in R"," , the stories corresponding to","topic t","","are grouped together and are incorrectly identified","as being relevant to t","",". We classify the errors in this case as","precision errors and not as recall errors. Therefore, if even","if one of the stories relevant to t ","(S","","","S","",") had been","identified as being relevant to a topic other than t","",", recall","would have been below 1. Similarly, for responses R","",", and","R","",", the stories corresponding to t","","and t","","respectively are","grouped together but judged relevant to the wrong topics.","To illustrate how recall is computed, we compute the","recall for a new response, R","",", below. R","","classifies","S","","","S","","as relevant to t  , S","","S","","as relevant","to t","",", S","","","S","","as relevant to t","",", and S","","","S","","as relevant to t","",". Therefore, R ","","R","","","","","R","","","","","since for each of these stories, s",", the system identifies 200","stories as relevant to the topic that s","is actually relevant to","(t","","). In addition, R","",""," ","R","","","","","since for each","of these 200 stories, s",", the system identifies the 200 stories","relevant to the topic that s is actually relevant to (t","","). It","should be noted that the denominator in this case is 200,","and not 240, as the total number of stories deemed by truth","as relevant to topic that each of these stories is relevant to","is 200 (corresponding to topic t  ). Similarly, R","","","","","R","","","","","since for each of these stories the system identi-","fies 40 of the 50 stories in the topic that the story itself is ac-","tually relevant to t","",". Moreover, R","","","","","R","","","","",".","Finally, R","","","  R","","","","",". Therefore, the final","recall, the average of the recalls for the 500 stories, equals   ","      ","    ","    ","   ","     ","             ","  ",""]},{"title":"5. The Weighted Method","paragraphs":["As mentioned earlier, the weighted method assigns equal weights to all topics as opposed to equal weights for all stories. This section describes the weighted methods for both the current and the B-CUBED algorithms. 5.1. Current Algorithm","Figure 5 shows the formulae for P","Miss, and P","FA when the topics have equal weights. In the formulae, N","Topics equals the total number of topics. Therefore, for computing the miss rate, the formula for P","Miss calculates, for each topic, the percentage of relevant stories which were marked not relevant by the system. Similarly, the false alarm rate is computed by calculating, for each topic, the percentage of non relevant stories which are marked relevant by the system. Final miss and false alarm rates are computed by taking an average for the percentages obtained for each of the topics. P Miss ","N Topics X t  Topics    X s  Storiest n","","","","sys t s ","","ref","t","s"," o  X s  Storiest","","ref t s","    P FA ","N Topics X t  Topics    X s  Storiest n","","sys","t","s   ","","","ref","t","s"," o  X s  Storiest","","  ref","t","s","    Figure 5: Equations for the Current Evaluation Algorithm – Weighted Method","For example, consider the response R","","described earlier","in the paper. The miss rate for this response equals                  ","  "," Moreover, the false alarm rate for this response equals                  ","  ","","In comparison, the miss rate for response R  equals                  ","  ","","Finally, the false alarm rate for R  equals                  ","  ","","It should be noted that while the weighted version of the","current algorithm does penalize R  more than R","","for false","alarm error rates, the difference is very small.","5.2. B-CUBED Algorithm Figure 6 shows the formulae for computing Precision,","and Recall using the B-CUBED algorithm when the topics","have equal weights. In the formulae, NT","sys equals the to-","tal number of topics identified by the system in the response","while NT","ref equals the total number of topics identified in","the truth. Topics is the set of topics in the response. There-","fore, the precision formula assigns equal weights to only","those topics identified in the response while the recall for-","mula assigns equal weights to the topics in the truth. The formulae in Figure 6 can also be explained in terms","of the equations shown in Figure 4. While the unweighted","version of the B-CUBED algorithm assigns equal weights","to each story when computing the final precision and re-","call numbers, the weighted version of the algorithm assigns","equal weights to each topic when computing the final pre-","cision and recall numbers. Therefore, if a response identi-","fies three topics, t","","t","","t","",", with 5 stories relevant to t","",", then","the weights assigned to each of these five stories would be","","   ","","","",". Similarly, if the response identified 7 sto-","ries relevant to t","",", the the weights assigned to each of them","would be","",". The final precision and recall is computed by taking this weighted average of the precision and recall numbers corresponding to each story. For example, using the formulae in Figure 6, the precision for response R","","described earlier in the paper equals       ","       ","     ","        ","                ","  ","","In comparison, the precision for response R  equals       ","       ","       ","     ","               ","  "," As explained earlier, the recall for both these responses","is 1. For example, the recall for response R","","is calculated","as shown below       ","       ","       ","       ","         ","","However, the recall for response R  equals       ","       ","       ","       ","       ","             ","  ","The weighted B-CUBED algorithm computes, for each topic, the precision and recall for the set of stories corresponding to the topic irrespective of the number of documents that are relevant to the topic. In other words, precision computes, for each topic, the percentage of stories correctly identified by the response as relevant to the topic out of the total number of stories identified by the response","P  ","NT sys X t  Topics   P","s","","ST sys","t","s","    X s  ST","","sys t s","","   n P s   ST P","t","","","Topics","ref","t","","s","",""," ref t","","s","","","","","sys","t","s",""," o P","s","","","ST sys","t","s","","  A     ","R  ","NT ref X t  Topics   P s  ST P","t","","","Topics","ref","t","","s  ","","ref","t","","s","","    X s  ST","","sys t s","","   n P s   ST P","t","","","Topics","ref","t","","s","",""," ref t","","s","","","","","sys","t","s",""," o P s   ST P","t","","","Topics","ref","t","","s  ","","ref","t","","s","","  A      Figure 6: Equations for the B-CUBED Algorithm – Weighted Method","as relevant to the topic. Similarly, recall computes, for each","topic, the number of stories correctly identified by the re-","sponse as relevant to the topic out of the total number of","stories that are actually relevant to the topic. This feature","is illustrated by the example below. In this example, we","proportionally reduce the size of the topics (in terms of the","number of stories actually relevant to the topics) used in","the previous example by a factor of 50. Therefore, the truth","now assigns documents S","","S","","to topic t","",", S","","S","","to topic t","",", S","","to t","",", and S","","to t","",". Suppose response R","","assigns S","","","S","","to t  , S","","S","","to t","",", and S","","to t","",".In","other words R","","mirrors R","","described earlier, the only dif-","ference being the size of the topics. The precision for R","","equals                   ","                          ","   which is the same as the one for R","",". Similarly, the recall for R ","is also 1. Due to this feature of the weighted B-CUBED algorithm, we prefer the unweighted B-CUBED algorithm. However, there may be scenarios when the weighted version is preferable."]},{"title":"6. Conclusion","paragraphs":["Given the build-test-improve cycle of development that is dominant in the development of natural language processing systems today, there is greater reliance on the use of standard evaluation algorithms than ever before. The type of scoring algorithm used greatly influences system development, and tuning. Therefore, it is extremely important that these scoring algorithms measure diverse aspects of the system performance so that the developers can use the collective scores to tune their systems accordingly. The B-CUBED algorithm presented in this paper is meant to enhance the current TDT tracking evaluation algorithm by providing two additional aspects: precision and recall to measure the performance of these systems."]},{"title":"7. References","paragraphs":["1998. The TDT Web Page. NIST, http://www.nist.gov/ speech/tdt3/tdt3.htm.","Bagga, Amit and Breck Baldwin, 1998. Algorithms for Scoring Coreference Chains. In The First International Conference on Language Resources and Evaluation Workshop on Linguistics Coreference."]}]}