{"sections":[{"title":"Coping With Ambiguity in a Large-Scale Machine Translation System","paragraphs":["Kathryn L. Baker, Alexander M. Franz, Pamela W. Jordan,","Teruko Mitamura, Eric H. Nyberg, 3rd Center for Machine Translation Carnegie Mellon University","Pittsburgh, PA 15213 Topical Paper: machine translation, parsing"]},{"title":"Abstract","paragraphs":["In an interlingual knowledge-based machine translation system, ambiguity arises when the source 1.qnguage analyzer produces more than one interlingua expression for a source sentence. This can have a negative impact on translation quality, since a target sentence may be produced from an unintended meaning. In this paper we describe the ,nethods nsed in the KANT machine translation system to reduce or eliminate ambiguity in a large-scale application domain. We also test these methods on a large corpus of test sentences, in order to illustrate how the different disambiguation methods redtuce the average number of parses per sentence, 1 Introduction The KANT system [Mitamura etal., 1991] is a system for Knowledge-basexl, Accurate Natural-language Translation. The system is used in focused technical domains for multi-lingual translation of controlled source language documents. KANT is an interlingua-based system: the sonrce language analyzer produces an interlingua expression for each source sentence, and this interlingua is processed to produce the corresponding target sentence. The problen3 el'"]},{"title":"ambiguity","paragraphs":["arises when the system produces more that~ ()tie interlingua representation for a single input sentence. If the goal is to automate translation and produce output that does not require post-editing, then the presence of ambiguity has a negative impact on translation quality, since a target sentence may he produced from an unintended meaning. When it is possible to limit tile interpretations of a sentence to just those that are coherent in the translation domain, then the accuracy of the MT system is enhanced.","Ambiguity can occnr at different levels of processing in source analysis. In this paper, we describe how we cope with ambiguity in the KANT controlled lexicon, grammar, and semantic domain model, and how these :ire designed to reduce or eliminate ambiguity in a given translation domain."]},{"title":"2","paragraphs":["Constraining the Source Text The KANT domain lexicon and grammar are a constrained subset of the general source language lexicon and gra,nmar. The strategy of constraining the source text has three main I"]},{"title":"I","paragraphs":["l-:igurc 1: The KANT System goals. First, it encourages clear and direct writing, which is beneficial to both the reader of tile source text and to the translation process. Second, it facilitates consistent writing among tile many authors who use the system and across all document types. And third, the selection of unambiguous words :111(I constructions to be used during authoring reduces the necessity for ambiguity resolution during the auto,natic stages of processing. It is important to reduce the processing overhead associaled wilh amhiguity resolution in order tokeep tile system fast enough for on-line use. 2.1 The l)omain Lexicml The domain lexicon is built using corpt, s analysis. Lists of terms, arranged by part of speech, are automatically extracted from the corpus [Mitamura etal., 1993]. \"File lexicon consists of"]},{"title":"closed-class general words, open-class general words, idioms,","paragraphs":["and"]},{"title":"nomenclature phrases.","paragraphs":["Closed-class general words (e.g."]},{"title":"the, with. should)","paragraphs":["are taken from general English. Open-class general words (e.g."]},{"title":"drain, run, hot)","paragraphs":["are limited in the lexicon to one sense per part of speech with some exceptions ~. Idioms (e.g."]},{"title":"on and off)","paragraphs":["and nomencl> tnre phrases (e.g."]},{"title":"summing valve)","paragraphs":["are domain-specilic and are limited to those phrases identilied in the domain corpus. Phrases, too, are delined with a single sense. Special vncab-","t Far example, in the heavy-equipment lexicon, there are a few hundred terms out of 60,000 which have more than one sense per part of speech. 90 ulary items, including symbols, abbreviations, and the like, ,are restricted in use and are chosen for the lexicon in collaboration with domain experts. Senses for prepositions, which are highly ambiguous and context-dependent, are determined (luring processing using the semantic domain model (of. Section 4).","Nominal compounds in the domain may be several words long. Because of the potential ambiguity associated wit h compositional parsing of nominal compounds, non-productive nominal compounds are listed explicitly ill tile lexicon as idioms or nomenclature phrases.","2.2 Controlled Grammar","Some constructions in the general source l,'nlgtmge that arc in-","herently ambiguous are excluded from the restricted grammar,","since they may l~td to multiple analyses during processing: ® Conjunction of VPs, ADJs, or ADVs e.g. *Extend and retract the cylinder. • Pronominal reference, e.g. *Start the engine and keel) it","running. • Ellipsis, e.g. reduced relative clauses: *the tools !~t for the procedure • Long-distance dependencies, snch as interrogatives and object-gap relative clauses, e.g. The parts which the service representative ordered. • Nominal compounding which is not explicitly coded in","the phrasal lexicon.","On the other h,'md, tim grammar inchules the following con-","structions: • Active, passive and imperative sentences, e.g. Start the","engine. • Conjunction of NPs, PPs or Ss. Sentences may be conjoined using coordinate or subordinate con jr, notions, e.g.","If you are on the last parameter, ~zen lhe program pro-","ceeds to the lop. • Subject-gap relative clauses, e.g. The service represen-","tative can determine the parts which are faulty. Tile recommendations in tile controlled grammar include","guidelines for authoring, such as how to rewrile a text from","general English into the domain language. Authors are ad-","'vised, for example, to choose the most concise terms available in the lexicon and to rewrite long, conjoined sentences into short, simple ones. The recommendations are useful both for rewriting old text and creating new text (set l:igure 2 for examples). Example 1: Rewrite Anaphnric Use (1t' Numerals Problematic Text: Suggested Rewrite: Loosen tile smaller (me first. Loosen the smaller bolt ftrst. Example 2: Use Concise Vocabulary","Problematic Text: The parts must be put ba_ck toget h!'L. Suggested Rewrite: The parts must be (easse~tbl#d. Figure 2: Grammar Recommendatiml Examl)les 2.3 SGML Text Markup q'he grammar makes use of Standard Generalized Markup Language (SGML) text markl,p tags. The set of markup tags for our applicatiou were developed in conjunction with do- ,nain experts. A set of domain-specific tags is used not only to demarcate tile text but also to identify tile content of potentially ambiguous expressions, and to help during vocabl,lary checking. For example, at the lexical level, number tags identify numerals as diagram callouts, part munbers, product model numbers, or parts of measurement exl)ressions. At the syntactic level, rules for tag combinations restrict how phrases rnay be constructed, as with tagged part rmmbers an(l part names (see Figure 3 for an example). '['tie <p~l/\"t;no> 4S152-1 </parLno> <parLr~amo> Hose A~{sornt)]y </parLname> <Cd]IouL> l </ca llout:> el t.he <paltrlo> 5'['65-']q < / [)El #'L NO> < f)/l [ [ lID, Ill(}-\" [~i~ El k (. ~ COl\"tLr o [ GlTOklp <~/~)~I/~LEIglmQZ > IIILI~;L nOW k)() connOcLed Lo Lho <parLno> 4K2986 </part:no> <partzname> Arl(:!lOi TOO </D~lrt.r/,lm~?:, . t.'igure 3: Sample S(;ML Text Mark-Up 3 Granun'w Design Issues","The parser in KANT is based on the \"Universal Parser\"","[Tonfita and Carbonell, 19871. \"File gramnmr consists of","context-free rules that define tile input's constitt,ent struc-","ture (c-structure) and these rules are annotated with con-","straint equations that define the input's functional structure","(f-structure). 'l'omita's parser compiles the gratnnmr into an","l.R-table, and the constraint equations into Lisp code. Al-","though this compilation results in f.'lst run-time parsing, the","need to minimize ambiguity still exists. One source of ambiguity is the attachment site for a prepo-","sitional phrase, llowever, many of the PP attachments are","encoded directly.in the gramma,\" because tile syntactic con-","text indicates an unanfl)iguous attaclunent site. For example: + A partitive where the PP attaches to the noun: a gallon","of antifreeze.",") )) • A pre-sentential I P where tile I [ attaches to the sentence:","For this test, ensttre thor a signal line is connected from","lhe pump outpul to the pump compensator. • A PI' attaches to the verb be when there is no predicale","adjective: The trm:'k is in the shop. • A ditransitive verb where the PP attaches to the verb:","Give your suggestions to tile dealer. ,, A stand-alone PP inside ;m SGML tag such as QUAL-","II\"IER where tile PP attaches to tile MDLDESC tag","contents: Inspect <mdldesc> all track-type trac-","tors <qualifier> with hydraulic heads </qualifier>","</mdldesc>. 3.1 l'assive vs. Cnpt,l'lr with Participial? There are many adjectives in English that have tile same form",". as ,'m -ed participle, l:or example:","7\"tie radius is poorly formed. (adjective)","The calibration mode is enabled by moving the","rocker switch. (participle)","i \"R} distinguish the qdjectival from the participial form we have added two heuristics to tile constraint rules of the grammar. The litst is to use verb class mapping information, If the 91 verb is classified as being more active than stative, then tile passive reading is preferred. So, for example, an intransitive verb would indicate an adjectival reading:","The display is faded. (adjective)","The second heuristic uses the notion of \"quasi-agents\". There are several prepositions that can introduce \"quasiagents\" [Quirk et al., 197211, such as: about, at, over, to, with. If the domain model indicates that the -ed verb is a possible attachment site for a prepositional phrase occurring in the sentence, then the passive reading is preferred.","These two heuristics are incorporated into tile constraints of rules involving predicate adjectives. If the -ed feral is classified as active, or if there is at PP in the sentence that can attach to the -ed verb form, tfien tile adjectival reading is ruled out. In the constraints of rules for the passive, tim passive reading is ruled out if the -ed form is classified as smtive. 3.2 Adverb or Adjective? For tile most part, eacfi word in the system is limited to one meaning per part of speeclt. So while we have nearly eliminated one source of lexical ambiguity, there is still the l)roblem of ambiguity between the various parts of speech for a particuhlr word. While ambiguity between, lbr example, a noun and a verb is usually resolved by the syntactic context, parts of speech that participate in similar contexts are still a problem. For example, the content of the SGML tag, POSITION, can be an adjective or adverb phrase and \"as [ <adj >l<ad v >] as\" can contain either an adjective or an adverb. This means that an input such as \"as fast as\" would have two analyses. We Imve found witll our domain that tile COtTeCt thing to do is to prefer the adverb reading. We put this preference directly into the constraints of rules involvingadjectives for which the same context allows an adverb. If the word is also an adverb then tim adjective rule will fail. This allows tile adverb reading to be preferred."]},{"title":"4 Semantic Donmin Model","paragraphs":["We have implemented a practical method for integrating semantic rules intoan LR parser. The resulting system combines the merits of a semantic domain nlodel with the generality and wide coverage of syntactic parsing, and is fast and efficient enough to remain practical. 4.1 Interleaved vs. Sentence-final Constraints Some previous knowledge-l)ased natural hmglmge analysis systems have constructed tile semantic represent'ilion for the sentence in tandem with syntactic parsing, lit this schenle semantic constr;fints from tile domain model filter out semantically ill-lormed representations and kill tile associated pro'sing path. Examples include AIISITY tHirst, 19861 and KBMT-89 [Goodman anti Nireuburg, 1991]. Other inevious systems have delayed semantic interpretation and al)plicalion of semantic well-formedness constraints until after tile syntactic parse.","Both of these schemes entail performance problems. The solution to this probleln lies ill importing the right type and right amount of semantic information into syntactic lmrsing. Iu KANT, the relevant knowledge sonrces are reorganized into data structures that arc optimized for ambiguity resolution during parsing. 4.2 Example of Attachment Ambiguity Tile knowledge-based disambiguation scheme covers Prepositional Phrase attachment, Noun-Notre conlponnding, and Adjective-Noun attachment. The remainder of this section discusses examples involving PP-attacl/m~nt. The syntactic grammar contains two rules that allow these attachments:","VP ,---- VP PP","NP ,--- NP PP Consider tim sentence Measure the voltage with the voltmeter. Syntactically, the PP with the voltmeter can modify either tile verb measure, or tile noun voltage. 4.3 Slructure and Content of tile I)omain Model We use knowledge abol,t the domain to resolve ambiguities like PP-attachment. Tile domain model cent;fins all of the semantic concepts in the domain. Leaf concepts, such as *O-VOLTMETER, correspond closely to linguistic expressions. The concepls are. arranged in an inheritance hierarchy, and other concepts, such as *O-MEASURING-DEVICE, represent abstract concepts. The domain model is implemented as a hierarchy of concepts. Constraints on possible attributes of concepls, along with semantic constraints on the fillers, are inherited through this hierarchy. Figure 4 shows an example."]},{"title":"£","paragraphs":["( \"a- Pl A 61~JSl I('-,~C'll O~ \"N) (INglRtlMFNr ~'}- M E& ~ URDdE brr. [&~'l(l:')"]},{"title":"&B.A","paragraphs":["Figure 4: Excerp! t'rnm l)unutiu Model 4.4 Using Semantics in tile Syntax In order tO keel) parsing traclable, the domain model is consuited at the earliest possible stage during parsing. Every grannnar rule that involves an attachment decision that is subject to knowledge-based disamhigv'ltion calls a function that consults the domain model, and allows the gramn/ar rule to succeed only if the attachmcut is sclnantically licensed. The grammar formalism allows procedural calls to be made directly fron/tim gramnmr rules. The function that performs or deuies attachment based on the domaiu model is called sere-attach.","The inpttts to the sem-att ach function are the functiomll structures (f-strttcturcs) lk)r the potential attachment site, tile structure to be attached, and the type of attachment (e.g., PP = t:'repositional Phrase). sere-attach consults inlbrm,'ltion from the domain model to decide whether the attachment is semantically licensed. This process is described in the next subsection. 4.5 Steps in Sem,'mtle Disaml)iguation I There are three main steps in selnanlic disambiguation of possib]e syntactic attachments: (1) mapping from syntax to 92 semantic concepts using tile lexical mappiug rules; (2) checking inform,'ttkm from the domain model; and (3) determining semantic roles using tile semantic interpretation rules."]},{"title":"?7 v :iiii","paragraphs":["Figure 5: Lexical Mapping Rules","Lexical Mapping Rules. The first step is to real I from syntactic structures to semantic concepts. The lexical mapping rnles associate syntactic lexicon entries with concepls from the Domain Model (Figure 5).","Domain Model. \"File second step consists in looking up the"]},{"title":"appropriate concepts","paragraphs":["in the Domain Model (Figure"]},{"title":"4). Semantic Interpretation Rules.","paragraphs":["The third step consists of consulting the semantic interpretation rules to determine whether the concepts from tile sentence can lo,'m approl~riate modilication relationships. Semantic interpretation rules describe the mapping from the syntactic representation to the frmne-based semantic representation, An interpretation rub consisls of a syntactic path (an index into tile f-structure), a semantic Imth (an index into tile senmntic frame), and an op. tional syntactic constraint on the mapping rule. For exmnple, below is mt interpretation rule for the INSTRUMENT role: (::~yn-pat.h IPP OBJ) :sem~path ZN.qTRUNENT : sys-consttraint","((pp ((root (*OR* \"wlth .... by\")) })) ) Eflicient Run-time Use. In order to make this process as efticient as possible, aml to minimize delays during parsing, the knowledge described in this section is reorganized offline I)cfore parsing. The result of this reorganization are data strtletnres known as S&*;'lalllic\" restrictors. The SelllLIIlliL' restrictors have three main properties: 1. They are indexed by head concept, and provide a list of","all approl)riate modiiiers. 2. All inheritance in the Domain Model is performed off-","line, so that the restrictors contain all necessary informa..","tion. 3. The semantic restrictors are stored in a Slmce-efficient","structure+shared manne, +."]},{"title":"S","paragraphs":["Author l)isambiguatiot! Once KANT has analyzed a source sentence and all l)OSsine disambiguations h;tve been performed, there may still be more than one interlingua representation for tim sentence. This occurs when the sentence is truly ambiguous, i.e., it hns more than one acceptable domain interpretation. In this case, KANT makes use of disambiguation by the author -- tile ambiguity is described to the author and the author is then pmml)ted to select the desired interpretation. The choice is \"remembered\" by placing extra in fomuttion into tile input text at the point of alnbiguity. There are two types of ambiguity cnrrently addressed by author disambit~uation:","• Lexical Ambiguity. When more than one interlingua is","produced because a certain word or phrase ean be in","terpreted in more than one way (iv. as two different","concepts), then the author is prompted to select the de-","sired meaning.","* Structural kmbigt.tity. When more than one attachment site is possible for a phrase like a prel)o~ilional phrase, the different attachments are glossed for tile :luther, who is then prompted to select tile desired inteq)retation.","Since author disambiguation is utilized only when the sentence cannot I)c disambiguated by other nteans, it will not occur very frequently once tile system is complete. On tile other hand, having such a mech,'mism available during system development is very helpful, since it helps to point out where there is residu-d ambiguity left to be addressed by knowledge St} tlrce ieli neltlent. 6 Testing l)isamlfigualion Methods When disambiguation methods are int,oduced, the number of parses per sentence can be reduced dramatically. If we use a general lexicon and grammar to parse texts lro[n ~.1 specialized dolnain corpus (rather than a general corpus), then more lmrses will be assigned than those thai are desired in the dOlnain. Figure 6 illustrates how the successive introduction of disambiguation ntethods reduces the set of l)ossiblc parses to just those desired in tile domain. The smallest set of interpretations is that remaining after tile controlled lexicon, gla[nllrar, seln[illlic restrictions, and author disambigtmtion have [)cell applied; in practice this sel ,,viii contain just one interpretation, since the author will select only the intended interpretation. # Inlerprelalion~ Ouin~l General"]},{"title":"I","paragraphs":["N Inle llWUt aliot~ U ~+ing Dotu,~irl hllelptelullolll [}i~ambi~ualiolL Followia~ A~Jlllo¢ l:igure 6: Reducing the Set of Possible lnterl)retatinns","Wc have experimented with tile KANT analyzer in order to determine the effects of the different disambiguation strategies mentioned above. We used a test suite containing 891 sentences which is used fo, regression testing during system development. The sentences in the test suite range in lenglh fronl t word to over 25 words.","General lexicon entries were derived automatically from the online version of Wcbster's 7th dictionary. Webster's includes 55,000 reels that are in at least one open class category (verh, ram,, adjective, adverb). One diclionary entry was created for each sense of one of these dalegories, This resulted in 117,000 lexicon entries. The constrained lexicon consists of 10,000 words and 50,000 phrases talk)red to the application 9.3 domain. For the results listed below, the \"general lexicon\" consists of the constrained lexicon plus the general entries from Webster's.","The constrained grammar has been tailored to the restricted source language for the domain (of. Section 2). In ,'tddition, it includes a number of constraint annotations and parse preferences that limit the number of ambiguous parses (cf. Section 3). A general grammar was derived from the constrained grammar by removing most restrictions and constraints on specific rules, leaving only the most general constraints such as subject-verb agreement.","When noun-noun compounding is allowed, sequences of nouns may form NPs even if they ~u'e not listed as nomenclature phrases in the lexicon. Each such sequence is only parsed one way; the parser does not build different smmtures for the sequence of nouns, but just reads them into a list.","In order to reduce the exponential complexity of some of the longer sentences, all test results were produced using the \"shared packed forest\" method of ambiguity packing for ambiguity internal to a sentence [Tomita, 1986]. The results for \"parses per sentence\" is simply the average for all the sentences. Test LEX GRA N-N DM P 1 GEN GEN YES NO 27.0 2 GEN GEN NO NO 10.2 3 GEN CON YES NO 8.4 4 CON GEN YES NO 1.7 5 CON GEN NO NO 1.6 6 CON CON NO YES 1.5 LEX: Lexicon GEN: General GILA: Grammar CON: Constrained N-N: Noun-Noun Compounding DM: Semantic Restriction with l)omain Model Figure 7: Testing Disambiguation Methods (12/17/93)","The results of this testing ~u'e shown in Figure 7. Test 1 is the baseline result for parsing with a general lexicon, general grammar, noun-noun compounding and no semantic restrictions. As expected, the average number of parses per sentence is quite high (27.0). Limiting noun-noun compounding ('lest 2) cuts this number by more than hal f, yielding 1(/.2 parses per sentence. Note that a similar effect is achieved if we run the test with a controlled grammar and noun-noun compounding (Test 3, 8.4 parses per sentence).","Constraining the lexicon seems to achieve the largest reduction in the average number of parses per sentence (Tests 4, 5, 6), with elimination of noun-notre compounding yielding only slight improvements when the lexicon has already been restricted. As expected, the best results are achieved when the system is run with constrained lexicon and grammar, no noun-noun compounding, and semantic restriction with a domain model (Test 6).","We expect that tile primary reason wily tile addition of semantic restrictions from a domain model does not have a greater impact is due to tile incomplete natttre of the domain model we used in the experiment. The domain model used in the experiment captures the domain relationships associated with prepositional phrase attachment to VP and object N1; lint there are several areas of the domain model still under development. When complete, these will further reduce ambiguity by placing additional limitations on the following:","• The semantic classification of words inside particular","SGML tags;",",, Attachment of prepositional phrases to suhject NP;","• Attachment of inlinitive clauses;",",, Attachment of relative clauses.","This testing has proved extremely useful ill prioritizing the level of effort expended oil different disambiguation methods dnring system development. As is often the case, theoretically interesting or difticult issues (such as noun-noun componnding) are reduced in effect when other domain-related restrictions are put in place (such as a controlled lexicon). On the other hand, this type of testing can also identify Ihe areas of the system (such as the semantic domain model) which are not reducing ambiguity as much as expected. In our ongoing work, we will complete the domain model for the KANT he,'tvy-eqtfipment application in those areas mentioned above; in the process, we expect to rednce the average number of parses per sentence in the most constrained ease. 7 Acl(nowledgements We would like to thank Jaime Carbonell, Radha Rao, and Todd Kaufinann, ,'rod all of ottr colleagttes on the KANT project, inchtding James Altucher, Nicholas Brownlow, Mildred Galarza, Sue Hohn, Kathi lannamico, Kevin 1(eck, Marion Kee, Sarah Law, John Leavitt, Daniela Lonsdale, Deryle Lonsdale, Jeanne Mier, Venkatesh Narayan, Amalio Nieto, and Will Walker, our sponsors at Caterpillar Inc., and onr colleagues at Carnegie Group. References","[Chevalier et al., 1978] Chevalier, M., Danscre:m, J., and Poulin, G. (1978). TatmHnetco: description dn systOne. Technical report, Groupe (le recherches pour la traduction automatique, Universit6 de MontrS,ql.","[Goodman and Nirenburg, 19911 Goodman, K. and Nirenburg, S. (1991). The KBMT Project: A Case Study in Knowledge-Based Machine Translation. Morgan Kaufulan[1, S:.ln Mated, CA.","[Hirst, 1986] Hirst, G. (1986). Semantic Inlerpretation and tile Resolution of Ambiguity. Cambridge University Press, Cambridge.","[Mitamura et al., 1991] Mitamura, T., Nybcrg, E., and Carbonell, J. (1991). An efficient interlingua translation system for multi-lingtmldocument prodttction. In Proceedings of Machine Translation Summit I11, Washington, DC.","[Mitamnra et al., 19931 Mitamura, T., Nyberg, E., anti Carbonell, J. (1993). Automated corl)uS analysis ,'rod the acquisition of large, multi-lingual knowledge hases for MT. In 5th International Cotlference on 771eoretical and Methodological Issues in Machine Translation, Kyoto, Japan.","[Quirk et al., 1972] Quirk, R., Grccnbaum, S.. Leech, G., and Svartvik, J. (1972). A Grammar of Contemporary English. l_ongman Group UK Limited, Essex Engl,'md.","[Suzuki, 19921 Suzuki, M. (1992). A method of utilizing domain and langnage-sl)ecitic conslraints ill dialog translation. In Coling-92.","[Tonfita, 1986 ] Tom ira, M. (1986). Efficient Parsing for Natural Language. Kluwer Academic Publishers, Boston, MA.","[qk)mita and Carbonell, 19871 \"ll)mita, M. and Carbonell, J. (1987). \"i'he Universal Parser architecurc Rlr Km)wledge-based Machine Translation. Technical Report CMU-CMT-87-101, Center for Machine Translation, Carnegie Mellon University. 94"]}]}