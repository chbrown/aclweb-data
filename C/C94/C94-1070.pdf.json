{"sections":[{"title":"THE \"WHITEBOARD\" ARCHITECTURE: A WAY TO INTEGRATE HETEROGENEOUS COMPONENTS OF NLP SYSTEMS Christian Boitet Mark Scligman","paragraphs":["GETA, IMAG (UJF & CNRS), 150 rue de la Chimie, BP 53 38041 Grenoble Cedex 9, Fr,'mce Christian .Boitet@imag. fr","NI'R Interpreting Teleconununicafions Research Labs","2-2 ltikari-dai, Seika-cho, Somku-gun","Kyoto 619-02, Japan","seligman@it i. atr. co. jp ABSTRACT We present a new softw,'u'e architecture for NLP systems made of heterogeneous components, and demonstrate an architectural prototype we have built at ATR iu the context of Speech Translation. KEYWORDS: Distributed NLP systems, Software architectures, Whiteboard."]},{"title":"INTRODUCTION","paragraphs":["Speech translation systems must integrate components handling speech recognition, machine translation and speech synthesis. Speech recognition often uses special hardware. More components may be added in the future, for task understanding, multimodal interaction, etc. In more traditional NLP systems, such ,'cq MT systems for written texts, there is also a trend towards distributing various tasks on various machines. Sequential ,architectures [10, 11] offer ,an easy solntion, but lead to loss of information and lack of robustness. On the other hand, reports on experimenls with blackboard architectures [16, 13, 20] show they also have problems. We ,are exploring an intermediate architecture, in which components are integrated under a coordinator, may be written in various programming languages, may use their own data structures and algorithms, and may run in parallel on different machines. The coordinator maintains in a whiteboard an image of the input and output data structures of each component, at a suitable level of detail. The whitehoard fosters reuse of partial results and avoids wasteful recomputation. Each component process is encapsulated in a manager, which transforms it inlo a server, commuuicating with external clients (including the coordinator) via a system of mailboxes. Managers handle the conversions between internal (server) and external (client) data formats. This protocol enhances modularity and clarity, because one needs to to explicitly and completely declare fl~e appearance of the partial results of the components on the whileboard. Managers may also make batch components appear :is incremental components by delivering outputs in a piecewise fashion, thus taking a first step towards systems simulating simultaneous translation. We have prc~luced a rudimentary architectural prototype, KASUGA, to demonstrate the above ideas. In fl~e first section, our four main guidelines ,are detailed: (1) record overall progress of components in a whiteboard; (2) let a coordinator schedule the work of components; (3) encapsnlate components in managers; and (4) use the managers to simulate Incremental Processing. In the second section, some high-level aspects of the KASUGA prototype ,are first described, and a simple demonstration is discnssed, in which incremental speech translation is simulated. Lower-level details are then giveu on some internal aspects. I. TIlE WII1TEBOARD"]},{"title":"ARCHITECTURE","paragraphs":["1. Record overall progress in a whitelmard","The whiteboard ,architecture is inspired by the chart","architecture of the MIND system [8] and later systems or","formalisms for NLP [1, 5], as well as by the blackbo~u'd","architecture, first introduced in HEARSAY-II [6, 13] for","speech recognition, llowever, there is a significant","difference: tile components do not access the whiteboard,","and need not even know of its existence.","There are 2 main problems with the sequential approach.","• Pl: loss of information If components ,are simply concatenated, as in Asnra [10, 11], it is difficult for them to share partial results. Information is lost at subsystem interfaces and work has to be duplicated. For example, the cited system uses an LR parser to drive speech recognition; but syntactic structures found are discarded when recognition candidates are passed to MT. Complete reparsing is thus needed.","• P2: lack of robustness Communication difficulties between subsystems may also dmnage robusmess. During reparsing for MT in ASURA, if no well-formed sentences are found, partied syntactic structures are discarded before semantic analysis; thus there is no chauce to tr,'mslate partially, or to use semantic inlonnation to complete the parse.","The pure blackboard approach solves P1, but not P2, and","introduces four other problems.","• P3: control of concurrent access In principle, all components are allowed to access the blackboard: complex protection and synchronization mechanisms must be included, and fast components may be considerably slowed down by having to wait for permission to read or write.","• P4: commnnication overloads The amount of information exchanged may I~ large. I1\" components rnn on different machines, such :is is often the case for speech-related componeuts, and may be the case for Example-Based MT con~ponents in the future, commmfication overloads may annihilate the bcuciit of using spcckdized or distributed hardware.","• P5: efficiency problems As components compute directly on the blackbo,'u'd, it is a compromise by necessity, and can not offer the optimal kind of data structure h~r each component.","• P6: debugging problems These ,are due to the complexity of writing each component with the complete blaekbo,'u'd in mind, and to the parallel nature of the whole computation.","In the \"whiteboard\" approach, the global data structure is","hidden from the components, and accessed only by a","\"coordinator\". (The whiteboard drawing is expanded later.) 426 \"1he \"Whiteboard\" Architecture: a way to integrate... Boitet & Seligman, COLING-94"]},{"title":"(.~Olllporlell[ I (~OlllpOIl(3ll[","paragraphs":["2"]},{"title":"COlllpollent3, ] o l","paragraphs":["Figure l: the \"whitelmard\" arc'h#ect,~re This simple change makes it possible to avoid problems P3-P6. It has also at least two good points: - It encourages developers to clearly define andpublish what their inputs and outputs are, at least to the level of detail necessary to represent them in the whiteboard. - The whiteboard can be the central place where graphical interfaces are developed to allow for e~Lsy inspection, at v,'u'ious levels of det~fil. As long as an NLP system uses a central record accessed ouly by a \"coordinator' and hidden fi'om the \"comlxmeuts\", it cau be said to use a whiteboard architecture. It remains open what dala structures the whiteboard itself should use. As in [21, we suggest the use of a time-aligned lattice, in which several types of nodes can be distinguished. In stating our preference for lattices, we must first distinguish them from grids, and then distinguish true lattices from 2 types of quasi-lattice, charts and Q-graphs (fig. 2 & 3).","Sent4","NP8","NPI NP2 ]e~.~W ~ ..=..__.41~_ ~~O _.._._._ii~ tornove and ~lrO und fur4 t,~k I I NPS I I I I./pa","Senti = ' ~ =","1 [ NP4"]},{"title":"_ ,..-","paragraphs":["I Senl2 Sent3 r~oruove tiller cap and ground fuel tank Fig. 2: chart built on a syntactically ambiguous a'et/tence N(l~ht,N(S), N(can,N(P},. V(spackle,P(3pI)..,'k.=.","A(Iklht,N(S ,~ N(can, NtS}. ,.} V{spar kle, P(3sffi ...~ G{M).f} ~,"]},{"title":"- -","paragraphs":[". V(spaflde,lnf,..) G(M}.. /","I 4(light,N(S), N(ean, N(S), IV(sparkle, N(PauI,PN, k(light,N(S,Pl I V(can,T(P), I"]},{"title":"lN(soa,~,","paragraphs":["~ ' AV(slight..,L~","n ~","~","iP\"-' ~ ~ O","Nb(sfl),Poss} I GIM,F}) ~ N(S),G(M}) , I v(light''') ! I N~,,.,l~. I IN! spark~' I G(M),..) N(P),G{M)) N(light,N(S), V (c in,T( P~ ,=~....e V(spathle,lnf,.,)","G(M)..,] ~V(mod)) Paul's ligh! can(s) sparldo(s) slightty Fig. 3: A Q-graph for a phonetically ambiguous sentence Grids have no arcs, but nodes Co,Tesponding to time spans. A ncxle N spanning It132] is implicitly connected to another node N' spanning [t'l,t'2] iff its time span begins earlier 01 gt'l ), ends strictly earlier (t2<t'2), and the respective sp,'ms (a) are not too far apart anti (b) don't overlap tc~) much (t2-max-gap_<t'l ~t2+max-ovorlap). max-gap and max-overlap are gapping and overlapping threshokts [12]. Because t2<t'2, there can be no cycles. ht a lattice, by contrast, nodes and arcs are explicit. Cycles are also forhiddcn, and there must be a unique first node and a unique last node. (;rids have often been used in NLP. l\"or example, Ihe output of the phonetic component of Kt~AL [121 was a word grid, and certain speech recognition programs at NI'R l~r(×luce phoneme grids 1. In gener~d, each uc~le bears a time span, a label, and a score. Grids can also be used to represent an input text obtained by scanning a bad original, or a stenotypy tape [9], and to implement some working structures (like flint of the Cocke algorithm). llowever, we will require explicit arcs in order to explicitly model possible sequences, sometimes with associated information concerning sequence probability. Thus mw grkls am insufficient for our whiteboards. Two kinds of quasi-lattices have been used extensively, in two wtrietics. First, chart structures have origi,mlly been intr(~luccd by M. Kay in the MIND system around 1965 [8], In a ch:ut, as understood tt~lay (Kay's charts were more general), the nodes ,are arranged in a row, so that there is always a path between any two given nodes. The arcs bear the information (label, score), not the nodes. Ch[u'ls are also used by many unification-based natural hmguage analyzers [ 141. Chart structures are unsuitable for represcnting restflts on a whiteboard, however, because they are tmable to represent alternate sequences. Consider the alternate word sequences of Figure 4. It is not possible to arr.'mge the words in a single mw so that all and only the proper sequences can be read out,"]},{"title":"1 1","paragraphs":["it if you came","I would like you to come emly tomorrow","earlier","Figure 4: A sentence with alternate formtdations A second type of quasi-lattice is the Q-graphs of [15] and their exteasiou [17], the basic data structure for text representation in tile METI~,O [14] aud TAUM-Aviation [71 systems. A Q-graph is a loop-free graph wilh a tmique entry node and a uni(lue exit node. As iu charts, the inlonnalion is carried on the arcs. It cousisls in labeled or atmotaled trees. As there may be no l)ath between two nixies, Q-graphs can indeed faithfully represent alternate sequences like those of Figure 4. But in this case it is necess;uy to use, on more thau one arc, identical labels referring to the same span of the input. For representation on a whitcl×mrd, such duplication is a drawback. To simplify bookkeeping and visual presentation, we prefer a representation in which a given label referring to a given span appeaJw in only one place. A true lattice, like flint of Figure 5, makes this possible. \"lhe decomposition of the laltice in htyers seems natural, aud leads to more clarity. Fach layer contains results of 1115, 16]. By contrast, tile IIWIM [20] system used a \"phonetic lattice\" on which an extended ATN operated. 427 The \"Whiteboard\" Architecture: a way to integrate... Boitet & Seligmcm, COLING-94 one component, selected to the \"appropriate level of detail\". Its time-aligned character makes it possible to organize it in such a way that everything which has been computed on a certain time interval at a certain layer may be found in the same region. Each layer has three dimensions, time, depth and label (or \"class\"). A node at position (i,j,k) corresponds to the input segment of length j ending at time i and is of label k. All realizations of label k corresponding to this segment are to be packed in this node, and all nodes corresponding to approximately equ,'d input segments am thus geometrically clustered. In other words, ambiguities are packed so that dynamic programming techniques may be applied on direct images of the whiteboard. Figure 6 gives an ex,'unple, Where the main NP has been obtained in two ways.","G Q III II % Figure 5: A word lattice (representing a sentence with alternate fornudations Arcs may optionally be augmented with activation or realistic choice of layers, however. inhibition weights, so that ideas from the fast-developing lield of neural networks may be applied. language u~q~ layers layers Figure 6: The whiteboard as a factorizing data structure The true lattice, then, is our preferred structure for the","whiteboard. We said that the whiteboard could be a central place for","transp,'u'ent inspection, at suitable levels of detail. We use","the notion of \"shaded nodes\" for this.","- \"White\" nodes are the real nodes of the lattice. They contain results of the computation of the component associated with their layer: a white node contains at least a label, legal in its layer, such as NP, AP, CARDP, VP... in the example above, and possibly more complex information, as allowed by the declaration of the layer in the whitelx~ard.","- \"Grey\" nodes may be added to show how the white nodes have been constructed. They don't belong to the lattice structure proper. In the example above, they stand for rule instances, with the possibility of m-->n rules. In other cases, they may be used to show the correspondences between nodes ot two layers."]},{"title":"©-@ .?.--@","paragraphs":["Figure 7: White and grey nodes corresponding to rule Rn:","X1 X2...Xp -> Y1 Y2...Yq","- \"Black\" nodes may be used to represent finer steps in","the computation of the component, e.g. to reflect the","active edges of a chart parser.","Whiteboard la rers are organized in a lc, op-li'ee dependency graph. Non-linguistic as well as linguistic information can be recorded in appropriate layers. For example, in a multimodal context, the syntactic analyzer might use selected information from a map layer, where pointing, etc. could be recorded. Interlayer dependencies should be decl~u'ed, with associated constraints, stating for instance that only nodes with certain labels can be related to other layers. Ilere is an illustration of that idea, wilhout any pretense to propose a","~Ot] ll|ellU","layer layer","Figure 8: A hierarchy of layers in an hypothetical","whiteboard for a nndtimodal NLP ,~'ystem 2. Let a coordin'ator schedule tile components In its simplest form, a coordinator only transmits the results of a component to Ihe next component(s). l lowever, it is in a position to carry out global strategies by filtering low-ranking hypotheses and transmitting only the most promising part of a whitcboard layer to its processiug component. Further, if certain components make uselhl predictions, the coordinator can pass these to other components as constraints, ,along with input. 3. Encapsulate components in managers Developers of components should be free to choose and vary their algorithms, data structures, programming languages, and possibly hardware (especially so lor speech-related components). Our approach is to encapsulate existing components in managers, which hide them and transform them into servers. This strategy has the furlher adv,'mtage of avoiding any direct call between coordinator and components. To plug in a new component, one just writes a new manager, a good part of which is generic. 428 The \"Whiteboard\" Architecture: a way to integrate... Boitet & Seligman, COLING-94 A m,'mager has a request box where clients send requests to open or close connections. A connection consists of a pair of in and out mailboxes, with associated locks, mid is opened with certain paraneters, such as its sleep time and codes indicating pre-agreed import and export formats. The coordinator puts work to do into in-boxes aid gets results in corresponding out-boxes. As illustrated in Figure 1 above, a client can open more than one connection with the sane manager. For exanple, au on-line dictionary might be called for displaying \"progressive\" word for word translation, as well as for ,'mswering ternfinological requests by a human interpretcr supervising several dialogues and l~ddng over if needed. And a malager can in principle have several clients. llowever, this potential is not used in KASUGA. 4. Simulate incremental processing In real life, simullanexms interpretation is often preferred over consecutive interpretation: although it may be less exact, one is not forced to wait, and one can react even before the end of tile speaker's utterance. Incremental processing will thus be an iinportant aspect of future machine interpretation systems. For instance, a sem.'mlic processor might begin working on the syntactic structures hypothesized for early parts of an utterance while later parts ,are still being syntactically an,'dyzed [19]. Even if a component (e.g., a W cun'ently existing speech recognizer) has to get to file end of the utterance before producing any result, its nmnager may still m;tke its processing appear incremental, by delivering its result piecewise and iu the desired order. I lence, this organiz'~tion makes it possible to siintfiate future incremental components. 11. TIlE KASUGA PROTOTYI'E 1. External level The coordinator (KAS.COORD) is writtcn in KEK"]},{"title":"TM,","paragraphs":["au object-oriented expert system shell with excellent interface-building tools. The whiteboard is declared ill KEF]s object language. KEE itself is written ill Common lisp. Three components are inw/lved: - speech recognition (SP.REC) providing :t 3-level grid,","progrmnmcd in C [15]; - ish'md-driven syntactic chart-parsing (SYNT.AN)","deriving words and higher-level syntactic units,","programned in C; - word-for-word translation (WW.TRANS) at file word","level, written in C aid running on another machine. The tanagers are written in Lisp, ,'rod run independently, in three Unix processes. Each manager ,and the c(gmlinator can rat in different Unix shells. Although WW.TRANS is already accessible as a server on a distant machine, we had to create a manager lbr it to get the intended behavior. With only these components, it is possible to produce a simple demonstration in which incremental speech translation is simulated and the transparency gained by using a whiteboard is illustrated. The phonemes produced by SP.REC are assembled into words and phrases by SYNT.AN. As this goes on, WW.TRANS produces possible word-for-word translations, which are presented on screen ,'u,~ a word lattice. KASUGA's whiteboard has only three layers: phonemes; source words and phrases; and equivalent target words. At the first layer, the phoneme lattice is represented with phonemes in nodes. At the second layer, we retain only the complete substructures produced by SYNT.AN, that is, the inactive exlges. Phonemes used in these slructures appear again at that layer. In KEE, we define a class of NODES, with subclasses WHITE.NODES, GREY.NODES, PIlON.LAYI~P,.NOI)ES, aud SYNT. I,AYER. NODES in tile syntactic htycr. NODES have a generic display method, and subclasses have specialized variants (e.g., the placing of white nodes depends on their time interval, while that of grey nodes depends on that of the white nodes they cermet0. 2. Internal level When a manager receives a Make.Conuection request frola a client, it creates an in box and an out box (and associated locks, used to prevent interference between components), through which information is p.'~ssed to and from the client. The Make.Connection request includes codes showing in which format(s) the client is expecting to deposit data in the iu box and read data from tile out box, lbr that connection. Mlhough data transfer could be programmed more efficiently, e.g. nsing lhfix sockets, our method is more general, as it uses only the file system, and we believe its overhead will be negligible in comparison with tile processing times required by the compouents. Ikn each out box, the client (KASUGA) actbatcs a reader process and tile relewmt mauagcr actiwttes a writer process. Conversely, for each in box, tile client activates at writer process and the manager activates a reader process. A zeader process wakes up regul:uly and checks whether its mailbox is both non-empty and nnlocked. If so, it locks the mailbox; reads ils contents; empties tile mailbox; unlocks it; and goes to sleep again. A writer process, by comparison, wakes up regul:uly and checks whether its mailbox is both empty and unlocked. If so, it locks the box, fills it with appropriate data, unlocks it, and goes back to sleep. For example, the writer associated with SYNT.AN will deposit in the appropriate out box the image of all tile inactive arcs created since the lm;t deposit. SItI?,EC provides, lor each of 40 prerecorded bunsetsu (elementary phrase), a set of about 25 phoneme malrices, one for each phoneme. A malrix cell contains the score for a given phoneme with a given begimfiug/ending speech frane pair. These nmtrices are then compared, and 3 other inatrices are computed. The tnp-scoring ln:llrix contains in each cell the tnl~-scnring phone and its score for Ihe corresponding begimliug/cnd. The 2nd-scoring a~d 3rdscoring matrices are computed sinfilarly. These three mauices are used to build the first layer of the whiteboard. To build the whilcboard's second layer, an ishmd-driven clmrt parser is used, where the matrices are cousklered as initialized charts. The over:dl best-scoring cell in the top matrix is established as the only anchor, and hi-directional searching is carried out wilhin the (handset) limits set by max-gap and max-overlap. A CFG written by J. llosaka for tile ASURA demos is now used as is. Parsing results are convertcd to syntact Jc:. ] a t< 5 ce. N (by Olt[ chart-to- la t t ice filter) and brought into KEF~. Then an image lattice, ww. la t t ] c e. N, is comptlted as the whiteboard's third layer, using a C-based ou-tine J-l{ dictionary. Each lexieal syntactic node gives rise to oue Fmglish word for each meafing. For example, ~ gives yes, yes-sir, the-lungs, ashes, etc. Layers of the whiteboard are represented by KEF, \"planes\". We can move planes rehtlive to e[ich olher; ztx~m 429 The \"Whiteboard\" Architecture: a way to integrate... Boitet & Seligman, COLING-94 in various ways; put various information in the nodes (label, rule responsible, id, time span, score); exp,'md the nodes; open & close the nodes selectively. And we can color the nodes according to their score. It is possible to show or hide various parts of the whiteboard. In Figure 9, the first layer, the time grid, the lattice lines, and the initial/final lattice nodes have been hidden. Alternatively, we could hide constnlction (dotted) lines, rule boxes, label boxes, etc. The view of any part of the whiteboard can he changed for emphasis: one can for instance interactively select only the nodes above a certain confidence threshold. Overall processing can be inten'upted for examination.","WW.lattice,N | ,~. ~ I start II"]},{"title":"I , I I ? I',,,,ose,s.,l II I I I \"--\"o I ~. I","paragraphs":["o~ # %%"]},{"title":"O'","paragraphs":["Figure 9: a view of KASUGA ' s whiteboard If this architecture is to be further developed in the future, one could use instead of KEE a general-purpose, portable interface building toolkit in order to avoid the oved~ead ,'rod overspecialization ,associated with using a complete expert system shell. KAS.COORD writes and reads data to and from the managers in a LISP-like format, and handles the transformation into KEE's internal fornmt. Each manager translates back ,and forth between that format and wbatever format its associated component happens to be using. Ilence, formats must be precisely defined. For inst,'mce, the edges produced by the speech recognizer are of the form (begin end phoneme score). The nodes and edges of the conesponding phoneme layer in the whiteboard are of Ihe form (node-id begin end phoneme score (in-arcs) (out-arcs)), with ares being of the form (are-id origin extremity weight). CONCLUSION Although the concept of the whiteboard architecture Ires emerged in the context of rese,-u'ch in Speech Translation, it can be useful in other areas of NLP. It has already been used, in a prelimin,'u'y form, in dialogue-b~sed MT [3]: the tasks are distributed between the authoring stations and an MT server, m~d the coordinator maintains in a unique data structure all intermediate stages of processing of all units of translation. The whiteboard ,architecture might be used with profit in all situations where it is important to integrate new or existing components, e.g. to build generic environments for developing heterogeneous NLP systems. Researchers would thereby gain twice: by getting a clearer view of what they (and others) ,are doing; and by being able to use generic interlace tools provided by the coordinator for debugging and illustrating purposes. ACKNOWI,EDGMENTS We are grateful to M. Fiorenthm from Intellicorp, Inc. and K. Kurokawa from CSK, Inc., for providing a demo copy of KEE TM and valuable technical support; to Dr. Y. Yamazaki, President of ATP,-IT, and T. Morimoto, llead of Dept. 4, for their support and encouragement; to II. Singer, T. ltayashi, Y. Kitagawa, I[. Kashioka, and J. Hosaka, for their help in developing the components; and to K. II. Loken-Kim, for sfimtflating discussions and proposing the term \"whitelx~ard\". REFERENCES [1] Barnett J., Knight K., Mani I. & Rich E. 119901 Knowledge and Natural Language Processing. Comm. ACM, 33/8, 50-71. [2] Boltet C. (19881 Representation and Computation of Units of Translation for Machine Interpretation of Spoken Texts. Comp. & AI, 6, 505--546. [3] Baiter C. & Blanchon 11. (1993) Dialogue-Based MT for Monolingual Authors and the LIDIA project. Proc. NLPRS'93, Fukuoka, 208--222. [4] Chandloux J. & Gu~rard M.q*'. (1981) METEO: un sysltSme ?t l'tCpreuve du temps. META, 1, 17--22. [5] Colmeraner A. (1970) Les systdmes-Q, un formalisme pour analyser et synthdtiser des phrases sur ordinateur. TAIJM, Univ. de MontrSal, dec. 1970. [61 Erman L. D. & Lesser V. R. (19801 771e Hearsay-H Speech Understanding System : A Tutorial. ht \"Trends in Speech Recognition\", W.A. Lea, ed., Prcntice-lhdl, 361-381. [7i lsabelle P. & Bourbeau L. (1984) TAUM-A VIA770N: its technical features and some experbnental results. Comp. Ling., I1/I, 18 27. [8] Kay M. (1973) 77~e MIND system. In \"Courant Computer Science Symposium 8: Natural Language Processing\", R. Rustin, ed., Algorithntics Press, 155-188. [9] M6rlaldo 1|. (1988) Multilevel decoding for Vety-Large-Size-Dictionaly speech recognition. IBM Journal of R&D, 32/2, March 1988, 227-237. [101 Morlmoto T., St, zuki M., Takezawa T., Klknl G.-(., Nagata M. & Tnmokiyo M. (19921 A Spoken Language Translation System: SL-TRANS2. Proc. COLING-92, Nantes, vol. 3/4, 1048--1052. [11] Morlmoto T., Takezawa T., Yato F., Sagayama S., Tashlro T., Nagata M. & al. (1993) ATR's Speech Translation System: ASURA. EuroSpeecb'93. [12] Qnlnton P. (19811) Contribution h la reconnaissance de la parole. Utilisation tie mdthodes heuristiques pour la reconnaissance tie phrases. TbSse d'Etat, Univ. de Rennes, 239 p. [13] Reddy R. (19811) Machine Models of Speech l'erception. In \"Perception and Production of Fluent Speech\", Cole, ed., Erlbaum, N.J., 215-242. [141 Schleber S. M. (19861 An it, troduction to unification-based approaches to grammar. CSLI Lect. Notes 4. [15] Singer II. & Sagayama S. (19921 Matrix Parser and its Application to 11MM-based Speech Recognition. IEICE, 111/SP92-76, I)SP92-61, 21-26. [16] Singer I1. & Sagayama S. (1992) Matrix Parsing applied to TDNN-based Speech Recognition. Japanese Journal of Speech Processing, 1992/3, 89-90. [17] Stewart G. (1975) Manuel du langage REZO. TAUM, Univ. de Montrfal, 120 p. [18] Tomlta M. (19911) 77~e Generalized LR Parser/Compiler V8-4 : a Software Package for Praclical NL l'rojects. Proc. COLIN(;-90, vol. 1/3, 59-63. [19] Waldster W. (1993) Planning Multimodal Discourse. Proc. ACL-93, Columbus, Ohio, 95-96. [20] Wolf J. J. & Woods W. A. (19811) 771e IIWIM Speech Understanding System. In \"Trends in Speech Recognition\", W. A. Lea, ed., Prentice-llall, 316-339.","-0-0-0-0-0-0-0-0-0-0-430"]}]}