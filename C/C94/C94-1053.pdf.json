{"sections":[{"title":"r O,, Plmming Argumcnl; l;iv0 l- ,xts Xiaorong Hua.ng Fachbereich Inforlnatik, Uniw:rsiti[t, des S+ul,rlandes 6(i0d]. Saarbriicken, (l(~rlltTi,liy, etnail: huang~3cs.uni-sb.de Abstract","paragraphs":["This paper presents PI£OVF, I¢,I\"f a text planner for argumentative texts. I~I~OVI']]~II~ Inain feature is that it combines global hierarchical planning and lltiphmned organization of text with respect to local derivation relations in a complementary way. The former splits the task of presenting a particular pronf into subtasks of llresenting sul)proot~. \"['lie lati, er silnli[al;cs ]iow the next intermediate eonchision to Im l)resenl,ed is chosell under the guida.nce o[ the local ['ocils. 1. Introduction This pallor presents a, text planner for l,h,' w,rlmliz~t-tion of natural deduction (ND) st, yle proofs [Gm,:Uq. Several similar attempi;s can be lbulld in previous work. I)eveloped before the era of NL genl!ral.))li, the system EXPOUND of D. (Thesl.er [Che76] call I>e characterized as an exatnl+le of direct translatio'a: Although a sophisticated linearizatioii is applied on the input ND proofs, the steps are then I:ranslated locally in a template driven way. ND i>rool:s were tested as input to an early version of the MUMI~,I,I'; systern of D. McDonald [McD83], the Irllain aim however, was to show the fl~asibility of the architecture. A more receitt attempt can be foilnd in 'l'lI[Nl(li;ll. [1']P93], whMI implements sew~ral interesting but isolated proof presentation strategies, witliout giving ;l comprehensive underlying model.","Our computational model can therelbre I>e viewed ;is the first serious attempt at a comprehensive coniputational model that produces adeqilate argillneill, al,iw~ te×l,s froln N]) si,yle proofs. The inaill nilll is I,+) sh+:,v how existing text planning techlliques Call [)t~ adapted for this particular apl)iication, q'o test its feasibility, this computational model is imldelilenl,ed lit a sysl,enl called PROVERH.","Most current NL text pialillers assiiltle thai, [aliguage generation is planned I>eh~vior ~tlid there-fore adopt a hiera.rchical platiliilig aplll'oach [Iiov88, Moo89, Da192, Rei91]. Nonetheless there! is psycllological ,~vidence that language has an ullplaluled, si~oti-. taneous aspect ms well [Och79]. Based on this ol~sei'val, ion, researchers have exploited organizing text with respect to some local relations. Sibun [Sib90] itnl+> merited a system generating descripticms for oltiects with a strong domain sl.ructure, such as houses, chips alld families. Once a discourse is st.;i,l'l,~;(I, local strllclures suggest the next objects awtilalfle. [nstead of planning globally, short-range sl;rategies ~tre cnlpl~tyed to ol'gallize ~t short seglrl<!lll, or' text. l?roni a collll)ltl, a-ti~mnl point of view, a hierarchical planner elaborates recursively on the initial commmiicatiw~ goal mitil the Ihtal sultgoals can be achieved by ++l>plyivg a prlmitiw'~ operator. A text generator based on the local organization, in contrast, repeatedly chooses a part of the renmiNing t.ask and carries it out.","The macroplanner of ]'IgOVER.B combines hierarcDical planTdng with local orgaMzation in a uniform planning framework. 'the hierarchical planning is realized by so-called top-dowu presentation operators that split the task of presenting a particular proof into subtasks of presenting subproofs. While the overall planning mechanism follows the RS'l.'-lmsed planning approach [Moo89, Reigl], the Idaiming operators lilt)re cl<+sely Feselllllle the schellHl+ta ill schema-based phtnning [McK85, Iqtr88]. l]ottom-up presentation Oilel'O, tOl'S ~tl'(~ (ll~vised to Sil/Itll;t|,e [,lie tLltpl,~tllll~d RSi>ect;, where the next intermediate cotichlsion to lm presenLed is chosen under the gttidance of the local fc>cus tnechanisnt in a lllore SpOlttarleous way. }'Jil:ce top-dmvn operal,ors enibody explicit coinnnnticative liOrlilS, they are ai',wiys given a higher priority. Only wheil lie l;op-dOWtl pr<?selil;atioit operator is apl>licabh;, will a bot,toiil-ttp present,alien operator be chosen.","This distinction betweeii plalllled alld illiplaltrled presentation loads to a w;ry natural seglnenl;ation of the discourse int.o an allciilional hierarchy, since, fol.. h>willg t.he theory of (.Irosz and Sidner [CS86], there is +l Oll+P-to-olle COl'l'esl)Oltdellce betweeli the ititelit, iolial hierarrhy and the al;tentional hierarchy. '['his atl;ent,iemal hierarchy iv used to itlltke,"]},{"title":"r@Jz'encc choices","paragraphs":["for inference ilietliods and for previously presented internimiiat,e conclusions, 'l'he inference choices itl'l~ the tllailt COliCerll of the iliieroitllmner of"]},{"title":"PR, O Vl'.'l~II(see","paragraphs":["[llua{)dlq). 2. (hmtext of Our Research","Tim text planner discussed hi this paper is the macroplanner of I'ROVI'H~,IJ, which translates machinef<mml iwool's in sew~rM steps into u~ttural lallgl.lage. PI:OVEI~II adopts a rcconslr'uclive approach: Once a l~roof in a m~u:hhie oriented forlnalistn is genel'~d;cd in the proof dewdopnwnt envh'onlnent fI--MKRP, a new proof' l, hat, m<+re resetnbles those found in mai;henmt,ical tex.tl)ooks is reconstructed [lhla94a]. The reconstrucl,<~d proo[\" is ~t proof"]},{"title":"lT\"ce,","paragraphs":["where"]},{"title":"proof","paragraphs":["nodes ave derived from their children by applying an inferc'nce tnel, hod (also called a justilical,ion). Most of the steps are justified by Lhe application of a definition 329","_ sgr(U, 1,'),, , U ~ ~ ~(U, 1,,, *),, u(ul, 1,,, *), Ul E U Du, ul C (;'"]},{"title":"_~z~zl)snl)gr[r,","paragraphs":["~ '1 \"","mo ; ~ _r,, --T-~UUl,, E U Ds, 'gr(F' *) D,"]},{"title":"_i!1: ,,, * 1,, --gl: I\" --'\" )2 -,rsol","paragraphs":["Figure 1: An Example lnlmt Proof or a theorem, the rest are justified by inference rules of the natural deduction (ND) ealcuhls, such as the \"Case\" rule. Figure 1 is an examph', of a segment of a possible input"]},{"title":"proof,","paragraphs":["where some nodes are labeled for convenience. The justifications \"Du\", \"Dsubgr\", \"Ds\", \"Dg\", and \"Tsol\" stand for the definitions of unit element, of subgroup, of subset, of group, and the theorem about solution, respectively.","The input proof tree is also augmented with an ordered list of nodes, being roots of subproofs planned in this order. The proof in Figure 1 is associated with the list: ([2], [a], [4], [l]). 3. The Framework of the Macroplanner Tl,e macroplanner of PROVERB elaborates on communicative goals, selects and orders pieces of inforrl~ation to fullill these goals. The output is an ordered sc'- quenee of proof communicative act iuteu~ions (PCAs). PCAs can be viewed as speech acts in our domain of application. Planning Pramework PROVERB combines the two above mentioned presentation modes by encoding communicat.ion knowledge for both top-down planning and bottom-up presentation in form of operators in a uniform planning framework. Since top-down presentation operators embody e×plieit communicatiw~ norms, they are given a higher priority. A botl.om-up presentation is"]},{"title":"chosen","paragraphs":["only"]},{"title":"when no","paragraphs":["top-down"]},{"title":"presentation operal,or","paragraphs":["applies. The overall planning framework is realized by the fimction present. Taking as inpul, a subproof, Present repeatedly executes a hasic planning cycle unl,il the inlmt subproof is eouw!yed. F, ach cycle carries out Olle presentation operal, or, where Present always tries first to choose and apply a top-down operator, if impossible, a Imttom.-up opc:rator will he chosen. ~l~he function Present is first called wil.}l t,he entire proof as the presentation task. The execution of a top-down presentation operator may generate subtasks by calling it recursively. The discourse produced by each call to Present tbrms an attentioual unit (compare the subsection below). The Discourse Model and the A.ttentiomd Hierarchy The discourse carried out so far is recorded in a discourse model. Rather than recording |he semantic ohjeets and their properties, our discourse model consists basically of the part of the input proof tree which has already been conveyed. The discourse model is also segmented into an allenlional hierarchy, where, subproofs posted by a top-down presentation operators as subtasks constitute attentional units. The. following are some notions useful for the formulation of the prese,]tation operators:","• Task is the subproof in the input proof whose presentation is the current task.","• Local focus is the intermediate conclusion lmst presented, while the semantic objects involved in t;he local tbcus are called the focal centers. Proof Comlnunieativc, Acts P(.,'As are the primil;ive actions plammd during the macroplanning to achiew. • communical;ive goals. Like speech acts, PCAs can be defined in terms of the commmlicative goals they fulfill as well as tlu-qr possible verbalizations. Based on an analysis of proofs in mathenuttical textlmoks, each PCA has as goal a combin-ation o17 the lbllowing sllhgoals:","1. CoIweying a st.ep of the deriwttion. '.Phe simplest ]'CA is the operator Derive. hlstantiated as be-","(Derive Reasons: lag ,5'I, EI C -- $2) Intermediate-Results : nil Derived-Formula: a G $2 Method: def-subset)","depending on the reference choices, a possible","verbalization is given as following: \"lb;cause a is an eh'.ment of 51 and ,%. is a subset of S,.,, according to the detinition of subset, a is an elelne/Lt of S:!.\"","2. I.Jpdates o[' I.he glob:d attentional structure. These I~CAs som,'t.imes also convey ~L partial plan for tim further l)resentation. IBlfects of this group of I'CAs include: creal, ing new attentional units, setting up partially premises and the goal of a new unit, closing t.he current unit, or l'ealloeal;ing the attention of the reader from one attentional unit to another. The PCA","(Begin-Cases Goal : l,'ormula Assumptions: (A I~)) creates two atteD.tional units with A and II as the assumptions, and Formula as the goal by producing the verbalization:","\"To prow\" Formula, let us consider the","two cases by assuming A and B.\"","Thirteen PCAs are currently employed in PRO-","VEI¢t3. See [Ilua94b] for more details. 330 Structure of the Plamfing OI)erators Although top-down and bottom-up presentation activities are of a eoneel)tually dift~rent nature, the corresponding communication knowledge is uniformly encoded as presentation oper'ators i|l a planning framework, similar to the plan operators in other generation systems [Hov88, Moo89, Da192, ILeigl]. In general, presentation operators map an original presentation task into a seqnenee of subtasks and finally into a sequence of PCAs. All of thenr haw~ the following four slots:","• Pro@ a proof schema, which characterizes the. syntactical structure of a proof segment for wllich this operator is designed. It plays 1;t1(.' role of the. goal slot in the traditional l)lanning franrework. • Applicability Condition: a pre([icate..","• Acts: a procedure which essentially carries out it seqtlellce of preselfl;atioli acts. They are either primitive PCAs, or are recursive calls to the procedure Present for subproofs.","• Features: a list of features which helps to select the best of a set of aI)l)licable operators. 4. Top-Down Planning 'I'his section elaborM;es oil the colnlntlnicative norms concerning how a proof to he presented can Im split into sitbproofs, as well ~us how the hierarchicallystructured subprooN can lie maplied onto some lineear order for presentation. In contrast with operators employed in RST-b~se(l plmuters that split goals according to the rhetorical structures, our operators encode standard schemata for presenting proofs, which (:oillain subgoals. The top-down presentation operators are roughly divided into two cate.gories:","• schemata-based operators encoding complex schemata for the presentation of proofs of a sl)ecilie pattern (twelve o1' tlwm are currently i,ltcgrated in PIgOVERII),","• general operators embodying general pr,~sentalion norms, concerning splitting proofs and ordering subgoals.","F t- [,' (1 i- (7 i"]},{"title":"~-rv-o ~ \",~,,,~:r Lcasl.:","paragraphs":["Figure 2: A Schmmt Involving Cases","Let us first look at an operator devised tbr proof segments containing eases. 'l'he. eorreslmnding schenra of such a proof tree is shown in Figure 2. Under two circumstances a writer lnay recognize that 11(; is confronted with a proof segment containing cases. First, when the snbproof that has the structure of l\"igure. 2 is the current presentation task, tested by (task ?L1) 1. Second, when the disjunction I,' V G has just been presented in the bottom-up mode, tested by (local- ['octls \"?L4). Under both circumstances, a teammatealien norm motiwttes the writer to First present the part leading to 1,' V G (in the second case this subgoal has ah'eady been aehiew3d), and then to proceed with the two cases. It enforces also that certain PCAs be used to mediat.e between 1)arts of l)roofs. This procedure is exactly captured by the presentation operator below. Case-Implicit • Proof: as given in lqgure 2","• Applicability Condition: ((task ?LI) V (local-l~,,',s '?1;4)) A (,,oi,-conveyed (?L., 7l~-,)) • Acts:","1. if ?L4 has not been conveyed, then l)resenl; '7174 (subgoal 1) 2. a PCA with the verbalization: \"First, let us","conside.r the first east., by assuming F.\" 3. preselfl; ?L2 (subgoal 2)","4. a PCA wit, h the vm'balization: \"Next, we consider the se.cond case by assuming (;.\" 5, presel,t '?La (subgoal 3) (i. mark \"71)1 as conw.ye(l • lL, atures: (top-down compulsory implicit)","q'he fl~atm'e values can be divided into two groups: those characterizing the style, of the 1;ext this operator produces, and those concerning other planning aspects. \"Implicit\" is a stylistie feature value, indicating that the splitting of the p,'oof into the three subgoals is not made explicit. In its explicit dnal Case-Explicit a PCA is added to the beginning of the Acts slot., wlfich l)ro(hiee.s tim verbalization: \"To prow~ Q, let us first prove F V G, and consider the two eases sel)arately.\"","The feature, wdue \"COmlmlsory\" indicates thai. if the applicallility condition is satisfied, and the style of the Ol)(~r;tl,{:,r (:Oll['orlns to the ghd)al style the texl. planner is (:olrlruitted to, this operator should be chosen. Two weaker vahms also retlect the speci[icii,y of plan operators: \"speci[ic\" and \"general\".","(h,neral l)resental.ion operators perform a simple task according to some general text organization principles. They either","• enforce a linearization on subprool~ to be presented, or","• split the task of the presentation of a proof with ordered snhproofs into sul)t.asks. t Labels stand fro\" the ¢m'respondhlg nodes 331","The first ordering operator operationalizes a general ordering strategy called"]},{"title":"minimal load principle.","paragraphs":["This principle predicates that a writer usually presents shorter branches beibre longer ones. The argument of Levelt is rather simple: When one branch is chosen to be described first, the writer has to have the"]},{"title":"choice node","paragraphs":["flagged in his memory for return. If he follows the shorter branch first., the duratiml of the load will be shorter. The eonerete operator is omitted.","Note that, the subproofs being ordered are subproofs conceptually planned while the correspm,ding proof is constructed. There are two other ordering operators based on general ordering principles: the"]},{"title":"local focus","paragraphs":["principle and the"]},{"title":"proof time order","paragraphs":["principle [IIua94b].","The invocation of an ordering operator is always followed by the invocation of a splitting operator, which actually posts subgoals by calling the function Present with the ordered goals subsequently. 5. Bottom-up Presentation The"]},{"title":"bottom-up presentation","paragraphs":["process simulates the unplanned part of proof presentation. Instead of splitting presentation goals into subgoals according to standard schernata, it follows the local derivation relation to find a next proof node or subproof to be presented, in this sense, it is similar to the local organization techniques used in [Sib90]. When no top-down presentation operator applies,"]},{"title":"I~ROVI'2RB","paragraphs":["chooses a bottom-up operator. The Local Focus The node to be presented next is suggested by the mechanism of"]},{"title":"local focus.","paragraphs":["Although logically any proof node having the local focus as a child could be choserl for the next step, usually the one with the greatest semantic overlapping with the"]},{"title":"focol cenier's","paragraphs":["is preferred. As mentioned above, focal centers are senmntic objects mentioned in the proof node which is the local focus. This is based on the observation that if one has proved a property about some semantic"]},{"title":"obje.cts,","paragraphs":["one tends to continue to talk about these particular objects before turning to new ohjects. Let ns examine the situation when the proof below is awn.iting I~,'ese,,L-ation."]},{"title":"j~] : or,,, b)' [a] .7-0(., b) A :e(I,, .)","paragraphs":["Assume that node [1] is the local focus, the set;"]},{"title":"{a, b}","paragraphs":["are the focal centers, [3] is a previously presented node and node [5] is the current task. [2] is chosen as the next node to be presented, since it, does not (re)introduce any new semantic object and it.s overlap with the focal centers ({a, b}) is larger than those of"]},{"title":"[4] ({~}).","paragraphs":["The Bottom-Up Presentatioll Operators Under different circumstances the deriwH, ion of the next-node is also presented in different ways, The corresponding presentation knowledge is encoded as bottom-np presentation operators. The one most frequent.ly used presents one. step of derivation: Derlve.-Bot tom-Up • Proof:"]},{"title":"?Nodel~...l?Node,~?M ? Noden+ l •","paragraphs":["Applicability Condition:"]},{"title":"?Noden+ 1","paragraphs":["is suggested by the focus mechar, ism as the next node, and"]},{"title":"?No&a,..., ?Node,,","paragraphs":["are conveyed. • Acts: a PCA that conveys the fact that"]},{"title":"?Node,+1","paragraphs":["is derived from the premises"]},{"title":"'?Nodq,..., ?Noden","paragraphs":["by applying ?M. • Features: (bottom-up general explicit detailed) If the conclusion"]},{"title":"?Node,+l,","paragraphs":["the premises and the method ?M are instantiated to a G S1, (a G ,92,"]},{"title":"S~ G S.,), def s'ubse.t","paragraphs":["respectively, the following w.~rbal-","ization can be produced: \"Since a is an element of S~, and $1 is a subset of S.,, a is an element of oe2 according to the definition of subset.\""]},{"title":"A lrivial","paragraphs":["suhproof may be presented as a single deriwttion by ornitting the intermediate nodes. 'this"]},{"title":"nezl s.ubproof","paragraphs":["is also suggested by the local focus. This is sinmlated by a bottom-up operator called Simplify-Bottom-Up. Currently seven bottom-up operators are it,l.egrated in"]},{"title":"PROVERB. 6.","paragraphs":["Verbalization of PCAs Macroplanning produces a sequence of PCAs. Our mieroplanner is restricted to the treatment of the re f-"]},{"title":"eremite choices","paragraphs":["for the inference methods and for the previously presented intermediate conclusions. While the former depelMs on static salience relating l,o the domain kuowledge, the latter is similar to subsequent refi.'rences, and is therelbre sensitive to the context, in particular to it:s segmentation into attenl,ional hierarchy. Dne to space restrictions, we only show the following piece of a"]},{"title":"prcverbal message","paragraphs":["as an example, he-i,lg a PCA enriched with reflq'ence dmices for reasons aml nn!l.hod by the microplanner [IluaDdh, IIua94b]. (Derive Reasons: (((ELE a U) explicit)","((SUBSET U F) omit))","Conclusion: (ELE a F)","Method: (Dof-Subsot omit))","Our surface generator TAG-GI'~N [Ki194] produces","the ul, terance: \"Since a is an element of U, a is an element of F.\"","Notice, only the l'[~ason labeled as"]},{"title":"\"explicit\"","paragraphs":["is verbalized. Finally, to demonstrate the type of proofs currently generated by"]},{"title":"PI~OVER.B,","paragraphs":["below is the complete out-l)ut ['or a proof constructed by f2-MKIIP: Thc, orem: Let /'~ be a grou I) and U a subgroup of F, if I and Iv are unit elements of F and U respectively, then 1 = 1¢:. 332 Proof:","Let F be a group, U be a subgroup of /,', 1 he a unit element of F and lu be a unit element of U. According to the definition of unit eleme,lt, Iu rE U. Therefore there is an X, X C U. Now suppose that ~tl is such an X. According to the definition of ilnit. [Che711] element, ut * 1u = ut. Since U is a subgrmtp of t:', U C F. \"lqterefore 1u E F. Similarly ul G F, since [[);,19'-'] ul G U. Since F is a group, F is asemigroui). Because ul* lcr = ul, 1u is asolution of tile equation \"l *X = [EP93] ul. Since 1 is a unit element of/\", vx * 1 = ul. Since 1 is a unit element of F, 1 C F. Because tq ~ [', 1 is a solution of the equation ul * X = ul. Since F is a group, lry = 1 by the uniqueness of solution. This [Gen35] conclusion is independent of the choice of the element ut. [CSsq 7. Conclusion and lhlture \"Work [ll~,vss] This paper puts forward an architecture that comlfines several estahlished NL generation techniques adapted for a particular application, namely the presentation of ND style proofs. We hope that this architecture is also of general interest beyond this particular application.","The most important feature of this model is that hierarchical planning and unplanned spontaneous presentation are integrated in a nnitbrm framework. ~Ibp-down hierarchical planning views language gmmr-ation ,'~s planned behavior. Bmsed on explicit colnn!unicative knowledge encoded as schemata, hierarchical planning splits a presentation task into sul~tasks. Although our overall presentation mechanism has much in common with that of H.ST-Imsed text planm.,rs, the [I,:i]9,1] top-down planning operators contain mostly complex presentation schemata, like those in schema-based plamfing. Since schemata-based planning covers only [Mct)83] proofs of some particular structure, it is complenlented by a mechanism called hottom-up presentation. Bottom-up presentation aims at simulating the unplanned part of proof presentation, where, a proof node [McI(.85] or a subproof awaiting present.ation is chosen as the next to he presented via the local derivation relations. [Moo89] Since more than one such node is often available, the local focus mechanism is employed to single out. the candidate having the strongest semantic links with the [Och7!J] focal centers. The distinction between l~lanned and unplanned behavior enables a very natural segment- [ParS8] ation of the discourse into an attentional hierarchy. This provide an appropriate basis for a discourse theory which handles reference choices [Iluagdb}. [l'~ei91]","Compared with proofs found in mathematical text-books, the"]},{"title":"output","paragraphs":["of PROVERB is still to, tedious and inflexible. The tediousness is largely ascril)ed to [Sibg0] the lack of plan level knowledge of the input proofs, which distinguishes crucial steps from unimportant details. Therefore, sophisticated plan recognition techniques are necessary. The inflexibility of text currently produced is partly inherited from the schemata-based approach, for which a fine-grained plamfing in terms of single PCAs might he a remedy. It is also partly due to the fixed lexicon choice, which we are currently reimplcnmntiug."]},{"title":"References","paragraphs":["[Ilua94a] [ltuag,ll,] [IIua94 b] l). Chester. The translation o[ formal proofs into English. Artificial Intelligence, 1976. R. Dale. Generating Re/erring I'Txpressio,Js. MVI\" Press, 1092. A. Edgar and P..]. Pelletier. Natural langnage explat)llttiott of lla.Lllr;tl de.dllctiolt proofs, in Pr'oe. of the first Conf. of the t'aeqic Assoc. for Comp. Linguistics, 1993. G. Qentzen. Untersuchungen (iber das logische Schliegen 1. Math. Zeitsehrift, 1935. B. J. Grosz and C. L. Sidaer. Attention, intentions, and the structure of discourse. Computational Liug.istics, 1986. E. It. [Iovy. (/enerating Natural Language under Progm,tie Coustrints. L;twrence Erlbaum Associates, [IillsdMe, 1988. X. IIuang. Reconstructing proofs at the asser-tion h-'vel. In Proc. o] l~th CADE, 199,1, fortheOlllitlg. X. IIuitng. Pl~tnning Reference Choices for Argumentatiw: ~D:xts. In Proe. of the 7th International Wo,'kshol~ on Natural Language Gene.ratiott, 199,1, forthcoming. X. IIuang. A Reconstructive Approach to Human Oriented Proof P~vsentation. PlID thesis, (lniversitiit des Satarlatndes, (-lermally, 1994, forthcoming. A. Kilger. Using U'I'AQs for increment:d ;tnd parallel generatitm. Computationallntelli- [lence, forthcoming, 11994. 1). l). McDonald. Natural language generation its ;t COmlmt,'ttionM prn[~leln. In Brady/llcrwick: Computational Models of Discourse. MI'].\" Press, 1983. [(. R. MeI(ec~wn. Te:L't (~eneratlon, Cambridge University Press, 1985. J. I). Moore. A Reactive Approach to l:2xphm-ation i. Expert arm Adoice-Gioin9 Systems. PhD thesis, Univ. of California, 1989. E. Ochs, I'lamled ;tml ulq)lanned discourse. Synt.x and £'em~mlies, 1979. C. P;tris. Taih~ring object descriptions to zt user's lewd of experti:;e. Compulutional Linguisl.:s, 1988. N. l{eithinger. Eine parallch~ Architektur zur inkrententeller Dialogbeitriige. Pill) thesis, Universitiit ties Saarlrtndes, 1991. P. Sibun. The loc;tl org,'tnization of text. In I(.R. McKeown eta1, editors, Proc. of the 5th International Workshop on Natural Language (?ener.tion, 1990. .~3"]}]}