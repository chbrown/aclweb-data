{"sections":[{"title":"Computational Complexity of Probabilistic Disambiguation by means of Tree-Grammars Khalil Sima'an* Research Institute for Language and Speech, Utrecht University, Trans 10, 3512 JK Utrecht, The Netherlands, Email: khalil.simaan(@let.ruu.nl. Abstract","paragraphs":["This paper studies the computational complexity of disambiguation under probabilistic tree-grammars as in (Bod, 1992; Schabes and Waters, 1993). It presents a proof that the following problems are NP-hard: computing the Most Probable Parse from a sentence or from a word-graph, and computing the Most Probable Sentence (MPS) from a word-graph. The NP-hardness of computing the MPS from a word-graph also holds for Stochastic Context-Free Grammars (SCFGs)."]},{"title":"1 Motivation","paragraphs":["Statistical disambiguation is currently a popular technique in parsing Natural Language. Among the models that implement statistical disambiguation one finds the models that employ Tree-Grammars such as Data Oriented Parsing (DOP) (Scha, 1990; nod, 1992) and Stochastic (Lexicalized) Tree-Adjoining Grammar (STAG) (Schabes and Waters, 1993). These models extend the domain of locality for expressing constraints from simple Context-Free Grammar (CFG) productions to deeper structures called elementary-trees. Due to this extension, the one to one mapping between a derivation and a parse-tree, which holds in CFGs, does not hold any more; many derivations might generate the same parse-tree, rl'his seemingly spurious ambiguity turns out crucial for statistical disambiguation as defined in (Bod, 1992) and in (Schabes and Waters, 1993), where the derivations are considered different stochastic processes and their probabilities all contribute to the probability of the generated parse. Therefore the Most Probable Deriva-tion (MPD) does not necessarily generate the","*Special thanks to Christer Samuelsson who pointed out and helped in solving a problem with a previous version. Thanks to Remko Scha, Rens Bod and Eric Aarts for valuable comments, and t~o Steven Krauwer and the STT for the support. Most Probable Parse (MPP).","The problem of computing the MPP in the DOP framework was put forward in (Bod, 1995). The solution which Bod proposes is Monte-Carlo estimation (Bod, 11993), which is essentially repeated random-sampling for minimizing errorrate. A Viterbi-style optimization for computing the MPP under I)OP is presented in (Sima'an et al., 1994), but it does not guarantee deterministic polynomial-time complexity. In this paper we present a proof that computing the MPP under the above mentioned stochastic tree grammars is NP-hard. Note that for computing the MPD there are deterministic polynomial-time algorithms (Schabes and Waters, 1993; Sima'an, 1996) 1. Another problem that turns out also NP-hard is computing the Most Probable Sentence (MPS) from a given word-graph. But this problem turns out NP-hard even for SCFGs.","Beside the mathematical interest, this work is driven by the desire to develop efficient algorithms for these problems. Such algorithms can be useflll for various applications that demand robust and faithful disambiguation e.g. Speech Recognition, information Retrieval. '['his proof provides an explanation for the source of complexity: and forms a license to redirect the research for solutions towards non-standard optimizations.","The structure of the paper is as follows. Section 2 briefly discusses the preliminaries. Section 3 presents the proofs. Section 4 discusses this result, points to the source of complexity and suggests some possible solutions. The presentation is for-real only where it, seemed necessary."]},{"title":"2 Preliminaries 2.1 Stochastic Tree-Substltution","paragraphs":["Grammar (STSG) STSGs and SCFGs are closely related. STSGs and SCFGs are equal in weak generative ca-","i The author notes that the actual accuracy figures of the experiments listed in (Sima'an, 1995) are much higher than the accuracy figures reported in the paper. The lower figures reported in that paper are due to a test-procedure. 1175 pacity (i.e. string languages). This is not the case for strong generative capacity (i.e. tree languages); STSGs can generate tree-languages that are not generatable by SCFGs. An STSG is a five-tuple"]},{"title":"(VN, VT, S, d, PT),","paragraphs":["where"]},{"title":"VN","paragraphs":["and"]},{"title":"VT","paragraphs":["denote respectively the finite set of non-terminal and terminal symbols, S denotes the start non-terminal, C is a finite set of elementary-trees (of arbitrary depth > 1) and"]},{"title":"PT","paragraphs":["is a function which assigns a value 0 <"]},{"title":"PT(t)","paragraphs":["< 1 (probability) to each elementary-tree t such that for all"]},{"title":"N EVN: Y].tee,","paragraphs":["root(tl=N"]},{"title":"PT(t)","paragraphs":["= 1 (where"]},{"title":"root(t)","paragraphs":["denotes the root of tree t). An elementary-tree in C has only non-terminals as internal nodes but may have both terminals and non-terminals on its frontier. A non-terminal on the frontier is called an Open-Tree (OT). If the left-most open-tree N of tree t is equal to the root of tree tl then t otl denotes the tree obtained by substituting tl for N in t. The partial function o is called left-most substitution. A left-most derivation (1.m.d.) is a sequence of left-most substitutions lmd = (...(tlOt2)o...)ot,,, wheretl,...,t,~ E d,"]},{"title":"root(tl) = S","paragraphs":["and the frontier of lmd consists of only terminals. The probability"]},{"title":"P(Imd)","paragraphs":["is defined as"]},{"title":"PT(tl) x ...x PT(t~). ~'or","paragraphs":["convenience,"]},{"title":"derivation","paragraphs":["in the sequel refers to 1.m. derivation. A Parse is a tree generated by a derivation. A"]},{"title":"parse is possibly generatable by many derivations.","paragraphs":["The probability of a parse is defined as the sum of the probabilities of the derivations that generate it. The probability of a sentence is the sum of the probabilities of all derivations that generate that sentence.","A word-graph over the alphabet Q is Q1 x"]},{"title":"• ..x Qm, whereQiC","paragraphs":["Q, foralll < i<_ m. We denote this word-graph with Qm if-Qi = Q, for alll< i< m. 2.2 The 3SAT problem It is sufficient to prove that a problem is NP-hard in order to prove that it is intractable. A problem is NP-hard if it is (at least) as hard as any problem that has been proved to be NP-complete (i.e. a problem that is known to be decidable on a non-deterministic Taring Machine in polynomial-time but not known to be decidable on a deterministic Turing Machine in polynomial-time). To prove that problem A is as hard as problem B, one shows a reduction from problem B to problem A. The reduction must be a deterministic polynomial time transformation that preserves answers.","The NP-complete problem which forms our starting-point is the 3SAT (satisfiability) problem. An instance INS of 3SAT can be stated as follows~:","Given an arbitrary a Boolean formula in","3-conjunctive normal form (3CNF) over :In the sequel, INS, INS's formula and its symbols","refer to this particular instance of 3SAT. 3Without loss of generality we assume that the for-the variables ul,..., un. Is there an assignment of values true or false to the Boolean variables such that the given formula is true ? Let us denote the given formula by C1 A C2 A. • • ACm for ra > 1 where 6'/ represents (d¢1 V dis V dis), for 1 < i < m, 1 < j _< 3, and dij represents a literal"]},{"title":"uk","paragraphs":["or ~k for some","1< k< n.","Optimization problems are known to be (at least) as hard as their decision counterparts (Garey and Johnson, 1981). The decision problem related to maximizing a quantity M which is a function of a variable V can be stated as follows: is there a value for V that makes the quantity M greater than or equal to a predetermined value m. The decision problems related to disambiguation under DOP can be stated as follows, where G is an STSG,"]},{"title":"WG","paragraphs":["is a word-graph, w~isasentence and0 < p < 1: MPPWG Does the word-graph"]},{"title":"WG","paragraphs":["have any parse, generatable by the STSG G, that has probability value greater than or equal to p ?","MPS Does the word-graph"]},{"title":"WG","paragraphs":["contain any sentence, generatable by the STSG G, that has probability value greater than or equal to p ?","MPP Does the sentence w~ have a parse generatable by the STSG G, that has probability value greater than or equal to p ?","Note that in the sequel MPPWG / MPS / MPP","denotes the"]},{"title":"decision","paragraphs":["problem corresponding to the problem of computing the MPP / MPS / MPP from a word-graph / word-graph / sentence re-"]},{"title":"spectively. 3 Complexity of MPPWG, MPS and MPP","paragraphs":["3.1 3SAT to MPPWG and MPS The reduction from the 3SAT instance INS to an MPPWG problem must construct an STSG and a word-graph in deterministic polynomial-time. Moreover, the answers to the MPPWG instance must correspond exactly to the answers to INS. The presentation of the reduction shall be accompanied by an example of the following 3SAT instance (Barton et al., 1987): (ul V E2 V ua) A (~1 V l/,2 V U3). Note that a 3SAT instance is satisfiable iff at least one of the literals in each conjunct is assigned the value True. Implicit in this, but crucial, the different occurrences of the literals of the same variable must be assigned values"]},{"title":"consistently.","paragraphs":["Reduction: The reduction constructs an STSG and a word-graph. The STSG has start-symbol S, two terminals represented by T and F , non-terminals which include (beside S) all Ck, for mula does not contain repetition of conjuncts. 1176 2/13 S 2/13 S • , C! '"]},{"title":"f?-.","paragraphs":["Ul 132 tl3 ii1 112 113 nl 112 ll3 )ll 112 U3"]},{"title":"I , I","paragraphs":["/ T F F T 1/3"]},{"title":"i","paragraphs":["'l ii2 u3 W 2/13 2/13 S S t:2 Cl (:2"]},{"title":"_/7\"--._","paragraphs":["F T T F","2/13 2/13 s s I/3 C1","u I ~ 2 u 3 / T 113","u I ii2 i 13 / W"]},{"title":"/","paragraphs":["C2 CI C2"]},{"title":"_iT'--_ '3 3","paragraphs":["T F F T 1/21/21/21/2 1/21/21/2 1/2 1/2 [/21/2 I/2 1 I 112 u3 il3 ~i3 ~3"]},{"title":"i' i\" r i\" f i | / /","paragraphs":["T F F F F T F' F T T T T 1/3"]},{"title":"j@-.._ i","paragraphs":["l I u 2 u 3 W 1/3 Ul u2 u3"]},{"title":"L","paragraphs":["W 1/3","Ul u2 ~3 / T 1/13 S c I Figure 1: The elementary-trees for the example 3SAT instance 1 < k < rn, and both literals of each Boolean variable of the formula of INS. The set of elementary-trees and probability function and the word-graph are constructed as follows:","1. For each Boolean variable ui, 1 < i < n, construct two elementary-trees that correspond to assigning the values true and false to ui consistently through the whole formula. Each of these elementary-trees has root S, with children Ck, 1 5 k < rn, in the same order as these appear in the formula of INS; subsequently the children of Ck are the non-terminals that correspond to its three disjuncts dkl, dk2 and dk3. And finally, the assignment of true (false) to ui is modeled by creating a child terminal T (resp. F ) to each non-terminal ui and P (resp. T ) to each ul. The two elementary-trees for u~, of our example, are shown in the top left corner of figure 1.","2. The reduction constructs three elementary-trees for each conjunct Ck. The three elementary-trees for conjunct Ck have the same internal structure: root Ck, with three children that correspond to the disjuncts dkl, dk2 and dk3 In each of these 3. 4. . elementary-trees exactly one of the disjuncts has as a child the terminal T ; in each of them this is a different one. Each of these elementary-trees corresponds to the conjunct; where one of the three possible literals is assigned the value T . For the elementary-trees of our example see the top right corner of figure l. The reduction constructs for each of tile two literals of each variable ni two elementary-trees where the literal is assigned in one case T and in the other F . Figure 1 shows these elementary-trees for variable ul in the bottom left corner. The reduction constructs one elementary-tree that has root S with children Ck, 1 < k < rn, in the same order as these appear in the formula of INS (see the bottom right corner of figure 1). The probabilities of the elementary-trees that have the same root non-terminal sum up to 1. The probability of an elementary-tree with root S that was constructed in step 1 of this reduction is a value Pi, 1 _< i < n, where ui is the only variable of which the literals in the elementary-tree at hand are lexical-1177 ized (i.e. have terminal children). Let ni denote the number of occurrences of both literals of variable ui in the formula of INS. Then Pi = 0 (1⁄2)ni for some real 0 that has to fulfill some conditions which will be derived next. The probability of the tree rooted with S and constructed at step 4 of this reduction must then bep0 = [1 - 2~i=lpl]. The probability of the elementary-trees of root Ck (step 2) is (1), and of root ui or ul (step 3)is (1⁄2). For our example some suitable probabilities are shown in figure 1.","6. Let Q denote a threshold probability that shall be derived hereunder. The MPPWG (MPS) instance is: does the STSG generate a parse (resp. sentence) of probability > Q, for the word-graph WG = {T, F} 3m ? Deriving the probabilities: The parses generated by the constructed STSG differ only in the sentences on their frontiers. Therefore, if a sentence is generated by this STSG then it has exactly one parse. This justifies the choice to reduce 3SAT to MPPWG and MPS simultaneously.","One can recognize two types of derivations in this STSG. The first type corresponds to substituting for an open-tree (i.e literal) of any of the 2n elementary-trees constructed in step 1 of the reduction. This type of derivation corresponds to assigning values to all literals of some variable ui in a consistent manner. For all 1 < i < n the probability of a derivation of this type is","13m n pi( ) - ' =","The second type of derivation corresponds to substituting the elementary-trees rooted with Ck in S -+ C1 ... C,~, and subsequently substituting in the open-trees that correspond to literals. This type of derivation corresponds to assigning to at least one literal in each conjunct the value true. The probability of any such derivation is 12ml m \" i","P0(~) (~) : [1-20~_~_,(~)n'](~)~\"~(~) TM","i=l","Now we derive both the threshold Q and the parameter 0. Any parse (or sentence) that fulfills both the \"consistency of assignment\" requirements and the requirement that each conjunct has at least one literal with child T , must be generated by n derivations of the first type and at least one derivation of the second type. Note that a parse can never be generated by more than n derivations of the first type. Thus the threshold"]},{"title":"q","paragraphs":["is: n","q = + [ -2o i=I However, 0 must fulfill some requirements for our reduction to be acceptable:","1. For all i: 0<pi < 1. This means that for 1 < i <_ n: 0 < 0(1⁄2) '~' < 1, and 0 < P0 < 1. However, the last requirement on P0 implies that 0 < 20V\"~z--,i=l~2/(!~'~ < 1, which is a stronger requirement than the other n requirements. This requirement can also be stated as follows:","1 0 < 0 < ,.","2. Since we want to be able to know whether a parse is generated by a second type derivation only by looking at the probability of the parse, the probability of a second type derivation must be distinguishable from first type derivations. Moreover, if a parse is generated by more than one derivation of the second type, we do not want the sum of the probabilities of these derivations to be mistaken for one (or more) first type derivation(s). For any parse, there are at most 3 TM second type derivations (e.g. the sentence T ...T ). Therefore we require that: i:1 Which is equal to 0 > 2~=,(~¢__","3. For the resulting STSG to be a probabilistic model, the \"probabilities\" of parses and sentences must be in the interval (0, 1]. This is taken care of by demanding that the sum of the probabilities of elementary-trees that have the same root non-terminal is 1, and by the definition of the derivation's probability, the parse's probability, and the sentence's probability.","There exists a 0 that fulfills all these requirements","1 is because the lower bound 2}--2~=1(~)~ 1 ~ + (1⁄2)~ always larger than zero and is strictly smaller than","1 the upper bound 2~i_~(~)","Polynomlality of the reduction: This reduc-","tion is deterministic polynomial-time in n because","it constructs not more than 2n + 1 + 3m + 4n","elementary-trees of maximum number of nodes 4","7m+ 1.","The reduction preserves answers: The","proof concerns the only two possible answers.","Yes If INS's answer is Yes then there is an as-","signment to the variables that is consistent","and where each conjunct has at least one lit-","eral assigned true. Any possible assignment","is represented by one sentence in WG. A","sentence which corresponds to a \"successful\"","assigmnent must be generated by n deriva-","tions of the first type and at least one deriva-","tion of the second type; this is because the","4Note than m is polynomial in n because the formula does not contain two identical conjuncts. 1178 sentence w 3m fulfills n consistency requirements (one per Boolean variable) and has at least one W as Wak+l, Wak+2 or W3k+3 , for all 0 < k < m. Both this sentence and its corresponding parse have probability > Q. Thus MPPWG and MPS also answer Yes.","No IfINS's answer is No, then all possible assignmcnts are either not consistent or result in at least one conjunct with three false disjuncts, or both. The sentences (parses) that correspond to non-consistent assignments each have a probability that cannot result in a Yes answer. This is the case because such sentences have fewer than n derivations of the first type, and the derivations of the second type can never compensate for that (the requirements on 0 take care of this). For the sentences (parses) that correspond to consistent assignments, there is at least some 0 < k < m such that wak+l , wak+2 and Wak+3 are all F . These sentences do not have second type derivations. Thus, there is no sentence (parse) that has a probability that can result in a Yes answer; the answer of MPPWG and MPS is NO. We conclude that MPPWG and MPS are both NP-hard problems.","Now we show that MPPWG and MPS are in NP. A problem is in NP if it is decidable by a non-deterministic Turing machine. The proof here is informah we show a non-deterministic algorithm that keeps proposing solutions and then checking each of them in deterministic polynomial time cf. (Barton et al., 1987). If one solution is successful then the answer is Yes. One possible non-deterministic algorithm for the MPPWG and MPS, constructs firstly a parse-forest for WG in deterministic polynomial time based on the algorithms in (Schabes and Waters, 1993; Sima'an, 1996), and subsequently traverses this parse-forest (bottom-up for example) deciding at each point what path to take. Upon reaching the start non-terminal S, it retrieves the sentence (parse) and evaluates it in deterministic polynomial-time (Sima'an et al., 1994), thereby answering the decision problem.","This concludes the proof that MPPWG and MPS are both NP-complete. 3.2 NP-eompletetless of MPP The NP-completeness of MPP can be easily deduced from the previous section. In the reduction the terminals of the constructed STSG are new symbols vii, 1 < i < m and 1 < j < 3, instead of T and F that becomc non-terminals. Each of the elementary-trees with root S or Ck is also represented here but each T and F on the frontier has a child vkj wherever the T or F appears as the child of the jth child (a literal) of Ck. For each elementary-tree with root ui or ui, there are 3m elementary-trees in tile new STSG that correspond each to creating a child ~)ij for the T or F on its frontier. The probability of an elementary-tree rooted by a literal is 1 The probabilities of elementary-trees rooted gm\" with Ck do not change. And the probabilities of the elementary-trees rooted with S are adapted from the previous reduction by substituting for","1 every (1⁄2) the value ~. The threshold Q and the requirements on 0 are also updated accordingly. The input sentence which the reduction constructs is simply v11.., v3,~. The decision problem is whether there is a parse generated by the resulting STSG for this sentence that has probability larger than or equal to Q.","The rest of the proof is very similar to that in section 3. Therefore the decision problem MPP is NP-complete. 3.3 MPS under SCFG The decision problem MPS is NP-complete also under SCFG. The proof is easily deducible from the proof concerning MPS for STSGs. The reduction simply takes the elementary-trees of the MPS for STSGs and removes their internal structure, thereby obtaining simple CFG productions. Crucially, each elementary-tree results in one unique CI\"G production. The probabilities are kept the same. The word-graph is also the same word-graph as in MPS for STSGs. The problem is: does the SCFG generate a sentence with probability _> Q, for the word-graph W G -- {T, F} 3m. The rest of the proof follows directly from section 3."]},{"title":"4 Conclusion and discussion","paragraphs":["We conclude that computing the MI)P / MPS / MPP from a sentence / word-graph / word-graph respectively is NP-hard under DOP. Computing the MPS from a word-graph is NP-hard even under SCI,'Gs. Moreover, these results are applicable to STAG as in (Schabes and Waters,"]},{"title":"1993).","paragraphs":["The proof of the previous section helps in understanding why computing tt, e MPP in DOP is such a hard problem. The fact that MPS under SCFG is also NP-hard implies that the complexity of the MPPWG, MPS and MPP is due to the definitions of the probabilistic model rather than the complexity of tile syntactic model.","The main source of NP-completeness is the following common structure of these problems: they all search for an entity that maximizes the sum of the probabilities of processes which depend on that entity. For the MPS problem of SCFGs for example, one searches for the sentence which maximizes the sum of the probabilities of the parses that generate that sentence (i.e. the probability of a parse is also a function of whether it generates the sentence at: hand or not). This is not the 1179 case, for example, when computing the MPD under STSGs (for sentence or even a word-graph), or when computing the MPP under SCFGs (for a sentence or a word-graph).","The proof in this paper is not a mere theoretical issue. An exponential algorithm can be comparable to a deterministic polynomial algorithm if the grammar-size can be neglected and if the exponential formula is not much worse than the polynomial for realistic sentence lengths. But as soon as the grammar size becomes an important factor (e.g. in DOP), polynomiality becomes a very desirable quality. For example tGI e ~ and IGI n a for n < 7 are comparable but for n = 12 the polynomial is some 94 times faster. If the grammar size is small and the comparison is between 0.001 seconds and 0.1 seconds this might be of no practical importance. But when the grammar size is large and the comparison is between 60 seconds s and 5640 seconds for a sentence of length 12, then things become different.","To compute the MPP under DOP, one possible solution involves some heuristic that directs the search towards the MPP; a form of this strategy is the Monte-Carlo technique. Another solution might involve assuming Memory-based be-havior in directing the search towards the most \"suitable\" parse according to some heuristic evaluation function that is inferred from the probabilistic model. And a third possible solution is to adjust the probabilities of elementary-trees such that it is not necessary to compute the MPP. The probability of an"]},{"title":"elementary-tree can","paragraphs":["be redefined as the sum of the probabilities of all derivations that generate it in the given STSG. This redefinition can be applied by off-line computation and normalization. Then the probability of a parse is redefined as the probability of the MPD that generates it, thereby collapsing the MPP and MPD. This method assumes full independence beyond the borders of elementary-trees, which might be an acceptable assumption.","Finally, it is worth noting that the solutions that we suggested above are merely algorithmic. But the ultimate solution to the complexity of probabilistic disambiguation under the current models lies, we believe, only in further incorporation of the crucial elements of the human processing ability into these models. References","G. Edward Barton, Robert Berwick, and Eric Sven Ristad. 1987."]},{"title":"Computational Complexity and Natural Language.","paragraphs":["A Bradford Book, The MIT Press.","SThis is a realistic figure from experiments on the ATIS.","Rens Bod. 1992. A computational model of language performance: Data Oriented Parsing. In"]},{"title":"Proceedings COLING'g2,","paragraphs":["Nantes. Rens Bod. 1993. Monte Carlo Parsing. In"]},{"title":"Proceedings Third International Workshop on Parsing Technologies, Tilburg/Durbuy.","paragraphs":["Rens Bod. 1995. The Problem of Computing the Most Probable Tree in Data-Oriented Parsing and Stochastic Tree Grammars. In"]},{"title":"Proceedings Seventh Conference of The European Chapter of the A CL, Dublin,","paragraphs":["March. Michael Garey and David Johnson. 1981."]},{"title":"Computers and Intractability.","paragraphs":["San Fransisco: W.H. Freeman and Co. John Hopcroft and Jeffrey Ullman. 1979."]},{"title":"Introduction to Automata Theory, Lanaguges, and Computation.","paragraphs":["Reading, MA: Addison Wesley.","Aravind Joshi and Yves Schabes. 1991. Tree-Adjoining Grammars and Lexicalized Grammars. In M. Nivat and A. Podelski, editors,"]},{"title":"Tree Automata and Languages.","paragraphs":["Elsevier Science Publishers. Harry Lewis and Christos Papadimitriou. 1981."]},{"title":"Elements of the Theory of Computation.","paragraphs":["Englewood-Cliffs, N.J., Prentice-Hall.","Philip Resnik. 1992. Probabilistic Tree-Adjoining Grammar as a Framework for Statistical Natural Language Processing. In"]},{"title":"Proceedings COL-ING'92,","paragraphs":["Nantes.","Arto Salomaa. 1969. Probabilistic and Weighted Grammars."]},{"title":"Inf. Control,","paragraphs":["15:529-544.","Rcmko Scha. 1990. Language Theory and Language Technology; Competence and Performance (in Dutch). In Q.A.M. de Kort and G.L.J. Leerdam, editors,"]},{"title":"Computertoepassingen in de Neerlandistiek,","paragraphs":["Almere: LVVN-jaarboek.","Yves Schabes and Richard Waters. 1993. Stochastic Lexicalized Context-Free Grammar. In"]},{"title":"Proceedings Third IWPT,","paragraphs":["Tilburg/Durbuy.","Khali] Sima'an, Rens Bod, Steven Krauwer, and Remko Scha. 1994. Efficient Disambiguation by means of Stochastic Tree Substitution Grammars. In"]},{"title":"Proceedings International Conference on New Methods in Language Processing.","paragraphs":["CCL, UMIST, Manchester.","Khalil Sima'an. 1995. An optimized algorithm for Data Oriented Parsing. In"]},{"title":"Proceedings RANLP,","paragraphs":["Tzigov Chark, Bulgaria.","Khalil Sima'an. 1996. An optimized algorithm for Data Oriented Parsing. In R. Mitkov and N. Nicolov, editors,"]},{"title":"Recent Advances in Natural Language Processing 1995,","paragraphs":["volume 136 of"]},{"title":"Current Issues in Linguistic Theory.","paragraphs":["John Benjamins, Amsterdam. 1180"]}]}