{"sections":[{"title":"Using Discourse Predictions for Ambiguity Resolution Yan Qu, Carolyn P. Ros6 and Barbara Di Eugenio","paragraphs":["Computational Linguistics Program Department of Philosophy Carnegie Mellon University"]},{"title":"Pittsburgh, PA 15213","paragraphs":["{yqu,cprose} (6) cs.cmu.edu,dieugeni~cmu.cdu"]},{"title":"Abstract","paragraphs":["In this paper we discuss how we all-ply discourse predictions along with non context-based predictions to the problem of parse disambiguation in Enthusiast, a Spanish-to-English translation system (Woszcyna et al., 1993; Snhm et al., 1994; Levin el; al., 1995). We discuss extensions to our plan-based discourse processor in order to make this possible. We evaluate those extensions and demonstrate the advantage of exploiting context-based predictions over a purely non context-based approach."]},{"title":"1 Introduction","paragraphs":["A system which processes spoken language must address all of the ambiguities arising when processing written language, plus other ambiguities specitie to the speech processing task. These include ambiguities derived from speech disfluencies, speech recognition errors, and the lack of clearly marked sentence boundaries. Because a large flexible grammar is necessary to handle these features of spoken language, as a side-effect the number of ambiguities increases. In this paper, we discuss how we apply discourse predictions along with non context-based predictions to the problem of parse disambiguation. This work has been carried out in the context of Enthusiast, a Spanish-to-English speech-to-speech translation system (Woszcyna et al., 1993; Suhm et al., 1994; Levin et al., 1995), which currently translates spontaneous dialogues between two people trying to schedule a meeting time.","A key feature of our approach is that it allows multiple hypotheses to be processed through the system in parallel, and uses context to disambiguate among alternatives in the linal stage of the process, where knowledge can be exploited to the fullest extent. In our system, numerical predictions based on the more local utterance level are generated by tile parser. The larger discourse context is processed and maintained by a plan-based discourse processor, which also produces context-based predictions for ambiguities. Our goal was to combine the predictions from the context-based discourse processing approach with those from the non context-based parser approach.","In developing our discourse processor for disambiguation we needed to address three major issues. First, most plan-based or finite state automaton based discourse processors (Allen and Schubert, 1991; Smith, Hipp, and Biermann, 1995; Lambert, 1993; Reithinger and Maim:, 1995), including tile one we initially developed (l~.osd et al., 1995), only take one semantic representation as input at a time: thus, we had to extend the discourse processor so thai; it can handle multiple hypotheses as input. Secondly, we needed to quantify the disambiguating predictions made by the plan-based discourse processor in order to combine these predictions with the non context-based ones. Finally, we needed a method for combining context-based and non context-based predictions in such a way as to reflect not only which factors are important, but also to what extent they are important, and under what circumstances. We assume that knowledge from different sources provides different perspectives on the disambiguation task, each specializing in different types of ambiguities.","In this paper, we concentrate on the first two issues which are imperative to integrate a traditional plan-based discourse processor into the disambiguation module of a whole system. The third issue is very important for successful confl)ination of predictions from different knowledge sources. We address this issue elsewhere in (Rosd and Qu, 1995).","The paper is organized as follows: Fh'st, we briefly introduce the Enthusiast speech translation system and discuss the ambiguity problem in Enthusiast. Then we discuss our discourse processor, focusing on those characteristics needed to generate predictions lbr disambiguation. Finally, we evaluate our performance, and demonstrate that tile use of discourse context improves performance on disambiguation tasks over a purely non context-based approach in the absence of cumulative error. 358"]},{"title":"2 System Description","paragraphs":["The main modules of our system include speech recognition, parsing, discourse processing, and generation. Processing begins with tim speech input in the source language. The top best hypothesis of the speaker's utterance is then passed to Lhe parser. The GLR* parser (Lavie, 1995) produces a set of interlingua texts, or ILFs, for a given sentence. For robustness, the. (-ILI{,* parser can skil) words in the inpu/, sentence in order to find a partial parse for a sentence which otherwise would not be parsable. An 11:I' is a frame-based language, independent meaning reprcsen ration of a sentence. The main components of an 11:1' are the sl)eech act (e.g., suggest, accept, reject), the sentence type (e.g., state, query-J.g, fragraent), and the main semantic frame (e.g., Iree, busy). An example of an IUI' is shown in Figure 1. The parser may produce many Ilfl's for a single sentencej sometimes as many as one hundred or lnore."]},{"title":"((when","paragraphs":["((fi'ame *simple-time) (day-of-week wednesday)"]},{"title":"(tin, e-or-day ,noruing)))","paragraphs":["(a-speech-act (*multiple* *suggest *accept))"]},{"title":"(who ((frame *i)))","paragraphs":["(frame *free) (sentence-type *state))) Sentence: 1 couhl do it; Wednesday niol'ning too. Figure 1: An Example ILT","'['he resulting set of llTs is then sent to the discourse processor. The discourse l)rocessor, based on I,ambert's work ([,ambert and Carberry, 1992; I,ambert, 1993), disaml)iguates tile sl)eech act of e~(;h sentence~ normalizes temporal expressions (¥oin context, and incorl)orates the seltt, enee into tile discourse context represented by a plan tree. The discourse l)roeessor also updal;es a calendar which keeps track of what the speakers h~we said M)out their schedules. We will discuss the discourse i>rocessor and how we extended it for the disambiguation task in Sectiou 4."]},{"title":"3 Ambiguity it, Enthusias(;","paragraphs":["Because t;he spontalleous sche([lding dialogues 3,re unrestricted, ambiguity is a major problenl in Enthusiast. We gange ambiguities in terms of differences between members of the set of ILTs produced by the parse, r for the sail~|e source sentence. As we mentioned e, arlier, the disaanbiguation task benelits from both non (-ontexL- and context-l)~sed methods. We observed that some classes of ambiguities can be more l)erspieuously dealt with in one way or the other. 3.1 Non Context-Based Disambiguation When the parser produces more than one IlJl' for a single sentence, it scores these ambiguities according to three diti'e.rent non context-based disaml)iguation inethods. The first method, based on (Carroll and Briscoc, 1993), assigns probal)ilities to actions in the (~I,R,* l)arser's 1)arse table. The probabilities of the parse actions induce st,a-tistical scores on alternative parse trees, which are then used for parse disambiguation. The resuiting score is called the slalislical score. The second method the parser uses to score the II/l's makes use of penalties mammlly assigned to different rules in the l)arsing grammar, rl'he resulting score from this method is called the gr'ammar pr'cfercucc score. The third score, called the parser score, is a heuristic combination of the previous two scores ldUS other information such as the number of words skil)ped. These three llOll context-based scores will be referred to later when we discuss comt)ining non eontext-I)ased l)redict, ions with context-based ones.","Error analysis of parser disambiguation output shows that the C, IA{* parser handles well ambiguities which are not strongly dependent upon the context for a reasonable interpretation, laBr ex-- ample, the Sl)anish word uua can mean either ouc or a, as an indefinite reference. The parser always chooses the indelinite reference meaning since the vast, majority of training examples use this sense of the word. Moreover, since in this case incorrect disambiguation does not adversely affect translation quality, it; ramies sense to handle this ambiguity in a purely non context-based manner. 3.2 Context-Based Disambiguation","While a broad range of ambiguities can I)e hal> died well in ~ non context-basel] manner, some ambiguities must be treated in a contexl, se, nsi tive manner in order to be translated correctly. Table 1 lists some examples of these tyt)es of atn-- biguities. Each type of ambiguity is categorized by COml)aring either difl'erent slots in alternative ll;l's or dilt'erenL values in ambiguous II2F slol.s given [;he same input utteran(;e.","For example, one. type o1\" ambiguity l)est hat> dh'd with ~ contextd)ase(I approactl is the day vs hour ~md)iguity, exenq)lified by tim phrase dos a cua&v. It can mean either Ihc second al J'o'a% lhc second lo the Jburlh or lwo go four. Out of conte.x|., it is iml)ossil)le to tell which is the I)cst intert)retation. (~ontextua.l inlk)rmation makes il; possible to choose the correct interpreLal, ion. I¢or (;xaml)le, if l,h(: sl)eakers are trying to estal)lish a dab: when they can meet,, then the sccoud to the Jourlh is t;hc most liD~ly itd;erl)retatiotJ. Itowcver, 359 Types of Ambiguity Description","day vs hour a temporal expression can be recognized as a (lay or all hour","state vs qaery-lf ambiguity between sentence type state or query-if","speaker reference ambiguity between pro-drop pronouns","tense ambiguity between past tense and present tense","how vs greet ambiguity between frame how and greet","when vs where ambiguity between when slot and where slot Exalnples dos a cuatro second at four or second to fourth or two to ,four est~ bien It's OK or [s it OK? tambidn podr[a ese d[a also i could that day or also you could that day d6nde nos encontramos where are we meeting or where were we meetinq qu~ tal How are you? or How is that? s£bado quince Saturday the fifteenth or Saturday building 15 Table 1: Examples of Context-Sensitive Ambiguities if the speakers have already chosen a date and are negotiating the exact time of the meeting, then only the meaning two to four makes sense.","Some sentence type ambiguities are also context-based. For example, l'Sstd bien can be either the statement It is good or the question Is it good?. This is an example of what we call the state vs query-i:f ambiguity: in Spanish, it is impossible to tell out of context, and without information about intonation, whether a sentence is a statement or a yes/no question. However, if the same speaker has just made a suggestion, then it is more likely that the speaker is requesting a response from the other speaker by posing a question. ht contrast, if the previous speaker has just made a suggestion, then it is more likely that the current speaker is responding with an accepting statement than posing a question.","In generM, we base our context-based predictions for disambiguation on turn-taking information, the stage of negotiation, and the speakers' cMendar information. This information is encoded in a set of context-based scores produced by the discourse processor for each ILT."]},{"title":"4 Discourse Processing and Disambiguation","paragraphs":["Context-based ranking of ambiguities is performed by the plan-based discourse processor described in (Rosd et aL., 1995) which is based on (Lambert and Carberry, 1992; Lambert, 1993). OriginMly, our discourse processor took as its input the single best parse returned by the parser. q'he main task of the discourse processor was to relate that representation to the context, i.e., to the plan tree. In generaL, plan inference starts from the surface [brms of sentences. Then speech-acts are inferred. Multiple speech-acts can be in-ferred for one ILT. A separate inference chain is created for each potential speech act performed by the associated ILT. Preferences for picking one inference chain over another were determined by the focusing heuristics, which provide ordered expectations of discourse actions given the existing plan tree. Our focusing heuristics, described in detail in (l{os6 et al., 1995), arc an extension of those described in (Lambert, 1993). In determin-ing how the inference chain attaches to the plan tree, the speech-act is recognized, since each inference chain is associated with a single speech-act.","As mentioned in the introduction, for a plan-based disconrse processor to deal with ambiguities, three issues need to be addressed:","1. The discourse processor must be able to deal with more than one semantic representation as input at a time. Note that simply extend-ing the discourse processor to accept multiple ILTs is not the whole solution to the disambiguation problem: finer distinctions must be made in terms of coherence with the context in order to produce predictions detailed enough to distinguish between alternative LLTs.","2. Before context-based predictions can be combined with quantitative non context-based predictions, they must be quantified, it was necessary to add a mechanism to produce more detailed quantifiable predictions than those produced by the original focusing heuristics described in (Ros6 et al., 1995).","3. Finally, context-based predictions must be combined successfully with non-context-based ones. The discourse processor must be able to weigh these various predictions in of der to determine which ones to believe in specific circumstances.","Thus, we extended our original discourse processor as follows. It takes multiple ambiguous lI,Ts fi'om the parser and computes three quantified discourse scores for each ambiguity. The discourse scores are derived by taking into accotmt 360"]},{"title":"attachment preferences","paragraphs":["to the discourse tree, as reflected by two kinds of focusing scores, and |,he score returned by the"]},{"title":".qradcd conslrainls, a","paragraphs":["new type of constraint we introduced. Then for each ambiguity the discourse processor combines these three kinds of context-based scores with the non context-based scores l)roduced by other modules of the system to make tire final choice, and returns the chosen IUI'. As in the first version of the discourse processor, the chosen I I,T is attached to the plan tree and a speech act is assigned to it. We discuss now how the discourse scores are derived. Note that lower wdues for all scores are preferred. 4.1 Focusing scores The focusing scores are derived from focusing heuristics based Ott (Sidner, 198l; l,ambert, 199:f; Rosd et al., 1995). The focusing heuristics identify the most coherent relationship between a new inference chain and the discourse |)Inn tree. Atl,ach meat preferences by the Focusing heuristics are translated into numerical preference scores based on attachment positions and the length of the in-- ference chains. The assignment of focusing scores reflects the assumption thai, the ntost coherent move in a diMogue is to continue the most salient focused actions, namely, the ones on the rightfl,ost frontier of the plan tree. The first feet(sing score is a boolean"]},{"title":"focusing fla(l.","paragraphs":["It returns 0 if the inference chain for the associated 11,'1' attaches t,o the rightmost fl'outier of the plan tree, 1 if it either attaches to the tree but trot to tit(.', right frontier or doesn't attach to the tree. The second focusing score, the"]},{"title":"J'ocusing score","paragraphs":["i)roper, assigns a score between 0 and t indicating [tow far up the rightmost frontier the inference chain attaches. The maximal score is assigned in the case that the inference chain does not attach. 4.2 Graded constraints Once the. discourse processor was extended to accept multiple ILTs as input, it became clear that Ibr most ambignous parses the original focusing heuristics did not provide enough information to distinguish among the alternatives. Our sohttion was to modity the discourse processor's constraint processing mechanism, making it possible to bring more domain knowledge to bear on the disambiguation task. In the original discourse processor, all of the constraints on plan operators, which we (:all"]},{"title":"elimination constraints,","paragraphs":["were used solely [or the purpose of binding w~riables and eliminat-ing certain inference possibilities. Their purpose was to eliminate provably wrong inferences, and it, this way to give the focusing heuristics a higher likelihood of selecting the torte.c( inference chain from the remaining set.","We introduced a different type of constraint,"]},{"title":"graded conslraints,","paragraphs":["inspired by the concept of graded unification discussed it, (Kim, 1994). Or,- like elimination constraints, they neither bind variables not\" eliminate any inferences. Graded constraints always return true, so they cannot eliminate inferences. However, they assign numerical penalties or preferences to inference chains based on domain specific information. This information is then used to rank the set of possible inferences Left after the elimination constraints are I)r°cessed.","For example, consider the day versus hour ambiguity we discussed earlier. In most cases inference chains for Ilfl's with this ambiguity have tit(; same focusing scores. We introduce the possible-time constrMnt to (he.ok whether the temporal constraints conflict with the dynamic calendar or the recorded dialogue (late when the inference chains are built. If the temporal information represented in an II,T is in conflict with the dialogue record date (e.g., scheduling a time before the record date) or with the temporal constraints already in the calendar (e.g., propose a time that is ah'eady rqiected), a penalty score is assigned to that inference chain; otherwise, a default value (i.e. no penalty) is returned. Several graded constraints may be fired in one inference chain. Penalties or preferences for all graded constraints in the inference chain are summed together. 'Phe result is the graded constraint score for that ambiguity.","Introducing graded constraints has two adwm-- tages over adding more elimination constraints. As far as tile systetn in ge, neral is COlmerned, graded constraints only give preferences, they do not rule out inferencing and attachment possibilities:"]},{"title":"thtls,","paragraphs":["introducing new constraints will not damage the broad coverage of the system. As far as the discourse processor is concerned, it; would be possible to achieve the same effect by adding more elimination constraints, but this wouht make it, necessary to introduce more fine-tuned plan operators geared towards specilic cases. By introducing graded constraints we avoid expanding the search space among the plan operators. 4.3 Combining Predict, ions Once the information from the graded constraints and the focusing scores is awdlable, the challeng-ing problem of combining these context-based predictions with tile non context-based ones arises. We experimented with two methods of automat-- really learning functions for combining our six scores into one composite score, namely a genetic progranmfing approach and a neural net approach. The basic assumption of our disambigua~ tion approach is that the context-based attd non context-based scores provide different perspectives on the disambiguation task. They act to-gether, each specializing in different types of cases, to constrain the final result. Thus, we want our learning approach to learn not only which factors are important, but also to what extent they are 361 important, and under what circumstances. The genetic progranlming and neural net approaches are ideal in this respect.","Genetic programming (Koza, 1992; Koza, 1994) is a method for \"evolving\" a program to accomplish a particular task, in this case a flmction for computing a composite score. This technique can learn functions which are efficient and humanly understandable and editable. Moreover, because this technique samples different parts of the search space in parallel, it avoids to some extent the problem of selecting locally optimal solutions which are not globally optimal.","Connectionist approaches have been widely used ['or spoken language processing and other areas of computational linguistics, e.g., (Wermpter, 1994; Miikkulainen, 1993) to name only a few. Connectionist approaches are able to learn the structure inherent in the input data, to make fine distinctions between input patterns in the pres-ence of noise, and to integrate difl'erent information sources.","We refer the reader to (l{osd and Qu, 1995) for fall details about the motivations underlying the choice of these two methods as well as the advantages and disadvantages of each. both kinds of testing are the same becanse cumulative error is only an issue for context-based approaches.","Our results show that the discourse processor is indeed making nsefld predictions for disambiguation: when we abstract away the problem of cumulative error, we can achieve an improvement of 13% with the genetic programming approach and of 2.5% with the neural net approach over the parser's non-context based statistical disambiguatiou technique. For example, we were able to achieve almost perfect performance on the state vs query-if ambiguity, missing only one case with the genetic programming approach; thus, for this ambiguity, we can trust the discourse processor's prediction.","However, our results also indicate that we have not solved the whole problem of combining non context- and context-based predictions for disambiguation. [n the face of cumulative error, both of the two discourse combination approaches suffer fl'om performance degradation, though to a different extent. Our current direction is to seek a solution to the cumulative error problem. Some preliminary results in this regard are discussed in (Qu et al., 1996). 5 Evaluation Both combination methods, the genetic programming approach and the neural net approach, were trained on a set of 15 Spanish scheduling dialogues. They were both tested on a set of five previously unseen dialogues. Only sentences with multiple ILTs, at least one of which was correct, were used as training and testing data. Altogether 115 sentences were used for training and 76 for testing.","We evaluated the performance of our two methods by comparing them to two non context-based ones: a baseline method of selecting a parse randomly, and a Statistical Parse Disambiguation method. The Statistical Parse Disambiguation method makes use of the three non context-based scores described in Section 3. The two context-based approaches combine the three non context-based scores as well as the three context-based scores, namely the focusing flag, the focusing score, and the graded constraint score.","Table 2 reports the percentages of ambiguous sentences correctly disambiguated by each method. We present two types of performance statistics on the testing set: without cumulative error"]},{"title":"Testing without CE","paragraphs":["and with cumulative error"]},{"title":"Testing with CE.","paragraphs":["Cumulative error builds up when an incorrect hypothesis is chosen and incorporated into the discourse context, causing future predictions based on discourse context to be in-accurate. Notice that for the two non context-based approaches, the performance figures for 6 Conclusions In this article we have discussed how we apply predictions from our plan-based discourse processor to the problem of disambiguation. Our evaluation demonstrates the advantage of incorporat-ing context-based predictions into a purely non context-based approach. While our results indicate that we have not solved the whole problem of combining non context- and context-based predictions for disambiguation, they show that the discourse processor is making usefld predictions and that we have combined this information successflllly with the non context-based predictors.","Our current efforts are aimed at solving the cumulative error problem in using discourse context. We noticed that cumulative error is especially a problem in spontaneous speech systems where unexpected inpnt, disfluencies, out-of-domain sentences and missing information cause the deterio- :ration of the quality of context. One possibility is to reassess and reestablish the context state when a conflict is detected between context and other predictions. A second proposal is to keep the n-best hypotheses and to choose one only after hav-ing processed a sequence of inputs. Preliminary experiments show that both t)roposals help reduce the adverse effect of the cumulative error problem.","Our results also suggest another possible avenue of future development. Instead of trying to learn a general function for combining various information sources, we could decide which source of information to trust in a particular case and classify 362 _ II,lindoin Statistical Parse Bislunl)iguation"]},{"title":"[","paragraphs":["D]P genelle Progrtllnlliillg DP Noltlcal N(!t 'l~ainixlg ] Testing without CE 32% I 45% 76.5~ /","76 3% 91.(;% l 89.5% s5,2% _[ 78.8% Testing with CE 45% 76.3% 60% 71 .a% 'l'~tble 2: Disanfliiguation (if All hnllliguous S(;ntencos the type of ambiguity at ti;md with the best ap-1)ro~tch for thL<s ~mil)iguity. This could be ace, omplished, for exa3nl)h; , with a decision tree le~trning ~1 )preach. Acknowledgements The authors would like to thank I,ori Levin, Alert I,~vie lind Alex Waibel for COllllllelltS Oi~l the work reported here a, tid th;mk the two ~tnoilyiiiOllS reviewers for COllillieAltS eli the, earlier version of t, he 1)~q)er. The work is supl)orLod in pa, l'l; by ~L gi':~iilt ['1'OI11 the l)epa,rtlllellt of I)e['ellse. l¢eferences Allen, J. F. and 1, K. Schubert. 1991."]},{"title":"7'he Trains Project.","paragraphs":["Ph.I). thesis, University of Rochester, ,qchool of (7oHipllter Science.","(J;~rroll, 3. ~tti([ T. Ilriscoe. 1993. (Jenera, lized probabilistic 1,1{, ptu'siiig of natured lal> {e, ua,ge ((:orpol'a 0 with unlflc~d;hm-bascd graAulIl;%l'S."]},{"title":"(,*o?ltpnlalioltal Lingnislics,","paragraphs":["19(1).","Sidner, C. [,. 1981. Focusing for [nterl)retation of ])rono/lllS. dlmcrican"]},{"title":"Journal of Computational Linguistics,","paragraphs":["7(4):217 23 I.","Kim, A. 1994. Graded unitieation: A I)'amework for interactive processing. In"]},{"title":"Proceedings of the Association for Computational Linguistics.","paragraphs":["Koza, .1. 1992."]},{"title":"Genetic l'~vgramming: On the I\"tvgramming of Computers by Means of Nalu- 'ral ,qeleclion.","paragraphs":["MI'I' Press. Koza, a. 1994."]},{"title":"Gc.netie Programming 1I.","paragraphs":["MIT Pl'eSS. l,~mibert, I,. 1993."]},{"title":"Recognizing Complex Dis-course Acts: A Tripa;<>titc Plan-Based Model of Dialogue.","paragraphs":["Ph.D. thesis, Department of Coinpurer Science, University of l)elaw~-~rc.","Lambert, L. and S. (Tarl)erry. 1992. Modeling negotiation subdialogues, hi"]},{"title":"Proceedings @ the ACL.","paragraphs":["Lavie, A. 19{)5."]},{"title":"A Grammar Based Robnsl l'arser","paragraphs":["/,br"]},{"title":"5'ponlaneous Speech.","paragraphs":["I'h.1). thesis, School of Computer Science, Carnegie Mellon University.","Levin, 1,., O. Glickman, Y. Qu, D. Gates, A. I,avie, C. P. l{osd, 17 Van Ess-Dykema, and A. Waibe]. 1995. Using context in machine translation of spoken l~mgmtge. In"]},{"title":"Theoretical and Methodological Issues in Machine Transla-tion.","paragraphs":["Miikkulainen, R,. 1993."]},{"title":",Sub.symbolic Natural Language P~vcessing: An Intcgrated Model of ,%ripls, Lexicon, and Memory.","paragraphs":["The M I'F I)ress.","Qu, Y., B. l)i F, uge, nio, A. I,avie, I,. S. l,evin, ~uid C, P. l{os6. 1996. Minimizing Cumulative Error in I)iscourse (kmtext. To appear in"]},{"title":"ICUAI Workshop Proceedings on Dialogue f'rocessing in ,S'poken Language Systems.","paragraphs":["lh;ithinger, N. ~md E. Meier. 1995. Utilizing st~- tisticM diMogue act, t)rocessing in Verbmobil. In"]},{"title":"Proceedings of the A CL.","paragraphs":["Rosd, C. P., B. l)i Igugenio, L. S. l,evin, mid (J. Vl-%ll [']ss-l)ykema. 1995. /)iseourse processing of dialogues with multiph\" threads, lit"]},{"title":"Proceedings of the ACL. l{,os6, C. P. ~md Y.","paragraphs":["Qu. 1995."]},{"title":"Automatically Learning to Use Discourse Information l'br I)isambiguation.","paragraphs":["Center for M~chine 'l'rmisl~- tion, (;arnegie Mellon University. '['echnical R,eport.",",qiiiith, R. W., l). II,. Hipp, iliid A. W. lJierin~liti. 1995. An architecture for voice dialogue systel[iS based on prolog-style theoreli+i proving."]},{"title":"()omputalional Lin(luislics,","paragraphs":["21 (3):218 320.","Suhm, B., 1,. Levin, N. Coccaro, J. CarboneU, K. Iloriguehi, R,. Isotani, A. [mvie, L. Maylield, C. |). l{os(, C. Van-Ess Dykcma, ~md A. W~dbel. 1994. Speech-hmguage integration in a multilingual speech translation systeni. In"]},{"title":"Proceedings of the AAAI Workshop on lnleg#nlion of Nalnral Language and ,qpeech Processing.","paragraphs":["Werml)ter , S. 1994. (7onnectionisl, learning of flat synt~mtic an,~lysis for speech/language systems."]},{"title":"In Proceedings of the International ConJerence on Artificial Neural Networks.","paragraphs":["Woszeyna, M., N. Coeearo, A. Eisele, A. Lavie, A. McNair, '['. Polzin, I. Regime, C. P. 1£os6, T. Slobod~, M. 'Pomita, a. Tsutsumi, N. Waibel, A. Waibel, and W. W~rd. 1993. R,ecent ~dvances in JAN US: a speech translation system."]},{"title":"In l'roceedings of the ARI'A lluman Languages 7'ethnology Workshop.","paragraphs":["363"]}]}