{"sections":[{"title":"Handling Sparse Data by Successive Abstraction Christer Samuelsson Universit£t des Saarlandes, FR 8.7, Computcrlinguistik Postfach 1150, D-66041 Saarbrfickcn, Germany Internet:","paragraphs":["christer©coli.uni-sb, de"]},{"title":"Abstract","paragraphs":["A general, practical method for handling sparse data that avoids held-out data and iterative reestimation is derived from first principles. It has been tested on a part-of-speech tagging task and outperformed (deleted) interpolation with context-independent weights, even when the latter used a globally optimal parameter setting determined a posteriori."]},{"title":"1 Introduction","paragraphs":["Sparse data is a perennial problem when applying statistical techniques to natural language processing. The fundamental problem is that there is often not enough data to estimate the required statistical parameters, i.e., the probabilities, directly from the relative frequencies. This problem is accentuated by the fact that in the search for more accurate probabilistic language models, more and more contextual information is added, resulting in more and more complex conditionings of the corresponding conditional probabilities. This in turn means that the number of observations tends to be quite small for such contexts. Over the years, a number of techniques have been proposed to handle this problem.","One of two different main ideas behind these techniques is that complex contexts can be generalized, and data from more general contexts can be used to improve the probability estimates for more specific contexts. This idea is usually referred to as back-off smoothing, see (Katz 1987). These techniques typically require that a separate portion of the training data be held out from the parameter-estimation phase and saved for determining appropriate back-off weights. Further~ more, determining the back-off weights usually requires resorting to a time-consuming iterative reestimation procedure. A typical example of such a technique is \"deleted interpolation\", which is described in' Section 5.1 below.","The other main idea is concerned with improving the estimates of low-frequency, or no-frequency, outcomes apparently without trying to generalize the conditionings. Instead, these techniques are based on considerations of how population frequencies in general tend to behave. Examples of this are expected likelihood estimation (ELE), see Section 5.2 below, and Good-Turing estimation, see (Good 1953).","We will here derive from first principles a practical method for handling sparse data that does not need separate training data for determining the back-off weights and which lends itself to direct calculation, thus avoiding time-consuming reestimation procedures."]},{"title":"2 Linear Successive Abstraction","paragraphs":["Assume that we want to estimate the conditional probability"]},{"title":"P(x I C)","paragraphs":["of tile outcome x given a context C from the number of times N~ it occurs in N = ICI trials, but that this data is sparse. Assume further that there is abundant data in a more general context"]},{"title":"C t D C","paragraphs":["that we want to use to get a better estimate of"]},{"title":"P(x I C).","paragraphs":["The idea is to let the probability estimate/5(x I C) in context C be a flmction g of the relative frequency"]},{"title":"f(x I C)","paragraphs":["of the outcome x in context C and the probability estimate P(x [C') ill context C':"]},{"title":"IV) = g(f( I IV'))","paragraphs":["Let us generalize this scenario slightly to the situation were wc have a sequence of increasingly more general contexts"]},{"title":"Cm C Urn-1 C ... C C1,","paragraphs":["i.e., where there is a linear order of the various contexts Ck. We can then build the estimate of"]},{"title":"P(x I Ck)","paragraphs":["on the relative frequency"]},{"title":"f(x I Ck)","paragraphs":["in context Ck and the previously established estimate of"]},{"title":"P(x I Ck-1).","paragraphs":["Wc call this method"]},{"title":"linear successive abstraction.","paragraphs":["A simple example is estimating the probability"]},{"title":"P(x I/n-j+l¢...,","paragraphs":["In) of word class x given"]},{"title":"l,-j+l,...,ln,","paragraphs":["tile last j letters of a word ll,...,l,. In this case, the estimate will be based on the relative frequencies"]},{"title":"f(x I l,,_~+,,..., l,,),..., f(x [ In), f(x).","paragraphs":["We will here consider the special case when the flmction g is a weighted sum of the relative frequency and the previous estimate, appropriately 895 renormalized:"]},{"title":"f(x I + 0 P(x I P(x I Ck) =","paragraphs":["1+0 We want the weight 0 to depend on the context Ck, and in particular be proportional to some measure of how spread out the relative frequencies of the various outcomes in context Ck are from the statistical mean. The variance is the quadratic moment w.r.t, the mean, and is thus such a measure. However, we want the weight to have the same dimension as the statistical mean, and the dimension of the variance is obviously the square of the dimension of the mean. The square root of the variance, which is the standard deviation, should thus be a suitable quantity. For this reason we will use the standard deviation in Ck as a weight, i.e., 0 = ~r(Ck). One could of course multiply this quantity with any reasonable real constant, but we will arbitrarily set this constant to one, i.e., use ~r(Ck) itself.","In linguistic applications, the outcomes are usually not real numbers, but pieces of linguistic structure such as words, part-of-speech tags, grammar rules, bits of semantic tissue, etc. This means that it is not quite obvious what the standard deviation, or the statistical mean for that matter, actually should be. To put it a bit more abstractly, we need to calculate the standard deviation of a non-numerical random variable. 2.1 Deriving the Standard Deviation So how do we find the standard deviation of a non-numerical random variable? One way is to construct an equivalent numerical random variable and use the standard deviation of the latter. This can be done in several different ways. The one we will use is to construct a numerical random variable with a uniform distribution that has the same entropy as the non-numerical one. Whether we use a discrete or continuous random variable is, as we shall see, of no importance.","We will first factor out the dependence on the context size. Quite in general, if ~N is the sample mean of N independent observations of any numerical random variable ( with variance a0 2, i.e.,","-} N"]},{"title":"= ~-,i=1 (i,","paragraphs":["then ~2 = Var[~N] = 1N 1 N ~","---- Var[ '~-~(i] = ~ ~Var[(i]----","i=1 i=1 In our case, the number of observations N is simply the size of the context"]},{"title":"Ck,","paragraphs":["by which we mean the number of times Ck occurred in the training data, i.e., the frequency count of"]},{"title":"Ck,","paragraphs":["which we will denote ]Ck[. Since the standard deviation is the square root of the variance, we have"]},{"title":"o-(cn","paragraphs":["="]},{"title":"Vic,,I","paragraphs":["Here ~r0 does not depend on the number of observations in'cofftext Ck, only on the underlying probability distribution conditional on context Ck.","To estimate cr0(Ck), we assume that we have either a discrete uniform distribution on {1,..., M} or a continuous uniform distribution on [0, M] that is as hard to predict as the one in Ck in the sense that the entropy is the same. The entropy H[~] of a random variable ~ is the expectation value of the logarithm of P((). In the discrete case we thus have H[(] = E[-lnP(()] :"]},{"title":"~-~-P(xi) lnP(xi)","paragraphs":["i tIere P(xi) is the probability of the random variable ( taking the value xi, which is ~ for all possible outcomes"]},{"title":"xi","paragraphs":["and zero otherwise. Thus, the entropy is In M: M"]},{"title":"E-P(xi) lnP(xi)","paragraphs":["= E-~-lnM------ = lnM i i=1 The continuous case is similar. We thus have that ln M = H[Ck] or M = e IIICk] The variance of these uniform distributions is M 2 1--T in the continuous case and ~ in the discrete case. We thus have","M 1 1 cr°(Ck) = X/~ x/r~M -1 - xff2e -H[ckl Unfortunately, the entropy It[Ck] depends on the probability distribution of context Ck and thus on"]},{"title":"Cro(Ck).","paragraphs":["Since we want to avoid trying to solve highly nonlinear equations, and since we have access to an estimate of the probability distribution of context Ck-1, we will make the following approximation: O'0(Ck-1) 1 ~(Ck) ~ It is starting to look sensible to specify ~r- 1 instead of ~, i.e., instead of ~ we will write lq-o\" ' c~-I q-1 \" 2.2 The Final Recurrence Formula We have thus established a recurrence formula for the estimate of the probability distribution in context Ck given the estimate of the probability distribution in context Ck-1 and the relative frequencies in context Ck: P(xICk) = (1)"]},{"title":",r(Ck)-ly(x C~) + p(x","paragraphs":["I C~-l) ~(Ck) -~ + 1 and"]},{"title":"(cn =","paragraphs":["We will start by estimating the probability distribution in the most general context C1, if necessary 896 directly from the relative frequencies. Since this is the most general context, this will be the context with the most training data. Thus it stands the best chances of the relative frequencies being acceptably accurate estimates. This will allow us to calculate an estimate of the probability distribution in context C2, which in turn will allow ns to calculate an estimate of the probability distribution in context Ca, etc. We can thus calculate estimates of the probability distributions in all contexts C1,..., Cm.","We will next consider some examples from part-of-speech tagging. 3 Examples from PoS Tagging Part-of-speech (PoS) tagging consists in assigning to each word of an input text a (set of) tag(s) from a finite set of possible tags, a tag palette or a tag set. The reason that this is a research issue is that a word can in general be assigned different tags depending on context. In statistical tagging, the relevant information is extracted from a training text and fitted into a statistical language model, which is then used to assign the most likely tag to each word in the input text.","The statistical language model usually consists of lexical probabilities, which determine the probability of a particular tag conditional on the particular word, and contextual probabilities, which determine the probability of a particular tag conditional on the surrounding tags. The latter conditioning is usually on the tags of the neighbouring words, and very often on the N - 1 previous tags, so-called (tag) N-gram statistics. These probabilities can bc estimated either from a pretagged training corpus or from untagged text, a lexicon and an initial bias. We will here consider the former case.","Statistical taggers usually work as follows: First, each word in the input word string 1471, • .., W, is assigned all possible tags according to the lexicon, thereby creating a lattice. A dynamic programming technique is then used to find tag the sequence 5/],..., ~, that maximizes"]},{"title":"P(T1,...,Tn I Wl, . .., Wn) =","paragraphs":["tt"]},{"title":"= IIP(Tk T1,...,Tk-1;Wl,...,Wn)","paragraphs":["k=l 1:=1 P(Tk Tk-N+I,...,Tk-1; VIZk)"]},{"title":"• 7' \".P(T~ wk) P(Tk","paragraphs":["~k-N+l,..., k-l) [ k=l"]},{"title":"fl P(Tk P(T,) Tk-N+~,..., ~-~)\" P(Wk I Tk)","paragraphs":["~:~"]},{"title":"P(Wk)","paragraphs":["Since the maximum does not depend on the factors"]},{"title":"P(Wk),","paragraphs":["these can be omitted, yielding the standard statistical PoS tagging task: max"]},{"title":"]-[ P(Tk IU~-~V+~,...,Tk-J.P(Wk JT~)","paragraphs":["TI ,...,T~, t~l=","This is well-described in for example (DeRose","1988). We thus have to estimate the two following sets","of probabilities: • Lexical probabilities:","The probability of each tag T i conditional on","the word W that is to be tagged,"]},{"title":"p(r' I I wr! i","paragraphs":["Often the converse probabilities"]},{"title":"P(W","paragraphs":["are given instead, but we will for reasons soou to become apparent use the former formula-tion.","® Tag N-grams: The probability of tag T i at position k in the input string, denoted"]},{"title":"T~,","paragraphs":["given that tags 7~.-N+1 T ,..., k-1 have been assigned to the previous N- 1 words• Often N is set to two or three, and thus bigralns or trigrams are employed. When using trigram statistics, this quantity is"]},{"title":"P(T~ ]7' k-~,Tk-1).","paragraphs":["3.1 N-gram Back-off Smoothing We will first consider estimating the N-gram probabilities"]},{"title":"P(T~ ]Tk-N+I,...,Tk-1).","paragraphs":["IIere, there is an obvious sequence of generalizations of the context 5/~-N+1,..., 7~-1 with a linear order, namely ~/~--N+I ~ C Tk-N+2, ,Tk-1 C ,..., k-1 ... •"]},{"title":"cT","paragraphs":["• . k-1 C fl, where f~ means \"no information\", corresponding to the nnigram probabilities. Tiros we will repeatedly strip off the tag furthest from the current word and use the estimate of the probability distribution in this generalized context to improve the estimate in the current context. This means that when estimating the (j + 1)-gram probabilities, we back off to the estimate of the j-gram probabilities.","7' So when estimating"]},{"title":"P(T[ I Tk-j,..., ~-~),","paragraphs":["we simply strip off the tag 5~_j and apply Eq. (1):","~-~(~'~ [ Tk-j,...,Tk-i) = --1 ,i r,"]},{"title":"f(% I :tk-j,..., rk-~) = + + P(T","paragraphs":["and ~-1+1"]},{"title":"~-~+1,..., Tk-O ~-1 + 1","paragraphs":["• ., 7}~_ 1[ e-ll[Tk_j+l ..... T~._ 11 3.2 Handling Unknown Words We will next consider improving the probability estimates for unknown words, i.e., words that do not occur in tile training corpus, and for which we therefore have no lexical probabilities, The same technique could actually be used for improving the estimates of the lexical probabilities of words that 897 do occur in the training corpus. The basic idea is thaF there is a substantial amount of information in the word suffixes, especially for languages with a richer morphological structure than English. For this reason, we will estimate the probability distribution conditional on an unknown word from the statistical data available for words that end with the same sequence of letters. Assume that the word consists of the letters I1,. • •, I,~. We want to know the probabilities"]},{"title":"P(T i I ll,...,ln)","paragraphs":["for the various tags"]},{"title":"Ti. 1","paragraphs":["Since the word is unknown, this data is not available. However, if we look at the sequence of generalizations of \"ending with same last j letters\", here denoted ln-j+l, • •.,"]},{"title":"In,","paragraphs":["we realize that sooner or later, there will be observations available, in the worst case looking at the last zero letters, i.e., at the unigram probabilities.","So when estimating"]},{"title":"P(T i I In-j+l,...,ln),","paragraphs":["we simply omit the jth last letter"]},{"title":"In-j+l","paragraphs":["and apply Eq. (1):"]},{"title":"P(T I = = er-lf( Ti I l,-j+l,...,ln)","paragraphs":["+ ~-1+1"]},{"title":"P( Ti I In-i+2,..., 1,,) +","paragraphs":["a-l+l and e r-1 ="]},{"title":"ln-j+l,...,l, l e","paragraphs":["-tl[1\"-j+= ..... M This data can be collected from the words in the training corpus with frequencies below some threshold, e.g., words that occur less than say ten times, and can be indexed in a tree on reversed suffixes for quick access. 4 Partial Successive Abstraction If there is only a partial order of the various generalizations, the scheme is still viable. For example, consider generalizing symmetric trigram statistics, i.e., statistics of the form"]},{"title":"P(T I Tz, Tr).","paragraphs":["Here, both"]},{"title":"Tt,","paragraphs":["the tag of the word to the left, and Tr, the tag of the word to the right, are one-step generalizations of the context 7}, Tr, and both have in turn the common generalization ~ (\"no information\"). We modify Eq. (1) accordingly:"]},{"title":"D(T I Tt,T~ ) = cr(Tt,T~) -~ f(T I Tz,T~ )","paragraphs":["o'(r/,Tr) -1 q- 1 -~ I"]},{"title":"P(TIT0 + P(TIT~ ) +-","paragraphs":["2 ~(T,,T~)-~ + i and"]},{"title":"P(T I T~) P(TIT,) a(Ti) -1 f(T I T~) + P(T) a(Tz) -1 + 1","paragraphs":["~r(Tr) -1"]},{"title":"f(T ITs) + P(T) ~(T~) -I","paragraphs":["+ 1 1Or really,"]},{"title":"P(T i I 1o, 11,..., ln)","paragraphs":["where lo is a special symbol indicating the beginning o1 the word. We call this"]},{"title":"partial successive abstraction.","paragraphs":["Since we really want to estimate cr in the more specific context, and since the standard deviation (with the dependence on context size factored out) will most likely not increase when we specialize the context, we will use:","1 ~r(Tt, Tr)"]},{"title":"= ~","paragraphs":["min(a0(T~), c~0(Tr))","In the general case, where we have M one-step generalizations C~ of C, we arrive at the equation"]},{"title":"P(x I c) = -1/(x I c) + EY=I I el)","paragraphs":["and"]},{"title":"+ 1 1","paragraphs":["min a0(C~) o'(C) -- [X//~ 16{i ..... M}"]},{"title":"-1 = v, iT -ntca","paragraphs":["By calculating the estimates of the probability distributions in such an order that whenever estimating the probability distribution in some particular context, the probability distributions in all more general contexts have already been estimated, we can guarantee that all quantities necessary for the calculations are available. 5 Relationship to Other Methods We will next compare the proposed method to, in turn, deleted interpolation, expected likelihood estimation and Katz's back-off scheme. 5.1 Deleted Interpolation Interpolation requires that the training corpus is divided into one part used to estimate the relative frequencies, and a separate held-back part used to cope with sparse data through back-off smoothing. For example, tag trigram probabilities carl be estimated as follows:"]},{"title":"P(Tj","paragraphs":["[Tk-2, Tk-1) ~ Alf(7~) +"]},{"title":"+ Auf(Tik ITk_1) + Aaf(Tik I Tk-2,T~_I)","paragraphs":["Since the probability estimate is a linear combination of the various observed relative frequencies, this is called linear interpolation. The weights 13. may depend on the conditionings, but are required to be nonnegative and to sum to one over j. An enhancement is to partition the training set into n parts and in turn perform linear interpolation with each of the n parts held out to determine the back-off weights and use the remaining n - 1 parts for parameter estimation. The various back-off weights are combined in the process. This is usually referred to as"]},{"title":"deleted interpolation.","paragraphs":["The weights Aj are determined by maximizing the probability of the held-out part of the training data, see (Jelinek & Mercer 1980). A locally"]},{"title":"898","paragraphs":["optimal weight setting can be found using Baum-Welch reestimation, see (Baum 1972). Baum-Welch reestimation is however prohibitively time-consuming for complex contexts if the weights are allowed to depend on the contexts, while successive abstraction is clearly tractable; the latter effectively determines these weights directly from the same data as the relative frequcncies. 5.2 Expected Likelihood Estimation Expected likelihood estimation (ELE) consists in assigning an extra half a count to all outcomes. Thus, an outcome that didn't occur in the training data receives half a count, an outcome that occurred once receives three half counts. This is equivalent to assigning a count of one to the occurring, and one third to the non-occurring outcomes. To give an indication of how successive abstraction is related to ELE, consider the following special case: If we indeed have a uniform distribution with M outcomes of probability M ! in context Ck-1 and there is but one observation of one single outcome in context Ck, then Eq. (1) will assign to this outcome the probability vqh+l and","vqh+M to the other, non-occurring, outcomes 1 So","v~+m' if we had used 2 instead of vq2 in Eq. (1), this would have been equivalent to assigning a count of one to the outcome that occurred, anti a count of one third to the ones that didn't. As it is, the latter outcomes are assigned a count of 1 4i5+~\" 5.3 Katz's Back-Off Scheme The proposed method is identical to Katz's back-off method (Katz 1987) up to the point of suggesting a, in the general case non-linear, retreat to more general contexts:"]},{"title":"P(= I c) = g(f(= I It'))","paragraphs":["Blending the involved distributions"]},{"title":"f(x ] C)","paragraphs":["and /5( x I C'), rather than only backing oft\" to C' if"]},{"title":"f(x ] C)","paragraphs":["is zero, and in particular, instantiating the flmction"]},{"title":"g(f, P)","paragraphs":["to a weighted sum, distinguishes the two approaches. 6 Experiments A standard statistical trigram tagger has been implemented that uses linear successive abstraction for smoothing the trigram and bigram probabilities, as described in Section 3.1, and that handles unknown words using a reversed suffix tree, as described in Section 3.2, again using linear successive abstraction to improve the probability estimates. This tagger was tested on the Susanne Corpus, (Sampson 1995), using a reduced tag set of 62 tags. The size of the training corpus A was almost 130,000 words. There were three separate test corpora B, C and D consisting of approximately 10,000 words each. Test corpus Tagger Error rate (%) - tag omissions -unknown words Unknown words Error rate (%)","B bigram trigram HMM 4.41 4,36 4.49","0.67 1.36 1.20 1.52","6.18 22.1 19.4 24,5","Test corpus C bigram HMM trigram Tagger","Error rate (%)","- tag omissions","- unknown words","Unknown words","Error rate (%)","4.26 3.93 4.03","0.68","1.43 1.30 1.34","7.78","18.3 16.8 17.3","Test corpus D","bigram HMM Tagger","Error rate (%)","- tag omissions","- unknown words","Unknown words","Error rate (%)","trigram","5.14 4.81 5.13","0.94","1.80 1.63 2.02","8.06","22.3 20.2 25.0 Figure 1: Results on the Susanne Corpus","Tile performance of the tagger was compared with that of an tlMM-based trigram tagger that uses linear interpolation for N-gram smoothing, but where the back-off weights do not depend on the eonditionings. An optimal weight, setting was determined for each test corpus individually, and used in the experiments. Incidentally, this setting varied considerably from corpus to corpus. Thus, this represented the best possible setting of back-off weights obtainable by linear interpolation, and in particular by linear deleted interpolation, when these are not allowed to depend on the context.","In contrast, the successive abstraction scheme determined the back-off weights automatically from the training corpus alone, and the same weight setting was nsed for all test corpora, yielding results that were at least on par with those obtained using linear interpolation with a globally optimal setting of contcxt-independent back-off weights determined a posteriori. Both taggers handled unknown words by inspecting the suffixes, but the HMM-based tagger did not smooth the probability distributions.","The experimental results are shown in Figure 1. Note that the absolute performance of the trigram tagger is around 96 % accuracy in two cases and distinctly above 95 % accuracy in all cases, which is clearly state-of-the-art results. Since each test corpus consisted of about 10,000 words, and the error rates are between 4 and 5 %, the 5 percent significance level for differences in error rate is between 0.39 and 0.43 % depending on the error rate, and the 10 percent significance level is between 0.32 and 0,36 %. 899","We see that the trigram tagger is better than the bigram tagger in all three cases and significantly better at significance level 10 percent, but not at 5 percent, in case C. So at this significance level, we can conclude that smoothed trigram statistics improve on bigram statistics alone. The trigram tagger performed better than the HMM-based one in all three cases, but not significantly better at any significance level below 10 percent. This indicates that the successive abstraction scheme yields back-off weights that are at least as good as the best ones obtainable through linear deleted interpolation with context-independent back-off weights. 7 Summary and Further Directions In this paper, we derived a general, practical method for handling sparse data from first principles that avoids held-out data and iterative reestimation. It was tested on a part-of-speech tagging task and outperformed linear interpolation with context-independent weights, even when the latter used a globally optimal parameter setting determined a posteriori.","Informal experiments indicate that it is possible to achieve slightly better performance by replacing the expression for ~ro~(Ck) with a fixed global con-","1 stant (while retaining the factor I~kl' which is most likely a quite accurate model of the dependence on context size). However, the optimal value for this parameter varied more than an order of magnitude, and the improvements in performance were not very large. Furthermore, suboptimal choices of this parameter tended to degrade performance, rather than improve it. This indicates that the proposed formula is doing a pretty good job of approximating an optimal parameter choice. It would nonetheless be interesting to see if the formula could be improved on, especially seeing that it was theoretically derived, and then directly applied to the tagging task, immediately yielding the quoted results. Acknowledgements The work presented in this article was funded by the N3 \"Bidirektionale Linguistische Deduktion (BiLD)\" project in the Sonderforschungsbereich 314 Kiinstliche Intelligeuz -- Wissensbasierte Systeme.","I wish to thank greatly Thorsten Brants, Slava Katz, Khalil Sima'an, the audiences of seminars at the University of Pennsylvania and the University of Sussex, in particular Mark Liberman, and the anonymous reviewers of Coling and ACL for pointing out inaccuracies and supplying useful comments and suggestions to improvements."]},{"title":"References","paragraphs":["L. E. Baum. 1972. \"An inequality and as-sociated maximization technique in statistical estimation for probabilistic functions of Markov processes\". Inequalities IIh 1-8. Steven J. DeRose. 1988. \"Grammatical Category Disambiguation by Statistical Optimization\". Computational Linguistics, 14(1): 31-39.","I. J. Good. 1953. \"The population frequencies of species and the estimation of population parameters\". Biometrika, 40: 237-264. Frederick Jelinek and Robert L. Mercer. 1980. \"Interpolated Estimation of Markov Source Paramenters from Sparse Data\". Pattern Recognition in Practice: 381-397. North Holland. Slava M. Katz. 1987. \"Estimation of Probabilities from Sparse Data for the Language Model Component of a Speech Recognizer\". IEEE Transactions on Acoustics, Speech, and Signal Processing, 35(3): 400-401.","Geoffrey Sampson. 1995. English for the Computer. Oxford University Press. 900"]}]}