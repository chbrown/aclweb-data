{"sections":[{"title":"Modelling Variations in Goal-Directed Dialogue","paragraphs":["Jean Carletta* University of Edinburgh","Depa.rtment of Artificial Intelligence jcc~aipna.ed.ac.uk"]},{"title":"1 Introduction","paragraphs":["This research investigates human dialogue variations by having simulated agents converse about a simple map navigation task using a computational theory of human dialogue strategies. The theory to be tested is that agents have a variety of strategies which they use in goM-directed dialogue for deciding what information to include, how to present it, how to construct references, and how to handle interactions. Agents choose strategies for each of these a.speets of the dialogue depending on the complexity of the current task, responses from the partner, and the task's importance; in general, agents try to minimize the collaborative effort spent on the task but still complete it satisfactorily. The software allows simulated agents to be constructed using any combination of strategies, showing how the strategies interact and allowing the decisions made in human dialogues about the same task to be modelled. Currently, the system works on a subset of the strategies found in Shadbolt [6], but a corpus of human dialogues is being studied to construct, an improved theory of dialogue strategies, and these strategies will be incorporated into later versions of the system."]},{"title":"2 The Dialogue Domain","paragraphs":["The task domain ['or the dialogues involves navigating around a simple map containing approximately fifteen landmarks. Two participants are given maps which differ slightly; each map may contain some features omitted on the other, and features in the same location on the two maps may have different labels. The first participant also has a route from the labelled start point to one map feature. The second participant must draw the","* Supported by a scholarship from the Marshall Aid Conunemoration Commission. Thanks to Chris Mellish for supervising me and Alison Cawsey for helpful comments, route on his or her map. In this task, the participants must cooperate because neither of them knows enough about the other's map to be able to construct accurate descriptions. At the same time, small changes in the map test how participants hrmdle referential ambiguities, how information is carried from one part of the dialogue to the next, and when agents decide to replan rather than repair an existing plan. Despite the possibilities for referential difficulties, this task minimizes the use of real world knowledge as long as all participants understand how to navigate. The task is simple enough to be completed in computer-simulated dialogue, but admits the dialogue variations to be tested in the research."]},{"title":"3 The Central Idea","paragraphs":["The central idea behind the research is that agents need multiple strategies for engaging in goM-directed dialogue because they do not necessarily know the best way to communicate with a given partner. Self [5] shows that dialogue is crucial where neither agent has all of the relevant domain knowledge. Dialogue is also necessary for any explanations where agents don't have accurate models of their partners [3]. Even if agents have all of the relevant domain knowledge, they may not know how best to present that knowledge, especially since explanations are about exactly that part of the task which is not mutually known to the dialogue partners [1]. Shadbolt [6] presents evidence that humans handle uncertainties about what information to give and how to present it by having a set of strategies for each aspect of the dialogue. Then the agent can tailor explanations to a particular partner by using the strategy that best fits the situation. For instance, human agents who believe that much domain information will have to be communicated structure their presentation carefully and often elicit feedback from the partner, like participant A of Sha.dbolt's [6] example 6.16: A: have you got wee palm trees aye? B: uhu A: right go just + a wee bit along to them have","you got a swamp? n~ er A: right well just go + have you got a watt>","fall7 Al,~ents who believe that most domain in[brmation is ki~own to their partner are more likely to rely on interru ptions fl'om the partner and replanning, a.s in example 6.:i 1: and \"around\". The agents converse in an artificial language resembling their shared planning language, but substituting referring expressions for internal feature identifiers. Under these constraints, the agents use dialogue strategies to decide on the content and form of the dialogue. The existing system is a prototype de-. signed to show that incorporating such strategies can explain some variations in human dialogue and make agents more flexible. An improved set of strategies is being extracted from the corpus of human dialogues. The end result of the project will be a theory of how communicative strategies control variations in dialogue, and software in which computer-simulated agents use these strategies to complete the navigation task. A: and then q- go up about and over the bridge B: I've not got a bridge I've got a lion's den","and a wood A: have you got a river'? Ol~e way to make computer generated explanations look m(,re natural is to plan them using strategies modelled on ~.he human ones. Although strategies like these could be built into the way a system plans an explanation, making strategy choices explicit allows the strategies themselves to be investigated, providiug a way to test oul. how variatio,~s affect the ensuing dialogue. The go~d of the present research is both to show how using dialogue strategies can improve tile \"naturalness\" of computer-generated task explanations and to provide insight into the dialogue strategies which humans use and how they interact."]},{"title":"4 The Project","paragraphs":["The project involves creating a theory of human dialogist strategies a.m:l modelling it. usiug two cotnputer processes that converse. Communication for the comi)llter agents, bg~sed or~ the model in Power [4] and I[oughtou [2], is simplified in a number of ways. A convener wakes the agents in turn and interactions are made by placing messages in mailboxes, leaving out the complications of turn-taking and interruptiom Rather than reason from \"visual\" images of the maps, agents begin with sets of beliefs about the positional relationship.~ among objects and share knowledge about both dialogue conventions, expressed a.s"]},{"title":"interactio'n flames","paragraphs":["[2], ~md navigational concel)tS like \"toward\", \"between\","]},{"title":"5 The Program","paragraphs":["The current version of the software uses dialogue strategies adapted from Shadbolt [6]. tte lists seven dif~ fercnt aspects of dialogue along which strategies may be developed. Agents may vary strategies tbr"]},{"title":"feedback","paragraphs":["(how they handle the partner's utterances),"]},{"title":"speciJicao lion","paragraphs":["(how they construct and resolve referring expressions),"]},{"title":"o~lology","paragraphs":["(how they decide from what features are available how to construct route descriptions),"]},{"title":"foe~zs","paragraphs":["(the amount of explMt focus intbrmation given),"]},{"title":"differonce","paragraphs":["(the effort spent determining what the partner's utterances mean),"]},{"title":"decenlering","paragraphs":["(whether intbrmation is presented using the agent's or the partner's names tbr fe.atures), and"]},{"title":"hypolhesi~ formalion","paragraphs":["(the effort spent making hypotheses about the partner's knowledge). Agents choose strategies tbr each of these aspects depending on how explicit they want to be, which in turn depends on how likely the partner is to misunderstand each aspect of the dialogue. Some of Shadbolt's aspects are interrelated; for instance, agents that provide explicit information about the current focus do not need to construct referring expressions as carefully as agents who provide no focusing information at all. Our own work divides the strategies slightly differently so that they ea.n be divided into sets depending on whether they atfect planning the dialogue interaction, planning the content, planning the presentation, or realizing references; the goal is to make the strategies ms modular as possible so that they can be modelled simply. Each simulated agent takes on a set of strategies for tile duration of' a dialogue. Currently, the prototype varies how much intbrmation about tile structttre of the dialogue is explicitly given, which features are included in a route description depending on a model of the partner's be-325 liefs, how often an agent allows interactions from the partner, and how much repair an agent is willing to do rather than replan a description. The agents also use heuristics to prefer plans where the partner already understands the plan's prerequisites. The output of the program is a simulated dialogue where each agent keeps the same strategies for the course of the dialogue; an obvious future step is to allow agents to adapt to a particular partner or part of the task by varying their strategies within a dialogue."]},{"title":"6 Examples","paragraphs":["The system currently has several strategies which affect how much structuring information is given in a dialogue and how often feedback is elicited from the partner. The following dialogue, an English gloss of two simulated agents conversing, shows how agent A might act if it believed that the maps had many differences: A: I'nl going to tell you how to get to the","buried treasure. I'm going to tell you how","to navigate the first part of the route. Do","you have a pahn beach? B: Yes. A: Do you have a swamp? B: No. A: Do you have a waterfall? B: Yes. A: The swamp is between the pMm beach and","the waterfall. OK? B: Yes. A: The route goes to the left of the pMm beach","and around the swamp. OK? B: Yes. If agent A believes that there will be few misunderstandings, or that B will understand enough to say what it misunderstood, it might choose to give information first and repair afterwards:"]},{"title":"7 Conclusion","paragraphs":["The theory and program are designed to show how varb ations that occur in human dialogue can be ex.plained in terms of deciding among communicative strategies governing the form of interaction, the content and presentation of information, and the construction of referring expressions. The strategies found by examining a human corpus of goM-directed dialogues are implemented in a dialogue system where two computer processes using an artificial language and simplified turn-taking complete the task. This approach is useful in itself for determining what makes human dialogues seem natural} but it also has implications for human-computer interaction, since it is one step towards making computer dialogues with humans operate flexibly to fit in with human expectations."]},{"title":"References [1] [3] [4] [5] [6]","paragraphs":["S. Garrod. Explanations in dialogues as attempts to overcome problems in coordination. In"]},{"title":"Proceedings of the Third Alvey Explanation Workshop,","paragraphs":["September 1987. G. IIoughton."]},{"title":"The Production of Language in Dialogue: A Computational Model.","paragraphs":["PM) thesis, University of Sussex, April 1986. J. Moore and W. Swartout. A reactive approach to explanation. In"]},{"title":"Proceedings of the I~'ouvth International Workshop on Nat~tral Language Generation,","paragraphs":["1988. 1L J. D. Power."]},{"title":"A Computer Model of Conversation.","paragraphs":["PhD thesis, University of Edinburgh, 1974. J. Self. Bypassing the intractable problem of student modelling. In"]},{"title":"Intelligent Tutoring Systems,","paragraphs":["1988. N. 1{. Shadbolt."]},{"title":"Constit~tting reference in naluraI language: the problem of referential opacity.","paragraphs":["PhD thesis, Edinburgh University, 1.984. A: The route goes to the left of the pahn beach","and around the swamp. OK? B: Where's the swamp? A: The swamp is between the pMm beach and","the waterfall. OK? B: Yes. 326"]}]}