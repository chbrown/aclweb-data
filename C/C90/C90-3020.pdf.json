{"sections":[{"title":"Linear Encodings of Linguistic Analyses","paragraphs":["Samuel S. Epstein","Bell Communications Research 445 South Street, 2Q-350 Morristown, NJ 07960-1910","USA epstein@ flash.bellcore.com"]},{"title":"1. Introduction","paragraphs":["Natural languages contain families of expressions such that the number of readings of an expression is an exponential function of the length of the expression. Two well-known cases involve prepositional phrase attachment and coordination. Other cases discussed below involve anaphora and relative operator scope. For example, in (l) John said that Bill said that ... that Harry said that he thinks that he thinks that ... that he thinks that it is raining. each he can refer to any of John, Bill ..... Harly? Thus if (1) contains n names and m occurrences of he, this sentence has n m readings (assuming that all anaphoric relationships are intrasentential). We discuss below families of expressions whose ambiguities grow as various exponential functions (factorial, Fibonacci, Catalan) of expression length.","It appears, then, that exhaustive linear-time processing of natural language is impossible. An exponentially long answer cannot be produced in linear time. 2 On the other hand, human processing of natural language seems to be at least linearly fast. The ready explanation for this is that people do not recover all readings of ambiguous expressions. This is clearly correct, as far as it goes.","This paper shows how to encode in linear space the exponentially large numbers of readings associated with various families of expressions. The availability of these encodings removes an apparent obstacle to exhaustive analyses of these expressions in linear time. The encodings may thus be useful for practical computational purposes. They may also provide a better","1. (1) is of course highly unnatural in a sense. However, it effectively isolates for study a phenomenon that is intrinsic to natural language. Similar observations apply to the examples below.","2. It is of course also the case that an exponentially long answer caunot be produced in polynomial time. If the problem cannot be reformulated so that answers are not exponentially long, the question of tractability does not arise. See [Garey and Johnson 79] and [Barton, Berwick, and Ristad 87] for related discussions. basis than exponential-space encodings for explanations of how humans process language.","For each of the linguistic constructions discussed in this paper, there is a simple program that generates analyses of the construction. If there are no constraints on what counts as a linguistic analysis, then a specification of a program, which requires constant space, together with a specification of an input expression, which requires linear space, could count as a linear encoding of an analysis of the input. Intuitively, there is a vast qualitative divide between a (program,input) pair on one hand, and, for instance, a forest of constituent structure trees on the other hand. More generally, a question arises of how to distinguish analyses from procedures that yield analyses. This paper will not attempt to answer this question definitively. The analyses presented in Sections 2 - 4 all satisfy a notion of \"legal\" analysis that excludes (program,input) pairs. Sections 2 and 3 discuss polynomial space analyses. Section 4 adds a representational device to the repertory of Sections 2 and 3, so that linear space analyses are possible. Section 5 infol~mally discusses a variety of issues, including the distinction between analysis and procedure."]},{"title":"2. Analyses in Conjunctive Normal Form","paragraphs":["Assume that example (1) involves no ambiguities except for antecedents of pronouns. Assume further that the length of the analysis of (1), aside from the specification of antecedents of pronouns, grows linearly) Let the proposition q comprise all aspects of the analysis of (1), aside from specifications of antecedents of pronouns. Let the proposition p.. comprise the specification that the j-th name in (1I 'J is the antecedent of the i-th occurrence of he. (For example, Pl,2 comprises the","3. These assumptions, and similar assumptions for other examples below, permit a briefer discussion than would otherwise be possible. Reservations about these assumptions do not affect the substance of the discussion. Our concern with (1) focuses on exponentially growing possibilities for assigning antecedents to pronouns. 108 1 specification that Bill is the antecedent of the most shallowly embedded he.) Let n be the number of names in (1) and let m be the number of occun'ences of he. Then an exhaustive analysis of (1) can take the following form:","(l-a) (q & Pl,~ & P2,1 & \"'\" & Pro,1 ) v (q & Pl,1 & P2,I & \"'\" & Pro-l,1 & Pra,2 ) v","(q & Pl,n & P2,n & \"'\" & Pm,n ) (l-a), which contains n m disjuncts, is in Disjunctive Normal Form (DNF). Each disjunct fully specifies a possible interpretation of (1). It is an implicit assumption in much of the literature that the proper form for linguistic analyses is DNF. An analysis in DNF amounts to a listing of possible global interpretations.","(l-a) is logically equivalent to the following :aatement in Conjunctive Normal Form (CNF):","(l-b) q & (Pl,1 v Pl,2 v ... v Pl.n ) & (P2,1 v P2,2 v ... v P2,n ) (3) the block in the box on the table ... in the kitchen As [Church and Patil 82] discuss, examples like (3) are similar to other structures with systematic attachment ambiguities, such as coordination structures. While the number of readings of (3)4is thus exponential in the length of (3), (3) has an O(n ) length analysis in CNF as follows: (3-a) q & (Pl,0) & (3~a-2) (P2,0 v P2,1 ) & (3-a-3) (P3,0 v P3,1 v P3,2 ) & (P3,1 zo P2,1 ) &","(3-a-4) (P4,0 v P4,1 v P4,2 v P4,3 ) & (P4,1 D P2,l ) & (P4,1 D (P3,I v P3,2)) & (P4,2 D P3,2 ) &","(Pro,1 v Pro.2 v ... v pm,n ) (l-b) contains m+l conjuncts. The length of an exhaustive analysis of (1) is exponential in the number of pronouns in (1) when the analysis is given in DNF, but linear in the number of pronouns when the analysis is given in CNF. However, (l--b) is not linear in the length of (1), because each of m conjuncts contains n clisjuncts, so that a total of mxn literals is required to specify anaphoric possibilities.","The following example has an analysis in DNF that grows as the factorial of the length of the input: (2) John told Bill that Tom told him that Fred told him tha! ... that Jim told him that Harry told him that it is"]},{"title":"raining.","paragraphs":["The first occurrence of him can have John or Bill as antecedent. The second occurrence of him can have John or Bill or Tom as antecedent, and so on. (2) has an obvious analysis in CNF whose length is a quadratic function of the length of the input, namely","(2:a) q & (PL1 v Pl 2 ) &; (P2,1 vP2,2v, -~2,3 )&","(Pro,1 v Pro,2 v ... v Pm,m+l ) where the notation follows the same conventions as in (l-a,b).","The number of readings for the following noun phrase grows as the Catalan of the number of prepositional phrases: (3-a-k) (Pk,0 v Pk,1 V ... V Pk,k_l ) &; (3-a-k,1) (Pk,l D P2,1 ) & (Pk,l D (P3,1 v P3,2)) & (Pk,1 D (Pk-l,l v Pk-l,2 v ... v Pk_l,k.2)) & (3-a-k,m) (Pk,m ~ Pm+l,m ) & (Pk,,n D (Pm+2,m v Pm+2,m+l)) & (Pk,m D (Pk-l,m V Pk-l,m+l V ... V Pk_l,k_2)) & In (3-a), Pi' comprises the specification that constituent i",",J .... attaches to constmmnt j, where the block ~s constituent 0, in the box is constituent 1, on the table is constituent 2, and so on. Constituent k must attach to some constituent that lies to its left. If constituent k attaches to 2 109 constituent m, then the constituents between constituent m and constituent k cannot attach to constituents to the left of constituent m. 4 For each pair (k,m), the number of atoms in (3-a-k,m) is fl(k,m) = ,~'i. fl(k,m) is quadratic in k. For each k, then, the number of atoms in (3-a-k) is f2(k) = k+ l(k,i), a cubic function in k. The number of atoms in (3-a) (excluding atoms hidden in q) is thus =f2(i), a quartic function in n. (3-a) is certainly not the most compressed CNF analysis of (3). It is, however, easy to describe.","Given an exhaustive analysis in DNF, choosing a global interpretation requires exactly one operation of selecting a disjunct. Foi\" (l-b) and (2-a), choosing a global interpretation requires a number of selections that is linear in the length of the input. I am aware of no other reason for preferring DNF to CNF for analyses of examples like (1) and (2). In favor of preferring CNF there is the practical advantage of polynomial-space output, with its implications for speed of processing. There is also the possibility of more accurate psycholinguistic modeling. It seems likely that people make decisions on antecedents for pronouns in examples like (1) and (2) locally, on a pronoun-by-pronoun basis, and that they do not choose among global analyses. 5 In contrast, the conjuncts of (3-a) clearly do not correspond one-to-one with processing decisions. Section 4 discusses an analysis of (3) whose components may correspond to local decisions on attachment sites."]},{"title":"3. Encodings with non-atomic propositional constants","paragraphs":["It is possible to get a cubic length analysis of (3) by introducing constants tbr non-atomic propositions. For m<k, let r. be v Pk_l.k_2 ). K,m (Pk-I m+l V Pk-I m+2 v ... Then (3-a-k,m) is equ3)alent to: '","(3-b-k,m) (Pk m D Pm+l m ) & (Pk,~n D (Pm+2,m v Pm+2,m+t )) & (Pkm D (Pk 2 m V rk. 1 m) ) (Pklm D (Pk~l',m v ,'k,n~)) Of course, the space required to define the r km must figure in the space required to encode an analys[s of (3) along the lines of (3-b-k,m). rk,m_ l -= (Pk-l,m v rk,m) , so","4. (Pl ~ (P2 v ... v pj)) is equivalent to (-'Pl v P2 v ... v pj), SO that (3-a) is in CNF.","5. This is not to suggest that people produce an exhaustive analysis in CNF prior to choosing a reading. The hypothesis is rather that fragments of a CNF representation are produced (in some sense) during processing. it requires quadratic space to define all the rk. m. A revised version of (3) with (3-b-k,m) in place of (3-a-k,m) throughout requires cubic space. 6","Tree representations of single readings for examples like (3) may be viewed as follows: edges correspond to atomic propositions that comprise specifications like \"constituent i attaches to constituent j\" or \"constituent i projects to constituent j.,,7 A non-terminal node A corresponds to a constituent, but also corresponds to the conjunction of the atomic propositions that correspond to edges that A dominates. Thus the root node of the tree corresponds to a proposition that comprises a full specification of constituent structure.","The situation is essentially the same ['or shared forests. ([Tomita 87] discusses shared forests and packed shared forests.) Edges ill shared forests correspond to atomic propositions, and non-terminal nodes correspond to non-atomic propositions. To extend this perspective, shared forests compress the information in non-shared forests by exploiting the introduction of constants for non-atomic propositions. In a shared forest, the subtree that a node dominates is written only once. In effect, then, a constant is introduced that represents the conjunction that corresponds to the node. This constant is a constituent of the fornmlas that correspond to superior nodes. While shared forests are more compressed than unshared forests, the number of nodes in the shared forest representation of (3) is still exponential in the length of (3).","In a packed shared forest, a packed node that does not dominate any packed nodes corresponds to a disjunction of conjunctions of atomic propositions. Packed nodes that dominate other packed nodes correspond to disjunctions of conjunctions of atomic and non-atomic propositions. In effect, for each node (packed or non-packed), a constant is introduced that abbreviates the formula that corresponds to the node. Exploitation of constants for non-atomic propositions pemfits more significant compression for packed shared forests than for shared forests. The packed root node of a packed shared forest for (3) cotxesponds to a disjunction of conjunctions whose size in atoms is exponential in the length of (3). However, the number of nodes of a packed shared forest for (3) goes up as the square of the length of (3). The number of edges of the packed shared forest (a more authentic measure of the size of the forest) goes up as the cube of the length.","6. Further compression is possible if we allow quantification over subscript indices. However, quantification over artifacts of representation may uncontroversially involve crossing the divide between analysis and procedure.","7. Details of constituent structure are not relevant to the discussion here. For example, we will not distinguish \"X attaches to V\" from \"X attaches to VP.\" 2 110 3"]},{"title":"4. Encodings that introduce structural constants","paragraphs":["A linear length encoding of an analysis of (1) is possible if we use the constant A = {John, Bill ..... Harry} in the encoding as follows:","(l-c) q & (antecedent(pronoun l) e A) & (antecedent(pronoun 2) e A) & (antecedent(pronounm) e A) Note that \"x ~ Y\" is short-hand for the disjunction of the statements \"x = y,\" where y ranges over Y, so that (l-c) is not very different from (l-b). Examples below involve tYeer use of constants that correspond to sets of linguistic entities, I will call such constants \"structural.\"","A linear analysis of (2) is possible if we introduce constants A 1 ..... A , where A. = {John, Bill}, A; = A 1 u {Tom}, A 3 = A 2 ~ {Fred}, ..l., Am = Am-I U {Jim}:","(2-b) q & (matecedent(pronoun t) E A1) & (antecedent(pronoun2) ~ A 2) & quantifier Qi takes scope over Qi-I to its immediate left, then the quantifier Qi+l to the immediate right of Qi cannot take scope over Qi\" (See [Epstein 88] for a discussion of relative operator scope.) It follows that the number of relative operator scope readings for (4) grows as the Fibonacci of the length of (4). 8 However, a linear encoding of an exhaustive analysis of (4) is as follows: (4-a) q & [((Q1 > Q2 ) & (L] = Q2)) v ((Q2 > Q1 ) & (L1 = T))[ & [Q1 > Q3 ] & [((L 1 > Q3 ) & (L 2 = Q3)) v ((Q3 > L1) & (L2 = T))] & [Qk-2 > Qk+l ] & [Qk-t > Qk+l ] & [((Lk_ 1 > Qk+l ) & (L k = Qk+l)) v ((Qk+l > Lk-1) & (Lk = T))] & (antecedent(pronounm) E Am) Because A. can be defined in terms of Ai_l, only linear 1 space is required to define these constants. It is convenient to mix definitions of constants with other aspects of the encoding of (2), as follows:","(2-c) q & A a = {John, Bill} & (antecedent(pronounl) ~ A i &","A 2 = (A 1 u {Tom})) & (antecedent(pronoun2) E A 2 & A 3 = (A 2 u {Fred})) & Here q represents aspects of the analysis of (4) aside from the specification of relative operator scope, and Qi represents the i-th quantifier in (4), reading from the left. The L. are introduced constants corresponding to","1 quantifiers that can have lower scope than some more deeply embedded quantifier. \"Q > Q.\" means that Q. i has higher scope than Qj. For all Q, ''~ < T\" is true and \"Q > T\" is false. 9 Note that if we delete from (4-a) propositions that assign values to introduced constants, such as \"(L l = Q2),\" the resulting statement is in CNF.","Section 3 discussed cubic length analyses of (3) with propositional constants. (3) has a linear analysis with structural constants as follows: (antecedent(pronoun .) ~ A . & I11-2 1111-1","A m = (Am. 1 u {Jim})) & (antecedent(pronoun m) ~ A m ) For (2), the introduction of structural constants permits a linear encoding. For the following example, the introduction of structural constants likewise permits a linear encoding: (4) Many teachers expect several students to expect many teachers to expect several students to ... to expect many teachers to expect several students to read some book. Each quantifier in (4) can take scope over the quantifier to its immediate left (if any), and can take scope over the quantifier to its immediate right (if any). However, if a","8. The most deeply embedded clause in (4) has 2 possible relative scope readings. The second most deeply embedded clansc in (4) has 3 possible relative scope readings (many>several>some, many>some>several several>many>some). Let S. be the k-th .... k most deeply embedded clause m (4). (S k ts immediately embedded in S..;)' Given that' S_ has a total ofn (relative operator) scope read~l~s, and that S ~tas a total of m scope readings then the • k-! \" \" ' subject of S. can take scope over all the quantifiers in S •","k+[ ,",".","k' accounting f6r n global readings over S..,. Alternatively, the subject of S can take scope over the subj[~ of S Then both . k k+.l\" these subjects take scope over all the qu'mtifiers m S . The k-l . second alternative thus accounts for m additional global readings over Sk+ 1.","9. (4-a) does not explicitly state, for example that Q > Q. but this t . 3' fact can be derived from (4-a) through apphcatmn of the transitivity of relative operator scope• Generally speaking, linguistic representations don't explicitly include all their consequences. 4 111 (3-c) q &","[(ap(PP1) = NP) & (AP 1 = NP) & (RE 1 = {NP, PP1})I &","[(ap(PP2) e REI) & (AP 2 = ap(PP2)) & (RE 2 = (RE 1 q\" AP2) u {PP2})] & (5-a) q &","[(ap(PPt) e {VP, NP}) & (AP t = ap(PP1)) & (RE~ = {VP, NP} T AP~) & (OG 1 = {VP}- {API))] &","[(ap(PP2) E REI) & (AP 2 = ap(PP2)) & (RE 2 = (RE l $ AP2) u {PP2} ) & (OG 2 = OG t - {AP2})] &","[(ap(PP k) ~ REk. 1) & (AP k = ap(PPk) ) & (RE k = (REk_ 1 1\" APk) u {PPk})] & Here q represents aspects of the analysis of (3) aside from the specification of attachment points for the prepositional phrases. The desired solutions consist of specifications of attachment possibilities, stated in the form \"ap(PPk) e X\" (\"attachment point of the k-th PP is one of the elements of X' ) in (3~c). The AP k and RE K are introduced constants. AP k is the attachment point ot","10 o PPk\" RE k represents the right edge of a constituent structure tree for the string consisting of the block and the first k PP's. (3-c) is in a sort of relaxed CNF, as discussed above in connection with (4-a), and in Section 5 below. \"T\" in (3-c) is defined so that RE TAP = {AP} u {X ~ RE I X precedes AP}. (When PPk to the right of PP. attaches above PP., PP. is not in the right edge of","1 1 l . the resulting structure, and is unavadable for attachment by material to the right of PPk.)","As for (3), the number of readings of the following exmnple (from [Church and Patil 82]) grows as the Catalan of the number of prepositional phrases: (5) Put the block in the box on the table ... in the kitchen. However, there is an important difference between (3) and (5). In (3), any number of PP's can attach to the block, any number of PP's can attach to the box, and so on, No NP in (3) requires complements. (Dr the box must attach to the block, but only because the block is the only NP that lies to the left of in the box.) In (5), on the other hand, put requires one NP argument and one PP argument, and cannot accept any other complements. 11 An analysis of (5) along the lines of (3-c) would incorrectly include readings where more than one PP attaches to put, and readings where no PP attaches to put. A linear analysis of (5) is as follows:","10. \"PP attaches to PP \" really means that PP attaches to the object of i .. k . i . . the preposmon head of PP. Thts usage permits a brtefer • k . discussion than would otherwise be possible.","1 l. This characterization of put is not strictly speaking correct, but the necessary qualifications are irrelevant to the discussion here.","[(ap(PP k) E REk_ l) & (AP k = ap(PPk) ) & (RE k = (REk. 1 T APk) L.) {PPk} ) & (OG k = OGk. 1 - {APk})] & [(ap(PPn) E OGn_ l [] REn.1)] (5-a) is similar to (3-c), but includes the additional constants OG OG is the open (theta-)gnd for the","k\" k substructure corresponding to put the block followed by the first k PP's. OG k is either {VP}, if none of the first k PP's is attached to V, or is empty Non-empty OG","\" k indicates that for each constituent X in OG., some PP.,",". . K 1 k<i~n, must attach to X. [] in (5-a) is defined so that A [] B is equal to A if A is non-empty, and is otherwise equal to B. The final conjunct in (5-a) captures the requirement that if none of the first n-1 PP's attaches to put, then the final PP must attach to this verb."]},{"title":"5. Issues","paragraphs":["The example constructions presented above illustrate a variety of abstract cases. In (1), local ambiguities are independent of each other. The assignment of an antecedent to a pronoun in (1) does not affect the possibilities of antecedent assignment for other pronouns in (1). An analysis of (1) in CNF need not include more than one appearance of any literal. (2) is similar to (1) in this respect. In (4), local ambiguities are interdependent, but local ambiguity possibilities depend only on ambiguity possibilities in neighboring clauses. There is thus a bound on how many ambiguities can interact. In (3), on the other hand, there is no such bound. Choosing an attachment site for PP.",".... J affects the attachment posslbdlttes for PP ..... no matter how large k is. (5) is similar to (3), but ats*~ involves a global filter associated with the verb put. (3-c) and (5-a) employ a richer repertory of operators on structural constants (-, , q') than is found in (l-c), (2-b), (2-c), and (4-a).","(l-b) may qualify relatively easily as an exhaustive analysis of (1), according to a common conception of what constitutes an analysis. (3-c), on the other hand, appears to have some of the ealxnarks of a procedure. The similarity of introduced constants to local variables is obvious. In particular, the constants AP; and RE; of 112 c 5 (3-c) conld be replaced with two local variables AP and RE that receive destructive assignment. (3--c) also en'@oys the operators \"$\" and \"~\", which might be regarded as corresponding to procedures. Whether (3--c) is an analysis or a procedure for computing analyses is ultimately a matter of selecting a definition for \"linguistic analysis.\"","Criteria Iota successful definition of \"linguistic analysis\" might appeal to psychological reality. One possible requirement is that components of analyses correspond to partial analyses built during human processing. When definitions of constants (assignments to local variables) are blended into what is otherwise a CNF formula, as in (2-c), (3-c), (4-a), and (5-a), the result might be called \"relaxed CNF.\" Somewhat more precisely, suppose that a formula in \"relaxed CNF\" is a conjunction of \"relaxed disjunctions,\" where a \"relaxed disjunction\" is the conjunction of a \"generalized disjunction\" with an \"assignment formula.\" A \"generalized disjunction\" rnay be either a disjunction of atoms, or a statement of the form \"x c A.\" An \"assignment formula\" is a conjunction of statements that assign values to constants. Given such a relaxed CNF formula as an exhaustive analysis, obtaining an analysis of a single reading requires for' each generalized disjunction the choice of a disjunct or the selection of an element. Such single--reading analyses may be produced by deterministic variants of non-detemfinistic processing models that produce exhaustive analyses in relaxed CNF. Relaxed CNF is compatible with a variety of processing models. For example, a component of the form \"x e A\" might be produced before components that specify the corrtents of A.","Recent work on Kolmogorov complexity might provide alternative criteria for the definition of \"linguistic analysis.\" ([Li and Vitffnyi 89] is a recent survey ol' work on Kolmogorov complexity.) In particular, notions of time-bounded algorithmic complexity, such as the \"logical depth\" of [Bennett 88], may be relevant. Following a third alternative, a satisfactory definition of \"analysis\" may involve a correspondence principle, along lhe lollowing lines:","N","13\"","it every component of a l%al analysis specifically mentions one or more components of the input. For this to work, \"component\" and \"mention\" themselves require appropriate definitions. Arbitrary fragments of analyses cannot count as \"components.\" \"Mention\" should be transitive.","Analyses in relaxed CNF may be more compatible with principle-based grammars than are tree-based analyses. ([Chomsky 81] is the seminal work on principle-based grammars.) Constituent structure does not occupy as central a place in the principleobased paradigm as in other' grammatical paradigms. Each generalized disjunction (or its deterministic counterpart) supplies a piece of information about tire analyzed expression. Assignment formulas capture logical dependencies among thcse pieces. Each of the examples in this oar~er illustrates a sinele Nlenomenon. Relaxed CNF can also capture interactions among ptmn~mmr~a.","Analyses like (3-c) are probably less easily readable than packed shared forests. Full analyses that specify constituent structure information together with relative operator scope information, information on anaphora, and so on, will be even less readable. It may be possible to devise a more graphically oriented notation for linear encodings of linguistic analyses.","Wlmtever cozrception of \"linguistic analysis\" may ultimately prove most useful, it seems clear that working with relaxed Conjunctive Norrnal Form has advantages over working with Disjunctive Normal Form lor computational applications. Relaxed CNF also appears to have advantages over DNF for psycholinguistic modeling. Introduced constants (local variables) have obvious utility in implementations. They may also play a role in human processing of language. In particular, as human processing proceeds, explicit details of previously encountered structure may recede into the background yet remain accessible."]},{"title":"Acknowledgments","paragraphs":["1 am indebted for comments and discussions to Steven Abney, Yves Caseau, and Andrew Ogielski. Responsibility tbr errors is entirely mine."]},{"title":"References","paragraphs":["E. Barton, R. Berwick, and E. Ristad,"]},{"title":"Computational Complexity and Natural Ixmguage,","paragraphs":["M_IT Press, Cmnbridge, Massachusetts, 1987. C. Bennett, \"Logical Depth and Physical Complexity,\" in R. Herken (ed.),"]},{"title":"The Universal J~ring Machine; A Half-Century Survey,","paragraphs":["pp. 227-258, Oxford University Press, Oxford, 1988. N. Chomsky,"]},{"title":"Lectures on Government and Binding,","paragraphs":["Foris Publications, Dordrecht, 1981. K. Church and R. Patil, \"Coping with Syntactic Ambiguity or How to Put the Block in the Box on the Table,\""]},{"title":"American Journal of Computational Linguistics,","paragraphs":["8:3-4, pp. 139-:149, 1982. S. Epstein, \"Principle-Based Interpretation of Natural Language Quantifiers,\""]},{"title":"Proceedings of the Seventh National Conference o n Artificial Intelligence (AAAI-88),","paragraphs":["pp. 718~723, 1988. M. Garey and D. Johnson,"]},{"title":"Computers and Intractability,","paragraphs":["W. H. Freeman, San Francisco, 1979. M. Li and P. Vitgnyi,"]},{"title":"Kolmogorov Complexity and Its Applications (Revised Version),","paragraphs":["Report CS-R8901, Centre tor Mathematics and Computer Science, Amsterdam, 1989. M. Tomita, \"An Efficient Augmented-Context-Free Parsing Algorithm,\""]},{"title":"Computational Linguistics,","paragraphs":["13:1-2, pp. 31-46, 1987. 6 113"]}]}