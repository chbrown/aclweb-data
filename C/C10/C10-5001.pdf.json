{"sections":[{"title":"","paragraphs":["Coling 2008: Kernel Engineering for Fast and Easy Design of Natural Language Applications–Tutorial notes, pages 1–91, Beijing, August 2010"]},{"title":"Kernel Engineering for Fast and Easy Design of Natural Language Applications Alessandro Moschitti","paragraphs":["Department of Information Engineering and Computer Science","University of Trento Email: moschitti@disi.unitn.it","The 23rd International Conference on Computational Linguistics August 22, 2010 Beijing, China"]},{"title":"Schedule 14:00 - 15:30 First part 15:30 - 16:00 Coffee break 16:00 - 17:30 Second part","paragraphs":["1"]},{"title":"Outline (1) ","paragraphs":["Motivation","Kernel-Based Machines Perceptron Support Vector Machines","Kernel Definition Kernel Trick Mercer’s conditions Kernel operators","Basic Kernels Linear Kernel Polynomial Kernel Lexical Kernel"]},{"title":"Outline (2) ","paragraphs":["Structural Kernels String and Word Sequence Kernels Tree Kernels","Subtree, Syntactic, Partial Tree Kernels","Applied Examples of Structural Kernels Semantic Role Labeling (SRL) Question Classification (QC) SVM-Light-TK Experiments in classroom with SRL and QC Inspection of the input, output, and model files 2"]},{"title":"Outline (3) ","paragraphs":["Kernel Engineering Structure Transformation Syntactic Semantic Tree kernels Kernel Combinations Kernels on Object Pairs Kernels for re-ranking","Practical Question and Answer Classifier based on SVM-Light-TK Combining Kernels Conclusion and Future Work"]},{"title":"Motivation (1) Feature design most difficult aspect in designing a learning system ","paragraphs":["complex and difficult phase, e.g., structural feature representation: deep knowledge and intuitions are required design problems when the phenomenon is described by many features 3"]},{"title":"Motivation (2) Kernel methods alleviate such problems ","paragraphs":["Structures represented in terms of substructures High dimensional feature spaces Implicit and abstract feature spaces "]},{"title":"Generate high number of features ","paragraphs":["Support Vector Machines “select” the relevant features Automatic Feature engineering side-effect"]},{"title":"Part I: Kernel Methods Theory","paragraphs":["4"]},{"title":"A simple classification problem: Text Categorization","paragraphs":["Sport Cn Politic C1 Economic C2"]},{"title":". . . . . . . . . . .","paragraphs":["Bush declares war Wonderful Totti Yesterday match Berlusconi acquires Inzaghi before elections Berlusconi acquires Inzaghi before elections Berlusconi acquires Inzaghi before elections"]},{"title":"Text Classification Problem ","paragraphs":["Given: a set of target categories: the set T of documents, define "]},{"title":"f : T → 2C ","paragraphs":["VSM (Salton89’) Features are dimensions of a Vector Space. Documents and Categories are vectors of feature weights. d is assigned to if €  d ⋅  C i > th €","C = C1 ,..,C n"]},{"title":"{ }","paragraphs":["i"]},{"title":"C","paragraphs":["5"]},{"title":"More in detail In Text Categorization documents are word vectors The dot product counts the number of features in common This provides a sort of similarity  € Φ(d","paragraphs":["x"]},{"title":") =  x = (0,..,1,..,0,..,0,..,1,..,0,..,0,..,1,..,0,..,0,..,1,..,0,..,1) buy acquisition stocks sell market zx  ⋅  € Φ(d","paragraphs":["z"]},{"title":") =  z = (0,..,1,..,0,..,1,..,0,..,0,..,0,..,1,..,0,..,0,..,1,..,0,..,0) buy company stocks sell Linear Classifier ","paragraphs":["€ f (  x ) =  x ⋅  w + b = 0,  x ,  w ∈ R n ,b ∈ R "]},{"title":"The equation of a hyperplane is is the vector representing the classifying example is the gradient of the hyperplane The classification function is x w","paragraphs":["( ) sign( ( ))h x f x= 6 "]},{"title":"Mapping vectors in a space where they are linearly separable x x x x o o o o The main idea of Kernel Functions )( xx  φ→","paragraphs":[")x(φ )x(φ )x(φ )x(φ )(oφ )(oφ )(oφ )(oφ φ"]},{"title":"A mapping example ","paragraphs":["Given two masses m1 and m2 , one is constrained Apply a force fa to the mass m1 Experiments","Features m1 , m2 and fa We want to learn a classifier that tells when a mass m1 will get far away from m2 2 21 21 ),,( rmm Crmmf = If we consider the Gravitational Newton Law we need to find when f(m1 , m2 , r) < fa 7"]},{"title":"A mapping example (2)","paragraphs":["))(),...,(()(),...,( 11 xxxxxx nn  φφφ =→= The gravitational law is not linear so we need to change space )ln,ln,ln,(ln),,,(),,,( 2121 rmmfzyxkrmmf aa =→ zyxcrmmCrmmf 2ln2lnlnln),,(ln 2121 −++=−++= (ln m1,ln m2,-2ln r)⋅"]},{"title":"(x,y,z)- ln f","paragraphs":["a"]},{"title":"+ ln C =","paragraphs":["0, we can decide without error if the mass will get far away or not As 0lnln2lnlnln 21 =−+−− Crmmfa We need the hyperplane"]},{"title":"A kernel-based Machine Perceptron training ","paragraphs":["€  w 0 ←  0 ;b0 ← 0;k ← 0;R ← max1≤i≤l ||  x i || do for i = 1 to  if yi(  w k ⋅  x i + bk ) ≤ 0 then   w k+1 =  w k + ηyi  x i bk+1 = bk + ηyiR 2 k = k + 1 endif endfor while an error is found return k,(  w k,bk ) 8 9"]},{"title":"Novikoff’s Theorem","paragraphs":["Let S be a non-trivial training-set and let Let us suppose there is a vector and with γ > 0. Then the maximum number of errors of the perceptron is: * * , || || 1 =w w","* * ( , ) , 1,..., ,i iy b i lγ+ ≥ =w x 2 * 2 , R t","γ ","=     1max || || .i i lR x≤ ≤ = 10 "]},{"title":"In each step of perceptron only training data is added with a certain weight So the classification function Note that data only appears in the scalar product Dual Representation for Classification ","paragraphs":["€"," w = α j j=1.."]},{"title":"∑","paragraphs":["y j  x j  € sgn(  w ⋅"," x + b) = sgn α j j=1.."]},{"title":"∑","paragraphs":["y j  x j ⋅  x + b        "]},{"title":"Dual Representation for Learning as well as the updating function The learning rate only affects the re-scaling of the hyperplane, it does not affect the algorithm, so we can fix","paragraphs":["1.η = η  €","if yi( α j j=1.."]},{"title":"∑","paragraphs":["y j  x j ⋅  x i + b) ≤ 0 then αi = αi + η 11 We can rewrite the classification function as As well as the updating function"]},{"title":"Dual Perceptron algorithm and Kernel functions ","paragraphs":["€ h( x) = sgn(  w φ ⋅ φ(  x ) + bφ ) = sgn( α j","j=1.."]},{"title":"∑","paragraphs":["y jφ(  x j ) ⋅ φ(  x ) + bφ ) =","= sgn( α j i=1.."]},{"title":"∑","paragraphs":["y j k(  x j ,  x ) + bφ )  €","if yi α j j=1.."]},{"title":"∑","paragraphs":["y j k(  x j,  x i ) + bφ         ≤ 0 allora αi = αi + η"]},{"title":"Support Vector Machines Hard-margin SVMs Soft-margin SVMs","paragraphs":["12"]},{"title":"Which hyperplane do we choose? Classifier with a Maximum Margin","paragraphs":["Var1 Var2 Margin Margin IDEA 1: Select the hyperplane with maximum margin 13"]},{"title":"Support Vectors","paragraphs":["Var1 Var2 Margin Support Vectors"]},{"title":"Support Vector Machines","paragraphs":["Var1 Var2 kbxw −=+⋅  kbxw =+⋅  0=+⋅ bxw "]},{"title":"k k","paragraphs":["w The margin is equal to 2 k w 14"]},{"title":"Support Vector Machines","paragraphs":["Var1 Var2 kbxw −=+⋅  kbxw =+⋅  0=+⋅ bxw "]},{"title":"k k","paragraphs":["w The margin is equal to 2 k w We need to solve  € max 2 k ||  w ||  w ⋅  x + b ≥ +k, if  x is positive  w ⋅  x + b ≤ −k, if  x is negative"]},{"title":"Support Vector Machines","paragraphs":["Var1 Var2 1w x b⋅ + = −  1w x b⋅ + =  0=+⋅ bxw "]},{"title":"1 1","paragraphs":["w There is a scale for which k=1. The problem transforms in:  € max 2 ||  w ||  w ⋅  x + b ≥ +1, if  x is positive  w ⋅  x + b ≤ −1, if  x is negative 15"]},{"title":"Final Formulation","paragraphs":["€ ⇒  € max 2 ||  w ||  w ⋅  x i + b ≥ +1, yi = 1  w ⋅  x i + b ≤ −1, yi = -1 € max 2 ||  w || yi (  w ⋅  x i + b) ≥ 1  € min ||  w || 2 yi (  w ⋅  x i + b) ≥ 1  € min ||  w ||2 2 yi (  w ⋅  x i + b) ≥ 1 € ⇒ € ⇒ € ⇒"]},{"title":"Optimization Problem Optimal Hyperplane: ","paragraphs":["Minimize Subject to "]},{"title":"The dual problem is simpler ","paragraphs":["libxwy ww ii ,...,1,1))(( 2 1 )( 2 =≥+⋅ =   τ 16"]},{"title":"Lagrangian Definition Dual Optimization Problem","paragraphs":["17"]},{"title":"Dual Transformation To solve the dual problem we need to evaluate: Given the Lagrangian associated with our problem Let us impose the derivatives to 0, with respect to w Dual Transformation (cont’d) and wrt b Then we substituted them in the objective function","paragraphs":["18"]},{"title":"The Final Dual Optimization Problem Khun-Tucker Theorem Necessary and sufficient conditions to optimality","paragraphs":["19"]},{"title":"Properties coming from constraints ","paragraphs":["Lagrange constraints: Karush-Kuhn-Tucker constraints Support Vectors have not null To evaluate b, we can apply the following equation  € ai i=1 l"]},{"title":"∑","paragraphs":["yi = 0,"," w = α i i=1 l"]},{"title":"∑","paragraphs":["yi  x i libwxy iii ,...,1,0]1)([ ==−+⋅⋅  α iα"]},{"title":"Soft Margin SVMs","paragraphs":["Var1 Var2 1w x b⋅ + = −  1w x b⋅ + =  0=+⋅ bxw "]},{"title":"1 1","paragraphs":["w iξ slack variables are added Some errors are allowed but they should penalize the objective function iξ 20"]},{"title":"Soft Margin SVMs","paragraphs":["Var1 Var2 1w x b⋅ + = −  1w x b⋅ + =  0=+⋅ bxw "]},{"title":"1 1","paragraphs":["w iξ The new constraints are The objective function penalizes the incorrect classified examples C is the trade-off between margin and the error  € yi(  w ⋅  x i + b) ≥ 1− ξi ∀  x i where ξi ≥ 0  € min 1 2 ||  w ||2","+C ξi i"]},{"title":"∑ Dual formulation ","paragraphs":["By deriving wrt €  w ,  ξ and b 21"]},{"title":"Partial Derivatives Substitution in the objective function of Kronecker","paragraphs":["ij"]},{"title":"δ","paragraphs":["22"]},{"title":"Final dual optimization problem Soft Margin Support Vector Machines ","paragraphs":["The algorithm tries to keep ξi low and maximize the margin NB: The number of error is not directly minimized (NP-complete problem); the distances from the hyperplane are minimized If C→∞, the solution tends to the one of the hard-margin algorithm Attention !!!: if C = 0 we get = 0, since If C increases the number of error decreases. When C tends to infinite the number of errors must be 0, i.e. the hard-margin formulation |||| w  € min 1 2 ||  w ||2 +C ξii"]},{"title":"∑ ","paragraphs":["€ yi (  w ⋅  x i + b) ≥ 1 − ξ i ∀  x i ξi ≥ 0 "]},{"title":"€ y","paragraphs":["i"]},{"title":"b ≥ 1− ξ","paragraphs":["i"]},{"title":"∀  x","paragraphs":["i 23"]},{"title":"Robusteness of Soft vs. Hard Margin SVMs","paragraphs":["i ξ Var1 Var2 0=+⋅ bxw  ξi Var1 Var2 0=+⋅ bxw  Soft Margin SVM Hard Margin SVM"]},{"title":"Kernels in Support Vector Machines ","paragraphs":["In Soft Margin SVMs we maximize: By using kernel functions we rewrite the problem as: 24"]},{"title":"Kernel Function Definition Kernels are the product of mapping functions such as  €  x ∈ R","paragraphs":["n"]},{"title":",  φ (  x ) = (φ","paragraphs":["1"]},{"title":"(  x ),φ","paragraphs":["2"]},{"title":"(  x ),...,φ","paragraphs":["m"]},{"title":"(  x )) ∈ R","paragraphs":["m"]},{"title":"The Kernel Gram Matrix ","paragraphs":["With KM-based learning, the sole information used from the training data set is the Kernel Gram Matrix If the kernel is valid, K is symmetric definite-positive . 25"]},{"title":"Valid Kernels Valid Kernels cont’d If the matrix is positive semi-definite then we can find a mapping φ implementing the kernel function","paragraphs":["26"]},{"title":"Mercer’s Theorem (finite space) ","paragraphs":["Let us consider  € K = K(  x i,  x j )"]},{"title":"( )","paragraphs":["i, j=1 n K symmetric ⇒ ∃ V: for Takagi factorization of a complex-symmetric matrix, where: Λ is the diagonal matrix of the eigenvalues λt of"]},{"title":"K ","paragraphs":["are the eigenvectors, i.e. the columns of V Let us assume lambda values non-negative € K = VΛ ′ V  €  v t = vti"]},{"title":"( )","paragraphs":["i =1 n "]},{"title":"€ φ :  x","paragraphs":["i"]},{"title":"→ λ","paragraphs":["t"]},{"title":"v","paragraphs":["ti"]},{"title":"( )","paragraphs":["t =1 n"]},{"title":"∈ R","paragraphs":["n"]},{"title":", i = 1,.., n Mercer’s Theorem (sufficient conditions) ","paragraphs":["€ Φ(  x i) ⋅ Φ("," x j ) = λtvti t=1 n"]},{"title":"∑","paragraphs":["vtj = VΛ ′ V"]},{"title":"( )","paragraphs":["ij = Kij = K (  x i,  x j )  Therefore , which implies that K is a kernel function 27"]},{"title":"Mercer’s Theorem (necessary conditions) ","paragraphs":["€  z 2 =  z ⋅  z = Λ ′ V  v s Λ ′ V  v s =  v s' V Λ Λ ′ V  v s =   v s' K  v s =  v s' λs  v s = λs  v s 2 < 0 Suppose we have negative eigenvalues"]},{"title":"λ","paragraphs":["s and eigenvectors the following point has the following norm: this contradicts the geometry of the space. "]},{"title":"€  v","paragraphs":["s  €  z = vsi Φ(  x i ) i=1 n"]},{"title":"∑","paragraphs":["= vsi λt vti"]},{"title":"( )","paragraphs":["t = i=1 n"]},{"title":"∑","paragraphs":["Λ ′ V  v s"]},{"title":"Is it a valid kernel? It may not be a kernel so we can use M·́M","paragraphs":["28"]},{"title":"Valid Kernel operations k(x,z) = k","paragraphs":["1"]},{"title":"(x,z)+k","paragraphs":["2"]},{"title":"(x,z) k(x,z) = k","paragraphs":["1"]},{"title":"(x,z)*k","paragraphs":["2"]},{"title":"(x,z) k(x,z) = α k","paragraphs":["1"]},{"title":"(x,z) k(x,z) = f(x)f(z) k(x,z) = k","paragraphs":["1"]},{"title":"(φ(x),φ(z)) k(x,z) = x'Bz Basic Kernels for unstructured data Linear Kernel Polynomial Kernel Lexical kernel String Kernel","paragraphs":["29"]},{"title":"Linear Kernel In Text Categorization documents are word vectors The dot product counts the number of features in common This provides a sort of similarity  € Φ(d","paragraphs":["x"]},{"title":") =  x = (0,..,1,..,0,..,0,..,1,..,0,..,0,..,1,..,0,..,0,..,1,..,0,..,1) buy acquisition stocks sell market zx  ⋅  € Φ(d","paragraphs":["z"]},{"title":") =  z = (0,..,1,..,0,..,1,..,0,..,0,..,0,..,1,..,0,..,0,..,1,..,0,..,0) buy company stocks sell Feature Conjunction (polynomial Kernel) ","paragraphs":["The initial vectors are mapped in a higher space More expressive, as encodes Stock+Market vs. Downtown+Market features We can smartly compute the scalar product as",")1,2,2,2,,(),( 21212 2 2 121 xxxxxxxx →><Φ ),()1()1( 1222 )1,2,2,2,,()1,2,2,2,,( )()( 22 2211 22112121 2 2 2 2 2 1 2 1 2121 2 2 2 12121 2 2 2 1 zxKzxzxzx zxzxzzxxzxzx zzzzzzxxxxxx zx Poly   =+⋅=++= =+++++= =⋅= =Φ⋅Φ )( 21xx 30"]},{"title":"Document Similarity","paragraphs":["industry telephone market company product Doc 1 Doc 2"]},{"title":"Lexical Semantic Kernel [CoNLL 2005] The document similarity is the SK function: where s is any similarity function between words, e.g. WordNet [Basili et al.,2005] similarity or LSA [Cristianini et al., 2002] Good results when training data is small € SK (d","paragraphs":["1"]},{"title":", d","paragraphs":["2"]},{"title":") = s(w","paragraphs":["1"]},{"title":", w","paragraphs":["2"]},{"title":")","paragraphs":["w1 ∈d1 ,w2 ∈d 2"]},{"title":"∑","paragraphs":["31"]},{"title":"Using character sequences zx  ⋅  € φ(\"bank\") =  x = (0,..,1,..,0,..,1,..,0,......1,..,0,..,1,..,0,..,1,..,0) counts the number of common substrings","paragraphs":["bank ank bnk bk b "]},{"title":"€ φ(\" rank\") =  z = (1,..,0,..,0,..,1,..,0,......0,..,1,..,0,..,1,..,0,..,1)","paragraphs":["rank ank rnk rk r "]},{"title":"€  x ⋅  z = φ(\"bank\") ⋅ φ(\" rank\") = k(\"bank\",\" rank\") String Kernel Given two strings, the number of matches between their substrings is evaluated E.g. Bank and Rank B, a, n, k, Ba, Ban, Bank, Bk, an, ank, nk,.. R, a , n , k, Ra, Ran, Rank, Rk, an, ank, nk,.. String kernel over sentences and texts Huge space but there are efficient algorithms","paragraphs":["32"]},{"title":"Formal Definition","paragraphs":[", where , where i1 +1"]},{"title":"Kernel between Bank and Rank","paragraphs":["33"]},{"title":"An example of string kernel computation Efficient Evaluation Dynamic Programming technique Evaluate the spectrum string kernels Substrings of size p Sum the contribution of the different spectra","paragraphs":["34"]},{"title":"Efficient Evaluation An example: SK(“Gatta”,”Cata”) First, evaluate the SK with size p=1, i.e. “a”, “a”,”t”,”t”,”a”,”a” Store this in the table","paragraphs":["€ SK p=1 35"]},{"title":"Evaluating DP2 Evaluate the weight of the string of size p in case a character will be matched This is done by multiplying the double summation by the number of substrings of size p-1 Evaluating the Predictive DP on strings of size 2 (second row) ","paragraphs":["Let’s consider substrings of size 2 and suppose that: we have matched the first “a” we will match the next character that we will add to the two strings We compute the weights of matches above at different string","positions with some not-yet known character “?”","If the match occurs immediately after “a” the weight will be λ1+1","x λ1+1 =","λ4","and we store just λ2","in the DP entry in [“a”,”a”] 36"]},{"title":"Evaluating the DP wrt different positions (second row) ","paragraphs":["If the match for “gatta” occurs after “t” the weight will be λ1+2 (x λ2 =","λ5",") since the substring for it will be with “a☐?”"," We write such prediction in the entry [“a”,”t”] Same rationale for a match after the second “t”: we have","the substring “a☐☐?” (matching with “a?” from “catta”) for","a weight of λ3+1","(x λ2",")"]},{"title":"Evaluating the DP wrt different positions (third row) ","paragraphs":["If the match occurs after “t” of “cata”, the weight will be λ2+1","","(x λ2 =","λ5",") since it will be with the string “a☐?”, with a weight","of λ3","If the match occurs after “t” of both “gatta” and “cata”, there are two ways to compose substring of size two:","“a☐?” with","weight λ4 or “t?” with weight λ2",""]},{"title":"⇒","paragraphs":["the total is λ2 +λ4"," 37"]},{"title":"Evaluating the DP wrt different positions (third row) ","paragraphs":["The final case is a match after the last “t” of both “cat” and “gatta”","There are three possible substrings of “gatta”:","“a☐☐?”, “t☐?”, “t?” for “gatta” with weight λ3",", λ2","or λ, respectively.","There are two possible substrings of “cata”","“a☐?”, “t?” with weight λ2","and λ","Their match gives weights: λ5 , λ3",", λ2","⇒ by summing: λ5","+ λ3","+ λ2"]},{"title":"Evaluating SK of size 2 using DP2 ","paragraphs":["The number (weight) of substrings of size 2 between “gat” and “cat” is λ4","= λ2"," ([“a”,”a”] entry of DP) x λ2","(cost of one character), where a = “t” and b = “t”.","Between “gatta” and “cata” is λ7 + λ5","+ λ4",", i.e the matches of “a☐☐a”, “t☐a”, “ta” with “a☐a” and “ta” € SK p= 2 38"]},{"title":"Tree kernels Subtree, Subset Tree, Partial Tree kernels Efficient computation Example of a parse tree “John delivers a talk in Rome”","paragraphs":["S → N VP VP → V NP PP PP → IN N N → Rome N Rome S N NP D N VP V John in delivers a talk PP IN 39"]},{"title":"The Syntactic Tree Kernel (STK) [Collins and Duffy, 2002]","paragraphs":["NP D N VP V delivers a talk NP D N VP V delivers a NP D N VP V delivers NP D N VP V NP VP V"]},{"title":"The overall fragment set","paragraphs":["40"]},{"title":"The overall fragment set NP D VP a Children are not divided Explicit kernel space zx  ⋅ ","paragraphs":["€ φ(Tx ) =  x = (0,..,1,..,0,..,1,..,0,..,1,..,0,..,1,..,0,..,1,..,0,..,1,..,0) "]},{"title":"counts the number of common substructures ","paragraphs":["€ φ(Tz ) =  z = (1,..,0,..,0,..,1,..,0,..,1,..,0,..,1,..,0,..,0,..,1,..,0,..,0) 41"]},{"title":"Efficient evaluation of the scalar product  €  x ⋅  z = φ(T","paragraphs":["x"]},{"title":") ⋅ φ(T","paragraphs":["z"]},{"title":") = K (T","paragraphs":["x"]},{"title":",T","paragraphs":["z"]},{"title":") = =","paragraphs":["nx ∈Tx"]},{"title":"∑ Δ(n","paragraphs":["x"]},{"title":", n","paragraphs":["z"]},{"title":")","paragraphs":["nz ∈Tz"]},{"title":"∑ Efficient evaluation of the scalar product ","paragraphs":["[Collins and Duffy, ACL 2002] evaluate"]},{"title":"Δ","paragraphs":["in O(n2 ):"]},{"title":"€ Δ(n","paragraphs":["x"]},{"title":", n","paragraphs":["z"]},{"title":") = 0, if the productions are different else Δ(n","paragraphs":["x"]},{"title":", n","paragraphs":["z"]},{"title":") = 1, if pre - terminals else Δ(n","paragraphs":["x"]},{"title":", n","paragraphs":["z"]},{"title":") = (1 + Δ(ch(n","paragraphs":["x"]},{"title":", j),ch(n","paragraphs":["z"]},{"title":", j)))","paragraphs":["j=1 nc(nx )"]},{"title":"∏  €  x ⋅  z = φ(T","paragraphs":["x"]},{"title":") ⋅ φ(T","paragraphs":["z"]},{"title":") = K (T","paragraphs":["x"]},{"title":",T","paragraphs":["z"]},{"title":") = =","paragraphs":["nx ∈Tx"]},{"title":"∑ Δ(n","paragraphs":["x"]},{"title":", n","paragraphs":["z"]},{"title":")","paragraphs":["nz ∈Tz"]},{"title":"∑","paragraphs":["42"]},{"title":"Other Adjustments Normalization € Δ(n","paragraphs":["x"]},{"title":", n","paragraphs":["z"]},{"title":") = λ, if pre - terminals else Δ(n","paragraphs":["x"]},{"title":", n","paragraphs":["z"]},{"title":") = λ (1 + Δ(ch(n","paragraphs":["x"]},{"title":", j),ch(n","paragraphs":["z"]},{"title":", j)))","paragraphs":["j=1 nc(nx )"]},{"title":"∏ € ′ K (T","paragraphs":["x"]},{"title":",T","paragraphs":["z"]},{"title":") = K (T","paragraphs":["x"]},{"title":",T","paragraphs":["z"]},{"title":") K (T","paragraphs":["x"]},{"title":",T","paragraphs":["x"]},{"title":") × K (T","paragraphs":["z"]},{"title":",T","paragraphs":["z"]},{"title":")  Decay factor SubTree (ST) Kernel [Vishwanathan and Smola, 2002] ","paragraphs":["NP D N a talk D N a talk NP D N VP V delivers a talk V delivers 43"]},{"title":"Evaluation € Δ(n","paragraphs":["x"]},{"title":", n","paragraphs":["z"]},{"title":") = 0, if the productions are different else Δ(n","paragraphs":["x"]},{"title":", n","paragraphs":["z"]},{"title":") = 1, if pre - terminals else Δ(n","paragraphs":["x"]},{"title":", n","paragraphs":["z"]},{"title":") = (1 + Δ(ch(n","paragraphs":["x"]},{"title":", j),ch(n","paragraphs":["z"]},{"title":", j)))","paragraphs":["j=1 nc(nx )"]},{"title":"∏ Given the equation for the SST kernel Evaluation € Δ(n","paragraphs":["x"]},{"title":", n","paragraphs":["z"]},{"title":") = 0, if the productions are different else Δ(n","paragraphs":["x"]},{"title":", n","paragraphs":["z"]},{"title":") = 1, if pre - terminals else Δ(n","paragraphs":["x"]},{"title":", n","paragraphs":["z"]},{"title":") = Δ(ch(n","paragraphs":["x"]},{"title":", j),ch(n","paragraphs":["z"]},{"title":", j))","paragraphs":["j=1 nc(nx )"]},{"title":"∏ Given the equation for the SST kernel","paragraphs":["44"]},{"title":"Fast Evaluation of STK [Moschitti, EACL 2006] where P(n","paragraphs":["x"]},{"title":") and P(n","paragraphs":["z"]},{"title":") are the production rules used at nodes n","paragraphs":["x"]},{"title":"and n","paragraphs":["z"]},{"title":"€ K (T","paragraphs":["x"]},{"title":",T","paragraphs":["z"]},{"title":") = Δ(n","paragraphs":["x"]},{"title":", n","paragraphs":["z"]},{"title":")","paragraphs":["nx ,nz ∈NP"]},{"title":"∑ NP = n","paragraphs":["x"]},{"title":", n","paragraphs":["z"]},{"title":"∈ T","paragraphs":["x"]},{"title":"× T","paragraphs":["z"]},{"title":": Δ(n","paragraphs":["x"]},{"title":", n","paragraphs":["z"]},{"title":") ≠ 0{ } = = n","paragraphs":["x"]},{"title":", n","paragraphs":["z"]},{"title":"∈ T","paragraphs":["x"]},{"title":"× T","paragraphs":["z"]},{"title":": P(n","paragraphs":["x"]},{"title":") = P(n","paragraphs":["z"]},{"title":"){ }, Algorithm","paragraphs":["45"]},{"title":"Observations We order the production rules used in T","paragraphs":["x"]},{"title":"and T","paragraphs":["z"]},{"title":", at loading time At learning time we may evaluate NP in |T","paragraphs":["x"]},{"title":"|+|T","paragraphs":["z"]},{"title":"| running time If T","paragraphs":["x"]},{"title":"and T","paragraphs":["z"]},{"title":"are generated by only one production rule ⇒ O(|T","paragraphs":["x"]},{"title":"|×|T","paragraphs":["z"]},{"title":"| )... Observations We order the production rules used in T","paragraphs":["x"]},{"title":"and T","paragraphs":["z"]},{"title":", at loading time At learning time we may evaluate NP in |T","paragraphs":["x"]},{"title":"|+|T","paragraphs":["z"]},{"title":"| running time If T","paragraphs":["x"]},{"title":"and T","paragraphs":["z"]},{"title":"are generated by only one production rule ⇒ O(|T","paragraphs":["x"]},{"title":"|×|T","paragraphs":["z"]},{"title":"| )...Very Unlikely!!!!","paragraphs":["46"]},{"title":"Labeled Ordered Tree Kernel","paragraphs":["NP D N VP V gives a talk NP D N VP V a talk NP D N VP a talk NP D N VP a NP D VP a NP D VP NP N VP NP N NP NP D N D NP ... VP "]},{"title":"SST satisfies the constraint “remove 0 or all children at a time”. If we relax such constraint we get more general substructures [Kashima and Koyanagi, 2002] Weighting Problems Both matched pairs give the same contribution. Gap based weighting is needed. A novel efficient evaluation has to be defined","paragraphs":["NP D N VP V gives a talk NP D N VP V a talk NP D N VP V gives a talk gives JJ good NP D N VP V gives a talk JJ bad 47"]},{"title":"Partial Trees, [Moschitti, ECML 2006]","paragraphs":["NP D N VP V brought a cat NP D N VP V a cat NP D N VP a cat NP D N VP a NP D VP a NP D VP NP N VP NP N NP NP D N D NP ... VP "]},{"title":"SST + String Kernel with weighted gaps on Nodes’ children Partial Tree Kernel By adding two decay factors we obtain:","paragraphs":["48"]},{"title":"Efficient Evaluation (1) ","paragraphs":["In [Taylor and Cristianini, 2004 book], sequence kernels with weighted gaps are factorized with respect to different subsequence sizes. We treat children as sequences and apply the same theory"]},{"title":"Dp Efficient Evaluation (2) ","paragraphs":["The complexity of finding the subsequences is Therefore the overall complexity is where ρ is the maximum branching factor (p = ρ) 49"]},{"title":"Running Time of Tree Kernel Functions SVM-light-TK Software Encodes ST, SST and combination kernels in SVM-light [Joachims, 1999] Available at http://dit.unitn.it/~moschitt/ Tree forests, vector sets The new SVM-Light-TK toolkit will be released asap","paragraphs":["50"]},{"title":"Data Format “What does Html stand for?” ","paragraphs":["1 |BT| (SBARQ (WHNP (WP What))(SQ (AUX does)(NP (NNP S.O.S.))(VP (VB stand)(PP (IN for))))(. ?)) |BT| (BOW (What *)(does *)(S.O.S. *)(stand *)(for *)(? *)) |BT| (BOP (WP *)(AUX *)(NNP *)(VB *)(IN *)(. *))","|BT| (PAS (ARG0 (R-A1 (What *)))(ARG1 (A1 (S.O.S. NNP)))(ARG2 (rel stand)))","|ET| 1:1 21:2.742439465642236E-4 23:1 30:1 36:1 39:1 41:1 46:1 49:1 66:1 152:1 274:1 333:1","|BV| 2:1 21:1.4421347148614654E-4 23:1 31:1 36:1 39:1 41:1 46:1 49:1 52:1 66:1 152:1 246:1 333:1 392:1 |EV|"]},{"title":"Basic Commands Training and classification ./svm_learn -t 5 -C T train.dat model ./svm_classify test.dat model Learning with a vector sequence ./svm_learn -t 5 -C V train.dat model Learning with the sum of vector and kernel sequences ./svm_learn -t 5 -C + train.dat model","paragraphs":["51"]},{"title":"Part II: Kernel Methods for Practical Applications Kernel Engineering approaches Basic Combinations Canonical Mappings, e.g. object transformations Merging of Kernels","paragraphs":["52"]},{"title":"Kernel Combinations an example Kernel Combinations:","paragraphs":["3 3 3 3 33 , , pTree pTree PTree p p Tree Tree PTree pTreePTreepTreePTree KK KK K K K K K K KKKKKK × × =+×= ×=+×= ×+ ×+ γ γ kernel Tree featuresflat of kernel polynomial 3 Tree p K K"]},{"title":"Object Transformation [Moschitti et al, CLJ 2008] Canonical Mapping, φ","paragraphs":["M"]},{"title":"() ","paragraphs":["object transformation,","e. g. a syntactic parse tree into a verb subcategorization frame tree. "]},{"title":"Feature Extraction, φ","paragraphs":["E"]},{"title":"() ","paragraphs":["maps the canonical structure in all its fragments different fragment spaces, e. g. ST, SST and PT. "]},{"title":"),()()( ))(())(()()(),(","paragraphs":["2121 212121"]},{"title":"SSKSS OOOOOOK","paragraphs":["EEE MEME"]},{"title":"=⋅= ⋅=⋅= φφ φφφφφφ","paragraphs":["53"]},{"title":"Predicate Argument Classification In an event: ","paragraphs":["target words describe relation among different entities","the participants are often seen as predicate's arguments. "]},{"title":"Example:","paragraphs":["Paul gives a talk in Rome"]},{"title":"Predicate Argument Classification In an event: ","paragraphs":["target words describe relation among different entities","the participants are often seen as predicate's arguments. "]},{"title":"Example:","paragraphs":["[ Arg0 Paul] [ predicate gives ] [ Arg1 a talk] [ ArgM in Rome] 54"]},{"title":"Predicate-Argument Feature Representation","paragraphs":["Given a sentence, a predicate p: 1. Derive the sentence parse tree","2. For each node pair <Np,Nx> a. Extract a feature representation set","F","b. If Nx exactly covers the Arg-i, F is one of its positive examples c. F is a negative example otherwise"]},{"title":"Vector Representation for the linear kernel","paragraphs":["Phrase Type Predicate Word Head Word Parse Tree Path Voice Active Position Right 55"]},{"title":"Kernel Engineering: Tree Tailoring PAT Kernel [Moschitti, ACL 2004]","paragraphs":["S N NP D N","VP  V Paul in delivers a talk PP IN NP jj Fv,arg.0 formal N style Arg. 0","a) S N NP D N","VP  V Paul in delivers a talk PP IN NP jj formal N style Fv,arg.1 b) S N NP D N","VP  V Paul in delivers a talk PP IN NP jj formal N style Arg. 1 Fv,arg.M c) Arg.M These are Semantic Structures Given the sentence: [ Arg0 Paul] [ predicate delivers] [ Arg1 a talk] [ ArgM in formal Style] 56"]},{"title":"In other words we consider...","paragraphs":["NP D N VP V delivers a talk S N Paul in PP IN NP jj formal N style Arg. 1"]},{"title":"Sub-Categorization Kernel (SCF) [Moschitti, ACL 2004]","paragraphs":["S N NP D N VP V Paul in delivers a talk PP IN NP jj formal N style Arg. 1 Arg. M Arg. 0 Predicate 57"]},{"title":"Experiments on Gold Standard Trees PropBank and PennTree bank ","paragraphs":["about 53,700 sentences Sections from 2 to 21 train., 23 test., 1 and 22 dev. Arguments from Arg0 to Arg5, ArgA and ArgM for a total of 122,774 and 7,359 "]},{"title":"FrameNet and Collins’ automatic trees ","paragraphs":["24,558 sentences from the 40 frames of Senseval 3 18 roles (same names are mapped together) Only verbs 70% for training and 30% for testing"]},{"title":"Argument Classification with Poly Kernel","paragraphs":["58"]},{"title":"PropBank Results Argument Classification on PAT using different Tree Fragment Extractor","paragraphs":["0.75 0.78 0.80 0.83 0.85 0.88","0 10 20 30 40 50 60 70 80 90 100 % Training Data","Accuracy --- ST SST Linear PT 59"]},{"title":"FrameNet Results ProbBank arguments vs. Semantic Roles Kernel Engineering: Node marking","paragraphs":["60"]},{"title":"Marking Boundary nodes Node Marking Effect","paragraphs":["61"]},{"title":"Different tailoring and marking CMST MMST Experiments PropBank and PennTree bank ","paragraphs":["about 53,700 sentences Charniak trees from CoNLL 2005 "]},{"title":"Boundary detection: ","paragraphs":["Section 2 training Section 24 testing PAF and MPAF 62"]},{"title":"Number of examples/nodes of Section 2 Predicate Argument Feature (PAF) vs. Marked PAF (MPAF) [Moschitti et al, ACL-ws-2005]","paragraphs":["63"]},{"title":"Merging of Kernels [ECIR 2007]: Question/Answer Classification Syntactic/Semantic Tree Kernel Kernel Combinations Experiments Merging of Kernels [Bloehdorn & Moschitti, ECIR 2007 & CIKM 2007]","paragraphs":["64"]},{"title":"Merging of Kernels","paragraphs":["NP D N VP V gives a talk N good NP D N VP V gives a talk N solid"]},{"title":"Delta Evaluation is very simple","paragraphs":["65"]},{"title":"Question Classification Definition","paragraphs":[": What does HTML stand for?","Description: What's the final line in the Edgar Allan Poe poem \"The Raven\"? Entity: What foods can cause allergic reaction in people? Human: Who won the Nobel Peace Prize in 1992? Location: Where is the Statue of Liberty? Manner: How did Bob Marley die? Numeric: When was Martin Luther King Jr. born? Organization: What company makes Bentley cars?"]},{"title":"Question Classifier based on Tree Kernels ","paragraphs":["Question dataset (http://l2r.cs.uiuc.edu/~cogcomp/Data/QA/QC/) [Lin and Roth, 2005]) Distributed on 6 categories: Abbreviations, Descriptions, Entity,","Human, Location, and Numeric. Fixed split 5500 training and 500 test questions Cross-validation (10-folds)","Using the whole question parse trees Constituent parsing Example “What is an offer of direct stock purchase plan ?” 66"]},{"title":"Kernels BOW, POS are obtained with a simple tree, e.g. PT (parse tree) PAS (predicate argument structure)","paragraphs":["... BOX is What an offer an * * * * * 67"]},{"title":"Question classification Similarity based on WordNet","paragraphs":["68"]},{"title":"Question Classification with S/STK Multiple Kernel Combinations","paragraphs":["69"]},{"title":"TASK: Question/Answer Classification [Moschitti, CIKM 2008] The classifier detects if a pair (question and answer) is correct or not A representation for the pair is needed The classifier can be used to re-rank the output of a basic QA system Dataset 2: TREC data 138 TREC 2001 test questions labeled as “description” 2,256 sentences, extracted from the best ranked paragraphs (using a basic QA system based on Lucene search engine on TREC dataset) 216 of which labeled as correct by one annotator","paragraphs":["70"]},{"title":"Dataset 2: TREC data 138 TREC 2001 test questions labeled as “description” 2,256 sentences, extracted from the best ranked paragraphs (using a basic QA system based on Lucene search engine on TREC dataset) 216 of which labeled as correct by one annotator A question is linked to many answers: all its derived pairs cannot be shared by training and test sets Bags of words (BOW) and POS-tags (POS) To save time, apply STK to these trees:","paragraphs":["... BOX is What an offer of * * * * * ... BOX VBZ WHNP DT NN IN * * * * * 71"]},{"title":"Word and POS Sequences What is an offer of...? (word sequence, WSK)  What_is_offer  What_is WHNP VBZ DT NN IN...(POS sequence, POSSK)  WHNP_VBZ_NN  WHNP_NN_IN Syntactic Parse Trees (PT)","paragraphs":["72"]},{"title":"Predicate Argument Structure for Partial Tree Kernel (PASPTK) ","paragraphs":["[ARG1 Antigens] were [AM−TMP originally] [rel defined] [ARG2 as nonself molecules].","[ARG0 Researchers] [rel describe] [ARG1 antigens][ARG2 as foreign molecules] [ARGM−LOC in the body]"]},{"title":"Kernels and Combinations Exploiting the property: k(x,z) = k","paragraphs":["1"]},{"title":"(x,z)+k","paragraphs":["2"]},{"title":"(x,z) BOW, POS, WSK, POSSK, PT, PAS","paragraphs":["PTK"]},{"title":"⇒ BOW+POS, BOW+PT, PT+POS, ...","paragraphs":["73"]},{"title":"Results on TREC Data (5 folds cross validation)","paragraphs":["20 22 24 26 28 30 32 34 36 38 40","BOW POS POS_SK","WSK PT PAS_SSTK PAS_PTK","BOW+POS BOW+PT  POS_SK+PT  WSK+PT  POS_SK+PT+P AS_SSTK POS_SK+PT+P AS_PTK F1-measure Kernel Type"]},{"title":"Results on TREC Data (5 folds cross validation)","paragraphs":["20 22 24 26 28 30 32 34 36 38 40","BOW POS POS_SK","WSK PT PAS_SSTK PAS_PTK","BOW+POS BOW+PT  POS_SK+PT  WSK+PT  POS_SK+PT+P AS_SSTK POS_SK+PT+P AS_PTK F1-measure Kernel Type 74"]},{"title":"Results on TREC Data (5 folds cross validation)","paragraphs":["20 22 24 26 28 30 32 34 36 38 40","BOW POS POS_SK","WSK PT PAS_SSTK PAS_PTK","BOW+POS BOW+PT  POS_SK+PT  WSK+PT  POS_SK+PT+P AS_SSTK POS_SK+PT+P AS_PTK F1-measure Kernel Type"]},{"title":"Results on TREC Data (5 folds cross validation)","paragraphs":["20 22 24 26 28 30 32 34 36 38 40","BOW POS POS_SK","WSK PT PAS_SSTK PAS_PTK","BOW+POS BOW+PT  POS_SK+PT  WSK+PT  POS_SK+PT+P AS_SSTK POS_SK+PT+P AS_PTK F1-measure Kernel Type 75"]},{"title":"Results on TREC Data (5 folds cross validation)","paragraphs":["20 22 24 26 28 30 32 34 36 38 40","BOW POS POS_SK","WSK PT PAS_SSTK PAS_PTK","BOW+POS BOW+PT  POS_SK+PT  WSK+PT  POS_SK+PT+P AS_SSTK POS_SK+PT+P AS_PTK F1-measure Kernel Type"]},{"title":"Results on TREC Data (5 folds cross validation)","paragraphs":["20 22 24 26 28 30 32 34 36 38 40","BOW POS POS_SK","WSK PT PAS_SSTK PAS_PTK","BOW+POS BOW+PT  POS_SK+PT  WSK+PT  POS_SK+PT+P AS_SSTK POS_SK+PT+P AS_PTK F1-measure Kernel Type 76"]},{"title":"Results on TREC Data (5 folds cross validation)","paragraphs":["20 22 24 26 28 30 32 34 36 38 40","BOW POS POS_SK","WSK PT PAS_SSTK PAS_PTK","BOW+POS BOW+PT  POS_SK+PT  WSK+PT  POS_SK+PT+P AS_SSTK POS_SK+PT+P AS_PTK F1-measure Kernel Type"]},{"title":"BOW ≈ 24 POSSK+STK+PAS_PTK≈ 39 ⇒62 % of improvement Kernels for Re-ranking","paragraphs":["77"]},{"title":"Re-ranking Framework Local classifier generates the most likely set of hypotheses. These are used to build annotation pairs, . ","paragraphs":["positive instances if hi more correct than hj",", "]},{"title":"A binary classifier decides if h","paragraphs":["i"]},{"title":"is more accurate than h","paragraphs":["j"]},{"title":". Each candidate annotation h","paragraphs":["i"]},{"title":"is described by a structural representation € h","paragraphs":["i"]},{"title":", h","paragraphs":["j"]},{"title":"Re-ranking framework","paragraphs":["Local Model 78"]},{"title":"Syntactic Parsing Re-ranking Pairs of parse trees (Collins and Duffy, 2002) Re-ranking concept labeling [Dinarelli et al, 2009] I have a problem with my monitor hi: I NULL have NULL a NULL problem PROBLEM-B with NULL my NULL monitor HW-B hj: I NULL have NULL a NULL problem HW-B with NULL my NULL monitor","paragraphs":["79"]},{"title":"Flat tree representation (cross-language structure) Multilevel Tree","paragraphs":["80"]},{"title":"Enriched Multilevel Tree FST CER from 23.2 to 16.01 Re-ranking for Named-Entity Recognition [Vien et al, 2010] CRF F1 from 84.86 to 88.16","paragraphs":["81"]},{"title":"Re-ranking Predicate Argument Structures [Moschitti et al, CoNLL 2006] SVMs F1 from 75.89 to 77.25 Conclusions ","paragraphs":["Kernel methods and SVMs are useful tools to design language applications Kernel design still requires some level of expertise","Engineering approaches to tree kernels Basic Combinations Canonical Mappings, e.g.","Node Marking Merging of kernels in more complex kernels","Easy modeling produces state-of-the-art accuracy in many tasks, RTE, SRL, QC, NER, RE SVM-Light-TK efficient tool to use them 82"]},{"title":"Future (on going work) ","paragraphs":["Once we have found the right kernel, are we satisfied? What about knowing the most relevant features? Can we speed up learning/classification at real-application scenario level?","The answer is reverse kernel engineering: [Pighin&Moschitti, CoNLL2009, EMNLP2009, CoNLL2010] Mine the most relevant fragments according to SVMs gradient Use the linear space Software for reverse kernel engineering available in the next months"]},{"title":"Thank you","paragraphs":["83"]},{"title":"References ","paragraphs":["Alessandro Moschitti and Silvia Quarteroni, Linguistic Kernels for Answer Re-ranking in Question Answering Systems, Information and Processing Management, ELSEVIER, 2010.","Yashar Mehdad, Alessandro Moschitti and Fabio Massimo Zanzotto. Syntactic/ Semantic Structures for Textual Entailment Recognition. Human Language Technology - North American chapter of the Association for Computational Linguistics (HLT-NAACL), 2010, Los Angeles, Calfornia.","Daniele Pighin and Alessandro Moschitti. On Reverse Feature Engineering of Syntactic Tree Kernels. In Proceedings of the 2010 Conference on Natural Language Learning, Upsala, Sweden, July 2010. Association for Computational Linguistics.","Thi Truc Vien Nguyen, Alessandro Moschitti and Giuseppe Riccardi. Kernel-based Reranking for Entity Extraction. In proceedings of the 23rd","International Conference on Computational Linguistics (COLING), August 2010, Beijing, China."]},{"title":"References ","paragraphs":["Alessandro Moschitti. Syntactic and semantic kernels for short text pair categorization. In Proceedings of the 12th Conference of the European Chapter of the ACL (EACL 2009), pages 576–584, Athens, Greece, March 2009. Association for Computational Linguistics.","Truc-Vien Nguyen, Alessandro Moschitti, and Giuseppe Riccardi. Convolution kernels on constituent, dependency and sequential structures for relation extraction. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 1378–1387, Singapore, August 2009. Association for Computational Linguistics.","Marco Dinarelli, Alessandro Moschitti, and Giuseppe Riccardi. Re-ranking models based-on small training data for spoken language understanding. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 1076–1085, Singapore, August 2009. Association for Computational Linguistics.","Alessandra Giordani and Alessandro Moschitti. Syntactic Structural Kernels for Natural Language Interfaces to Databases. In ECML/PKDD, pages 391–406, Bled, Slovenia, 2009. 84"]},{"title":"References ","paragraphs":["Alessandro Moschitti, Daniele Pighin and Roberto Basili. Tree Kernels for Semantic Role Labeling, Special Issue on Semantic Role Labeling, Computational Linguistics Journal. March 2008.","Fabio Massimo Zanzotto, Marco Pennacchiotti and Alessandro Moschitti, A Machine Learning Approach to Textual Entailment Recognition, Special Issue on Textual Entailment Recognition, Natural Language Engineering, Cambridge University Press., 2008","Mona Diab, Alessandro Moschitti, Daniele Pighin, Semantic Role Labeling Systems for Arabic Language using Kernel Methods. In proceedings of the 46th Conference of the Association for Computational Linguistics (ACL'08). Main Paper Section. Columbus, OH, USA, June 2008.","Alessandro Moschitti, Silvia Quarteroni, Kernels on Linguistic Structures for Answer Extraction. In proceedings of the 46th Conference of the Association for Computational Linguistics (ACL'08). Short Paper Section. Columbus, OH, USA, June 2008."]},{"title":"References ","paragraphs":["Yannick Versley, Simone Ponzetto, Massimo Poesio, Vladimir Eidelman, Alan Jern, Jason Smith, Xiaofeng Yang and Alessandro Moschitti, BART: A Modular Toolkit for Coreference Resolution, In Proceedings of the Conference on Language Resources and Evaluation, Marrakech, Marocco, 2008.","Alessandro Moschitti, Kernel Methods, Syntax and Semantics for Relational Text Categorization. In proceeding of ACM 17th Conference on Information and Knowledge Management (CIKM). Napa Valley, California, 2008.","Bonaventura Coppola, Alessandro Moschitti, and Giuseppe Riccardi. Shallow semantic parsing for spoken language understanding. In Proceedings of HLT-NAACL Short Papers, pages 85–88, Boulder, Colorado, June 2009. Association for Computational Linguistics.","Alessandro Moschitti and Fabio Massimo Zanzotto, Fast and Effective Kernels for Relational Learning from Texts, Proceedings of The 24th Annual International Conference on Machine Learning (ICML 2007). 85"]},{"title":"References ","paragraphs":["Alessandro Moschitti, Silvia Quarteroni, Roberto Basili and Suresh Manandhar, Exploiting Syntactic and Shallow Semantic Kernels for Question/Answer Classification, Proceedings of the 45th Conference of the Association for Computational Linguistics (ACL), Prague, June 2007.","Alessandro Moschitti and Fabio Massimo Zanzotto, Fast and Effective Kernels for Relational Learning from Texts, Proceedings of The 24th Annual International Conference on Machine Learning (ICML 2007), Corvallis, OR, USA.","Daniele Pighin, Alessandro Moschitti and Roberto Basili, RTV: Tree Kernels for Thematic Role Classification, Proceedings of the 4th International Workshop on Semantic Evaluation (SemEval-4), English Semantic Labeling, Prague, June 2007.","Stephan Bloehdorn and Alessandro Moschitti, Combined Syntactic and Semanitc Kernels for Text Classification, to appear in the 29th European Conference on Information Retrieval (ECIR), April 2007, Rome, Italy.","Fabio Aiolli, Giovanni Da San Martino, Alessandro Sperduti, and Alessandro Moschitti, Efficient Kernel-based Learning for Trees, to appear in the IEEE Symposium on Computational Intelligence and Data Mining (CIDM), Honolulu, Hawaii, 2007"]},{"title":"References ","paragraphs":["Alessandro Moschitti, Silvia Quarteroni, Roberto Basili and Suresh Manandhar, Exploiting Syntactic and Shallow Semantic Kernels for Question/Answer Classification, Proceedings of the 45th Conference of the Association for Computational Linguistics (ACL), Prague, June 2007.","Alessandro Moschitti, Giuseppe Riccardi, Christian Raymond, Spoken Language Understanding with Kernels for Syntactic/Semantic Structures, Proceedings of IEEE Automatic Speech Recognition and Understanding Workshop (ASRU2007), Kyoto, Japan, December 2007","Stephan Bloehdorn and Alessandro Moschitti, Combined Syntactic and Semantic Kernels for Text Classification, to appear in the 29th European Conference on Information Retrieval (ECIR), April 2007, Rome, Italy.","Stephan Bloehdorn, Alessandro Moschitti: Structure and semantics for expressive text kernels. In proceeding of ACM 16th Conference on Information and Knowledge Management (CIKM-short paper) 2007: 861-864, Portugal. 86"]},{"title":"References ","paragraphs":["Fabio Aiolli, Giovanni Da San Martino, Alessandro Sperduti, and Alessandro Moschitti, Efficient Kernel-based Learning for Trees, to appear in the IEEE Symposium on Computational Intelligence and Data Mining (CIDM), Honolulu, Hawaii, 2007.","Alessandro Moschitti, Efficient Convolution Kernels for Dependency and Constituent Syntactic Trees. In Proceedings of the 17th European Conference on Machine Learning, Berlin, Germany, 2006.","Fabio Aiolli, Giovanni Da San Martino, Alessandro Sperduti, and Alessandro Moschitti, Fast On-line Kernel Learning for Trees, International Conference on Data Mining (ICDM) 2006 (short paper).","Stephan Bloehdorn, Roberto Basili, Marco Cammisa, Alessandro Moschitti, Semantic Kernels for Text Classification based on Topological Measures of Feature Similarity. In Proceedings of the 6th IEEE International Conference on Data Mining (ICDM 06), Hong Kong, 18-22 December 2006. (short paper)."]},{"title":"References ","paragraphs":["Roberto Basili, Marco Cammisa and Alessandro Moschitti, A Semantic Kernel to classify texts with very few training examples, in Informatica, an international journal of Computing and Informatics, 2006.","Fabio Massimo Zanzotto and Alessandro Moschitti, Automatic learning of textual entailments with cross-pair similarities. In Proceedings of COLING-ACL, Sydney, Australia, 2006.","Ana-Maria Giuglea and Alessandro Moschitti, Semantic Role Labeling via FrameNet, VerbNet and PropBank. In Proceedings of COLING-ACL, Sydney, Australia, 2006.","Alessandro Moschitti, Making tree kernels practical for natural language learning. In Proceedings of the Eleventh International Conference on European Association for Computational Linguistics, Trento, Italy, 2006.","Alessandro Moschitti, Daniele Pighin and Roberto Basili. Semantic Role Labeling via Tree Kernel joint inference. In Proceedings of the 10th Conference on Computational Natural Language Learning, New York, USA, 2006. 87"]},{"title":"References ","paragraphs":["Roberto Basili, Marco Cammisa and Alessandro Moschitti, Effective use of Wordnet semantics via kernel-based learning. In Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL 2005), Ann Arbor (MI), USA, 2005","Alessandro Moschitti, A study on Convolution Kernel for Shallow Semantic Parsing. In proceedings of the 42-th Conference on Association for Computational Linguistic (ACL-2004), Barcelona, Spain, 2004.","Alessandro Moschitti and Cosmin Adrian Bejan, A Semantic Kernel for Predicate Argument Classification. In proceedings of the Eighth Conference on Computational Natural Language Learning (CoNLL-2004), Boston, MA, USA, 2004."]},{"title":"An introductory book on SVMs, Kernel methods and Text Categorization","paragraphs":["88"]},{"title":"Non-exhaustive reference list from other authors ","paragraphs":["V. Vapnik. The Nature of Statistical Learning Theory. Springer, 1995.","P. Bartlett and J. Shawe-Taylor, 1998. Advances in Kernel Methods - Support Vector Learning, chapter Generalization Performance of Support Vector Machines and other Pattern Classifiers. MIT Press.","David Haussler. 1999. Convolution kernels on discrete structures. Technical report, Dept. of Computer Science, University of California at Santa Cruz.","Lodhi, Huma, Craig Saunders, John Shawe Taylor, Nello Cristianini, and Chris Watkins. Text classification using string kernels. JMLR,2000","Schölkopf, Bernhard and Alexander J. Smola. 2001. Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond. MIT Press, Cambridge, MA, USA."]},{"title":"Non-exhaustive reference list from other authors ","paragraphs":["N. Cristianini and J. Shawe-Taylor, An introduction to support vector machines (and other kernel-based learning methods) Cambridge University Press, 2002","M. Collins and N. Duffy, New ranking algorithms for parsing and tagging: Kernels over discrete structures, and the voted perceptron. In ACL02, 2002.","Hisashi Kashima and Teruo Koyanagi. 2002. Kernels for semi-structured data. In Proceedings of ICML’02.","S.V.N. Vishwanathan and A.J. Smola. Fast kernels on strings and trees. In Proceedings of NIPS, 2002.","Nicola Cancedda, Eric Gaussier, Cyril Goutte, and Jean Michel Renders. 2003. Word sequence kernels. Journal of Machine Learning Research, 3:1059–1082. D. Zelenko, C. Aone, and A. Richardella. Kernel methods for relation extraction. JMLR, 3:1083–1106, 2003. 89"]},{"title":"Non-exhaustive reference list from other authors ","paragraphs":["Taku Kudo and Yuji Matsumoto. 2003. Fast methods for kernel-based text analysis. In Proceedings of ACL’03.","Dell Zhang and Wee Sun Lee. 2003. Question classification using support vector machines. In Proceedings of SIGIR’03, pages 26–32.","Libin Shen, Anoop Sarkar, and Aravind k. Joshi. Using LTAG Based Features in Parse Reranking. In Proceedings of EMNLP’03, 2003","C. Cumby and D. Roth. Kernel Methods for Relational Learning. In Proceedings of ICML 2003, pages 107–114, Washington, DC, USA, 2003.","J. Shawe-Taylor and N. Cristianini. Kernel Methods for Pattern Analysis. Cambridge University Press, 2004.","A. Culotta and J. Sorensen. Dependency tree kernels for relation extraction. In Proceedings of the 42nd","Annual Meeting on ACL, Barcelona, Spain, 2004."]},{"title":"Non-exhaustive reference list from other authors ","paragraphs":["Kristina Toutanova, Penka Markova, and Christopher Manning. The Leaf Path Projection View of Parse Trees: Exploring String Kernels for HPSG Parse Selection. In Proceedings of EMNLP 2004.","Jun Suzuki and Hideki Isozaki. 2005. Sequence and Tree Kernels with Statistical Feature Mining. In Proceedings of NIPS’05.","Taku Kudo, Jun Suzuki, and Hideki Isozaki. 2005. Boosting based parse reranking with subtree features. In Proceedings of ACL’05.","R. C. Bunescu and R. J. Mooney. Subsequence kernels for relation extraction. In Proceedings of NIPS, 2005.","R. C. Bunescu and R. J. Mooney. A shortest path dependency kernel for relation extraction. In Proceedings of EMNLP, pages 724–731, 2005.","S. Zhao and R. Grishman. Extracting relations with integrated information using kernel methods. In Proceedings of the 43rd Meeting of the ACL, pages 419–426, Ann Arbor, Michigan, USA, 2005. 90"]},{"title":"Non-exhaustive reference list from other authors ","paragraphs":["J. Kazama and K. Torisawa. Speeding up Training with Tree Kernels for Node Relation Labeling. In Proceedings of EMNLP 2005, pages 137– 144, Toronto, Canada, 2005.","M. Zhang, J. Zhang, J. Su, , and G. Zhou. A composite kernel to extract relations between entities with both flat and structured features. In Proceedings of COLING-ACL 2006, pages 825–832, 2006.","M. Zhang, G. Zhou, and A. Aw. Exploring syntactic structured features over parse trees for relation extraction using kernel methods. Information Processing and Management, 44(2):825–832, 2006.","G. Zhou, M. Zhang, D. Ji, and Q. Zhu. Tree kernel-based relation extraction with context-sensitive structured parse tree information. In Proceedings of EMNLP-CoNLL 2007, pages 728–736, 2007."]},{"title":"Non-exhaustive reference list from other authors ","paragraphs":["Ivan Titov and James Henderson. Porting statistical parsers with data-defined kernels. In Proceedings of CoNLL-X, 2006","Min Zhang, Jie Zhang, and Jian Su. 2006. Exploring Syntactic Features for Relation Extraction using a Convolution tree kernel. In Proceedings of NAACL.","M. Wang. A re-examination of dependency path kernels for relation extraction. In Proceedings of the 3rd International Joint Conference on Natural Language Processing-IJCNLP, 2008. 91"]}]}