{"sections":[{"title":"20","paragraphs":["1965 International Conference on Computational Linguistics ENDOCENTRIC CONSTRUCTIONS AND TIlE COCKE PARSING LOGIC Jane J. Robinson The RAND Corporation 1700 Main Street Santa Monica D California 90406","h.- Uo/~ x I _~, ..~.' ~ '%. \"~,.\\ Robinson 2 ABSTRACT Automatic syntactic analysis is simplified by disengaging the grammatical rules, by means of a parsing logic, from the computer routines that apply them. A case in point is the John Cocke logic. It iterates on five simple parameters and finds all structures permitted by the grammar, thus testing the rules, which can then be changed without changing the routines. The rules themselves need not be ordered so far as the logic of the system is concerned. However, in operating with an IC grammar, rules for bracketing endocentric constructions must be made quite complex merely to avoid multiple analyses of unambiguous or trivially ambiguous expressions. The rules can be simplified if they are classified and if the system is provided with an additional capability for applying them in a specified order. Although an additional parameter is introduced into the system, the disengagement of grammar from routine is preserved. The additional parameter controls the direction, left-to-right or right-to-left, in which constructions are put together. The decision as to which direction should be specified is a grammatical decision, and is related to Yngve's hypothesis of asymmetry in language. It does not affect the opera-tion of the parsing logic. Robinson 3 ACKNOWLEDGMENTS I wish to acknowledge the assistance of M. Kay and S. Marks in discussing points raised in the paper and in preparing the flowchart. A more general acknowledgment is due to D. G. Hays, who first called my attention to the problem of orderin~ the attachment of elements. Robinson 4","ENDOCENTRIC CONSTRUCTIONS AND THE COCKE PARSING LOGIC Automatic sentence structure determination (SSD) is greatly simplified if, through the intervention of a parsing logic, the grammatical rules that determine the structure are partially disengaged from the computer routines that apply to them. Some earlier parsing programs analyzed sentences by routines that branched according to the grammatical properties or signals encountered at particular points in the sentence, making the routines themselves serve as the rules. This not only required separate programs for each language, but led to extreme proliferation in the routines, requiring extensive rewriting and debu~gin~ with every discovery and incorporation of a new ~rammatical feature. More recently, programs for SSD have employed generalized parsing logics, applicable to different languages and providing primarily for an exhaustive and systematic application of a set of rules. (1,2,5,5) The rules themselves can be changed without changing the routines that apply them, and the routines"]},{"title":"consequently","paragraphs":["take fuller advantage of the speed with which digital"]},{"title":"computers","paragraphs":["can repeat the same sequence of instructions over and over again, changing only the values of some parameters at each cycle. Robinson S The case in point is the parsing logic (PL) devised by John Cocke in 1960, for applying the rules of a contextfree phrase structure grammar (PSG), requiring"]},{"title":"that","paragraphs":["each structure recognized by the grammar be analyzed into two and only two immediate"]},{"title":"constituents.(I)","paragraphs":["Although all PSGs appear to be inadequate in some important respects to the task of handling natural language,"]},{"title":"they","paragraphs":["still form the base of the more powerful transformational grammars, which are not yet"]},{"title":"automated","paragraphs":["for SSD. Moreover, even their severest critic acknowledges"]},{"title":"that","paragraphs":["\"The PSG conception of grammar...is a quite reasonable theory of natural language which unquestionably formalizes many actual properties of human language.\"(6,P \"78) Both"]},{"title":"theoretically","paragraphs":["and empirically the development and"]},{"title":"automatic","paragraphs":["application of PSGs are of interest to linguists. The PSG on which the Cocke Pl, operates is essentially a table of constructions. Its rules have three entries, one for the code (a descriptor) of the construction, the other two specifying the codes of the ordered pair of immediate constituents out of which it may be formed. The logic iterates in five nested loops, controlled by three simple parameters and two codes supplied by the grammar. They are: i) the string length, starting with length 2, of the segment being tested for constructional Robinson 6 status; 2) the position of the first word in the tested string; 3) the length of the first constituent; 4) the codes of the first constituent; and 5) the codes of the second constituent. After a dictionary lookup routine has assi~.ned grammar codes to all the occurrences in the sentence or total string to be parsed (it need not be a sentence), the PL operates to offer the codes of pairs of adjacent segments to a parsing routine that tests their connectability by looking them up in the stored table of constructions, i.e., in the grammar. If the ordered pair is matched by a pair of ICs in the table, tile code of the construction formed by the ICs is added to the list of codes to be offered for testin~ when iterations are performed on longer strings. In the RAND program for parsing English, the routines produce a labeled binary-branching tree for every complete structural analysis. There will be one tree if the grammar recognizes the string as well-formed and syntactically unambiguous; more than one if it is recognized as ambiguous. Even if no complete analysis is made of the whole string, a resum~ lists all constructions found in the process, including those which failed of inclusion in larger constructions. (8,9)","*This interaction between a PL and a routine for testing the connectability of two items is described in somewhat greater detail in Hays (2). Rob ins on 7 Besides simplifying the problem of revising the grammar by separating it from the problem of application to sentences, the PL, because it leads to an exhaustive application of the rules, permits a rigorous evaluation of the grammar's ability to assign structures to sentences and also reveals many unsuspected yet legitimate ambiguities in those sentences.(4, 7) But because of the difficulties in-herent in specifying a sufficiently discriminatory set of rules for sentences of any natural language and because of the very many syntactic ambiguities, resolvable only through lar~er context, this method of parsing produces a long list of intermediate constructions for sentences of even modest length, and this in turn raises a storage prob lem. By way of illustration, consider a string of four occurrences, x I x 2 x 3 x4, a dictionary that assigns a single grammar code to each, and a grammar that assigns a unique construction code to every different combination of adjacent segments. Given such a grammar, as in Table I, the steps in its application to the string by the parsing routines operating with the Cocke PL are represented in Table II. (The preliminary dictionary lookup assigning the original codes to the occurrences is treated as equivalent to iterating with the parameter for string length set to I). Robinson 8 Table I Rule"]},{"title":"# ICl IC2 CO","paragraphs":["I. A B E 2o B C F 3. c D o 4. A F H 5. E C I 6. B G J"]},{"title":"7. F D I<","paragraphs":["ICl: IC2: code of first constituent code of second constituent Rule # iCl 102 CO 8. A J L 9. A K M lO. E G N ii. H D 12. I D P 13. A C Q 14. etc. CC: code of construction Table II","Steps ¢i ~ M W P C(P) C(Q) C(M) Rule ~ Combined Structure Assigned I. i I i A A 2. 1 2 i B B"]},{"title":"3. I 3 ] c c","paragraphs":["4. 1 4 1 D D Dictionary x I lookup x 2 assio~ning x 3 codes to: x 4 5. ? 6. 2 7. 2 8. 3 9. 3 I0. x"]},{"title":"ll. 3","paragraphs":["12. 4"]},{"title":"13. 4","paragraphs":["14. 4 15. 4 16. 4"]},{"title":"#:","paragraphs":["P:"]},{"title":"c(P).","paragraphs":["i 1 2 i 3 1 1 1 ! 2 2 I 2 1 1 A I I ,'. I o E i 3 H 1 3 i A R B C F C D G A F H ~ C I B G J F D K J L i,; M O >T D i' stel; number strinj ].en~Tth of segment It 3. o"]},{"title":"5. 6.","paragraphs":["7. ~o 9. ].0. Ii. 12. lenr'th of first construction s t,-ing code of first construction 1+2"]},{"title":"(Xl+X 2 ) 2+3 (x2~x 3) ~+4 (x3+x 4) 5+3 < (Xl+~2)~,\" 3 )","paragraphs":["• 2+7"]},{"title":"(x2(x3+x 4)","paragraphs":[") / X"]},{"title":"6~ (~x2+. 3)x4) c(M).","paragraphs":["i+i0 !+ll"]},{"title":"5+7","paragraphs":["8+4 9+4"]},{"title":"(x1(x2(x3+x4))) (Xl((x2+x3)%)) ( (:c I ,x~) (~ 3÷~4) ) ( (xj (x2+x 5) )xg) (((x l+x2)x ~)xL~)","paragraphs":["code of second const, string code for string, to be stored when C(P) and C(Q) are matched in the o~r_ammar. C(M) = CC of crammar. The boxed section represents the PL iterations. Robinson 9 With such a grammar, the number of constructions to be stored and processed through each cycle increases in proportion to the cube of the number of words in the sentence. If the dictionary and grammar assign more than one code to occurrences and constructions, the number may grow multiplicatively, making the storage problem still more acute. For example, if x I were assigned two codes instead of one, additional steps would be required for every string in which x I was an element and iteration on string length 4 would require twice as many cycles and twice as much storage. Of course, reasonable grammars do not provide for combining every possible pair of adjacent segments into a construction, and in actual practice the growth of the construction list is reduced by failure to find the two codes presented by the PL, when the grammar is consulted. If Rule i is omitted from the grammar in Table I, then steps S, 9, 14, and 16 will disappear from Table II and both storage requirements and processing time will be cut down. Increasing the discriminatory power of the grammar through refining the codes so that the first occurrence must belong to class Aa and the second to class Bb in order to form a construction provides this limiting effect in essentially the same way. Robinson I0 Another way o£ limiting the growth o£ the stored constructions is to take advantage of the fact that in actual grammars two or more different pairs of constituents sometimes combine to produce the \"same\" construction. Assume that A and F (Table I) combine to form a construction whose syntactic properties are the same, at least within the discriminatory powers of the grammar, as those of the construction formed by E and C. Then Rules 4 and S can assign the same code, }l, to their constructions. In consequence, at both steps 8 and 9 in the parsing (Table If), |1 will be stored as the construction code C(M) for the string x I x 2 x3, even though two substructures are recorded for it: i.e. (Xl(X 2 + x3) ) and ((x I + x2)x3). The string can be marked as having more than one structure, but in subsequent iterations on string length 4, only one concatenation of the string with x 4 need be made and step 16 can be omitted. When the parsing has terminated, all substructures of completed analyses are recoverable, including those of marked strings. Eliminating duplicate codes for the same string from the cycles of the PL results in dramatic savings in time and storage, partly because the elimination of any step has a cumulative effect, as demonstrated previously. In addition, opportunities to eliminate duplicates arise frequently, in English at least, because of the frequent Rob in s on 11 occurrence o£ endocentric constructions, .constructions whose syntactic properties are largely the same as those o£ one of their elements--the head. In English~ noun phrases are typically endocentric, and when a noun head is flanked by attributives as in a phrase consisting of article, noun, prepositional phrase (A N PP), the requirement that constructions have only two ICs promotes the assignment of two structures, (A(N+PP)) and (~A+N) PP), unless the grammar has been carefully formulated to avoid it. Since NPs of this type are ubiquitous, occurrinp, as subjects, objects of verbs, and objects of prepositions, duplicate codes for them are likely to occur at several points in a sentence. Consideration of endocentric constructions, however, raises other questions, some theoretical and some practical, suggesting modification of the grammar and the parsing routines in order to represent the language more accurately or in order to save storage, or both. Theoretically, the problem is the overstructuring of noun phrases by the insistence on two ICs and the doubtful propriety of permitting more than one way of structuring them. Practically, the problem is the elimination of duplicate construction codes stored for endocentric phrases when the codes are repeated for different string lengths. Robinson 12 Consider the noun phrase subject in All the old men on the corner sta.red. Its syntactic properties are essentially the same as that of men. But fifteen other phrases, all made up from the same elements but varying in length, also have the same properties. They are shown below: Table III Length I. 7 2. 6 5. 6 4. 6 5. 5 6. 5 7. 5. 8. 4 9. 4 i0. 3 ii. 3 12. 3 13. 2 14. 2 15. 2 16. 1 Noun phrase","All the old men on the corner","The old men on the corner","All the men on the corner","All old men on the corner","Old men on the corner","The men on the corner","All men on the corner","Men on the corner","All the old men","The old men","All the men","All old men","Old men","The men","All men","Men (stared) A reasonably good grammar should provide for the recognition of all sixteen phrases. This is not to say that sixteen separate rules are required, although this would be one way of doing it. Minimally, the grammar must provide two rules for an endocentric NP, one to combine the head noun or the string containing it with a preceding attributive and another to combine it with a following Robinson 13 attributive. The codes for all the resulting constructions may be the same, but even so, the longest phrase will receive four different structural assignments or bracketings as its adjacent elements are gathered together in pairs;"]},{"title":"(all (the (old (men (on the corner))))) (all (the ((old men) (on the corner)))) (all ((the (old men)) (on the corner))) and ((all (the (old men))) (on the corner)) If it is assumed that the same code, say that","paragraphs":["of"]},{"title":"a plural NP, has been assigned at each string length, it is true that only one additional step is needed to concatenate","paragraphs":["the string with the following verb when the PL iteration is performed for string length 8. But meanwhile a number of intermediate codes have been stored during iterations on string lengths 5, 6, and 7 as the position of the first word of the tested string was advanced, so"]},{"title":"that","paragraphs":["the list also contains codes for:","men on the corner stared (length 5) old men on the corner stared (length 6)","and the old men on the corner stared (length 7) Again, the codes may be the same, but duplicate codes will not be eliminated from processing if"]},{"title":"they","paragraphs":["are associated with different strings, and strings of different length are treated as wholly different by the PL, regardless of over-lap. If this kind of duplication is to be reduced or name"]},{"title":"ly:","paragraphs":["Robinson 14 avoided, a different procedure is required from that available for the case of simple duplication over the same string. But first a theoretical question must be decided. Is the noun phrase, as exemplified above, perhaps really four-ways ambiguous and do the four different bracketings correlate systematically with four distinct interpretations or assignments of semantic structure? (Cf\" 4,7) And if so, is it desirable to eliminate them? It is possible to argue that some of the different bracketings do correspond to different meanings or emphases, or--in earlier transformational terms--to different orderings in the embeddings of the men were old and the men were on the corner into all the men stared. Admittedly the native speaker can indicate contrasts in meaning by his intonation, emphasizing in one reading that all the men stared and in another that it was all the ol___dd men who stared; and the writer can resort to italics. But it seems reasonable to assume that there is a normal intonation for the unmarked and unemphatic phrase and that its interpretation is structurally unambiguous. In the absence of italics and other indications, it seems ~_~_reasonable to produce four different bracketings at every encounter with an NP of the kind exemplified. Robinson 15 One way to reduce the duplication is to write the grammar codes so that, with the addition of each possible element, the noun head is assigned a different construction code whose distribution as a constituent in larger constructions is carefully limited. For the sake of simplicity, assume that the elements of NPs have codes that reflect, in part, their ordering within the phrase and that the NP codes themselves reflect the properties of the noun head in first position and are subsequently differentiated by codes in later positions that correspond to those of the attributes. Let the codes for the elements be 1 (all), 2 (the), 3 (old), 4 (men), 5 (on the corner). Rules may be written to restrict the combinations, as follows: Robinson 16 Tab le IV R# ICI IC2 CC i, 1+4 ÷41 2. 2+4 ÷42 3. 3+4 ÷43 4. 4+5 ÷45 5. i + 42 ÷ 412 6. 1 + 43 ÷ 413 7. 2 + 43 ÷ 423 8. I + 423 ÷ 4123 9. 1 + 45 ÷ 41S 10. 2 ÷ 45 ÷ 42S II. 3 + 45 ÷ 435 12. 2 + 435 ÷ 4235 13. 1 ÷ 4235 ÷ 41235 (all men) (the men) (old men) (men on the corner) (all the men) (all old men) (the old men) (all the old men) (all men on the corner); but not \"41 + S ÷ 415 (the men on the corner); but not *42 + 5 ÷ 425 (old men on the corner); but not *43 + 5 ÷ 435 (the old men on the corner); but not *423 + 5 ÷ 4235 (all the old men on the corner); but not \"4123 + 5 ÷ 41235 With these rules, the Rrammar provides for only one structural assignment to the string: (all (the (old (men + on the corner)))). This method has the advantage of acknowledging the general endocentricity of the NP while allowing for its limitations, so that where the subtler differences among NPs are not relevant, they can be ignored by ignoring certain positions of the codes, and where they are relevant, the full codes are available. The method should lend Robinson 17 itself quite well to code matching routines for connectability. However, if carried out fully and consistently, it greatly increases the length and complexity of both the codes and the rules, and this may also be a source of problems in storage and processing time. (cf. Flays, 2) Another method is to make use of a classification of the rules themselves. Since the lowest loop of the PL (see Fig. I) iterates on the codes of the second constituents, the rules against which the paired strings are tested are stored as ordered by first IC codes and subordered by second IC codes. If the iterations of the logic were differently ordered, the rules would also be differently ordered, for efficiency in testing. In other words, the code of one constituent in the test locates a block of rules within which matches for all the codes of the other constituent are to be sought; but the hierarchy of ordering by one constituent or the other is a matter of choice so long as it is the same for the PL and for storing the table of rules that constitute the grammar. In writing and revising the rules, however~ it proves humanly easier if they are grouped according to construction types. Accordingly, all endocentric NPs in the RAND grammar are given rule identification tags with an A in first position. Within this grouping, it is natural to subclass the rules according to whether they attach attributives on the right Robinson 18 or on the left of the noun head. If properly formalized, this practice can lead to a reduction in the multiple analyses of NPs with fewer rules and simpler codes than those of the previous method. As applied to the example, the thirteen rules and five-place codes of Table IV can be reduced to two rules with one-place codes and an additional feature in the rule identification tag. *AI The rules can be written as: 1 N N 2 3 $A2 N 4 N Although the construction codes are less finely differentiated, the analysis of the example will still be unique, and the number of abortive intermediate constructions will be reduced. To achieve this effect, the connectability test routine must include a comparison of the rule tag associated with each C(P) and the rule tags of the grammar. If a rule of type *A is associated with the C(P), that is, if an *A rule assigned the construction code to the string P which is now being tested as a possible first constituent, then no rule of type $A can be used in the current test. For all such rules, there will be an automatic \"no match\" without checking the second constituent codes. (See Fig. I.) As a consequence of this restriction, in Robinson 19 the final analysis, the noun head will have been combined with all attributives on the right before acquiring any on the left. To be sure, the resume of intermediate constructions will contain codes for ol___dd men, the old men, and all the ol__.dd me__n_n , produced in the course of iterations on string lengths 2, 3, and 4, but only one structure is finally assigned to the whole phrase and the intermediate duplications of codes for strings of increasing length will be fewer because of the hiatus at string length 5. Of course, in the larger constructions in which the NP participates, the reduction in the number of stored intermediate constructions will be even greater. Provisions may be made in the rules for attaching still other attributives to the head of the NP without great increase in complexity of rules or multiplication of structural analyses. Rule $A2, for example, could include provision for attaching a relative clause as well as a prepositional phrase, and while a phrase like the men on the corner who were sad might receive two analyses unless the codes were sufficiently differentiated to prevent the clause from being attached to corner as well as to me___n, at least the further differentiation of the codes need not also be multiplied in order to prevent the multiple analyses arising from endocentricity. Robinson 20 Similarly, for verb phrases where the rule must allow for an indefinite number of adverbial modifiers, a single analysis can be obtained by marking the strings and the rules and forcing a combination in a single direction. In short, although the Cocke PL tends to promote multiple analysis of unambiguous or trivially ambiguous endocentric phrases, at the same time increasing the problem of storing intermediate constructions, the number of analyses can be greatly reduced and the storage problem greatly alleviated if the rules of the grammar recognize endocentricity wherever possible and if they are classified so that rules for endocentric constructions are marked as left (*) or right ($), and their order of application is specified. A final theoretical-practical consideration can at least be touched on, although it is not possible to develop it adequately here. The foregoing description provided for combining a head with its attributives (or dependents) on the right before combining it with those on the left, but either course is possible. Which is preferable depends on the type of construction and on the language generally. If Yngve's hypothesis that languages are essentially asymmetrical, tending toward right-branching constructions to avoid overloading the memory, is correct, then the Robinson 21 requirement to combine first on the right is preferable. (10) This is a purely grammatical consideration, however, and does not affect the procedure sketched above, in principle. For example, consider an endocentric construction of string length 6 with the head at position 3, so that its extension is predominantly to the right, thus: 1 2 (3) 4 5 6. If all combinations were allowed by the rules, there would be thirty-four analyses. If combination is restricted to either direction, left or right, the number of analyses is reduced to eleven. However, if the Cocke PL is used to analyze a left-branching language, making it preferable to specify prior combination on the left, then the order of nesting of the fourth and fifth loops of the PL should be reversed (Fig. I) and the rules of the grammar should be stored in order of their second constituent codes, subordered on those of the first constituents. Robinson 22 N\" P:"]},{"title":"Q: M:","paragraphs":["Fig. I FLOWCHART FOR THE COCKE PL sentence length string length of first constituent string length of second constituent P+Q - string length of construction","W\" L(W)"]},{"title":":","paragraphs":["C(P):"]},{"title":"c (Q)","paragraphs":[": number of first word of"]},{"title":"M","paragraphs":["N-M+I = limit of first word code of first constituent code of second constituent"]},{"title":"l~ M+I÷MI<~ >","paragraphs":["CONSTRUCTION CODE ASSOCIATED WITH M, AND KEEP TRACK INPUT SENTENCE"]},{"title":"OF","paragraphs":["LENGTH N do dictionary lookup associate grammar codes","with words and keep track"]},{"title":"[SET M EQUAL TO 2[","paragraphs":[">"]},{"title":"ISET L(W) EQUAL TO (N-M+I)","paragraphs":["l .... SET W EQUAL TO I"]},{"title":",< ~OMPARE W AND L(W)~ [SET p","paragraphs":["EQUAL iO"]},{"title":"I l ~OMPARE P AND_~M~---- \"] W+: OUTPUT l W+I÷W I no more t","paragraphs":[".... l P+I÷P mo re RESET TO FIRST C(Q), GET"]},{"title":"NEXT C (P) IM-P I COMPARE C(P), C(Q) WITH DE~ IRST IC CODE, SECOND IC CO IN THE GRAMMAR","paragraphs":["match ~no match"]},{"title":"STORE RULE # AND (~EST FOR MORE C(Q) S~ k'- -j","paragraphs":["no more ~ ~ more"]},{"title":"Robinson 23 REFERENCES","paragraphs":[". . . . . . . . . I0.","}lays, D. G., \"Automatic Language-Data Processing,\" Computer Applications in the Behavioral Sciences, Chapter 17, Prentice-Hall, 1962.","}lays, D. G. \"Connectability Calculations, syntactic Functions, and Russian Syntax,\" Mechanical Translatio.___~n, Vol. 8, No. 1 (August 1964)."]},{"title":"Kuno, S., and A. G. Oettinger, \"Multiple-path Syntactic","paragraphs":["Analyzer,\" Mathematical Linguistics and Automatic Translation, Report No. NSF-8, Sec. TT-The Computa-tion Laboratory of }larvard University, 1965.","Kuno, S., and A. G. Oettinger, \"Syntactic Structure and Ambiguity of English,\" AFIPS Conference Proceedings Vol. 24, 1965 Fall Joint--~mputer Con±erence.","National Physical Laboratory, 1961 International Conference o__n_n Machine Trans-T~ion of Languages and Applied Language Analysis, Vol. 2,-[[. M. Stationery Office, 1962.","Postal, P. M. Constituent Structure, Publication Thirty of the Indiana Univers~rch Center in Anthropology, Folklore, and Linguistics, January 1964.","Robinson, J.) \"Automated Grammars as Linguistics Tools,\" (Unpublished), Presented at the Thirty=ninth Annual Meeting of the Linguistic Society of America, New York, December 1964.","Robinson, J., The Automatic Recognition of Phrase Structure and Paraphrase, RM-4005-PR (Abridlged), The RAND Corporation, Santa Monica, December 1964.","Robinson, J., Preliminary Codes and Rules for the Automatic Parsing of English, RM-S339-PR, =The RAN]) Corporation, Santa Mort}ca, December 1962.","Yngve, V. li., \"A Model and an Hypothesis for Language Structure,\" Proceedings of the American Philosophical Society, Vol. 104, No. S--[Oct-~e-r 1960)."]}]}