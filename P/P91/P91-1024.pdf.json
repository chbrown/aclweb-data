{"sections":[{"title":"EXPERIMENTS AND PROSPECTS OF EXAMPLE-BASED MACHINE TRANSLATION","paragraphs":["Eiichiro SUMITA*","and Hitoshi HDA"]},{"title":"ATR Interpreting Telephony Research Laboratories","paragraphs":["Sanpeidani, Inuidani, Seika-cho Souraku-gun, Kyoto 619-02, JAPAN ABSTRACT EBMT (Example-Based Machine Translation) is proposed. EBMT retrieves similar examples (pairs of source phrases, sentences, or texts and their translations) from a d~t.hase of examples, adapting the examples to translate a new input. EBMT has the following features: (1) It is easily upgraded simply by inputting appropriate examples to the database; (2) It assigns a reliability factcr to the translation result; (3) It is acoelerated effectively by both indexing and parallel computing; (4) It is robust because of best-match reasoning; ~d (5) It well utilizes translator expertise. A prototype system has been implemented to deal with a difficult translation problem for conventional Rule-Based Machine Translation (RBMT), i.e., translating Japanese noun phrases of the form \"N~ no N2\" into English. The system has achieved about a 78% success rate on average. This paper explains the basic idea of EBMT, illustrates the experiment in detail, explains the broad applicability of EBMT to several difficult translation problems for RBMT and discusses the advantages of integrating EBMT with RBMT."]},{"title":"1 INTRODUCTION","paragraphs":["Machine Translation requires handcmt~ and complicated large-scale knowledge (Nirenburg 1987). Conventional machine translation systems use rules as the knowledge. This framework is called Rule-Based Machine Translation (RBMT). It is difficult to scale up from a toy program to a practical system because of the problem of building such a lurge-scale rule-base. It is also difficult to improve translation"]},{"title":"performance","paragraphs":["because the effect of adding a new rule is hard to anticipate, and because translation using a large-scule rule-based system is time-consuming. Moreover, it is difficult to make use of situational or domain-specific information for translation. their translations) has been implemented as the knowledge (Nagao 1984; Sumita and Tsutsumi 1988; Sato and Nagao 1989; Sadler 1989a; Sumita et al. 1990a, b). The translation mechanism retrieves similar examples from the database, adapting the examples to Wanslate the new source text. This framework is called Example-Based Machine Translation (EBMT).","This paper focuses on ATR's linguistic database of spoken Japanese with English translations. The corpus contains conversations about international conference registration (Ogura et al. 1989). Results of this study indicate that EBMT is a breakthrough in MT technology.","Our pilot EBMT system translates Japanese noun phrases of the form '~1 x no N2\" into English noun phrases. About a 78% success rate on average has been achieved in the experiment, which i s considered to outperform RBMT. This rate cm be improved as discussed below.","Section 2 explains the basic idea of EBMT. Section 3 discusses the broad applicability of EBMT and the advantages of integrating it with RBMT. Sections 4 and 5 give a rationale for section 3, i.e., section 4 illustrates the experiment of translating noun phrases of the form \"Nt no N2\" in detail, and section 5 studies other phenomena through actual dam from our corpus. Section 6 concludes this paper with detailed comparisons between RBMT and EBMT. 2 BASIC IDEA OF EBMT 2.1 BASIC FLOW","In this section, the basic idea of EBMT, which is general and applicable to many phenomena dealt with by machine translation, is shown.","In order to conquer these problems in machine translation, a database of examples (pairs of source phrases, sentences, or texts and * Currently with Kyoto University","Figure 1 shows the basic flow of EBMT using translation of \"kireru\"[cut/be sharp]. From here on, the literal English translations are bracketed.","(1) and (2) me examples (pairs of Japanese sentences and their English 185 translations) in the database.","Examples similar to the Japanese input sentence are retrieved in the following manner. Syntactically, the input is similar to Japanese sentences (1) and (2). However, semantically, \"kachou\" [chief] is far from \"houchou\" [kitchen knife]. But, \"kachou\" [chief] is semantically similar to \"kanojo\" [she] in that both are people. In other words, the input is similar to example sentence (2). By mimicking the similar example (2), we finally get \"The chief is sharp\".","Although it is possible to obtain the same result by a word selection rule using fme-tuned semantic restriction, note that translation here is obtained by retrieving similar examples to the input. • Example Database","(data for \"kireru'[cut / be sharp]) (1) houchou wa klrsru -> The kitchen knife cuts. (2) kanojo wa kireru -> She Is sharp. • Input","kachouwa klreru o>? • Retrieval of similar examples (Syntax) Input = (1), (2) (Semantics) kachou/== houehou","kachou ,= kanojo (Total) Input == (2) • OUt0Ut -> The chief Is ~ h a r D,","Figure I Mimicking Similar Examples"]},{"title":"2.2 DISTANCE","paragraphs":["Retrieving similar examples to the input is done by measuring the distance of the input to each of examples. The smaller a distance is, the more similar the example is to the input. To define the best distance metric is a problem of EBMT not yet completely solved. However, one possible definition is shown in section 4.2.2.","From similar examples retrieved, EBMT generates the most likely translation with a reliability factor based on distance and frequency. If there is no similar example within the given threshold, EBMT tells the user that it cannot translate the input."]},{"title":"3 BROAD APPLICABILITY AND INTEGRATION 3.1 BROAD APPLICABILITY","paragraphs":["EBMT is applicable to many linguistic phenomena that are regarded as difficult to translate in conventional RBMT. Some are well-known among researchers of natural language processing and others have recently been given a great deal of attention.","When one of the following conditions holds","true for a linguistic phenomenon, RBMT is less","suitable than EBMT.","(Ca) Translation rule formation is difficult.","(Cb) The general rule cannot accurately describe phenomena because it represents a special case, e.g., idioms.","(Cc) Translation cannot be made in a compositional way from target words (Nagao 1984; Nitta 1986; Sadler 1989b). This is a list (not exhaustive) of phenomena in J-E translation that are suitable for EBMT: • optional cases with a case particle ( \"- de\", \"~ hi\",...) • subordinate conjunction (\"- ba -\", \"~ nagara -\", \"~ tara -\",...,\"- baai ~\",...) • noun phrases of the form '~1 no N2\" • sentences of the form \"N~ wa N 2 da\" • sentences lacking the main verb (eg. sentences of the form \"~ o-negaishimasu\") • fragmental expressions Chai\", \"sou-desu\", \"wakarimashita\",...) (Furuse et al. 1990) • modality represented by the sentence ending C-tainodesuga\", \"~seteitadakimasu\", ...)","(Furuse et al. 1990) • simple sentences (Sato and Nagao 1989) This paper discusses a detailed experiment for \"N~ no N2\" in section 4 and prospects for other phenomena, \"N1 wa N2 da\" and \"~ o-negaishimasu\" in section 5.","Similar phenomena in other language pairs can be found. For example, in Spanish to English translation, the Spanish preposition \"de\", with its broad usage like Japanese \"no\", is also effectively Iranslated by EBMT. Likewise, in German to English translation, the German complex noun is also effectively translated by EBMT."]},{"title":"3.2 INTEGRATION It","paragraphs":["is not yet clear whether EBMT can or should deal with the whole process of translation. We assume that there are many kinds of phenomena. Some are suitable for EBMT, while others are suitable for RBMT.","Integrating EBMT with RBMT i s expected to be useful. It would be more acceptable for users if RBMT were first introduced as a base system, and then incrementally have its translation performance improved by attaching EBMT components. This is in the line with the proposal in Nagao (1984). Subsequently, we proposed a practical method of integration in 186 previous papers (Sumita et al. 1990a, b). 4 EBMT FOR \"N x no Nz\" 4.1 THE PROBLEM","\"N~ no N2\" is a common Japanese noun phrase form. \"no\" in the \"Nt no Nz\" is a Japanese adnominal particle. There are other variants, including \"deno\", \"karano\", \"madeno\" and so on. Roughly speaking, Japanese noun phrases of the form \"N~ no N2\" correspond to English noun phrases of the form \"N2 of N:\" as shown in the examples at the top of Figure 2."]},{"title":"Japanese English","paragraphs":["youka n o gogo the afternoon o f the 8th kaigi no mokuteki the object o f the conference .......................................... kaigi n o sankaryou the application fee for the conf.","?the application fee o fthe conf. kyoutodenokaigi theconf, in Kyoto",".'/the conf. o f Kyoto isshukan no kyuka a week' s holiday","?the holiday o f a week mittsu no hoteru three hotels","*hotels o fthree Figure 2 Variations in Translation of \"N1 no N2\" However, \"N2 of Nt\" does not always provide a natural translation as shown in the lower examples in Figure 2. Some translations are too broad in meaning to interpret, others axe almost ungrammatical. For example, the fourth one, \"the conference of Kyoto\", could be misconstrued as \"the conference about Kyoto\", and the last one, \"hotels of three\", is not English. Natural translations often require prepositions other than \"of\", or no preposition at all. In only about one-fifth of \"N~ no N2\" occurrences in our domain, \"N2 of Nt\" would be the most appropriate English translation. We cannot use any particular preposition as an effecdve de.fault value.","No rules for selecting the most appropriate translation for \"N~ no N2\" have yet been found. In other words, the condition (Ca) in section 3.1 holds. Selecting the translation for '~1~ no N2\" is still an important and complicated problem in J-E translation.","In contrast with the preceding research analyzing \"NI no N2\" (Shimazu et al. 1987; Hirai and Kitahashi 1986), deep semantic analysis is avoided because it is assumed that translations appropriate for given domain can be obtained using domain-specific examples (pairs of source md target expressions). EBMT has the advantage that it can directly return a translation by adapting examples without reasoning through a long chain of rules. 4.2 IMPLEMENTATION 4.2.1 OVERVIEW","The EBMT system consists of two databases: an example database and a thesaurus; and also three translation modules: analysis, example-based transfer, and generation (Figure 3).","Examples (pairs of source phrases and their translations) are extracted from ATR's linguistic database of spoken Japanese with English translations. The corpus contains conversations about registering for an international conference (Ogura 1989). Example Database (1) Analysis I","(2) Example-Based Transfer Thesaurus I (3) Generation I Figure 3 System Configuration The thesaurus is used in calculating the semantic distance between the content words in the input and those in the examples. It is composed of a hierarchical structure in accordance with the thesaurus of everyday Japanese written by Ohno and Hamanishi (1984). Analysis","kyouto deno kaigi Example-Based Transfer"]},{"title":"d Japanese English","paragraphs":["0.4 toukyou deno taizai the stay in Tokyo 0.4 honkon deno taizai the stay in Hongkong 0.4 toukyou deno go-taizai the stay in Tokyo 1.0 oosaka no kaigi the conf. in Osaka 1.0 toukyou no kaigi the conf. in Tokyo Generation","the conf. in Kyoto","Figure 4 Translation Procedure","Figure 4 illustrates the translation procedure with an actual sample. First, morphological analysis is performed for the input phrase,\"kyouto[Kyoto] deno kaigi [conference]\". In this case, syntactical 187 analysis is not necessary. Second, similar examples are retrieved from the database. The top five similar examples are shown. Note that the top three examples have the same distance and that they are all translated with \"in\". Third, using this rationale, EBMT generates \"the conference in Kyoto\". 4.2.2 DISTANCE CALCULATION","The distance metric used when retrieving examples is essential and is explained hem in detail.","we suppose that the input and examples (I, E) in the d~tAl~ase ~ r~ted in the same data structure, i.e., the list of words' syntactic and semantic attribute values (refeaxed to as and I~, E~) for each phrase. The attributes of the current target, \"Nt no N2\" , 8~ as follows: 1) for the nouns \"NI\" and \"N2\": the lexical subcategory of the noun, the existence of a prefix or suffix, and its semantic code in the thesaurus; 2) for the adnominal particle \"no\": the kinds of variants, \"deno\", \"karano\", \"madeno\" and so on. Here, for simplicity, only the semantic code and the kind of adnominal a=e considered.","Distances ae calculated using the following two expressions (Sumita et al. 1990a, b): (1) d(I,E)=d(li,Ei)"]},{"title":"\"w i i (2) wi=,~// ~. ( freq. of t. p. when Ei=li ) 2 t.p.","paragraphs":["The attribute distance, d(li, E.~ end the weight of attribute, w~ are explained in the following sections. Each Iranslation pattern (t.p.) is abstracted from an example md is stored with the example in the example d~mhase [see Figure 6]."]},{"title":"(a) ATTRIBUTE DISTANCE","paragraphs":["For the attribute of the adnominal particle \"no\", the distance is 0 or 1 depending on whether or not they match exactly, for example, d(\"deno\",\"deno\") = 0 and d(\"deno\", \"no\") = 1.","For semantic attributes, however, the distance varies between 0 and 1. Semantic distance d(0 < d < 1)is determined by the Most Specific"]},{"title":"Common Abstractlon(MSCA)","paragraphs":["(Kolodner and Riesbeck 1989) obtained from the thesaurus abstraction hierarchy. When the thesaurus is (n+l) layered, (k/n) is assigned to the concepts in the k-th layer from the bottom. For example, as shown with the broken line in Figure 5, the MSCACkaigi '' [conference], \"taizai\" [stay]) is \"koudou\" [actions] and the distance is 2/3. Of course, 0 is assigned when the MSCA is the bottom class, for instance, MSCACkyouto\"[Kyoto], \"toukyou\" [Tokyo])= \"timei\"[placc], or when nouns are identical ("]},{"title":"MSCA(N, N) for any N). Thesaurus Root","paragraphs":["[actions]"]},{"title":"(1/3) oural omings goings] setsumei tions] (o) I kaisetsu","paragraphs":["[commentary] //.,. \",\\"]},{"title":"[ taizai I I","paragraphs":["hatchaku"]},{"title":"I [stays] II [arrivals & [meetingslJ","paragraphs":["J"]},{"title":"J Jdepartures', II :o) , ili i kaigi taizai","paragraphs":["touchaku [conference] [stay] [arrive] Figure 5 Thesaurus(portion)"]},{"title":"(b) WEIGHT OF ATTRIBUTE The weight of the attribute is the degree to which the attribute influences the selection of the translation pattern(t.p.).","paragraphs":["We"]},{"title":"adopt","paragraphs":["the expression (2) used by Stanfill and Waltz (1986) for memory-based reasoning, to implement the intuition."]},{"title":"t.p. freq.","paragraphs":["B in A 12/27 AB 4/27 B from A 2/27 BA 2/27 BtoA 1/27"]},{"title":"(E l=timei) [place/ t.p. freq. B in A 313 (E2=deno) [in/ t.p. freq. B","paragraphs":["9/24 AB"]},{"title":"9/24","paragraphs":["B in A 2/24 A's B 1124 BonA 1/24"]},{"title":"(E3=soudan) [meetings]","paragraphs":["Figure 6 Weight of the i-th attribute"]},{"title":"188","paragraphs":["In Figure 6, all the examples whose E2 = \"deno\" aze translated with the same preposition, \"in\". This implies that when El= \"deno\", E2 is an attribute which heavily influences the selection of the translation pattern. In contrast to this, the translation patterns of examples whose E1 = \"timei\"[place], =e varied. This implies that when E1 -- \"timei\"[place], E~is an attribute which is less influential on the selection of the translation pattern.","According to the expression (2), weights for attributes, E~, E2 and E3me as follows:"]},{"title":"W1=,~(12/27) 2+(4127 ) 2+...+(1/27)2 = 0.49 W2=,,~(3/3) 2 = 1.0 w3=,~(9/24 ) 2+(9124 ) 2+. ..+(1/24) 2 ,= 0.54 (C) TOTAL DISTANCE","paragraphs":["The distance between the input and the first example shown in Figure 4 is calculated using the weights in section 4.2.2 Co), attribute distances as explained in section 4.2.2 (a) and expression (1) at the beginning of section 4.2.2."]},{"title":"d( \"kyouto'[Kyoto] \"deno'[in] \"kaigi'[ conference], \"toukyou'[Tokyo] \"deno'[in] \"taizai'[stay]) ,= d('kyouto','toukyou\" )*0.49+ d('deno\",'deno')*1.0+ d('kaigi\", \"taizai')*0.54 = 0\"0.49+0\"1.0+2/3\"0.54 = 0.4 4.3 EXPERIMENTS","paragraphs":["The current number of words in the corpus is about 300,000 and the number of examples is 2,550. The collection of examples from another domain is in progress."]},{"title":"4.3.1 JACKKNIFE TEST","paragraphs":["In ~ to roughly estimate translation performance, a jackknife experiment was conducted. We partitioned the example database(2,550) in groups of one hundred, then used one set as input(100) and translated them with the rest as an example database (2,450). This was repeated 25 times.","Figure 7 shows that the average success rate is 78%, the minimum 70% and the maximum 89% [see section 4.3.4].","It is difficult to fairly compare this result with the success rate of the existing MT system. However, it is believed that current conventional systems can at best output the most common translation pattern, for example, \"B of A\", as the default. In this case, the average success rate may only be about 20%."]},{"title":"success(%) MAXIMUM(89%) 100 80","paragraphs":["~~_ ,.."]},{"title":"60 AVERAGE(78%) MINIMUM(70%)","paragraphs":["0 I I"]},{"title":"1 11 21 test number Figure 7 Result of Jackknife Test 40","paragraphs":["20"]},{"title":"4.3.2 SUCCESS RATE PER NUMBER OF EXAMPLES","paragraphs":["Figure 8 shows the relationship between the success rate and the number of examples. Of the twenty-five cases in the previous jackknife test, three are shown: maximum, average, and minimum. This graph shows that, in general, the more examples we have, the better the quality [see section 4.3.4]."]},{"title":"success(%) MAXIMUM 80 t| /J~~~,~'~'~''--/-- .,, ~s' AVERAGE","paragraphs":["70 . _. - .. ---.'''--- 50"]},{"title":"1 11 21 no. of examples (x 100) Figure 8 Success Rate per No. of Examples 189 4.3.3 SUCCESS RATE PER DISTANCE","paragraphs":["Figure 9 shows the relationship between the success rate and the distance between the input and the most similar examples retrieved.","This graph shows that in general, the smaller the distance, the better the quality. In other words, EBMT assigns the distance between the input and the retrieved examples us a reliability factor. SUCCESS 0.9 r 1592/1790 0.8"]},{"title":"0.7 0.6 0.5 0.4 0.3 0.2 0.1 0 23137 100 / 169 • 19/33 • = • 35167 951162 • 74/148 8/24 7/14 3/56","paragraphs":["E• I I I I I 0 0.2 0.4 0.6 0.8 1 distance"]},{"title":"Figure 9 Success Rate per Distance 4.3.4 SUCCESSES AND FAILURES","paragraphs":["The following represents successful results: (1) the noun phrase \"kyouto-eki [Kyoto-station] no o-mise [store]\" is wansta_!ed according to the translation pattern \"B at A\" while the similar noun phrase, \"kyouto[Kyoto] no shiten [branch]\" is translated according to the translation pattern \"13 in A\"; (2) the noun phrase of the form \"N~ no hou\" is translated according to the translation pattern \"A\", in other words, the second noun is omitted. We ~e now studying the results carefully ~d are striving to improve the success rate. (a) About half of the failures are caused by a lack of similar examples. They are easily solved by adding appropriate examples. Co) The rest are caused by the existence of similar examples: (1) equivalent but different examples are retrieved, for instance, those of the form, \"B of A\" and \"AB\" for \"rolm-gatsu [June] no futsu-ka [second]\". This is one of the main reasons the graphs (Figure 7 and 8) show an up-and-down pattern. They can be regarded as a correct translation or the distance calculation may be changed"]},{"title":"to","paragraphs":["handle the problem; (2) Because the current distance calculation is inadequate, dissimilar examples are retrieved."]},{"title":"5 PHENOMENA OTHER THAN \"N 1 no Nz\"","paragraphs":["This section studies the phenomena, \"N1 wa N2 da\" and \"- o-negaishimasu\" with the same corpus used in the"]},{"title":"previous","paragraphs":["section."]},{"title":"5.1 \"N x wa N~ da\"","paragraphs":["A sentence of the form \"N] wa N2 da\" is called a \"da\" sentence. Here \"N{' and '~2\" ~e nouns, \"wa\" is a topical particle, and \"da\" is a kind of verb which, roughly speaking, is the English copula \"be\".","The correspondences between \"da\" sentences and the English equivalents are exemplified in Figure 10. Mainly, \"N~ wa N2 da\" corresvonds to '~ be Nz\" like (a-l) - (a-4).","However, sentences like (b) - (e) cannot be translated according to the translation pattern ,N~ be N2\". In example (d), there is no Japanese counterpart of \"payment should be made- by\". The English sentence has a modal, passive voice, the verb make, and its object,"]},{"title":"payment,","paragraphs":["while the Japanese sentence has no such correspondences. This translation cannot be made in a compositional way from the target words which ale selected from a normal dictionary. It is difficult to formulate rules for the translation and to explain how the translation is made. The conditions (Ca) and (Co) in section 3.1 hold true.","Conventional approaches lead to the understanding of\"da\" sentences using contextual and exwa-linguistic information. However, many translations exist that are the result of human translators' understanding. Translation can be made by mimicking such similar examples.","Example (e) is special, i.e., idiomatic. The condition (Co) in section 3.1 holds. (a) NI be N= watashi[I]"]},{"title":"kochira[this] denwa-bango[tel-no.] sanka-hi[fee] (b) N, cost N= yokoushuu[proc.] N, jonson[Johnson] jim ukyoku[secretariat] 06-951-0866106-951-0866] 85,000-en[85,000 yen] 30,000-en[30,000 yen] (c) for N,, the fee is N= kigyou[companies] 85,000-en[85,000 yen] (d) payment should be made by N= hiyou[fee] ginnkou-furikomi [bank-transfer] (e) the conference will end on N= saishuu-bi[final day] 10qatsu12nichi[Oct. 12th] Figure 1 0 Examples of \"N1 wa N2da\"","paragraphs":["The distribution of N] and N2 in the examples 190 of our corpus vary for each case. Attention should be given to 2-tuples of nouns, (N1, N2). N2s of (a-4), (13) and (c) are similar, i.e., both mean \"prices\". However N~s are not similar to each other. Nls of (a-4) and (d) ~e similar, i.e., both mean \"fee\". However, the N2s ~e not similar to each other. Thus, EBMT is applicable. 5.2 \"~ o-negaishimasu\"","Figure 11 exemplifies the conespondences between sentences of the form \"~ o-negaishimasu\" and the English equivalents. (a) may I speak to N (b) please give me N (c) please pay by N (d) yes, please (e) thank You","Figure 11 jim ukyoku[secretariat] o","o-negaishlmasu go-ju usyo[add ress] o... genkin[cash] de... hal... voroshiku...","Examples of \"~ o-negaishimasu\"","Translations in examples (b) and (c) are possible by finding substitutes in Japanese for give me and pay by, respectively. The conditions (Ca) and (Cc) in section 3.1 hold. Usually, this kind of supplement is done by contextual analysis. However, the connection between the missing elements and the noun in the examples is strong enough to reuse, because it is the product of a combination of translator expertise and domain specific restriction. Examples (a), (d) and (e) are idiomatic expressions. The condition (Cb) holds. The distribution of the noun and the particle in the examples of our corpus varies for each case in the same way as in the \"da\" sentence. EBMT is applicable. 6 CONCLUDING REMARKS Example-Based Machine Translation (EBMT) has been proposed. EBMT retrieves similar examples (pairs of source and target expressions), adapting them to translate a new source text.","The feasibility of EBMT has been shown by implementing a system which translates Japanese noun phrases of the form '~1 no N2\" into English noun phrases. The result of the experiment was encouraging. Bnaed applicability of EBMT was shown by studying the d~m from the text corpus. The advantages of integrating EBMT with RBMT were also discussed. The system has been written in Common Lisp, and is running on a Genera 7.2 Symbolics Lisp Machine at ATR. (1) IMPROVEMENT","The more elaborate the RBMT becomes, the less expandable it is. Considerably complex rules concerning semantics, context, and the real world, are required in machine translation. This is the notorious AI bottleneck: not only is it difficult to add a new rule to the database of rules that are mutually dependent, but it is also difficult to build such a rule database itself. Moreover, computation using this huge and complex rule database is so slow that it forces a developer to abandon efforts to improve the system. RBMT is not easily upgraded.","However, EBMT has no rules, and the use of examples is relatively localized. Improvement is effected simply by inputting appropriate examples into the database. EBMT is easily upgraded, which the experiment in section 4.3.2 has shown: the more examples we have, the better the quality. (2) RELIABILITY FACTOR","One of the main reasons users dislike RBMT systems is the so-called \"poisoned cookie\" problem. RBMT has no device to compute the reliability of the result. In other words, users of RBMT cannot trust any RBMT translation, because it may be wrong without any such indication from system. Consider the case where all translation processes have been completed successfully, yet, the result is incorrect.","In EBMT, a reliability factor is assigned to the translation result according to the distance between the input and the similar examples found [see the experiment in section 4.3.3]. In addition to this, retrieved examples that are similar to the input convince users that the translation is accurate. (3) TRANSLATION SPEED RBMT translates slowly in general because it is really a large-scale rule-based system, which consists of analysis, transfer, and generation modules using syntactic rules, semantic restrictions, structural transfer rules, word selections, generation rules, and so on. For example, the Mu system has about 2,000 rewriting and word selection rules for about 70,000 lexical items (Nagao et al. 1986).","As recently pointed out (Furuse et al. 1990), conventional RBMT systems have been biased toward syntactic, semantic, and contextual analysis, which consumes considerable computing time. However, such deep analysis is not always necessary or useful for translation. In contrast with this, deep semantic analysis is avoided in EBMT because it is assumed that translations appropriate for given domain can be obtained using domain-specific examples (pairs of source and target expressions). EBMT directly returns a translation without reasoning through a long chain of rules [see 191 sections 2 and 4].","There is fear that retrieval from a large-scale example database will prove too slow. However, it can be accelerated effectively by both indexing (Sumita and Tsutsumi 1988) and parallel computing (Sumita and Iida 1991). These processes multiply acceleration. Consequently, the computation of EBMT is acceptably efficient."]},{"title":"(4) ROBUSTNESS","paragraphs":["RBMT works on exact-match reasoning. It fails to translate when it has no knowledge that matches the input exactly.","However, EBMT works on best-match reasoning. It intrinsically translates in a fail-safe way [see sections 2 and 4]."]},{"title":"(5) TRANSLATORS EXPERTISE","paragraphs":["Formulating linguistic rules for RBMT is a difficult job and requires a linguistically trained staff. Moreover, linguistics does not deal with all phenomena occurring in real text (Nagao 1988).","However, examples necessary for EBMT ~Ee easy to obtain because a large number of texts and their translations are available. These are realization of translator expertise, which deals with all real phenomena. Moreover, as electronic publishing increases, more and more texts will be machine-readable (Sadler 1989b).","EBMT is intrinsically biased toward a sublanguage: strictly speaking, toward an example database. This is a good feature because it provides a way of automatically tuning itself to a sublanguage."]},{"title":"REFERENCES","paragraphs":["Furuse~ O., Sumita, E. and Iida, H. 1990 \"A Method for Realizing Transfer-Driven Machine Translation\", Reprint of W(~L 80-8, IPSJ, (in Japanese).","Hirei, M. and Kitahashi, T. 1986, \"A Semantic Classification of Noun Modifications in Japanese Sentences and Their Analysis\", Reprint of WGNL 58-1, IPSJ, (in Japanese).","Kolodner, J. and Riesbeek, C. 1989 \"Case-Based Reasoning\", Tutorial Textbook of 11 th UCAI. Nagao, M.","1984 \"A Framework of a Mechanical Translation Between Japanese and English by Analogy Principle\", in A. Elithom and R. Banerji (ed.), Artificial and Human Intelligence, North-Holland, 173-180.","Nagao, M. ,Tsujii, J. , Nakamura, J. 1986 \"Machine Translation from Japanese into English\", Proceedings of the IFI~.F., 74, 7.","Nagao, M.(chair) 1988 \"Language Engineering : The Real Bottleneck of Natural Language Processing\", Proceedings of the 12th International Conference on Computational Linguistics.","Nirenburg, S. 1987 Machine Translation, Cambridge University Press, 350.","Nitta, Y. 1986 'Idiosyncratic Gap: A Tough Problem to Structure-bound Machine Translation\", Proceedings of the 11th International Conference on Computational Linguistics, 107-111.","Ogura, K., Hashimoto, K., and Morimoto, T. 1989 \"Object-Oriented User Interface for Linguistic Database\", Proceedings of Working Conference on Data and Knowledge Base Integration, University of Keele, England.","Ohno, S. and Hamanishi, M. 1984 Ruigo-Shin-Jiten, Kadokawa, 93 2, (in Japanese).","Sadler, V. 1989a ''Translating with a Simulated Bilingual Knowledge Bank(BKB)\", BSO/Research.","Sadler. V. 1989b Working with Analogical Semantics, Foris Publications, 25 6.","Sato, S. and Nagao, M. 1989 \"Memory-Based Translation\", Reprint of WGNL 70-9, IPSJ, (in Japanese).","Sato, S. and Nagao, M. 1990 \"Toward Memory-Based Translation\", Proceedings of the 13th International Conference o n Computational Linguistics.","Shimazu, A. , Naito, S. , and Nomura, H. 1987 \"Semantic Structure Analysis of Japanese Noun Phrases with Adnominal Particles\", Proceedings of the 25th Annual Meeting of the Association for Computational Linguistics, 123-130.","Stanf'dl, C. and Waltz, D. 1986 'Toward Memory-Based Reasoning\", CACM, 29-12, 1213-1228.","Sumita, E. and Tsutsumi, Y. 1988 \"A Translation Aid System Using Flexible Text Retrieval Based on Syntax-Matching\", Proceedings of The Second International Conference on Theoretical and Methodological Issues in Machine Translation of NaturalLanguages, CMU, Pittsburgh. Sumita, E.,","Iida, H. and Kohyama, H. 1990a '~l'ranslating with Examples: A New Approach to Machine Translation\", Proceedings of The Third International Conference on Theoretical and Methodological Issues in Machine Translation of NaturalLanguages, Texas, 203-212.","Sumita, E. /ida, H. and Kohyama, H. 1990b \"Example-based Approach in Machine Translation\", Proceedings of lnfoJapan \"90, Part 2: 65-72.","Sumita, E. and Iida, H. 1991 \"Acceleration of Example-Based Machine Translation\", (manuscript). 192"]}]}
