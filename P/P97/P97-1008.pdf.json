{"sections":[{"title":"Similarity-Based Methods For Word Sense Disambiguation","paragraphs":["Ido Dagan","Dept. of Mathematics and Computer Science Bar Ilan University","Ramat Gan 52900, Israel"]},{"title":"dagan©macs,","paragraphs":["biu. ac. il Lillian Lee Fernando Pereira Div. of Engineering and AT&T Labs - Research Applied Sciences 600 Mountain Ave. Harvard University Murray Hill, NJ 07974, USA","Cambridge, MA 01238, USA pereira©research, att. corn llee©eecs, harvard, edu Abstract We compare four similarity-based estimation methods against back-off and maximum-likelihood estimation methods on a pseudo-word sense disambiguation task in which we controlled for both unigram and bigram frequency. The similarity-based methods perform up to 40% better on this particular task. We also conclude that events that occur only once in the training set have major impact on similarity-based estimates. 1 Introduction The problem of data sparseness affects all statistical methods for natural language process-ing. Even large training sets tend to misrepresent low-probability events, since rare events may not appear in the training corpus at all.","We concentrate here on the problem of estimating the probability of unseen word pairs, that is, pairs that do not occur in the training set. Katz's back-off scheme (Katz, 1987), widely used in bigram language modeling, estimates the probability of an unseen bigram by utilizing unigram estimates. This has the undesirable result of assigning unseen bigrams the same probability if they are made up of unigrams of the same frequency.","Class-based methods (Brown et al., 1992; Pereira, Tishby, and Lee, 1993; Resnik, 1992) cluster words into classes of similar words, so that one can base the estimate of a word pair's probability on the averaged cooccurrence probability of the classes to which the two words be-long. However, a word is therefore modeled by the average behavior of many words, which may cause the given word's idiosyncrasies to be ignored. For instance, the word \"red\" might well act like a generic color word in most cases, but it has distinctive cooccurrence patterns with respect to words like \"apple,\" \"banana,\" and so on.","We therefore consider similarity-based estimation schemes that do not require building general word classes. Instead, estimates for the most similar words to a word w are combined; the evidence provided by word w' is weighted by a function of its similarity to w. Dagan, Markus, and Markovitch (1993) propose such a scheme for predicting which unseen cooccurrences are more likely than others. However, their scheme does not assign probabilities. In what follows, we focus on probabilistic similarity-based estimation methods.","We compared several such methods, including that of Dagan, Pereira, and Lee (1994) and the cooccurrence smoothing method of Essen and Steinbiss (1992), against classical estimation methods, including that of Katz, in a decision task involving unseen pairs of direct objects and verbs, where unigram frequency was eliminated from being a factor. We found that all the similarity-based schemes performed al-most 40% better than back-off, which is expected to yield about 50% accuracy in our experimental setting. Furthermore, a scheme based on the total divergence of empirical dis-56 tributions to their average 1 yielded statistically significant improvement in error rate over cooccurrence smoothing.","We also investigated the effect of removing extremely low-frequency events from the training set. We found that, in contrast to back-off smoothing, where such events are often discarded from training with little discernible effect, similarity-based smoothing methods suffer noticeable performance degradation when singletons (events that occur exactly once) are omitted. 2 Distributional Similarity Models We wish to model conditional probability distributions arising from the coocurrence of linguistic objects, typically words, in certain configura-tions. We thus consider pairs (wl, w2) E Vi × V2 for appropriate sets 1/1 and V2, not necessarily disjoint. In what follows, we use subscript i for the i th element of a pair; thus"]},{"title":"P(w21wi)","paragraphs":["is the conditional probability (or rather, some empirical estimate, the true probability being unknown) that a pair has second element w2 given that its first element is wl; and"]},{"title":"P(wllw2)","paragraphs":["denotes the probability estimate, according to the base language model, that wl is the first word of a pair given that the second word is w2."]},{"title":"P(w)","paragraphs":["denotes the base estimate for the unigram probability of word w.","A similarity-based language model consists of three parts: a scheme for deciding which word pairs require a similarity-based estimate, a method for combining information from similar words, and, of course, a function measuring the similarity between words. We give the details of each of these three parts in the following three sections. We will only be concerned with similarity between words in V1.","1To the best of our \"knowledge, this is the first use of this particular distribution dissimilarity function in statistical language processing. The function itself is implicit in earlier work on distributional clustering (Pereira, Tishby, and Lee, 1993}, has been used by Tishby (p.e.) in other distributional similarity work, and, as suggested by Yoav Freund (p.c.), it is related to results of Hoeffding (1965) on the probability that a given sample was drawn from a given joint distribution. 2.1 Discounting and Redistribution Data sparseness makes the"]},{"title":"maximum likelihood estimate (MLE)","paragraphs":["for word pair probabilities unreliable. The MLE for the probability of a word pair (Wl, w2), conditional on the appearance of word wl, is simply PML(W2[wl) --"]},{"title":"c(wl, w2) (1) c( i)","paragraphs":["where"]},{"title":"c(wl,","paragraphs":["w2) is the frequency of (wl, w2) in the training corpus and"]},{"title":"c(wl)","paragraphs":["is the frequency of wt. However, PML is zero for any unseen word pair, which leads to extremely inaccurate estimates for word pair probabilities.","Previous proposals for remedying the above problem (Good, 1953; Jelinek, Mercer, and Roukos, 1992; Katz, 1987; Church and Gale, 1991) adjust the MLE in so that the total probability of seen word pairs is less than one, leav-ing some probability mass to be redistributed among the unseen pairs. In general, the adjustment involves either"]},{"title":"interpolation,","paragraphs":["in which the MLE is used in linear combination with an estimator guaranteed to be nonzero for unseen word pairs, or"]},{"title":"discounting,","paragraphs":["in which a reduced MLE is used for seen word pairs, with the probability mass left over from this reduction used to model unseen pairs.","The discounting approach is the one adopted by Katz (1987):"]},{"title":"/Pd(w2]wx) C(Wl, w2) > 0","paragraphs":["/5(w2lwl) ="]},{"title":"[o~(wl)Pr(w2[wl)","paragraphs":["o.w. (2) where"]},{"title":"Pd","paragraphs":["represents the Good-Turing discounted estimate (Katz, 1987) for seen word pairs, and Pr denotes the model for probability redistribution among the unseen word pairs. c~(wl) is a normalization factor.","Following Dagan, Pereira, and Lee (1994), we modify Katz's formulation by writing"]},{"title":"Pr(w2]wl)","paragraphs":["instead P(w2), enabling us to use similarity-based estimates for unseen word pairs instead of basing the estimate for the pair on unigram frequency"]},{"title":"P(w2).","paragraphs":["Observe that similarity","estimates are used for unseen word pairs only. We next investigate estimates for"]},{"title":"Pr(w21wl)","paragraphs":["57 derived by averaging information from words that are distributionally similar to Wl. 2.2 Combining Evidence Similarity-based models assume that if word w~ is \"similar\" to word wl, then w~ can yield information about the probability of unseen word pairs involving wl. We use a weighted average of the evidence provided by similar words, where the weight given to a particular word w~ depends on its similarity to wl.","More precisely, let"]},{"title":"W(wl, W~l)","paragraphs":["denote an in-creasing function of the similarity between wl and w[, and let $(Wl) denote the set of words most similar to Wl. Then the general form of similarity model we consider is a W-weighted linear combination of predictions of similar words:"]},{"title":"PSIM('W2IWl) = ~V(Wl, W~) E","paragraphs":["~ ~s(~1 ) (3) where = is a normalization factor. According to this formula, w2 is more likely to occur with wl if it tends to occur with the words that are most similar to"]},{"title":"WI.","paragraphs":["Considerable latitude is allowed in defining the set"]},{"title":"$(Wx), as","paragraphs":["is evidenced by previous work that can be put in the above form. Essen and Steinbiss (1992) and Karov and Edelman (1996) (implicitly) set"]},{"title":"8(wl) = V1.","paragraphs":["However, it may be desirable to restrict ,5(wl) in some fashion, especially if 1/1 is large. For instance, Dagan. Pereira, and Lee (1994) use the closest k or fewer words w~ such that the dissimilarity between wl and w~ is less than a threshold value t; k and t are tuned experimentally.","Now, we could directly replace"]},{"title":"P,.(w2[wl)","paragraphs":["in the back-off equation (2) with"]},{"title":"PSIM(W21Wl).","paragraphs":["However, other variations are possible, such as interpolating with the unigram probability"]},{"title":"P(w2):","paragraphs":["P,.(w2lwl) = 7P(w2) +"]},{"title":"(1","paragraphs":["- 7)PsiM(W2lWl), where 7 is determined experimentally (Dagan, Pereira, and Lee, 1994). This represents, in effect, a linear combination of the similarity estimate and the back-off estimate: if 7 -- 1, then we have exactly Katz's back-off scheme. As we focus in this paper on alternatives for PSlM, we will not consider this approach here; that is, for the rest of this paper,"]},{"title":"Pr(w2]wl) = PslM(W21wl).","paragraphs":["2.3 Measures of Similarity We now consider several word similarity functions that can be derived automatically from the statistics of a training corpus, as opposed to functions derived from manually-constructed word classes (Resnik, 1992). All the similarity functions we describe below depend just on the base language model P('I'), not the discounted model /5(.[.) from Section 2.1 above. 2.3.1 KL divergence"]},{"title":"Kullback-Leibler (KL) divergence","paragraphs":["is a standard information-theoretic measure of the dissimilarity between two probability mass functions (Cover and Thomas, 1991). We can apply it to the conditional distribution P(.[wl) induced by Wl on words in V2:"]},{"title":"D(wx[lW ) = P(w2lwl) log P(wu[wx) P(w21wl)\" (4)","paragraphs":["For"]},{"title":"D(wxHw~l)","paragraphs":["to be defined it must be the case that"]},{"title":"P(w2]w~l)","paragraphs":["> 0 whenever"]},{"title":"P(w21wl) >","paragraphs":["0. Unfortunately, this will not in general be the case for MLEs based on samples, so we would need smoothed estimates of"]},{"title":"P(w2]w~)","paragraphs":["that redistribute some probability mass to zero-frequency events. However, using smoothed estimates for"]},{"title":"P(w2[wl) as","paragraphs":["well requires a sum over all w2 6 172, which is expensive ['or the large vocabularies under consideration. Given the smoothed denominator distribution, we set"]},{"title":"l/V(wl, w~)","paragraphs":["="]},{"title":"lO -~D(wlllw'l) ,","paragraphs":["where/3 is a free parameter. 2.3.2 Total divergence to the average A related measure is based on the total KL divergence to the average of the two distribu-tions:","+ wl"]},{"title":"A(wx, W11) = D (w, wl )+D","paragraphs":["(w~[ + w~) 2 58 where"]},{"title":"(Wl ÷ w~)/2","paragraphs":["shorthand for the distribution"]},{"title":"1⁄2 (P(.IwJ + P(.Iw~))","paragraphs":["Since"]},{"title":"D('II-) > O, A(Wl,W~) >_ O. Furthermore,","paragraphs":["letting"]},{"title":"p(w2)","paragraphs":["="]},{"title":"P(w2[wJ, p'(w2)","paragraphs":["="]},{"title":"P(w2lw~)","paragraphs":["and C : {w2 :"]},{"title":"p(w2) > O,p'(w2)","paragraphs":["> O}, it is straightforward to show by grouping terms appropriately that"]},{"title":"A(wi,wb= -H(p(w2)) - H(p'(w2)) }","paragraphs":["+ 2 log 2, where"]},{"title":"H(x) = -x","paragraphs":["logx. Therefore,"]},{"title":"d(wl, w~)","paragraphs":["is bounded, ranging between 0 and 2log2, and smoothed estimates are not required because probability ratios are not involved. In addition, the calculation of"]},{"title":"A(wl, w~)","paragraphs":["requires summing only over those w2 for which"]},{"title":"P(w2iwJ","paragraphs":["and"]},{"title":"P(w2]w~)","paragraphs":["are both non-zero, which, for sparse","data, makes the computation quite fast. As in the KL divergence case, we set"]},{"title":"W(Wl, W~l)","paragraphs":["to be 10 -~A(~'wl). 2.3.3","LI norm The"]},{"title":"L1 norm","paragraphs":["is defined as"]},{"title":"n(wi, wl)","paragraphs":[": ~"]},{"title":"IP(w2lwj - P(w21w'Jl .","paragraphs":["(6) W2 By grouping terms as before, we can express"]},{"title":"L(wI, w~)","paragraphs":["in a form depending only on the \"common\" w2:"]},{"title":"n(wl, w~)","paragraphs":["= 2- E p(w2)- E p'(w2)"]},{"title":"w26C w2EC ÷ Ip(w2)-p'(w2)t.","paragraphs":["w2EC This last form makes it clear that 0 <"]},{"title":"L(Wl,","paragraphs":["w[) _< 2, with equality if and only if there are no words w2 such that both"]},{"title":"P(w2lwJ","paragraphs":["and"]},{"title":"P(w2lw[)","paragraphs":["are strictly positive. Since we require a weighting scheme that is","decreasing in L, we set"]},{"title":"W(wl, w~) = (2 - n(wl,","paragraphs":["W/l)) fl with fl again free. 2.3.4","Confusion probability Essen and Steinbiss (1992) introduced"]},{"title":"confu- sion probability","paragraphs":["2, which estimates the probability that word w~ can be substituted for word Wl:"]},{"title":"Pc(w lWl) = w(wl, = ~, P(wllw2)P(w~[w2)P(w2) w2 P(Wl)","paragraphs":["Unlike the measures described above, wl may not necessarily be the \"closest\" word to itself, that is, there may exist a word w~ such that"]},{"title":"Pc(W'l[Wl ) > Pc(w,[wl) .","paragraphs":["The confusion probability can be computed from empirical estimates provided all unigram estimates are nonzero (as we assume through-out). In fact, the use of smoothed estimates like those of Katz's back-off scheme is problem-atic, because those estimates typically do not preserve consistency with respect to marginal estimates and Bayes's rule. However, using consistent estimates (such as the MLE), we can rewrite Pc as follows: ' w P(w2lwl) ."]},{"title":"P(w21w'JP(w'J. Pc(W1[ 1)= ~ P(w2)","paragraphs":["W2 This form reveals another important difference between the confusion probability and the functions D, A, and L described in the previous sec-tions. Those functions rate w~ as similar to wl if, roughly,"]},{"title":"P(w21w~)","paragraphs":["is high when"]},{"title":"P(w21'wj","paragraphs":["is."]},{"title":"Pc(w~[wl),","paragraphs":["however, is greater for those w~ for which"]},{"title":"P(w~, wJ","paragraphs":["is large when"]},{"title":"P(w21wJ/P(w2)","paragraphs":["is. When the ratio"]},{"title":"P(w21wl)/P(w2)","paragraphs":["is large, we may think of w2 as being exceptional, since if w2 is infrequent, we do not expect"]},{"title":"P(w21wJ","paragraphs":["to be large. 2.3.5","Summary","Several features of the measures of similarity listed above are summarized in table 1. \"Base LM constraints\" are conditions that must be satisfied by the probability estimates of the base","2Actually, they present two alternative definitions. We use their model 2-B, which they found yielded the best experimental results. 59 language model. The last column indicates whether the weight"]},{"title":"W(wl, w~)","paragraphs":["associated with each similarity function depends on a parameter that needs to be tuned experimentally. 3 Experimental Results We evaluated the similarity measures listed above on a word sense disambiguation task, in which each method is presented with a noun and two verbs, and decides which verb is more likely to have the noun as a direct object. Thus, we do not measure the absolute quality of the assignment of probabilities, as would be the case in a perplexity evaluation, but rather the relative quality. We are therefore able to ignore constant factors, and so we neither normalize the similarity measures nor calculate the denominator in equation (3). 3.1 Task: Pseudo-word Sense"]},{"title":"Disambiguation","paragraphs":["In the usual word sense disambiguation problem, the method to be tested is presented with an ambiguous word in some context, and is asked to identify the correct sense of the word from the context. For example, a test instance might be the sentence fragment \"robbed the bank\"; the disambiguation method must decide whether \"bank\" refers to a river bank, a savings bank, or perhaps some other alternative.","While sense disambiguation is clearly an important task, it presents numerous experimental difficulties. First, the very notion of \"sense\" is not clearly defined; for instance, dictionaries may provide sense distinctions that are too fine or too coarse for the data at hand. Also, one needs to have training data for which the correct senses have been assigned, which can require considerable human effort.","To circumvent these and other difficulties, we set up a pseudo-word disambiguation experiment (Schiitze, 1992; Gale, Church, and Yarowsky, 1992) the general format of which is as follows. We first construct a list of"]},{"title":"pseudo- words,","paragraphs":["each of which is the combination of two different words in V2. Each word in V2 contributes to exactly one pseudo-word. Then, we replace each w2 in the test set with its corresponding pseudo-word. For example, if we choose to create a pseudo-word out of the words \"make\" and \"take\", we would change the test data like this:","make plans =~ {make, take} plans","take action =~ {make, take} action The method being tested must choose between the two words that make up the pseudo-word. 3.2 Data We used a statistical part-of-speech tagger (Church, 1988) and pattern matching and concordancing tools (due to David Yarowsky) to identify transitive main verbs and head nouns of the corresponding direct objects in 44 million words of 1988 Associated Press newswire. We selected the noun-verb pairs for the 1000 most frequent nouns in the corpus. These pairs are undoubtedly somewhat noisy given the errors inherent in the part-of-speech tagging and pattern matching.","We used 80%, or 587833, of the pairs so derived, for building base bigram language models, reserving 20.o/0 for testing purposes. As some, but not all, of the similarity measures require smoothed language models, we calculated both a Katz back-off language model (P = 15 (equation (2)), with"]},{"title":"Pr(w2[wl) = P(w2)),","paragraphs":["and a maximum-likelihood model (P = PML)- Furthermore, we wished to investigate Katz's claim that one can delete"]},{"title":"singletons,","paragraphs":["word pairs that occur only once, from the training set without affecting model performance (Katz, 1987); our training set contained 82407 singletons. We therefore built four base language models, summarized in Table 2. MLE Katz with singletons no singletons (587833 pairs) (505426 pairs) MLE-1 MLE-ol BO-1 BO-ol Table 2: Base Language Models","Since we wished to test the effectiveness of using similarity for unseen word cooccurrences, we removed from the test set any verb-object pairs 60 name D A L"]},{"title":"Pc","paragraphs":["range"]},{"title":"[0, co] [0, 2 log 2] [0, 2]","paragraphs":["[0, 1⁄2 maxw, P(w2)] base LM constraints"]},{"title":"P(w21w~l) ¢ 0","paragraphs":["if"]},{"title":"P(w2[wx) ~: 0","paragraphs":["none none","Bayes consistency Table 1: Summary of similarity function properties tune? yes yes yes no that occurred in the training set; this resulted in 17152"]},{"title":"unseen","paragraphs":["pairs (some occurred multiple times). The unseen pairs were further divided into five equal-sized parts, T1 through :/'5, which formed the basis for fivefold cross-validation: in each of five runs, one of the Ti was used as a performance test set, with the other 4 sets combined into one set used for tuning parameters (if necessary) via a simple grid search. Finally, test pseudo-words were created from pairs of verbs with similar frequencies, so as to control for word frequency in the decision task. We use error rate as our performance metric, defined as (# incorrect choices + (# of ties)/2) of where N was the size of the test corpus. A tie occurs when the two words making up a pseudo-word are deemed equally likely. 3.3 Baseline Experiments The performances of the four base language models are shown in table 3. MLE-1 and MLE-ol both have error rates of exactly .5 because the test sets consist of unseen bigrams, which are all assigned a probability of 0 by maximum-likelihood estimates, and thus are all ties for this method. The back-off models BO-1 and BO-ol also perform similarly. MLE-1 MLE-ol BO-1 BO-ol"]},{"title":"7'1 T~ % T4 % .5 .5 .5 .5 .5","paragraphs":["ir 0.517 0.520 0.512 0.513 0.516 0.517 0.520 0.512 0.513 0.516 Table 3: Base Language Model Error Rates","Since the back-off models consistently performed worse than the MLE models, we chose to use only the MLE models in our subsequent experiments. Therefore, we only ran comparisons between the measures that could utilize unsmoothed data, namely, the Lt norm,"]},{"title":"L(wx, w~);","paragraphs":["the total divergence to the average,"]},{"title":"A(wx,","paragraphs":["w~); and the confusion probability,"]},{"title":"Pc(w~lwx).","paragraphs":["3 In the full paper, we give detailed examples showing the different neighborhoods induced by the different measures, which we omit here for reasons of space. 3.4 Performance of Similarity-Based Methods Figure 1 shows the results on the five test sets, using MLE-1 as the base language model. The parameter/3 was always set to the optimal value for the corresponding training set. RAND, which is shown for comparison purposes, simply chooses the weights"]},{"title":"W(wl,w~)","paragraphs":["randomly."]},{"title":"S(wl)","paragraphs":["was set equal to Vt in all cases.","The similarity-based methods consistently outperform the MLE method (which, recall, always has an error rate of .5) and Katz's back-off method (which always had an error rate of about .51) by a huge margin; therefore, we conclude that information from other word pairs is very useful for unseen pairs where unigram frequency is not informative. The similarity-based methods also do much better than RAND, which indicates that it is not enough to simply combine information from other words arbitrarily: it is quite important to take word similarity into account. In all cases, A edged out the other methods. The average improvement in using A instead of"]},{"title":"Pc","paragraphs":["is .0082; this difference is significant to the .1 level (p < .085), according to the paired t-test.","3It should be noted, however, that on BO-1 data, KL-divergence performed slightly better than the L1 norm. 61 T1 T2","Err~","Rates","on","T~t Sets,","8aN","Language","MociJ","MLEI","\"RANOMLEI\" --","\"CONFMU~ I\" - ....","\"I.MLEI\" • ....","•","AMLEI • --"]},{"title":"ii","paragraphs":["T3 T4 T5 Figure 1: Error rates for each test set, where the base language model was MLE-1. The methods, going from left to right, are RAND, Pc, L, and A. The performances shown are for settings offl that were optimal for the corresponding training set. I3 ranged from 4.0 to 4.5 for L and from 10 to 13 for A.","The results for the MLE-ol case are depicted in figure 2. Again, we see the similarity-based methods achieving far lower error rates than the MLE, back-off, and RAND methods, and again, A always performed the best. However, with singletons omitted the difference between A and Pc is even greater, the average difference being .024, which is significant to the .01 level (paired t-test).","An important observation is that all methods, including RAND, were much more effective if singletons were included in the base language model; thus, in the case of unseen word pairs, Katz's claim that singletons can be safely ignored in the back-off model does not hold for similarity-based models. 4 Conclusions Similarity-based language models provide an appealing approach for dealing with data sparseness. We have described and compared the performance of four such models against two classical estimation methods, the MLE method and Katz's back-off scheme, on a pseudo-word disambiguation task. We observed that the similarity-based methods perform much better on unseen word pairs, with the measure based","E~or","~tes on TeSt","~.","~","Umgua91 Model MLE.ot .... F-] Tt"]},{"title":";)-I","paragraphs":["T2 1\"3 T4","\"RANDMLEol*--","\"CONFMLE01\"-",".... \"LMLEol\"-",".... \"7\"AMLEol","......"]},{"title":"\"°'!-...","paragraphs":["ii! : '","F T5 Figure 2: Error rates for each test set, where the base language model was MLE-ol. /~ ranged from 6 to 11 for L and from 21 to 22 for A. on the KL divergence to the average, being the best overall.","We also investigated Katz's claim that one can discard singletons in the training data, resulting in a more compact language model, without significant loss of performance. Our results indicate that for similarity-based language modeling, singletons are quite important; their omission leads to significant degradation of performance. Acknowledgments We thank Hiyan Alshawi, Joshua Goodman, Rebecca Hwa, Stuart Shieber, and Yoram Singer for many helpful comments and discussions. Part of this work was done while the first and second authors were visiting AT&:T Labs. This material is based upon work supported in part by the National Science Foundation under Grant No. IRI-9350192. The second author also gratefully acknowledges support from a National Science Foundation Graduate Fellowship and an AT&T GRPW/ALFP grant. References","Brown, Peter F., Vincent J. DellaPietra, Peter V. deSouza, Jennifer C. Lai, and Robert L. Mercer. 1992. Class-based n-gram models of natural language."]},{"title":"Computational Linguistics,","paragraphs":["18(4):467-479, December. 62","Church, Kenneth. 1988. A stochastic parts program and noun phrase parser for unrestricted text. In Proceedings of the Second Conference on Applied Natural Language Processing, pages 136-143.","Church, Kenneth W. and William A. Gale. 1991. A comparison of the enhanced Good-Turing and deleted estimation methods for estimating probabilites of english bigrams. Computer Speech and Language, 5:19-54.","Cover, Thomas M. and Joy A. Thomas. 1991. Elements of Information Theory. John Wiley.","Dagan, Ido, Fernando Pereira, and Lillian Lee. 1994. Similarity-based estimation of word cooccurrence probabilities. In Proceedings of the 32nd Annual Meeting of the ACL, pages 272-278, Las Cruces, NM. Essen, Ute and Volker Steinbiss. 1992. Cooccurrence smoothing for stochastic language modeling. In Proceedings of ICASSP, volume 1, pages 161-164.","Gale, William, Kenneth Church, and David Yarowsky. 1992. Work on statistcal methods for word sense disambiguation. In Working Notes, AAAI Fall Symposium Series, Probabilistic Approaches to Natural Language, pages 54-60.","Good, I.J. 1953. The population frequencies of species and the estimation of population parameters. Biometrika, 40(3 and 4):237-264.","Hoeffding, Wassily. 1965. Asymptotically optimal tests for nmttinomial distributions. Annals of Mathematical Statistics, pages 369-401.","Jelinek, Frederick, Robert L. Mercer, and Salim Roukos. 1992. Principles of lexical language modeling for speech recognition. In In Sadaoki Furui and M. Mohan Sondhi, editors, Advances in Speech Signal Processing. Mercer Dekker, Inc., pages 651-699.","Karov, Yael and Shimon Edelman. 1996. Learning similarity-based word sense disambiguation from sparse data. In 4rth Workshop on Very Large Corpora.","Katz, Slava M. 1987. Estimation of probabilities from sparse data for the language model component of a speech recognizer. IEEE Transac-tions on Acoustics, Speech and Signal Processing, ASSP-35(3) :400-401, March.","Pereira, Fernando, Naftali Tishby, and Lillian Lee. 1993. Distributional clustering of English words. In Proceedings of the 31st Annual Meeting of the ACL, pages 183-190, Columbus, OH.","Resnik, Philip. 1992. Wordnet and distributional analysis: A class-based approach to lexical discovery. AAAI Workshop on Statistically-based Natural Language Processing Techniques, pages 56-64, July.","Schiitze, Hinrich. 1992. Context space. In Working Notes, AAAI Fall Symposium on Probabilistic Approaches to Natural Language. 63"]}]}
