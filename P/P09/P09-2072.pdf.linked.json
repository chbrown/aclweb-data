{"sections":[{"title":"","paragraphs":["Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 285–288, Suntec, Singapore, 4 August 2009. c⃝2009 ACL and AFNLP"]},{"title":"Iterative Scaling and Coordinate Descent Methods for Maximum Entropy Fang-Lan Huang, Cho-Jui Hsieh, Kai-Wei Chang, and Chih-Jen Lin Department of Computer Science National Taiwan University Taipei 106, Taiwan {d93011,b92085,b92084,cjlin}@csie.ntu.edu.tw Abstract","paragraphs":["Maximum entropy (Maxent) is useful in many areas. Iterative scaling (IS) methods are one of the most popular approaches to solve Maxent. With many variants of IS methods, it is difficult to understand them and see the differences. In this paper, we create a general and unified framework for IS methods. This framework also connects IS and coordinate descent (CD) methods. Besides, we develop a CD method for Maxent. Results show that it is faster than existing iterative scaling methods1","."]},{"title":"1 Introduction","paragraphs":["Maximum entropy (Maxent) is widely used in many areas such as natural language processing (NLP) and document classification. Maxent models the conditional probability as:","Pw(y|x) ≡ Sw(x, y)/Tw(x), (1) Sw(x, y) ≡ eP","t wtft(x,y)",", Tw(x) ≡","∑","y Sw(x, y), where x indicates a context, y is the label of the context, and w ∈ Rn","is the weight vector. A function ft(x, y) denotes the t-th feature extracted from the context x and the label y.","Given an empirical probability distribution P̃ (x, y) obtained from training samples, Maxent minimizes the following negative log-likelihood: minw −","∑ x,y P̃ (x, y) log Pw(y|x) =","∑ x P̃ (x) log Tw(x) −","∑ t wt P̃ (ft), (2) where P̃ (x) =","∑ y","P̃ (x, y) is the marginal probability of x, and P̃ (ft) =","∑ x,y","P̃ (x, y)ft(x, y) is the expected value of ft(x, y). To avoid overfitting the training samples, some add a regularization term and solve: min w L(w) ≡ ∑ x P̃ (x)logTw(x)−∑ t wt P̃(ft)+ P tw2","t 2σ2 ,","(3) 1 A complete version of this work is at http:","//www.csie.ntu.edu.tw/c̃jlin/papers/","maxent_journal.pdf. where σ is a regularization parameter. We focus on (3) instead of (2) because (3) is strictly convex.","Iterative scaling (IS) methods are popular in training Maxent models. They all share the same property of solving a one-variable sub-problem at a time. Existing IS methods include generalized iterative scaling (GIS) by Darroch and Ratcliff (1972), improved iterative scaling (IIS) by Della Pietra et al. (1997), and sequential conditional generalized iterative scaling (SCGIS) by Goodman (2002). In optimization, coordinate descent (CD) is a popular method which also solves a one-variable sub-problem at a time. With these many IS and CD methods, it is uneasy to see their differences. In Section 2, we propose a unified framework to describe IS and CD methods from an optimization viewpoint. Using this framework, we design a fast CD approach for Maxent in Section 3. In Section 4, we compare the proposed CD method with IS and LBFGS methods. Results show that the CD method is more efficient.","Notation n is the number of features. The total number of nonzeros in samples and the average number of nonzeros per feature are respectively #nz ≡","∑ x,y","∑ t:ft(x,y)̸=0 1 and l̄ ≡ #nz/n."]},{"title":"2 A Framework for IS Methods 2.1 The Framework","paragraphs":["The one-variable sub-problem of IS methods is related to the function reduction L(w+zet)−L(w), where et = [0, . . . , 0, 1, 0, . . . , 0]T",". IS methods differ in how they approximate the function reduction. They can also be categorized according to whether w’s components are sequentially or parallely updated. In this section, we create a framework in Figure 1 for these methods.","Sequential update For a sequential-update algorithm, once a one-variable sub-problem is solved, the corresponding element in w is updated. The new w is then used to construct the next sub-problem. The procedure is sketched in 285 Iterative scaling Sequential update Find At(z) to approximate L(w + zet) − L(w) SCGIS Let At(z) = L(w+zet)−L(w) CD Parallel update Find a separable function A(z) to approximate L(w + z) − L(w)","GIS, IIS Figure 1: An illustration of various iterative scaling methods.","Algorithm 1 A sequential-update IS method","While w is not optimal","For t = 1, . . . , n","1. Find an approximate function At(z) sat-","isfying (4).","2. Approximately minz At(z) to get z̄t.","3. wt ← wt + z̄t.","Algorithm 1. If the t-th component is selected for","update, a sequential IS method solves the follow-","ing one-variable sub-problem:","minz At(z),","where At(z) bounds the function difference: At(z) ≥ L(w + zet) − L(w) =","∑ x","P̃ (x) log Tw+ze","t (x) Tw (x) + Qt(z) (4)","and Qt(z) ≡ 2wtz+z2","2σ2 − z P̃ (ft). (5) An approximate function At(z) satisfying (4) does not ensure that the function value is strictly decreasing. That is, the new function value L(w + zet) may be only the same as L(w). Therefore, we can impose an additional condition","At(0) = 0 (6) on the approximate function At(z). If A′","t(0) ̸= 0 and assume z̄t ≡ arg minz At(z) exists, with the condition At(0) = 0, we have At(z̄t) < 0. This inequality and (4) then imply L(w + z̄tet) < L(w). If A′","t(0) = ∇tL(w) = 0, the convexity of L(w) implies that we cannot decrease the function value by modifying wt. Then we should move on to modify other components of w.","A CD method can be viewed as a sequential IS method. It solves the following sub-problem:","minz ACD","t (z) = L(w + zet) − L(w) without any approximation. Existing IS methods consider approximations as At(z) may be simpler for minimization.","Parallel update A parallel IS method simultaneously constructs n independent one-variable sub-problems. After (approximately) solving all of them, the whole vector w is updated. Algorithm 2 gives the procedure. The differentiable function A(z), z ∈ Rn",", is an approximation of L(w + z) − L(w) satisfying A(z) ≥ L(w + z) − L(w), A(0) = 0, and A(z) =","∑ t At(zt). (7) Similar to (4) and (6), the first two conditions en-","Algorithm 2 A parallel-update IS method","While w is not optimal 1. Find approximate functions At(zt) ∀t satis-","fying (7). 2. For t = 1, . . . , n","Approximately minzt At(zt) to get z̄t. 3. For t = 1, . . . , n","wt ← wt + z̄t.","sure that the function value is strictly decreasing.","The last condition shows thatA(z)is separable, so","minz A(z) =","∑","t minzt At(zt). That is,we can minimizeAt(zt),∀t simultaneously, and then update wt ∀t together. A parallel-update method possesses nice implementation properties. However, since it less aggressively updates w, it usually converges slower. If A(z) satisfies (7), taking z = ztet implies that (4) and (6) hold for any At(zt). A parallel method could thus be transformed to a sequential method using the same approximate function, but not vice versa. 2.2 Existing Iterative Scaling Methods We introduce GIS, IIS and SCGIS via the proposed framework. GIS and IIS use a parallel update, but SCGIS is sequential. Their approximate functions aim to bound the function reduction L(w+z)−L(w) =","∑ x","P̃ (x) log Tw+z (x)","Tw (x) +∑","tQt(zt),","(8) where Tw(x) and Qt(zt) are defined in (1) and (5), respectively. Then GIS, IIS and SCGIS use similar inequalities to get approximate functions. They apply log α ≤ α − 1 ∀α > 0 to get (8) ≤ ∑ x,y P̃ (x)Pw(y|x)(eP","tztft(x,y) −1)+∑","t Qt(zt). (9) GIS defines f #","≡ max","x,y f # (x, y), f #","(x, y) ≡ ∑","t ft(x, y),","and adds a feature fn+1(x, y)≡f #","−f #","(x, y) with","zn+1 = 0. Assuming ft(x, y) ≥ 0, ∀t, x, y, and","using Jensen’s inequality","ePn+1 t=1 f t(x,y) f# (ztf#",") ≤","∑n+1 t=1 ft(x,y) f# eztf# and eP","t ztft(x,y) ≤ ∑","t ft(x,y) f# eztf# +","fn+1(x,y)","f# , (10)","we obtain n independent one-variable functions:","AGIS","t (zt) = ez","tf# −1 f#","∑ x,y P̃ (x)Pw(y|x)ft(x, y) + Qt(zt). 286 IIS applies Jensen’s inequality e P t f t(x,y) f# (x,y) (ztf#","(x,y)) ≤","∑ t ft(x,y) f#","(x,y) eztf#","(x,y)","on (9) to get the approximate function","AIIS","t (zt) = ∑ x,y","P̃ (x)Pw(y|x)ft(x, y) ez tf# (x,y)","−1 f# (x,y)","+ Qt(zt).","SCGIS is a sequential-update method. It replaces","f #","in GIS with f #","t ≡ maxx,y ft(x, y). Using ztet","as z in (8), a derivation similar to (10) gives","eztft(x,y)","≤ ft(x,y)","f#","t","eztf#","t","+ f#","t −ft(x,y)","f#","t .","The approximate function of SCGIS is","ASCGIS","t (zt) = ez tf# t −1 f# t","∑ x,y P̃ (x)Pw(y|x)ft(x, y)","+ Qt(zt). We prove the linear convergence of existing IS","methods (proof omitted):","Theorem 1 Assume each sub-problem As","t (zt) is","exactly minimized, where s is IIS, GIS, SCGIS, or","CD. The sequence {wk","} generated by any of these","four methods linearly converges. That is, there is","a constant μ ∈ (0, 1) such that","L(wk+1",")−L(w∗",") ≤ (1−μ)(L(wk",")−L(w∗",")), ∀k,","where w∗","is the global optimum of (3). 2.3 Solving one-variable sub-problems Without the regularization term, by A′","t(zt) = 0, GIS and SCGIS both have a simple closed-form solution of the sub-problem. With the regularization term, the sub-problems no longer have a closed-form solution. We discuss the cost of solving sub-problems by the Newton method, which iteratively updates zt by","zt ← zt − As","t ′ (z","t)/As t ′′ (z","t). (11)","Here s indicates an IS or a CD method.","Below we check the calculation of As","t ′ (z t) as","the cost of As t ′′ (z t) is similar. We have","As t ′ (z","t) =∑ x,y P̃ (x)Pw(y|x)ft(x, y)eztfs","(x,y)","+ Q′ t(zt) (12) where","f s (x, y) ≡   ","f # if s is GIS, f # t if s is SCGIS, f #","(x, y) if s is IIS. For CD, ACD t ′ (zt) = Q′","t(zt)+","∑ x,y P̃ (x)Pw+z","tet(y|x)ft(x, y).","(13) The main cost is on calculating Pw(y|x) ∀x, y, whenever w is updated. Parallel-update approaches calculate Pw(y|x) once every n sub-problems, but sequential-update methods evaluates Pw(y|x) after every sub-problem. Consider the situation of updating w to w + ztet. By (1), Table 1: Time for minimizing At(zt) by the Newton method CD GIS SCGIS IIS 1st Newton direction O(l̄) O(l̄) O(l̄) O(l̄) Each subsequent Newton direction O(l̄) O(1) O(1) O(l̄) obtaining Pw+ztet(y|x) ∀x, y requires expensive O(#nz) operations to evaluate Sw+ztet(x, y) and Tw+ztet(x) ∀x, y. A trick to trade memory for time is to store all Sw(x, y) and Tw(x), Sw+ztet(x, y) = Sw(x, y)eztft(x,y)",", Tw+ztet(x)=Tw(x)+","∑","ySw(x, y)(eztft(x,y)","−1). Since Sw+ztet(x, y) = Sw(x, y) if ft(x, y) = 0, this procedure reduces the the O(#nz) operations to O(#nz/n) = O(l̄). However, it needs extra spaces to store all Sw(x, y) and Tw(x). This trick for updating Pw(y|x) has been used in SCGIS (Goodman, 2002). Thus, the first Newton iteration of all methods discussed here takes O(l̄) operations. For each subsequent Newton iteration, CD needs O(l̄) as it calculates Pw+ztet (y|x) whenever zt is changed. For GIS and SCGIS, if","∑ x,y","P̃ (x)Pw(y|x)ft(x, y) is stored at the first Newton iteration, then (12) can be done in O(1) time. For IIS, because f #","(x, y) of (12) depends on x and y, we cannot store","∑ x,y","P̃ (x)Pw(y|x)ft(x, y) as in GIS and SCGIS. Hence each Newton direction needs O(l̄). We summarize the cost for solving sub-problems in Table 1."]},{"title":"3 Comparison and a New CD Method 3.1 Comparison of","paragraphs":["IS/CD methods From the above discussion, an IS or a CD method falls into a place between two extreme designs: At(zt) a loose bound ↔","At(zt) a tight bound Easy to minimize At(zt) Hard to minimizeAt(zt) There is a tradeoff between the tightness to bound the function difference and the hardness to solve the sub-problem. To check how IS and CD methods fit into this explanation, we obtain relationships of their approximate functions:","ACD","t (zt) ≤ ASCGIS","t (zt) ≤ AGIS","t (zt),","ACD","t (zt) ≤ AIIS","t (zt) ≤ AGIS","t (zt) ∀ zt. (14) The derivation is omitted. From (14), CD considers more accurate sub-problems than SCGIS and GIS. However, to solve each sub-problem, from Table 1, CD’s each Newton step takes more time. The same situation occurs in comparing IIS and GIS. Therefore, while a tight At(zt) can 287 give faster convergence by handling fewer sub-problems, the total time may not be less due to the higher cost of each sub-problem. 3.2 A Fast CD Method We develop a CD method which is cheaper in solving each sub-problem but still enjoys fast final convergence. This method is modified from Chang et al. (2008), a CD approach for linear SVM. We approximately minimize ACD","t (z) by applying only one Newton iteration. The Newton direction at z = 0 is now","d = −ACD","t ′","(0)/ACD t ′′","(0). (15) As taking the full Newton direction may not decrease the function value, we need a line search procedure to find λ ≥ 0 such that z = λd satisfies the following sufficient decrease condition: ACD t (z)−ACD","t (0) = ACD","t (z) ≤ γzACD","t ′","(0), (16) where γ is a constant in (0, 1/2). A simple way to find λ is by sequentially checking λ = 1, β, β2",", . . . , where β ∈ (0, 1). The line search procedure is guaranteed to stop (proof omitted). We can further prove that near the optimum two results hold: First, the Newton direction (15) satisfies the sufficient decrease condition (16) with λ = 1. Then the cost for each sub-problem is O(l̄), similar to that for exactly solving sub-problems of GIS or SCGIS. This result is important as other-wise each trial of z = λd expensively costs O(l̄) for calculating ACD","t (z). Second, taking one Newton direction of the tighter ACD","t (zt) reduces the function L(w) more rapidly than exactly minimizing a loose At(zt) of GIS, IIS or SCGIS. These two results show that the new CD method improves upon the traditional CD by approximately solving sub-problems, while still maintains fast convergence."]},{"title":"4 Experiments","paragraphs":["We apply Maxent models to part of speech (POS) tagging for BROWN corpus (http://www.nltk.org) and chunk-ing tasks for CoNLL2000 (http://www. cnts.ua.ac.be/conll2000/chunking). We randomly split the BROWN corpus to 4/5 training and 1/5 testing. Our implementation is built upon OpenNLP (http://maxent.sourceforge.net). We implement CD (the new one in Section 3.2), GIS, SCGIS, and LBFGS for comparisons. We include LBFGS as Malouf (2002) reported that it is better than other approaches including GIS","0","500","1000","1500","200010","−2 10 −1 10 0 10 1 Training Time (s) Relative function value difference "," CD SCGIS GIS LBFGS (a) BROWN","0","50","100","150","20010","−2 10 −1 10 0 10 1 10 2 Training Time (s) Relative function value difference "," CD SCGIS GIS LBFGS (b) CoNLL2000","0","500 1000","1500","200094 94.5 95 95.5 96 96.5 97 Training Time (s) Testing Accuracy   CD SCGIS GIS LBFGS (c) BROWN","0","50 100","150","20090 90.5 91 91.5 92 92.5 93 93.5 Training Time (s) F1 measure   CD SCGIS GIS LBFGS (d) CoNLL2000 Figure 2: First row: time versus the relative function value difference (17). Second row: time versus testing accuracy/F1. Time is in seconds. and IIS. We use σ2","= 10, and set β = 0.5 and γ = 0.001 in (16).","We begin at checking time versus the relative difference of the function value to the optimum:","L(w) − L(w∗",")/L(w∗","). (17) Results are in the first row of Figure 2. We check in the second row of Figure 2 about testing accuracy/F1 versus training time. Among the three IS/CD methods compared, the new CD approach is the fastest. SCGIS comes the second, while GIS is the last. This result is consistent with the tightness of their approximation functions; see (14). LBFGS has fast final convergence, but it does not perform well in the beginning."]},{"title":"5 Conclusions","paragraphs":["In summary, we create a general framework for explaining IS methods. Based on this framework, we develop a new CD method for Maxent. It is more efficient than existing IS methods."]},{"title":"References","paragraphs":["K.-W. Chang, C.-J. Hsieh, and C.-J. Lin. 2008. Coordinate descent method for large-scale L2-loss linear SVM. JMLR, 9:1369–1398.","John N. Darroch and Douglas Ratcliff. 1972. Generalized iterative scaling for log-linear models. Ann. Math. Statist., 43(5):1470–1480.","Stephen Della Pietra, Vincent Della Pietra, and John Lafferty. 1997. Inducing features of random fields. IEEE PAMI, 19(4):380–393.","Joshua Goodman. 2002. Sequential conditional generalized iterative scaling. In ACL, pages 9–16.","Robert Malouf. 2002. A comparison of algorithms for maximum entropy parameter estimation. In CONLL. 288"]}],"references":[{"authors":[{"first":"K.","middle":"-W.","last":"Chang"},{"first":"C.","middle":"-J.","last":"Hsieh"},{"first":"C.","middle":"-J.","last":"Lin"}],"year":"2008","title":"Coordinate descent method for large-scale L2-loss linear SVM","source":"K.-W. Chang, C.-J. Hsieh, and C.-J. Lin. 2008. Coordinate descent method for large-scale L2-loss linear SVM. JMLR, 9:1369–1398."},{"authors":[{"first":"John","middle":"N.","last":"Darroch"},{"first":"Douglas","last":"Ratcliff"}],"year":"1972","title":"Generalized iterative scaling for log-linear models","source":"John N. Darroch and Douglas Ratcliff. 1972. Generalized iterative scaling for log-linear models. Ann. Math. Statist., 43(5):1470–1480."},{"authors":[{"first":"Stephen","middle":"Della","last":"Pietra"},{"first":"Vincent","middle":"Della","last":"Pietra"},{"first":"John","last":"Lafferty"}],"year":"1997","title":"Inducing features of random fields","source":"Stephen Della Pietra, Vincent Della Pietra, and John Lafferty. 1997. Inducing features of random fields. IEEE PAMI, 19(4):380–393."},{"authors":[{"first":"Joshua","last":"Goodman"}],"year":"2002","title":"Sequential conditional generalized iterative scaling","source":"Joshua Goodman. 2002. Sequential conditional generalized iterative scaling. In ACL, pages 9–16."},{"authors":[{"first":"Robert","last":"Malouf"}],"year":"2002","title":"A comparison of algorithms for maximum entropy parameter estimation","source":"Robert Malouf. 2002. A comparison of algorithms for maximum entropy parameter estimation. In CONLL. 288"}],"cites":[{"style":0,"text":"Darroch and Ratcliff (1972)","origin":{"pointer":"/sections/2/paragraphs/19","offset":219,"length":27},"authors":[{"last":"Darroch"},{"last":"Ratcliff"}],"year":"1972","references":["/references/1"]},{"style":0,"text":"Pietra et al. (1997)","origin":{"pointer":"/sections/2/paragraphs/19","offset":290,"length":20},"authors":[{"last":"Pietra"},{"last":"al."}],"year":"1997","references":["/references/2"]},{"style":0,"text":"Goodman (2002)","origin":{"pointer":"/sections/2/paragraphs/19","offset":380,"length":14},"authors":[{"last":"Goodman"}],"year":"2002","references":["/references/3"]},{"style":0,"text":"Goodman, 2002","origin":{"pointer":"/sections/3/paragraphs/136","offset":243,"length":13},"authors":[{"last":"Goodman"}],"year":"2002","references":["/references/3"]},{"style":0,"text":"Chang et al. (2008)","origin":{"pointer":"/sections/4/paragraphs/9","offset":587,"length":19},"authors":[{"last":"Chang"},{"last":"al."}],"year":"2008","references":["/references/0"]},{"style":0,"text":"Malouf (2002)","origin":{"pointer":"/sections/5/paragraphs/0","offset":423,"length":13},"authors":[{"last":"Malouf"}],"year":"2002","references":["/references/4"]}]}
