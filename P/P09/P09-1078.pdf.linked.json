{"sections":[{"title":"","paragraphs":["Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 692–700, Suntec, Singapore, 2-7 August 2009. c⃝2009 ACL and AFNLP"]},{"title":"A Framework of Feature Selection Methods for Text Categorization   Shoushan Li1 Rui Xia2 Chengqing Zong 2 Chu-Ren Huang1   1 ","paragraphs":["Department of Chinese and Bilingual","Studies","The Hong Kong Polytechnic University {shoushan.li,churenhuang}","@gmail.com  2 National Laboratory of Pattern","Recognition","Institute of Automation Chinese Academy of Sciences","{rxia,cqzong}@nlpr.ia.ac.cn",""," "]},{"title":"Abstract","paragraphs":["In text categorization, feature selection (FS) is a strategy that aims at making text classifiers more efficient and accurate. However, when dealing with a new task, it is still difficult to quickly select a suitable one from various FS methods provided by many previous studies. In this paper, we propose a theoretic framework of FS methods based on two basic measurements: frequency measurement and ratio measurement. Then six popular FS methods are in detail discussed under this framework. Moreover, with the guidance of our theoretical analysis, we propose a novel method called weighed frequency and odds (WFO) that combines the two measurements with trained weights. The experimental results on data sets from both topic-based and sentiment classification tasks show that this new method is robust across different tasks and numbers of selected features."]},{"title":"1 Introduction","paragraphs":["With the rapid growth of online information, text classification, the task of assigning text documents to one or more predefined categories, has become one of the key tools for automatically handling and organizing text information.","The problems of text classification normally involve the difficulty of extremely high dimensional feature space which sometimes makes learning algorithms intractable. A standard procedure to reduce the feature dimensionality is called feature selection (FS). Various FS methods, such as document frequency (DF), information gain (IG), mutual information (MI), 2","χ -test (CHI), Bi-Normal Separation (BNS), and weighted log-likelihood ratio (WLLR), have been proposed for the tasks (Yang and Pedersen, 1997; Nigam et al., 2000; Forman, 2003) and make text classification more efficient and accurate.","However, comparing these FS methods appears to be difficult because they are usually based on different theories or measurements. For example, MI and IG are based on information theory, while CHI is mainly based on the measurements of statistic independence. Previous comparisons of these methods have mainly depended on empirical studies that are heavily affected by the experimental sets. As a result, conclusions from those studies are sometimes inconsistent. In order to better understand the relationship between these methods, building a general theoretical framework provides a fascinating perspective.","Furthermore, in real applications, selecting an appropriate FS method remains hard for a new task because too many FS methods are available due to the long history of FS studies. For example, merely in an early survey paper (Sebastiani, 2002), eight methods are mentioned. These methods are provided by previous work for dealing with different text classification tasks but none of them is shown to be robust across different classification applications.","In this paper, we propose a framework with two basic measurements for theoretical comparison of six FS methods which are widely used in text classification. Moreover, a novel method is set forth that combines the two measurements and tunes their influences considering different application domains and numbers of selected features.","The remainder of this paper is organized as follows. Section 2 introduces the related work on 692 feature selection for text classification. Section 3 theoretically analyzes six FS methods and proposes a new FS approach. Experimental results are presented and analyzed in Section 4. Finally, Section 5 draws our conclusions and outlines the future work."]},{"title":"2 Related Work","paragraphs":["FS is a basic problem in pattern recognition and has been a fertile field of research and development since the 1970s. It has been proven to be effective on removing irrelevant and redundant features, increasing efficiency in learning tasks, and improving learning performance.","FS methods fall into two broad categories, the filter model and the wrapper model (John et al., 1994). The wrapper model requires one predetermined learning algorithm in feature selection and uses its performance to evaluate and determine which features are selected. And the filter model relies on general characteristics of the training data to select some features without involving any specific learning algorithm. There is evidence that wrapper methods often perform better on small scale problems (John et al, 1994), but on large scale problems, such as text classification, wrapper methods are shown to be impractical because of its high computational cost. Therefore, in text classification, filter methods using feature scoring metrics are popularly used. Below we review some recent studies of feature selection on both topic-based and sentiment classification.","In the past decade, FS studies mainly focus on topic-based classification where the classification categories are related to the subject content, e.g., sport or education. Yang and Pedersen (1997) investigate five FS metrics and report that good FS methods improve the categorization accuracy with an aggressive feature removal using DF, IG, and CHI. More recently, Forman (2003) empirically compares twelve FS methods on 229 text classification problem instances and proposes a new method called 'Bi-Normal Separation' (BNS). Their experimental results show that BNS can perform very well in the evaluation metrics of recall rate and F-measure. But for the metric of precision, it often loses to IG. Besides these two comparison studies, many others contribute to this topic (Yang and Liu, 1999; Brank et al., 2002; Gabrilovich and Markovitch, 2004) and more and more new FS methods are generated, such as, Gini index (Shang et al., 2007), Distance to Transition Point (DTP) (Moyotl-Hernandez and Jimenez-Salazar, 2005), Strong Class Information Words (SCIW) (Li and Zong, 2005) and parameter tuning based FS for Rocchio classifier (Moschitti, 2003).","Recently, sentiment classification has become popular because of its wide applications (Pang et al., 2002). Its criterion of classification is the attitude expressed in the text (e.g., recommended or not recommended, positive or negative) rather than some facts (e.g., sport or education). To our best knowledge, yet no related work has focused on comparison studies of FS methods on this special task. There are only some scattered reports in their experimental studies. Riloff et al. (2006) report that the traditional FS method (only using IG method) performs worse than the baseline in some cases. However, Cui et al. (2006) present the experiments on the sentiment classification for large-scale online product reviews to show that using the FS method of CHI does not degrade the performance but can significantly reduce the dimension of the feature vector.","Moreover, Ng et al. (2006) examine the FS of the weighted log-likelihood ratio (WLLR) on the movie review dataset and achieves an accuracy of 87.1%, which is higher than the result reported by Pang and Lee (2004) with the same dataset. From the analysis above, we believe that the performance of the sentiment classification system is also dramatically affected by FS."]},{"title":"3 Our Framework","paragraphs":["In the selection process, each feature (term, or single word) is assigned with a score according to a score-computing function. Then those with higher scores are selected. These mathematical definitions of the score-computing functions are often defined by some probabilities which are estimated by some statistic information in the documents across different categories. For the convenience of description, we give some notations of these probabilities below.","( )P t : the probability that a document x contains term t ;","( )iP c : the probability that a document x does not belong to category ic ;","( , )iP t c : the joint probability that a document x contains term t and also belongs to category ic ;","( | )iP c t : the probability that a document x belongs to category ic under the condition that it contains term t. 693","( | )iP t c : the probability that, a document x does not contain term t with the condition that x belongs to category ic ; Some other probabilities, such as ( )P t , ( )iP c , ( | )iP t c , ( | )iP t c , ( | )iP c t , and ( | )iP c t , are similarly defined. In order to estimate these probabilities, statistical information from the training data is needed, and notations about the training data are given as follows:","1{ }m","i ic = : the set of categories;","iA : the number of the documents that contain the term t and also belong to category ic ;","iB : the number of the documents that contain the term t but do not belong to category ic ;","iN : the total number of the documents that belong to category ic ;","allN : the total number of all documents from the training data.","iC : the number of the documents that do not contain the term t but belong to category ic , i.e.,","i iN A−","iD : the number of the documents that neither contain the term t nor belong to category ic , i.e.,","all i iN N B− − ;","In this section, we would analyze theoretically six popular methods, namely DF, MI, IG, CHI, BNS, and WLLR. Although these six FS methods are defined differently with different scoring measurements, we believe that they are strongly related. In order to connect them, we define two basic measurements which are discussed as follows.","The first measurement is to compute the document frequency in one category, i.e., iA .","The second measurement is the ratio between the document frequencies in one category and the other categories, i.e., /i iA B . The terms with a high ratio are often referred to as the terms with high category information.","These two measurements form the basis for all the measurements that are used by the FS methods throughout this paper. In particular, we show that DF and MI are using the first and second measurement respectively. Other complicated FS methods are combinations of these two measurements. Thus, we regard the two measurements as basic, which are referred to as the frequency measurement and ratio measurement. 3.1 Document Frequency (DF) DF is the number of documents in which a term occurs. It is defined as 1( )","m iiDF A=="]},{"title":"∑","paragraphs":["The terms with low or high document frequency are often referred to as rare or common terms, respectively. It is easy to see that this FS method is based on the first basic measurement. It assumes that the terms with higher document frequency are more informative for classification. But sometimes this assumption does not make any sense, for example, the stop words (e.g., the, a, an) hold very high DF scores, but they seldom contribute to classification. In general, this simple method performs very well in some topic-based classification tasks (Yang and Pedersen, 1997). 3.2 Mutual Information (MI) The mutual information between term t and class ic is defined as ( | ) ( , ) log ( ) i i P t c I t c P t= And it is estimated as log ( )( ) i all i i i i A N MI A C A B× = + +  Let us consider the following formula (using Bayes theorem) ( | ) ( | )","( , ) log log ( ) ( ) i i i i P t c P c t I t c P t P c= = Therefore,","( , )= log ( | ) log ( )i i iI t c P c t P c− And it is estimated as log log log log 1 log(1 ) log / i i i i all i i i i all i i i all A N MI","A B N A B N","A N","N A B N","= − +","+ = − − = − + −  From this formula, we can see that the MI score is based on the second basic measurement. This method assumes that the term with higher category ratio is more effective for classification.","It is reported that this method is biased towards low frequency terms and the bias becomes extreme when ( )P t is near zero. It can be seen in the following formula (Yang and Pedersen, 1997)","( , ) log( ( | )) log( ( ))i iI t c P t c P t= − 694 Therefore, this method might perform badly when common terms are informative for classification.","Taking into account mutual information of all categories, two types of MI score are commonly used: the maximum score ( )maxI t and the average score ( )avgI t , i.e.,","1( ) max { ( , )}m","max i iI t I t c== , ","1( ) ( ) ( , )m avg i iiI t P c I t c== ⋅"]},{"title":"∑","paragraphs":[". We choose the maximum score since it performs better than the average score (Yang and Pedersen, 1997). It is worth noting that the same choice is made for other methods, including CHI, BNS, and WLLR in this paper. 3.3 Information Gain (IG) IG measures the number of bits of information obtained for category prediction by recognizing the presence or absence of a term in a document (Yang and Pedersen, 1997). The function is 1 1 1 ( ) { ( ) log ( )} +{ ( )[ ( | ) log ( | )] ( )[ ( | ) log ( | )]}","m i ii m","i ii m","i ii G t P c P c P t P c t P c t P t P c t P c t = = = = − +"]},{"title":"∑ ∑ ∑ ","paragraphs":["And it is estimated as 1 1 1 1 1 { log } +( / )[ log ] ( / )[ log ] m","i i i all all","m m","i i i alli i i i i i","m m","i i i alli i i i i i N N IG N N A A A N A B A B C C C N C D C D = = = = = = − + + + + +"]},{"title":"∑ ∑ ∑ ∑ ∑","paragraphs":["From the definition, we know that the information gain is the weighted average of the mutual information ( , )iI t c and ( , )iI t c where the weights are the joint probabilities ( , )iP t c and ( , )iP t c :","1 1( ) ( , ) ( , ) ( , ) ( , )m m i i i ii iG t P t c I t c P t c I t c= == +"]},{"title":"∑ ∑","paragraphs":["Since ( , )iP t c is closely related to the document frequency iA and the mutual information ( , )iI t c is shown to be based on the second measurement, we can say that the IG score is influenced by the two basic measurements.","3.4 2 χ Statistic (CHI) The CHI measurement (Yang and Pedersen, 1997) is defined as","2 ( )","( ) ( ) ( ) ( ) all i i i i i i i i i i i i N A D C B CHI A C B D A B C D⋅ − = + ⋅ + ⋅ + ⋅ + ","In order to get the relationship between CHI","and the two measurements, the above formula is","rewritten as follows","2","[ ( ) ( ) ]","( ) ( ) [ ( )] all i all i i i i i i all i i i all i i N A N N B N A B CHI N N N A B N A B⋅ − − − − = ⋅ − ⋅ + ⋅ − + ","For simplicity, we assume that there are two categories and the numbers of the training documents in the two categories are the same ( 2all iN N= ). The CHI score then can be written as","2 2","2 ( ) ( ) [2 ( )] 2 ( / 1) ","2 ( / 1) [ / ( / 1)] i i i i i i i i i i i i","i i i i i i i N A B CHI A B N A B N A B N","A B A B A B A − = + ⋅ − + − = + ⋅ ⋅ − +"," From the above formula, we see that the CHI score is related to both the frequency measurement iA ","and ratio measurement /i iA B . Also, when keeping the same ratio value, the terms with higher document frequencies will yield higher CHI scores. 3.5 Bi-Normal Separation (BNS)","BNS method is originally proposed by Forman","(2003) and it is defined as","1 1 ( , ) ( ( | )) ( ( | )i i iBNS t c F P t c F P t c− −","= − It is calculated using the following formula 1 1 ( ) ( )i i","i all i","A B","BNS F F","N N N− −","= − −  where ( )F x is the cumulative probability function of standard normal distribution.","For simplicity, we assume that there are two categories and the numbers of the training documents in the two categories are the same, i.e., 2all iN N= and we also assume that i iA B> . It should be noted that this assumption is only to allow easier analysis but will not be applied in our experiment implementation. In addition, we only consider the case when / 0.5i iA N ≤ . In fact, most terms take the document frequency","iA which is less than half of iN .","Under these conditions, the BNS score can be shown in Figure 1 where the area of the shadow part represents ( / / )i i i iA N B N− and the length of the projection to the x axis represents the BNS score. 695 From Figure 1, we can easily draw the two following conclusions: 1) Given the same value of iA , the BNS score","increases with the increase of i iA B− . 2) Given the same value of i iA B− , BNS score","increase with the decrease of iA ."," Figure 1. View of BNS using the normal probability distribution. Both the left and right graphs have shadowed areas of the same size. ","And the value of i iA B− can be rewritten as the following 1(1 ) / i i","i i i i","i i i","A B","A B A A","A A B−","− = ⋅ = − ⋅","The above analysis gives the following conclusions regarding the relationship between BNS and the two basic measurements: 1) Given the same iA , the BNS score increases","with the increase of /i iA B . 2) Given the same /i iA B , when iA increases,","i iA B− also increase. It seems that the BNS","score does not show a clear relationship with","iA .","In summary, the BNS FS method is biased towards the terms with the high category ratio but cannot be said to be sensitive to document frequency.","3.6 Weighted Log Likelihood Ratio (WLLR)","WLLR method (Nigam et al., 2000) is defined as ( | ) ( , ) ( | ) log ( | ) i i i i P t c WLLR t c P t c P t c= And it is estimated as","( ) logi i all i i i i A A N N WLLR N B N⋅ − = ⋅ ","The formula shows WLLR is proportional to the frequency measurement and the logarithm of the ratio measurement. Clearly, WLLR is biased towards the terms with both high category ratio and high document frequency and the frequency measurement seems to take a more important place than the ratio measurement. 3.7 Weighed Frequency and Odds (WFO) So far in this section, we have shown that the two basic measurements constitute the six FS methods. The class prior probabilities,","( ), 1, 2,...,iP c i m= , are also related to the selection methods except for the two basic measurements. Since they are often estimated according to the distribution of the documents in the training data and are identical for all the terms in a class, we ignore the discussion of their influence on the selection measurements. In the experiment, we consider the case when training data have equal class prior probabilities. When training data are unbalanced, we need to change the forms of the two basic measurements to","/i iA N and ( ) / ( )i all i i iA N N B N⋅ − ⋅ .","Because some methods are expressed in complex forms, it is difficult to explain their relationship with the two basic measurements, for example, which one prefers the category ratio most. Instead, we will give the preference analysis in the experiment by analyzing the features in real applications. But the following two conclusions are drawn without doubt according to the theoretical analysis given above. 1) Good features are features with high","document frequency; 2) Good features are features with high","category ratio.","These two conclusions are consistent with the original intuition. However, using any single one does not provide competence in selecting the best set of features. For example, stop words, such as ‘a’, ‘the’ and ‘as’, have very high document frequency but are useless for the classification. In real applications, we need to mix these two measurements to select good features. Because of different distribution of features in different domains, the importance of each measurement may differ a lot in different applications. Moreover, even in a given domain, when different numbers of features are to be selected, different combinations of the two measurements are required to provide the best performance.","Although a great number of FS methods is available, none of them can appropriately change the preference of the two measurements. A better way is to tune the importance according to the application rather than to use a predetermined combination. Therefore, we propose a new FS method called Weighed Frequency and Odds (WFO), which is defined as 696 ( | ) / ( | ) 1i iwhen P t c P t c >","1( | )","( , ) ( | ) [log ] ( | ) i i i i P t c WFO t c P t c","P t cλ λ− = ( , ) 0i else WFO t c =  And it is estimated as","1( )","( ) (log )i i all i i i i A A N N WFO N B Nλ λ−⋅ − = ⋅  where λ is the parameter for tuning the weight between frequency and odds. The value of λ varies from 0 to 1. By assigning different value to λ we can adjust the preference of each measurement. Specially, when 0λ = , the algorithm prefers the category ratio that is equivalent to the MI method; when 1λ = , the algorithm is similar to DF; when 0.5λ = , the algorithm is exactly the WLLR method. In real applications, a suitable parameter λ needs to be learned by using training data."]},{"title":"4 Experimental Studies 4.1 Experimental Setup Data Set:","paragraphs":["The experiments are carried out on both topic-based and sentiment text classification datasets. In topic-based text classification, we use two popular data sets: one subset of Reuters-21578 referred to as R2 and the 20 Newsgroup dataset referred to as 20NG. In detail, R2 consist of about 2,000 2-category documents from standard corpus of Reuters-21578. And 20NG is a collection of approximately 20,000 20-category documents 1",". In sentiment text classification, we also use two data sets: one is the widely used Cornell movie-review dataset2"," (Pang and Lee, 2004) and one dataset from product reviews of domain DVD3","(Blitzer et al., 2007). Both of them are 2-category tasks and each consists of 2,000 reviews. In our experiments, the document numbers of all data sets are (nearly) equally distributed cross all categories.","Classification Algorithm: Many classification algorithms are available for text classification, such as Naïve Bayes, Maximum Entropy, k-NN, and SVM. Among these methods, SVM is shown to perform better than other methods (Yang and Pedersen, 1997; Pang et al.,"," 1 http://people.csail.mit.edu/~jrennie/20Newsgroups/ 2 http://www.cs.cornell.edu/People/pabo/movie-review-data/ 3 http://www.seas.upenn.edu/~mdredze/datasets/sentiment/  2002). Hence we apply SVM algorithm with the help of the LIBSVM 4","tool. Almost all parameters are set to their default values except the kernel function which is changed from a polynomial kernel function to a linear one because the linear one usually performs better for text classification tasks.","Experiment Implementation: In the experiments, each dataset is randomly and evenly split into two subsets: 90% documents as the training data and the remaining 10% as testing data. The training data are used for training SVM classifiers, learning parameters in WFO method and selecting \"good\" features for each FS method. The features are single words with a bool weight (0 or 1), representing the presence or absence of a feature. In addition to the “principled” FS methods, terms occurring in less than three documents ( 3DF ≤ ) in the training set are removed.","4.2 Relationship between FS Methods and the Two Basic Measurements To help understand the relationship between FS methods and the two basic measurements, the empirical study is presented as follows.","Since the methods of DF and MI only utilize the document frequency and category information respectively, we use the DF scores and MI scores to represent the information of the two basic measurements. Thus we would select the top-2% terms with each method and then investigate the distribution of their DF and MI scores.","First of all, for clear comparison, we normalize the scores coming from all the methods using Min-Max normalization method which is designed to map a score s to 's in the range [0, 1] by computing ' s Min s Max Min− = −  where Min and Max denote the minimum and maximum values respectively in all terms’ scores using one FS method.","Table 1 shows the mean values of all top-2% terms’ MI scores and DF scores of all the six FS methods in each domain. From this table, we can apparently see the relationship between each method and the two basic measurements. For instance, BNS most distinctly prefers the terms with high MI scores and low DF scores. According to the degree of this preference, we"," 4 http://www.csie.ntu.edu.tw/~cjlin/libsvm/ 697","FS Methods","Domain","20NG R2 Movie DVD","DF score MI score DF score MI score DF score MI score DF score MI score MI 0.004 0.870 0.047 0.959 0.003 0.888 0.004 0.881 BNS 0.005 0.864 0.117 0.922 0.008 0.881 0.006 0.880 CHI 0.015 0.814 0.211 0.748 0.092 0.572 0.055 0.676 IG 0.087 0.525 0.209 0.792 0.095 0.559 0.066 0.669 WLLR 0.026 0.764 0.206 0.805 0.168 0.414 0.127 0.481 DF 0.122 0.252 0.268 0.562 0.419 0.09 0.321 0.111"," Table 1. The mean values of all top-2% terms’ MI and DF scores using six FS methods in each domain"," can rank these six methods as MI, BNS IG, CHI, WLLR DFf f , where x yf"," means method x prefers the terms with higher MI scores (higher category information) and lower DF scores (lower document frequency) than method y. This empirical discovery is in agreement with the finding that WLLR is biased towards the high frequency terms and also with the finding that BNS is biased towards high category information (cf. Section 3 theoretical analysis). Also, we can find that CHI and IG share a similar preference of these two measurements in 2-category domains, i.e., R2, movie, and DVD. This gives a good explanation that CHI and IG are two similar-performed methods for 2-category tasks, which have been found by Forman (2003) in their experimental studies.","According to the preference, we roughly cluster FS methods into three groups. The first group includes the methods which dramatically prefer the category information, e.g., MI and BNS; the second one includes those which prefer both kinds of information, e.g., CHI, IG, and WLLR; and the third one includes those which strongly prefer frequency information, e.g., DF. 4.3 Performances of Different FS Methods It is worth noting that learning parameters in WFO is very important for its good performance. We use 9-fold cross validation to help learning the parameter λ so as to avoid over-fitting. Specifically, we run nine times by using every 8 fold documents as a new training data set and the remaining one fold documents as a development data set. In each running with one fixed feature number m, we get the best ,i m bestλ − (i=1,..., 9) value through varying ,i mλ from 0 to 1 with the step of 0.1 to get the best performance in the development data set. The average value m bestλ − , i.e.,","9 ,1( ) / 9m best i m bestiλ λ− −=="]},{"title":"∑","paragraphs":["is used for further testing.","Figure 2 shows the experimental results when using all FS methods with different selected feature numbers. The red line with star tags represents the results of WFO. At the first glance, in R2 domain, the differences of performances across all are very noisy when the feature number is larger than 1,000, which makes the comparison meaningless. We think that this is because the performances themselves in this task are very high (nearly 98%) and the differences between two FS methods cannot be very large (less than one percent). Even this, WFO method do never get the worst performance and can also achieve the top performance in about half times, e.g., when feature numbers are 20, 50, 100, 500, 3000.","Let us pay more attention to the other three domains and discuss the results in the following two cases.","In the first case when the feature number is low (about less than 1,000), the FS methods in the second group including IG, CHI, WLLR, always perform better than those in the other two groups. WFO can also perform well because its parameters m bestλ − are successfully learned to be around 0.5, which makes it consistently belong to the second group. Take 500 feature number for instance, the parameters 500 bestλ − are 0.42, 0.50, and 0.34 in these three domains respectively.","In the second case when the feature number is large, among the six traditional methods, MI and BNS take the leads in the domains of 20NG and Movie while IG and CHI seem to be better and more stable than others in the domain of DVD. As for WFO, its performances are excellent cross all these three domains and different feature numbers. In each domain, it performs similarly as or better than the top methods due to its well-learned parameters. For example, in 20NG, the parameters m bestλ − are 0.28, 0.20, 0.08, and 0.01 when feature numbers are 10,000, 15,000, 20,000, and 30,000. These values are close to 0 698 (WFO equals MI when 0λ = ) while MI is the top one in this domain. 10 20 50 100 200 500 1000 2000 3000 42270.88 0.9 0.92 0.94 0.96 0.98 1 feature number a c c u r a c y Topic - R2   DF MI IG BNS CHI WLLR WFO  200 500 1000 2000 5000 10000 15000 20000 30000 320910.5 0.55 0.6 0.65 0.7 0.75 0.8 0.85 0.9 feature number a c c u r a c y Topic - 20NG   DF MI IG BNS CHI WLLR WFO  50 200 500 1000 4000 7000 10000 13000 151760.55 0.6 0.65 0.7 0.75 0.8 0.85 feature number a c c u r a c y Sentiment - Movie   DF MI IG BNS CHI WLLR WFO  20 50 100 500 1000 1500 2000 3000 4000 58240.5 0.55 0.6 0.65 0.7 0.75 0.8 feature number a c c u r a c y Sentiment - DVD   DF MI IG BNS CHI WLLR WFO"," Figure 2. The classification accuracies of the four domains using seven different FS methods while increasing the number of selected features. ","From Figure 2, we can also find that FS does help sentiment classification. At least, it can dramatically decrease the feature numbers without losing classification accuracies (see Movie domain, using only 500-4000 features is as good as using all 15176 features)."]},{"title":"5 Conclusion and Future Work","paragraphs":["In this paper, we propose a framework with two basic measurements and use it to theoretically analyze six FS methods. The differences among them mainly lie in how they use these two measurements. Moreover, with the guidance of the analysis, a novel method called WFO is proposed, which combine these two measurements with trained weights. The experimental results show that our framework helps us to better understand and compare different FS methods. Furthermore, the novel method WFO generated from the framework, can perform robustly across different domains and feature numbers.","In our study, we use four data sets to test our new method. There are much more data sets on text categorization which can be used. In additional, we only focus on using balanced samples in each category to do the experiments. It is also necessary to compare the FS methods on some unbalanced data sets, which are common in real-life applications (Forman, 2003; Mladeni and Marko, 1999). These matters will be dealt with in the future work."]},{"title":"Acknowledgments","paragraphs":["The research work described in this paper has been partially supported by Start-up Grant for Newly Appointed Professors, No. 1-BBZM in the Hong Kong Polytechnic University."]},{"title":"References","paragraphs":["J. Blitzer, M. Dredze, and F. Pereira. 2007. Biographies, Bollywood, Boom-boxes and Blenders: Domain adaptation for sentiment classification. In Proceedings of ACL-07, the 45th Meeting of the Association for Computational Linguistics.","J. Brank, M. Grobelnik, N. Milic-Frayling, and D. Mladenic. 2002. Interaction of feature selection methods and linear classification models. In Workshop on Text Learning held at ICML.","H. Cui, V. Mittal, and M. Datar. 2006. Comparative experiments on sentiment classification for online product reviews. In Proceedings of AAAI-06, the 21st National Conference on Artificial Intelligence.","G. Forman. 2003. An extensive empirical study of feature selection metrics for text classification. The Journal of Machine Learning Research, 3(1): 1289-1305. 699","E. Gabrilovich and S. Markovitch. 2004. Text categorization with many redundant features: using aggressive feature selection to make SVMs competitive with C4.5. In Proceedings of the ICML, the 21st International Conference on Machine Learning.","G. John, K. Ron, and K. Pfleger. 1994. Irrelevant features and the subset selection problem. In Proceedings of ICML-94, the 11st International Conference on Machine Learning.","S. Li and C. Zong. 2005. A new approach to feature selection for text categorization. In Proceedings of the IEEE International Conference on Natural Language Processing and Knowledge Engineering (NLP-KE).","D. Mladeni and G. Marko. 1999. Feature selection for unbalanced class distribution and naive bayes. In Proceedings of ICML-99, the 16th International Conference on Machine Learning.","A. Moschitti. 2003. A study on optimal parameter tuning for Rocchio text classifier. In Proceedings of ECIR, Lecture Notes in Computer Science, vol. 2633, pp. 420-435.","E. Moyotl-Hernandez and H. Jimenez-Salazar. 2005. Enhancement of DTP feature selection method for text categorization. In Proceedings of CICLing, Lecture Notes in Computer Science, vol.3406, pp.719-722.","V. Ng, S. Dasgupta, and S. M. Niaz Arifin. 2006. Examining the role of linguistic knowledge sources in the automatic identification and classification of reviews. In Proceedings of the COLING/ACL Main Conference Poster Sessions.","K. Nigam, A. McCallum, S. Thrun, and T. Mitchell. 2000. Text classification from labeled and unlabeled documents using EM. Machine Learning, 39(2/3): 103-134.","B. Pang, L. Lee, and S. Vaithyanathan. 2002. Thumbs up? Sentiment classification using machine learning techniques. In Proceedings of EMNLP-02, the Conference on Empirical Methods in Natural Language Processing.","B. Pang and L. Lee. 2004. A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts. In Proceedings of ACL-04, the 42nd Meeting of the Association for Computational Linguistics.","E. Riloff, S. Patwardhan, and J. Wiebe. 2006. Feature subsumption for opinion analysis. In Proceedings of EMNLP-06, the Conference on Empirical Methods in Natural Language Processing,.","F. Sebastiani. 2002. Machine learning in automated text categorization. ACM Computing Surveys, 34(1): 1-47.","W. Shang, H. Huang, H. Zhu, Y. Lin, Y. Qu, and Z. Wang. 2007. A novel feature selection algorithm for text categorization. The Journal of Expert System with Applications, 33:1-5.","Y. Yang and J. Pedersen. 1997. A comparative study on feature selection in text categorization. In Proceedings of ICML-97, the 14th International Conference on Machine Learning.","Y. Yang and X. Liu. 1999. A re-examination of text categorization methods. In Proceedings of SIGIR-99, the 22nd annual international ACM Conference on Research and Development in Information Retrieval. 700"]}],"references":[{"authors":[{"first":"J.","last":"Blitzer"},{"first":"M.","last":"Dredze"},{"first":"F.","last":"Pereira"}],"year":"2007","title":"Biographies, Bollywood, Boom-boxes and Blenders: Domain adaptation for sentiment classification","source":"J. Blitzer, M. Dredze, and F. Pereira. 2007. Biographies, Bollywood, Boom-boxes and Blenders: Domain adaptation for sentiment classification. In Proceedings of ACL-07, the 45th Meeting of the Association for Computational Linguistics."},{"authors":[{"first":"J.","last":"Brank"},{"first":"M.","last":"Grobelnik"},{"first":"N.","last":"Milic-Frayling"},{"first":"D.","last":"Mladenic"}],"year":"2002","title":"Interaction of feature selection methods and linear classification models","source":"J. Brank, M. Grobelnik, N. Milic-Frayling, and D. Mladenic. 2002. Interaction of feature selection methods and linear classification models. In Workshop on Text Learning held at ICML."},{"authors":[{"first":"H.","last":"Cui"},{"first":"V.","last":"Mittal"},{"first":"M.","last":"Datar"}],"year":"2006","title":"Comparative experiments on sentiment classification for online product reviews","source":"H. Cui, V. Mittal, and M. Datar. 2006. Comparative experiments on sentiment classification for online product reviews. In Proceedings of AAAI-06, the 21st National Conference on Artificial Intelligence."},{"authors":[{"first":"G.","last":"Forman"}],"year":"2003","title":"An extensive empirical study of feature selection metrics for text classification","source":"G. Forman. 2003. An extensive empirical study of feature selection metrics for text classification. The Journal of Machine Learning Research, 3(1): 1289-1305. 699"},{"authors":[{"first":"E.","last":"Gabrilovich"},{"first":"S.","last":"Markovitch"}],"year":"2004","title":"Text categorization with many redundant features: using aggressive feature selection to make SVMs competitive with C4","source":"E. Gabrilovich and S. Markovitch. 2004. Text categorization with many redundant features: using aggressive feature selection to make SVMs competitive with C4.5. In Proceedings of the ICML, the 21st International Conference on Machine Learning."},{"authors":[{"first":"G.","last":"John"},{"first":"K.","last":"Ron"},{"first":"K.","last":"Pfleger"}],"year":"1994","title":"Irrelevant features and the subset selection problem","source":"G. John, K. Ron, and K. Pfleger. 1994. Irrelevant features and the subset selection problem. In Proceedings of ICML-94, the 11st International Conference on Machine Learning."},{"authors":[{"first":"S.","last":"Li"},{"first":"C.","last":"Zong"}],"year":"2005","title":"A new approach to feature selection for text categorization","source":"S. Li and C. Zong. 2005. A new approach to feature selection for text categorization. In Proceedings of the IEEE International Conference on Natural Language Processing and Knowledge Engineering (NLP-KE)."},{"authors":[{"first":"D.","last":"Mladeni"},{"first":"G.","last":"Marko"}],"year":"1999","title":"Feature selection for unbalanced class distribution and naive bayes","source":"D. Mladeni and G. Marko. 1999. Feature selection for unbalanced class distribution and naive bayes. In Proceedings of ICML-99, the 16th International Conference on Machine Learning."},{"authors":[{"first":"A.","last":"Moschitti"}],"year":"2003","title":"A study on optimal parameter tuning for Rocchio text classifier","source":"A. Moschitti. 2003. A study on optimal parameter tuning for Rocchio text classifier. In Proceedings of ECIR, Lecture Notes in Computer Science, vol. 2633, pp. 420-435."},{"authors":[{"first":"E.","last":"Moyotl-Hernandez"},{"first":"H.","last":"Jimenez-Salazar"}],"year":"2005","title":"Enhancement of DTP feature selection method for text categorization","source":"E. Moyotl-Hernandez and H. Jimenez-Salazar. 2005. Enhancement of DTP feature selection method for text categorization. In Proceedings of CICLing, Lecture Notes in Computer Science, vol.3406, pp.719-722."},{"authors":[{"first":"V.","last":"Ng"},{"first":"S.","last":"Dasgupta"},{"first":"S.","middle":"M. Niaz","last":"Arifin"}],"year":"2006","title":"Examining the role of linguistic knowledge sources in the automatic identification and classification of reviews","source":"V. Ng, S. Dasgupta, and S. M. Niaz Arifin. 2006. Examining the role of linguistic knowledge sources in the automatic identification and classification of reviews. In Proceedings of the COLING/ACL Main Conference Poster Sessions."},{"authors":[{"first":"K.","last":"Nigam"},{"first":"A.","last":"McCallum"},{"first":"S.","last":"Thrun"},{"first":"T.","last":"Mitchell"}],"year":"2000","title":"Text classification from labeled and unlabeled documents using EM","source":"K. Nigam, A. McCallum, S. Thrun, and T. Mitchell. 2000. Text classification from labeled and unlabeled documents using EM. Machine Learning, 39(2/3): 103-134."},{"authors":[{"first":"B.","last":"Pang"},{"first":"L.","last":"Lee"},{"first":"S.","last":"Vaithyanathan"}],"year":"2002","title":"Thumbs up? Sentiment classification using machine learning techniques","source":"B. Pang, L. Lee, and S. Vaithyanathan. 2002. Thumbs up? Sentiment classification using machine learning techniques. In Proceedings of EMNLP-02, the Conference on Empirical Methods in Natural Language Processing."},{"authors":[{"first":"B.","last":"Pang"},{"first":"L.","last":"Lee"}],"year":"2004","title":"A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts","source":"B. Pang and L. Lee. 2004. A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts. In Proceedings of ACL-04, the 42nd Meeting of the Association for Computational Linguistics."},{"authors":[{"first":"E.","last":"Riloff"},{"first":"S.","last":"Patwardhan"},{"first":"J.","last":"Wiebe"}],"year":"2006","title":"Feature subsumption for opinion analysis","source":"E. Riloff, S. Patwardhan, and J. Wiebe. 2006. Feature subsumption for opinion analysis. In Proceedings of EMNLP-06, the Conference on Empirical Methods in Natural Language Processing,."},{"authors":[{"first":"F.","last":"Sebastiani"}],"year":"2002","title":"Machine learning in automated text categorization","source":"F. Sebastiani. 2002. Machine learning in automated text categorization. ACM Computing Surveys, 34(1): 1-47."},{"authors":[{"first":"W.","last":"Shang"},{"first":"H.","last":"Huang"},{"first":"H.","last":"Zhu"},{"first":"Y.","last":"Lin"},{"first":"Y.","last":"Qu"},{"first":"Z.","last":"Wang"}],"year":"2007","title":"A novel feature selection algorithm for text categorization","source":"W. Shang, H. Huang, H. Zhu, Y. Lin, Y. Qu, and Z. Wang. 2007. A novel feature selection algorithm for text categorization. The Journal of Expert System with Applications, 33:1-5."},{"authors":[{"first":"Y.","last":"Yang"},{"first":"J.","last":"Pedersen"}],"year":"1997","title":"A comparative study on feature selection in text categorization","source":"Y. Yang and J. Pedersen. 1997. A comparative study on feature selection in text categorization. In Proceedings of ICML-97, the 14th International Conference on Machine Learning."},{"authors":[{"first":"Y.","last":"Yang"},{"first":"X.","last":"Liu"}],"year":"1999","title":"A re-examination of text categorization methods","source":"Y. Yang and X. Liu. 1999. A re-examination of text categorization methods. In Proceedings of SIGIR-99, the 22nd annual international ACM Conference on Research and Development in Information Retrieval. 700"}],"cites":[{"style":0,"text":"Yang and Pedersen, 1997","origin":{"pointer":"/sections/3/paragraphs/2","offset":119,"length":23},"authors":[{"last":"Yang"},{"last":"Pedersen"}],"year":"1997","references":["/references/17"]},{"style":0,"text":"Nigam et al., 2000","origin":{"pointer":"/sections/3/paragraphs/2","offset":144,"length":18},"authors":[{"last":"Nigam"},{"last":"al."}],"year":"2000","references":["/references/11"]},{"style":0,"text":"Forman, 2003","origin":{"pointer":"/sections/3/paragraphs/2","offset":164,"length":12},"authors":[{"last":"Forman"}],"year":"2003","references":["/references/3"]},{"style":0,"text":"Sebastiani, 2002","origin":{"pointer":"/sections/3/paragraphs/4","offset":225,"length":16},"authors":[{"last":"Sebastiani"}],"year":"2002","references":["/references/15"]},{"style":0,"text":"John et al., 1994","origin":{"pointer":"/sections/4/paragraphs/1","offset":83,"length":17},"authors":[{"last":"John"},{"last":"al."}],"year":"1994","references":["/references/5"]},{"style":0,"text":"Yang and Pedersen (1997)","origin":{"pointer":"/sections/4/paragraphs/2","offset":172,"length":24},"authors":[{"last":"Yang"},{"last":"Pedersen"}],"year":"1997","references":["/references/17"]},{"style":0,"text":"Forman (2003)","origin":{"pointer":"/sections/4/paragraphs/2","offset":366,"length":13},"authors":[{"last":"Forman"}],"year":"2003","references":["/references/3"]},{"style":0,"text":"Yang and Liu, 1999","origin":{"pointer":"/sections/4/paragraphs/2","offset":777,"length":18},"authors":[{"last":"Yang"},{"last":"Liu"}],"year":"1999","references":["/references/18"]},{"style":0,"text":"Brank et al., 2002","origin":{"pointer":"/sections/4/paragraphs/2","offset":797,"length":18},"authors":[{"last":"Brank"},{"last":"al."}],"year":"2002","references":["/references/1"]},{"style":0,"text":"Gabrilovich and Markovitch, 2004","origin":{"pointer":"/sections/4/paragraphs/2","offset":817,"length":32},"authors":[{"last":"Gabrilovich"},{"last":"Markovitch"}],"year":"2004","references":["/references/4"]},{"style":0,"text":"Shang et al., 2007","origin":{"pointer":"/sections/4/paragraphs/2","offset":920,"length":18},"authors":[{"last":"Shang"},{"last":"al."}],"year":"2007","references":["/references/16"]},{"style":0,"text":"Moyotl-Hernandez and Jimenez-Salazar, 2005","origin":{"pointer":"/sections/4/paragraphs/2","offset":977,"length":42},"authors":[{"last":"Moyotl-Hernandez"},{"last":"Jimenez-Salazar"}],"year":"2005","references":["/references/9"]},{"style":0,"text":"Li and Zong, 2005","origin":{"pointer":"/sections/4/paragraphs/2","offset":1061,"length":17},"authors":[{"last":"Li"},{"last":"Zong"}],"year":"2005","references":["/references/6"]},{"style":0,"text":"Moschitti, 2003","origin":{"pointer":"/sections/4/paragraphs/2","offset":1134,"length":15},"authors":[{"last":"Moschitti"}],"year":"2003","references":["/references/8"]},{"style":0,"text":"Pang et al., 2002","origin":{"pointer":"/sections/4/paragraphs/3","offset":88,"length":17},"authors":[{"last":"Pang"},{"last":"al."}],"year":"2002","references":["/references/12"]},{"style":0,"text":"Riloff et al. (2006)","origin":{"pointer":"/sections/4/paragraphs/3","offset":472,"length":20},"authors":[{"last":"Riloff"},{"last":"al."}],"year":"2006","references":["/references/14"]},{"style":0,"text":"Cui et al. (2006)","origin":{"pointer":"/sections/4/paragraphs/3","offset":611,"length":17},"authors":[{"last":"Cui"},{"last":"al."}],"year":"2006","references":["/references/2"]},{"style":0,"text":"Ng et al. (2006)","origin":{"pointer":"/sections/4/paragraphs/4","offset":10,"length":16},"authors":[{"last":"Ng"},{"last":"al."}],"year":"2006","references":["/references/10"]},{"style":0,"text":"Pang and Lee (2004)","origin":{"pointer":"/sections/4/paragraphs/4","offset":193,"length":19},"authors":[{"last":"Pang"},{"last":"Lee"}],"year":"2004","references":["/references/13"]},{"style":0,"text":"Yang and Pedersen, 1997","origin":{"pointer":"/sections/6/paragraphs/0","offset":550,"length":23},"authors":[{"last":"Yang"},{"last":"Pedersen"}],"year":"1997","references":["/references/17"]},{"style":0,"text":"Yang and Pedersen, 1997","origin":{"pointer":"/sections/6/paragraphs/8","offset":166,"length":23},"authors":[{"last":"Yang"},{"last":"Pedersen"}],"year":"1997","references":["/references/17"]},{"style":0,"text":"Yang and Pedersen, 1997","origin":{"pointer":"/sections/7/paragraphs/0","offset":79,"length":23},"authors":[{"last":"Yang"},{"last":"Pedersen"}],"year":"1997","references":["/references/17"]},{"style":0,"text":"Yang and Pedersen, 1997","origin":{"pointer":"/sections/7/paragraphs/0","offset":385,"length":23},"authors":[{"last":"Yang"},{"last":"Pedersen"}],"year":"1997","references":["/references/17"]},{"style":0,"text":"Yang and Pedersen, 1997","origin":{"pointer":"/sections/10/paragraphs/1","offset":45,"length":23},"authors":[{"last":"Yang"},{"last":"Pedersen"}],"year":"1997","references":["/references/17"]},{"style":0,"text":"Nigam et al., 2000","origin":{"pointer":"/sections/10/paragraphs/47","offset":13,"length":18},"authors":[{"last":"Nigam"},{"last":"al."}],"year":"2000","references":["/references/11"]},{"style":0,"text":"Pang and Lee, 2004","origin":{"pointer":"/sections/11/paragraphs/2","offset":2,"length":18},"authors":[{"last":"Pang"},{"last":"Lee"}],"year":"2004","references":["/references/13"]},{"style":0,"text":"Blitzer et al., 2007","origin":{"pointer":"/sections/11/paragraphs/3","offset":1,"length":20},"authors":[{"last":"Blitzer"},{"last":"al."}],"year":"2007","references":["/references/0"]},{"style":0,"text":"Yang and Pedersen, 1997","origin":{"pointer":"/sections/11/paragraphs/4","offset":221,"length":23},"authors":[{"last":"Yang"},{"last":"Pedersen"}],"year":"1997","references":["/references/17"]},{"style":0,"text":"Forman (2003)","origin":{"pointer":"/sections/11/paragraphs/19","offset":639,"length":13},"authors":[{"last":"Forman"}],"year":"2003","references":["/references/3"]},{"style":0,"text":"Forman, 2003","origin":{"pointer":"/sections/13/paragraphs/1","offset":348,"length":12},"authors":[{"last":"Forman"}],"year":"2003","references":["/references/3"]},{"style":0,"text":"Mladeni and Marko, 1999","origin":{"pointer":"/sections/13/paragraphs/1","offset":362,"length":23},"authors":[{"last":"Mladeni"},{"last":"Marko"}],"year":"1999","references":["/references/7"]}]}
