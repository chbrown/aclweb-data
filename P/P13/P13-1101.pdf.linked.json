{"sections":[{"title":"","paragraphs":["Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1023–1032, Sofia, Bulgaria, August 4-9 2013. c⃝2013 Association for Computational Linguistics"]},{"title":"Subtree Extractive Summarization via Submodular Maximization Hajime Morita Tokyo Institute of Technology, Japan morita@lr.pi.titech.ac.jp Hiroya Takamura Tokyo Institute of Technology, Japan takamura@pi.titech.ac.jp Ryohei Sasano Tokyo Institute of Technology, Japan sasano@pi.titech.ac.jp Manabu Okumura Tokyo Institute of Technology, Japan oku@pi.titech.ac.jp Abstract","paragraphs":["This study proposes a text summarization model that simultaneously performs sentence extraction and compression. We translate the text summarization task into a problem of extracting a set of dependency subtrees in the document cluster. We also encode obligatory case constraints as must-link dependency constraints in order to guarantee the readability of the generated summary. In order to handle the subtree extraction problem, we investigate a new class of submodular maximization problem, and a new algorithm that has the approximation ratio 1","2 (1 − e−1","). Our experiments with the NTCIR ACLIA test collections show that our approach outperforms a state-of-the-art algorithm."]},{"title":"1 Introduction","paragraphs":["Text summarization is often addressed as a task of simultaneously performing sentence extraction and sentence compression (Berg-Kirkpatrick et al., 2011; Martins and Smith, 2009). Joint models of sentence extraction and compression have a great benefit in that they have a large degree of freedom as far as controlling redundancy goes. In contrast, conventional two-stage approaches (Zajic et al., 2006), which first generate candidate compressed sentences and then use them to generate a summary, have less computational complexity than joint models. However, two-stage approaches are suboptimal for text summarization. For example, when we compress sentences first, the compressed sentences may fail to contain important pieces of information due to the length limit imposed on each sentence. On the other hand, when we extract sentences first, an important sentence may fail to be selected, simply because it is long. Enumerating a huge number of compressed sentences is also infeasible. Joint models can prune unimportant or redundant descriptions without resorting to enumeration.","Meanwhile, submodular maximization has recently been applied to the text summarization task, and the methods thereof have performed very well (Lin and Bilmes, 2010; Lin and Bilmes, 2011; Morita et al., 2011). Formalizing summarization as a submodular maximization problem has an important benefit inthat the problem can be solved by using a greedy algorithm with a performance guarantee.","We therefore decided to formalize the task of simultaneously performing sentence extraction and compression as a submodular maximization problem. That is, we extract subsentences for mak-ing the summary directly from all available subsentences in the documents and not in a stepwise fashion. However, there is a difficulty with such a formalization. In the past, the resulting maximization problem has been often accompanied by thousands of linear constraints representing logical relations between words. The existing greedy algorithm for solving submodular maximization problems cannot work in the presence of such numerous constraints although monotone and non-monotone submodular maximization with constraints other than budget constraints have been studied (Lee et al., 2009; Kulik et al., 2009; Gupta et al., 2010). In this study, we avoid this difficulty by reducing the task to one of extracting dependency subtrees from sentences in the source documents. The reduction replaces the difficulty of numerous linear constraints with another difficulty wherein two subtrees can share the same word to-1023 ken when they are selected from the same sentence, and as a result, the cost of the union of the two subtrees is not always the mere sum of their costs. We can overcome this difficulty by tackling a new class of submodular maximization problem: a budgeted monotone nondecreasing submodular function maximization with a cost function, where the cost of an extraction unit varies depending on what other extraction units are selected. By formalizing the subtree extraction problem as this new maximization problem, we can treat the constraints regarding the grammaticality of the compressed sentences in a straightforward way and use an arbitrary monotone submodular word score function for words including our word score function (shown later). We also propose a new greedy algorithm that solves this new class of maximization problem with a performance guarantee 1","2 (1 − e−1",").","We evaluated our method on by using it to per-form query-oriented summarization (Tang et al., 2009). Experimental results show that it is superior to state-of-the-art methods."]},{"title":"2 Related Work","paragraphs":["Submodularity is formally defined as a property of a set function for a finite universe V . The function f : 2V","→ R maps a subset S ⊆ V to a real value. If for any S, T ⊆ V , f (S ∪ T ) + f (S ∩ T ) ≤ f (S) + f (T ), f is called submodular. This definition is equivalent to that of diminishing returns, which is well known in the field of economics: f (S ∪ {u}) − f (S) ≤ f (T ∪ {u}) − f (T ), where T ⊆ S ⊆ V and u is an element of V . Diminishing returns means that the value of an element u remains the same or decreases as S be-comes larger. This property is suitable for summarization purposes, because the gain of adding a new sentence to a summary that already contains sufficient information should be small. Therefore, many studies have formalized text summarization as a submodular maximization problem (Lin and Bilmes, 2010; Lin and Bilmes, 2011; Morita et al., 2011). Their approaches, however, have been based on sentence extraction. To our knowledge, there is no study that addresses the joint task of simultaneously performing compression and extraction through an approximate submodular maximization with a performance guarantee.","In the field of constrained maximization problems, Kulik et al. (2009) proposed an algorithm that solves the submodular maximization problem under multiple linear constraints with a performance guarantee 1 − e−1","in polynomial time. Although their approach can represent more flexible constraints, we cannot use their algorithm to solve our problem, because their algorithm needs to enumerate many combinations of elements. Integer linear programming (ILP) formulations can represent such flexible constraints, and they are commonly used to model text summarization (McDonald, 2007). Berg-Kirkpatrick et al. (2011) formulated a unified task of sentence extraction and sentence compression as an ILP. However, it is hard to solve large-scale ILP problems exactly in a practical amount of time."]},{"title":"3 Budgeted Submodular Maximization with Cost Function 3.1 Problem Definition","paragraphs":["Let V be the finite set of all valid subtrees in the source documents, where valid subtrees are defined to be the ones that can be regarded as grammatical sentences. In this paper, we regard subtrees containing the root node of the sentence as valid. Accordingly, V denotes a set of all rooted subtrees in all sentences. A subtree contains a set of elements that are units in a dependency structure (e.g., morphemes, words or clauses). Let us consider the following problem of budgeted monotone nondecreasing submodular function maximization with a cost function: maxS⊆V {f (S) : c (S) ≤ L} , where S is a summary represented as a set of subtrees, c(·) is the cost function for the set of subtrees, L is our budget, and the submodular function f (·) scores the summary quality. The cost function is not always the sum of the costs of the covered subtrees, but depends on the set of the covered elements by the subtrees. Here, we will assume that the generated summary has to be as long as or shorter than the given summary length limit, as measured by the number of characters. This means the cost of a subtree is the integer number of characters it contains.","V is partitioned into exclusive subsets B of valid subtrees, and each subset corresponds to the original sentence from which the valid subtrees derived. However, the cost of a union of subtrees from different sentences is simply the sum of the costs of subtrees, while the cost of a union of subtrees from the same sentence is smaller than the sum of the costs. Therefore, the problem can be represented as follows: 1024 max S⊆V { f (S) : ∑ B∈B c (B ∩ S) ≤ L } . (1) For example, if we add a subtree t containing words {w a, wb, wc} to a summary that already covers words {wa, wb, wd} from the same sentence, the additional cost of t is only c({wc}) because wa and wb are already covered1",".","The problem has two requirements. The first requirement is that the union of valid subtrees is also a valid subtree. The second requirement is that the union of subtrees and a single valid subtree have the same score and the same cost if they cover the same elements. We will refer to the single valid subtree as the equivalent subtree of the union of subtrees. These requirements enable us to represent sentence compression as the extraction of subtrees from a sentence. This is because the requirements guarantee that the extracted subtrees represent a sentence. 3.2 Greedy Algorithm We propose Algorithm 1 that solves the maximization problem (Eq.1). The algorithm is based on ones proposed by Khuller et al. (1999) and Krause et al. (2005). Instead of enumerating all candidate subtrees, we use a local search to extract the element that has the highest gain per cost. In the algorithm, Gi indicates a summary set obtained by adding element si to Gi−1. U means the set of subtrees that are not extracted. The algorithm iteratively adds to the current summary the element si that has the largest ratio of the objective function gain to the additional cost, unless adding it violates the budget constraint. We set a parameter r that is the scaling factor proposed by Lin and Bilmes (2010). After the loop, the algorithm compares Gi with the {s∗","} that has the largest value of the objective function among all subtrees that are under the budget, and it outputs the summary candidate with the largest value. Let us analyze the performance guarantee of Algorithm 12",".","1","Each subset B corresponds to a kind of greedoid constraint. V implicitly constrains the model such that it can only select valid subtrees from a set of nodes and edges.","2","Our performance guarantee is lower than that reported by Lin and Bilmes (2010). However, their proof is erroneous. In their proof of Lemma 2, they derive ∀u ∈ S∗","\\Gi−1, ρu(G","i−1)","Cr","u ≤","ρv i (G i−1) Cr v i",", for any i(1 ≤ i ≤ |G|), from line 4 of their Algorithm 1, which selects the densest element out of all available elements. However, the inequality does not hold for i, for which element u selected on line 4 is discarded on line 5 of their algorithm. The performance guarantee of their algorithm is actually the same as ours, since Algorithm 1 Modified greedy algorithm for budgeted submodular function maximization with a cost function . 1: G0 ← φ 2: U ← V 3: i ← 1 4: while U ̸= φ do 5: si ← arg maxs∈U f(Gi−1∪{s})−f(Gi−1)","(c(Gi−1∪{s})−c(Gi−1))r 6: if c({si} ∪ Gi−1) ≤ L then 7: Gi ← Gi−1 ∪ {si} 8: i ← i + 1 9: end if 10: U ← U \\{si} 11: end while 12: s̄ ← arg maxs∈V,c(s)≤L f({s}) 13: return Gf = arg maxS∈{{s̄},Gi} f(S) Theorem 1 For a normalized monotone submodular function f (·), Algorithm 1 has a constant approximation factor when r = 1 as follows: f (Gf ) ≥ ( 1 2","(1 − e−1 ))","f (S∗","), (2)","where S∗","is the optimal solution and, G f is the solution obtained by Greedy Algorithm 1. Proof. See appendix. 3.3 Relation with Discrete Optimization We argue that our optimization problem can be regarded as an extraction of subtrees rooted at a given node from a directed graph, instead of from a tree. Let D be the set of edges of the directed graph, F be a subset of D that is a subtree. In the field of combinatorial optimization, a pair (D, F ) is a kind of greedoid: directed branching greedoid (Schmidt, 1991). A greedoid is a generalization of the matroid concept. However, while matroids are often used to represent constraints on submodular maximization problems (Conforti and Cornuéjols, 1984; Calinescu et al., 2011), greedoids have not been used for that purpose, in spite of their high representation ability. To our knowledge, this is the first study that gives a constant performance guarantee for the submodular maximization under greedoid (non-matroid) constraints. the guarantee 1","2 (1 − e−1",") was already proved by Krause and Guestrin (2005). We show a counterexample. Suppose that V is { e1(density 4:cost 6), e2(density 2:cost 4), e3(density 3:cost 1), e4(density 1:cost 1) }, and cost limit K is 10. The optimal solution is S∗","= {e1, e2}. Their algorithm selects e1, e3, e4 in this order. However the algorithm selects e2 on line 4 after selecting e3, and it drops e2 on line 5. As a result, e4 selected by the algorithm does not satisfy the inequality ∀u ∈ S∗","\\Gi−1, ρu(G","i−1)","Cr","u ≤","ρv i(G i−1) Cr v i . 1025"]},{"title":"4 Joint Model of Extraction and Compression","paragraphs":["We will formalize the unified task of sentence compression and extraction as a budgeted monotone nondecreasing submodular function maximization with a cost function. In this formaliza-tion, a valid subtree of a sentence represents a candidate of a compressed sentence. We will refer to all valid subtrees of a given sentence as a valid set. A valid set corresponds to all candidates of the compression of a sentence. Note that although we use the valid set in the formaliza-tion, we do not have to enumerate all the candidates for each sentence. Since, from the requirements, the union of valid subtrees is also a valid subtree in the valid set, the model can extract one or more subtrees from one sentence, and generate a compressed sentence by merging those subtrees to generate an equivalent subtree. Therefore, the joint model can extract an arbitrarily compressed sentence as a subtree without enumerating all can-didates. The joint model can remove the redundant part as well as the irrelevant part of a sentence, because the model simultaneously extracts and compresses sentences. We can approximately solve the subtree extraction problem by using Algorithm 1. On line 5 of the algorithm, the subtree extraction is performed as a local search that finds maximal density subtrees from the whole documents. The maximal density subtree is a subtree that has the highest score per cost of subtree. We use a cost function to represent the cost, which indicates the length of word tokens in the subtree.","In this paper, we address the task of summarization of Japanese text by means of sentence compression and extraction. In Japanese, syntactic subtrees that contain the root of the dependency tree of the original sentence often make grammatical sentences. This means that the requirements mentioned in Section 3.1 that a union of valid subtrees is a valid and equivalent tree is often true for Japanese. The root indicates the predicate of a sentence, and it is syntactically modified by other prior words. Some modifying words can be pruned. Therefore, sentence compression can be represented as edge pruning. The linguistic units we extract are bunsetsu phrases, which are syntactic chunks often containing a functional word after one or more content words. We will refer to bunsetsu phrases as phrases for simplicity. Since Japanese syntactic dependency is generally defined between two phrases, we use the phrases as the nodes of subtrees.","In this joint model, we generate a compressed sentence by extracting an arbitrary subtree from a dependency tree of a sentence. However, not all subtrees are always valid. The sentence generated by a subtree can be unnatural even though the subtree contains the root node of the sentence. To avoid generating such ungrammatical sentences, we need to detect and retain the obligatory dependency relations in the dependency tree. We address this problem by imposing must-link constraints if a phrase corresponds to an obligatory case of the main predicate. We merge obligatory phrases with the predicate beforehand so that the merged nodes make a single large node.","Although we focus on Japanese in this paper, our approach can be applied to English and other languages if certain conditions are satisfied. First, we need a dependency parser of the language in order to represent sentence compression as dependency tree pruning. Moreover, although, in Japanese, obligatory cases distinguish which edges of the dependency tree can be pruned or not, we need another technique to distinguish them in other languages. For example we can distinguish obligatory phrases from optional ones by using semantic role labeling to detect arguments of predicates. The adaptation to other languages is left for future work. 4.1 Objective Function We extract subtrees from sentences in order to solve the query-oriented summarization problem as a unified one consisting of sentence compression and extraction. We thus need to allocate a query relevance score to each node. Off-the-shelf similarity measures such as the cosine similarity of bag-of-words vectors with query terms would allocate scores to the terms that appear in the query, but would give no scores to terms that do not appear in it. With such a similarity, sentence compression extracts nearly only the query terms and fails to contain important information. Instead, we used Query SnowBall (QSB) (Morita et al., 2011) to calculate the query relevance score of each phrase. QSB is a method for query-oriented summarization, which calculates the similarity between query terms and each word by using cooccurrences within the source documents. Although the authors of QSB also provided scores of word pairs to avoid putting excessive penalties 1026 on word overlaps, we do not score word pairs. The score function is supermodular as a score function of subtree extraction3",", because the union of two subtrees can have extra word pairs that are not included in either subtree. If the extra pair has a positive score, the score of the union is greater than the sum of the score of the subtrees. This violates the definition of submodularity, and invalidates the performance guarantee of our algorithms.","We designed our objective function by combin-ing this relevance score with a penalty for redundancy and too-compressed sentences. Important words that describe the main topic should occur multiple times in a good summary. However, excessive overlap undermines the quality of a summary, as do irrelevant words. Therefore, the scores of overlapping words should be lower than thoseof new words. The behavior can be represented by a submodular objective function that reduces word scores depending on those already included in the summary. Furthermore, a summary consisting of many too-compressed sentences would lack readability. We thus gives a positive reward to long sentences. The positive reward leads to a natural summary being generated with fewer sentences and indirectly penalizes too short sentences. Our positive reward for long sentences is represented as reward(S) = c(S) − |S|, (3) where c(S) is the cost of summary S, and |S| is the number of sentences in S. Since a sentence must contain more than one character, the reward consistently gives a positive score, and gives a higher score to a summary that consists of fewer sentences.","Let d be the damping rate, countS(w) be the number of sentences containing word w in summary S, words(S) be the set of words included in summary S, qsb(w) be the query relevance score of word w, and γ be a parameter that adjusts the rate of sentence compression. Our score function for a summary S is as follows: f (S) = ∑ w∈words(S)   ","countS(w)−1 ∑ i=0 qsb(w)di    + γ reward(S). (4)","An optimization problem with this objective function cannot be regarded as an ILP problem because it contains non-linear terms. It is also ad-3 The score is still submodular for the purpose of sentence","extraction. vantageous that the submodular maximization can deal with such objective functions. Note that the objective function is such that it can be calculated according to the type of word. Due to the nature of the objective function, we can use dynamic programming to effectively search for the subtree with the maximal density. 4.2 Local Search for Maximal Density","Subtree Let us now discuss the local search used on line 5 of Algorithm 1. We will use a fast algorithm to find the maximal density subtree (MDS) of a given sentence for each cost in Algorithm 1.","Consider the objective function Eq. 4, We can ignore the second term of the reward function while looking for the MDS in a sentence because the number of sentences is the same for every MDS in a sentence. That is, the gain function of adding a subtree to a summary can be represented as the sum of gains for words: g(t) = ∑ w∈t {gainS(w) + f reqt(w)c(w)γ},","gainS(w) = qsb(w)dcountS(w) , where f reqt(w) is the number of ws in subtree t, and gainS(w) is the gain of adding the word w to the summary S. Our algorithm is based on dynamic programming, and it selects a subtree that maximizes the gain function per cost.","When the word gain is a constant, the algorithm proposed by Hsieh et al. (2010) can be used to find the MDS. We extended this algorithm to work for submodular word gain functions that are not constant. Note that the gain of a word that occurs only once in the sentence, can be treated as a constant. In what follows, we will describe an extended algorithm to find the MDS even if there is word overlap.","For example, let us describe how to obtain the MDS in the case of a binary tree. First let us tackle the case in which the gain is always constant. Let n be a node in the tree, a and b be child nodes of n, c(n) be the cost of n, mdsc","a be the MDS rooted at a and have cost c. mdsn = {mds","c(n)","n , . . . , mdsL","n } denotes the set of MDSs for each cost and its root node n. The valid subtrees rooted at n can be obtained by taking unions of n with one or both of t1 ∈ mdsa and t2 ∈ mdsb. mdsc","n is the union that has the largest gain over the union with the cost of c (by enumerating all the unions). The MDS for 1027 the sentence root can be found by calculating each mdsc","n from the bottom of the tree to the top.","Next, let us consider the objective function that returns the sum of values of submodular word gain functions. When there is no word overlap within the union, we can obtain mdsc","n in the same manner as for the constant gain. In contrast, if the union includes word overlap, the gain is less than the sum of gains: g(mdsc","n) ≤ g(n) + g(mdsk","a) + g(mds","c−k−c(n)","b ), where k and c are variables. The score reduction can change the order of the gains of the union. That is, it is possible that another union without word overlaps will have a larger gain. Therefore, the algorithm needs to know whether each t ∈ mdsn has the potential to have word overlaps with other MDSs. Let O be the set of words that occur twice or more in the sentence on which the local seach focuses. The algorithm stores MDS for each o ⊆ O, as well as each cost. By storing MDS for each o and cost as shown in Fig. 1, the algorithm can find MDS with the largest gain over the combinations of subtrees.","Algorithm 2 shows the procedure. In it, t and m denote subtrees, words(t) returns a set of words in the subtree, g(t) returns the gain of t, tree(n) means a tree consisting of node n, and t ∪ m denotes the union of subtrees: t and m. subt indicates a set of current maximal density subtrees among the combinations calculated before. newt indicates a set of temporary maximal density subtrees for the combinations calculated from line 4 to 8. subt[cost,ws] indicates a element of subt that has a cost cost and contains a set of words ws. newt[cost,ws] is defined similarly. Line 1 sets subt to a set consisting of a subtree that indicates node n itself. The algorithm calculates maximal density subtrees within combinations of the root node n and MDSs rooted at child nodes of n. Line 3 iteratively adds MDSs rooted at a next child node to the combinations; the algorithm then calculates MDSs newt between subt and the MDSs of the child node. The procedure from line 6 to 8 selects a subtree that has a larger gain from the temporary maximal subtree and the union of t and m. The computational complexity of this algorithm is O(N C2",") when there is no word overlap within the sentence, where C denotes the cost of the whole sentence, and N denotes the number of nodes in the sentence. The complexity order is the same as that of the algorithm of Hsieh et al. (2010). When we treat word overlaps, we need to count Algorithm 2 Algorithm for finding maximal density subtree for each cost: MDSs. Function: MDSs Require: root node n 1: subt[c(n),words(n)∩O] = tree(n) 2: newt = φ 3: for i ∈ child node of n do 4: for t ∈ MDSs(i) do 5: for m ∈ subt do 6: index = [c(t ∪ m), words(t ∪ m) ∩ O] 7: newtindex = arg maxj∈{newtindex,t∪m} g(j) 8: end for 9: end for 10: subt = newt 11: end for 12: return subt Figure 1: Maximal density subtree extraction. The right table enumerates the subtrees rooted at w2 in the left tree for all indices. The number in each tree node is the score of the word. all unions of combinations of the stored MDSs. There are at most (C2|O|",") MDSs that the algorithm needs to store at each node. Therefore the total computational complexity is O(N C2","22|O|","). Since it is unlikely that a sentence contains many word tokens of one type, the computational cost may not be so large in practical situations."]},{"title":"5 Experimental Settings","paragraphs":["We evaluate our method on Japanese QA test collections from NTCIR-7 ACLIA1 and NTCIR-8 ACLIA2 (Mitamura et al., 2008; Mitamura et al., 2010). The collections contain questions and weighted answer nuggets. Our experimental settings followed the settings of (Morita et al., 2011), except for the maximum summary length. We generated summaries consisting of 140 Japanese characters or less, with the question as the query terms. We did this because our aim is to use our method in mobile situations. We used “ACLIA1 test data” to tune the parameters, and evaluated our method on “ACLIA2 test” data.","We used JUMAN (Kurohashi and Kawahara, 2009a) for word segmentation and part-of-speech tagging, and we calculated idf over Mainichi newspaper articles from 1991 to 2005. For the de-1028","POURPRE Precision Recall F1 F3 Lin and Bilmes (2011) 0.215 0.126 0.201 0.135 0.174 Subtree extraction (SbE) 0.268 0.238 0.213 0.159 0.190 Sentence extraction (NC) 0.278 0.206 0.215 0.139 0.183 Table 1: Results on ACLIA2 test data. pendency parsing, we used KNP (Kurohashi and Kawahara, 2009b). Since KNP internally has a flag that indicates either an “obligatory case” or an “adjacent case”, we regarded dependency relations flagged by KNP as obligatory in the sentence compression. KNP utilizes Kyoto University’s case frames (Kawahara and Kurohashi, 2006) as the re-source for detecting obligatory or adjacent cases.","To evaluate the summaries, we followed the practices of the TAC summarization tasks (Dang, 2008) and NTCIR ACLIA tasks, and computed pyramid-based precision with the allowance parameter, recall, and Fβ (where β is 1 or 3) scores. The allowance parameter was determined from the average nugget length for each question type of the ACLIA2 collection (Mitamura et al., 2010). Precision and recall are computed from the nuggets that the summary covered along with their weights. One of the authors of this paper manually evaluated whether each nugget matched the summary. We also used the automatic evaluation measure, POURPRE (Lin and Demner-Fushman, 2006). POURPRE is based on word matching of reference nuggets and system outputs. We regarded as stopwords the most frequent 100 words in Mainichi articles from 1991 to 2005 (the document frequency was used to measure the frequency). We also set the threshold of nugget matching as 0.5 and binarized the nugget matching, following the previous study (Mitamura et al., 2010). We tuned the parameters by using POURPRE on the development dataset.","Lin and Bilmes (2011) designed a monotone submodular function for query-oriented summarization. Their succinct method performed well in DUC from 2004 to 2007. They proposed a positive diversity reward function in order to define a monotone submodular objective function for generating a non-redundant summary. The diversity reward gives a smaller gain for a biased summary, because it consists of gains based on three clusters and calculates a square root score with respect to each sentence. The reward also contains a score for the similarity of a sentence to the query, for purposes of query-oriented summa-","Recall Length # of nuggets Subtree extraction 0.213 11,143 100 Reconstructed (RC) 0.228 13,797 108 Table 2: Effect of sentence compression. rization. Their objective function also includes a coverage function based on the similarity wi,j between sentences. In the coverage function min function limits the maximum gain α","∑","i∈V wi,j, which is a small fraction α of the similarity between a sentence j and the all source documents. The objective function is the sum of the positive reward R and the coverage function L over the source documents V , as follows: F(S) = L(S) + 3 ∑ k=1 λkRQ,k(S), L(S) = ∑ i∈V min    ∑ j∈S wi,j, α ∑ k∈V wi,k    , RQ,k = ∑ c∈Ck √ √ √ √ ∑","j∈S∪c ( β N ∑ i∈V wi,j + (1 − β)rj,Q), where α, β and λk are parameters, and rj,Q represents the similarity between sentence j and query Q. We tuned the parameters on the development dataset. Lin and Bilmes (2011) used three clusters Ck with different granularities, which were calculated in advance. We set the granularity to (0.2N , 0.15N , 0.05N ) according to the settings of them, where N is the number of sentences in a document.","We also regarded as stopwords “ (tell),” “ (know),” “ (what)” and their conjugated forms, which are excessively common in questions. For the query expansion in the baseline, we used Japanese WordNet to obtain synonyms and hypernyms of query terms."]},{"title":"6 Results","paragraphs":["Table 1 summarizes our results. “Subtree extraction (SbE)” is our method, and “Sentence extraction (NC)” is a version of our method without compression. The NC has the same objective function but only extracts sentences. The F1-measure and F3-measure of our method are 0.159 and 0.190 respectively, while those of the state-of-1029 the-art baseline are 0.135 and 0.174 respectively. Unfortunately, since the document set is small, the difference is not statistically significant. Comparing our method with the one without compression, we can see that there are improvements in the F1 and F3 scores of the human evaluation, whereas the POURPRE score of the version of our method without compression is higher than that of our method with compression. The compression im-proved the precision of our method, but slightly decreased the recall.","For the error analyses, we reconstructed the original sentences from which our method extracted the subtrees. Table 2 shows the statistics of the summaries of SbE and reconstructed summaries (RC). The original sentences covered 108 answer nuggets in total, and 8 of these answer nuggets were dropped by the sentence compression. Comparing the results of SbE and RC, we can see that the sentence compression caused the recall of SbE to be 7% lower than that of RC. However, the drop is relatively small in light of the fact that the sentence compression can discard 19% of the original character length with SbE. This suggests that the compression can efficiently prune words while avoiding pruning informative content.","Since the summary length is short, we can select only two or three sentences for a summary. As Morita et al. (2011) mentioned, answer nuggets overlap each other. The baseline objective function R tends to extract sentences from various clusters. If the answer nuggets are present in the same cluster, the objective function does not fit the situation. However, our methods (SbE and NC) have a parameter d that can directly adjust overlap penalty with respect to word importance as well as query relevance. This may help our methods to cover similar answer nuggets. In fact, the development data resulted in a relatively high parameter d (0.8) for NC compared with 0.2 for SbE."]},{"title":"7 Conclusions and Future Work","paragraphs":["We formalized a query-oriented summarization, which is a task in which one simultaneously performs sentence compression and extraction, as a new optimization problem: budgeted monotone nondecreasing submodular function maximization with a cost function. We devised an approximate algorithm to solve the problem in a reasonable computational time and proved that its approximation rate is 1","2 (1 − e−1","). Our approach achieved an F3-measure of 0.19 on the ACLIA2 Japanese test collection, which is 9.2 % improvement over a state-of-the-art method using a submodular objective function.","Since our algorithm requires that the objective function is the sum of word score functions, our proposed method has a restriction that we cannot use an arbitrary monotone submodular function as the objective function for the summary. Our future work will improve the local search algorithm to remove this restriction. As mentioned before, we also plan to adapt of our system to other languages."]},{"title":"Appendix","paragraphs":["Here, we analyze the performance guarantee of Algorithm 1. We use the following notation. S∗","is the optimal solution, cu(S) is the residual cost of subtree u when S is already covered, and i∗","is the last step before the algorithm discards a subtree s ∈ S∗","or a part of the subtree s. This is because the subtree does not belong to either the approximate solution or the optimal solution. We can remove the subtree s′","from V without changing the approximate rate. si is the i-th subtree obtained by line 5 of Algorithm 1. Gi is the set obtained after adding subtree si to Gi−1 from the valid set Bi. Gf is the final solution obtained by Algorithm 1. f (·) : 2V","→ R is a monotone submodular function.","We assume that there is an equivalent subtree with any union of subtrees in a valid set B: ∀t1, t2, ∃te, te ≡ {t1, t2}. Note that for any order of the set, the cost or profit of the set is fixed:∑","ui∈S={u1,...,u|S|} cui(Si−1) = c(S). Lemma 1 ∀X, Y ⊆ V, f (X) ≤ f (Y ) +∑","u∈X\\Y ρu(Y ), where ρu(S) = f (S ∪ {u}) − f (S). The inequality can be derived from the definition of submodularity. Lemma 2 For i = 1, . . . , i∗","+1, when 0 ≤ r ≤ 1,","f(S∗",")−f(Gi−1)≤ Lr |S∗","|1−r cs i(G","i−1) (f(Gi−1∪{si})−f(Gi−1)),","where cu(S)=c(S∪{u})−c(S). Proof. From line 5 of Algorithm 1, we have","∀u ∈ S∗ \\G i−1, ρu(Gi−1) cu(Gi−1)r ≤ ρsi(Gi−1) csi(Gi−1)r .","Let B be a valid set, and union be a function that returns the union of subtrees. We have 1030 ∀T ⊆ B, ∃b ∈ B, b = union(T ), because we have an equivalent tree b ∈ B for each union of trees T in a valid set B. That is, for any set of subtrees, we have an equivalent set of subtrees, where bi ∈ Bi. Without loss of generality, we can replace the difference set S∗","\\G i−1 with","a set T ′","i−1 = {b0, . . . , b|T ′","i−1|} that does not con-","tain any two elements extracted from the same","valid set. Thus when 0 ≤ r ≤ 1 and 0 ≤","i ≤ i∗","+ 1, ρs∗","\\G","i−1(Gi−1)","cS∗","\\G","i−1(Gi−1)r =","ρT′ i−1 (Gi−1)","cT′ i−1 (Gi−1)r , and","∀bj ∈ T ′ i−1, ρb j (Gi−1) cb j (Gi−1)r ≤ ρs i(Gi−1) cs i(Gi−1)r . Thus,","ρT′ i−1 (Gi−1) =","∑ u∈T′","i−1 ρu(Gi−1) ≤ ρs i(G","i−1) cs i(G","i−1)r","∑ u∈T′","i−1 cu(Gi−1)r ≤ ρs i(G","i−1)","cs i(G","i−1)r |T ′","i−1| ( ∑","u∈T′ i−1","cu(G i−1) |T′ i−1| ) r ≤ ρs i(G","i−1)","cs i(G","i−1)r |T ′","i−1|1−r (","∑ u∈T′","i−1 cu(φ) ) r ≤","ρs","i(G i−1)","cs","i(G i−1)r |S∗","|1−r","Lr",", where the second inequality is from Hölder’s inequality. The third inequality uses the submodularity of the cost function, cu(Gi−1) = c({u} ∪ Gi−1) − c(Gi−1) ≤ cu(φ)","and the fact that |S∗ | ≥ |S∗","\\G","i−1| ≥ |T ′ i−1|, and","∑ u∈T ′","i−1 cu(φ) = c(T ′","i−1) ≤ L .","As a result, we have","ρs∗ \\Gi−1(Gi−1) = ρT ′","i−1(Gi−1) ≤","ρsi(Gi−1)","csi(Gi−1)r |S∗ |1−r","Lr",".","Let X = S∗ and Y = Gi−1. Applying Lemma 1 yields","f (S∗ ) ≤ f (G","i−1) + ρu∈S∗ \\Gi−1(Gi−1). ≤ f (Gi−1) + ρsi(Gi−1) csi(Gi−1)","|S∗","|1−r","Lr",". The lemma follows as a result. Lemma 3 For a normalized monotone submodular f (·), for i = 1, . . . , i∗","+ 1 and 0 ≤ r ≤ 1 and letting si be the i-th unit added into G and Gi be the set after adding si, we have f (Gi) ≥ ( 1 − i ∏ k=1","( 1 −","csk (Gk−1)r Lr |S∗","|1−r ))","f (S∗ ). Proof. This is proved similarly to Lemma 3 of (Krause and Guestrin, 2005) using Lemma 2. Proof of Theorem 1. This is proved similarly to Theorem 1 of (Krause and Guestrin, 2005) using Lemma 3."]},{"title":"References","paragraphs":["Taylor Berg-Kirkpatrick, Dan Gillick, and Dan Klein. 2011. Jointly learning to extract and compress. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies - Volume 1, HLT ’11, pages 481–490, Stroudsburg, PA, USA. Association for Computational Linguistics.","Calinescu Calinescu, Chandra Chekuri, Martin Pál, and Jan Vondrák. 2011. Maximizing a monotone submodular function subject to a matroid constraint. SIAM Journal on Computing, 40(6):1740–1766.","Michele Conforti and Gérard Cornuéjols. 1984. Submodular set functions, matroids and the greedy algorithm: Tight worst-case bounds and some generalizations of the rado-edmonds theorem. Discrete Applied Mathematics, 7(3):251 – 274.","Hoa Trang Dang. 2008. Overview of the tac 2008 opinion question answering and summarization tasks. In Proceedings of Text Analysis Conference.","Anupam Gupta, Aaron Roth, Grant Schoenebeck, and Kunal Talwar. 2010. Constrained non-monotone submodular maximization: offline and secretary algorithms. In Proceedings of the 6th international conference on Internet and network economics, WINE’10, pages 246–257, Berlin, Heidelberg. Springer-Verlag.","Sun-Yuan Hsieh and Ting-Yu Chou. 2010. The weight-constrained maximum-density subtree problem and related problems in trees. The Journal of Supercomputing, 54(3):366–380, December.","Daisuke Kawahara and Sadao Kurohashi. 2006. A fully-lexicalized probabilistic model for japanese syntactic and case structure analysis. In Proceedings of the main conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics, HLT-NAACL ’06, pages 176–183, Stroudsburg, PA, USA. Association for Computational Linguistics.","Samir Khuller, Anna Moss, and Joseph S. Naor. 1999. The budgeted maximum coverage problem. Information Processing Letters, 70(1):39–45.","Andreas Krause and Carlos Guestrin. 2005. A note on the budgeted maximization on submodular functions. Technical Report CMU-CALD-05-103, Carnegie Mellon University. 1031","Ariel Kulik, Hadas Shachnai, and Tami Tamir. 2009. Maximizing submodular set functions subject to multiple linear constraints. In Proceedings of the twentieth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA ’09, pages 545–554, Philadelphia, PA, USA. Society for Industrial and Applied Mathematics.","Sadao Kurohashi and Daisuke Kawahara, 2009a. Japanese Morphological Analysis System JUMAN 6.0 Users Manual. http://nlp.ist.i. kyoto-u.ac.jp/EN/index.php?JUMAN.","Sadao Kurohashi and Daisuke Kawahara, 2009b. KN parser (Kurohashi-Nagao parser) 3.0 Users Manual. http://nlp.ist.i.kyoto-u.ac.jp/ EN/index.php?KNP.","Jon Lee, Vahab S. Mirrokni, Viswanath Nagarajan, and Maxim Sviridenko. 2009. Non-monotone submodular maximization under matroid and knapsack constraints. In Proceedings of the 41st annual ACM symposium on Theory of computing, STOC ’09, pages 323–332, New York, NY, USA. ACM.","Hui Lin and Jeff Bilmes. 2010. Multi-document summarization via budgeted maximization of submodular functions. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, HLT ’10, pages 912–920, Stroudsburg, PA, USA. Association for Computational Linguistics.","Hui Lin and Jeff Bilmes. 2011. A class of submodular functions for document summarization. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies - Volume 1, HLT ’11, pages 510–520, Stroudsburg, PA, USA. Association for Computational Linguistics.","Jimmy Lin and Dina Demner-Fushman. 2006. Methods for automatically evaluating answers to complex questions. Information Retrieval, 9(5):565– 587, November.","André F. T. Martins and Noah A. Smith. 2009. Sum-marization with a joint model for sentence extraction and compression. In Proceedings of the Workshop on Integer Linear Programming for Natural Langauge Processing, ILP ’09, pages 1–9, Stroudsburg, PA, USA. Association for Computational Linguistics.","Ryan McDonald. 2007. A study of global inference algorithms in multi-document summarization. In Proceedings of the 29th European conference on IR research, ECIR’07, pages 557–564, Berlin, Heidelberg. Springer-Verlag.","Teruko Mitamura, Eric Nyberg, Hideki Shima, Tsuneaki Kato, Tatsunori Mori, Chin-Yew Lin, Ruihua Song, Chuan-Jie Lin, Tetsuya Sakai, Donghong Ji, and Noriko Kando. 2008. Overview of the NTCIR-7 ACLIA Tasks: Advanced Cross-Lingual Information Access. In Proceedings of the 7th NTCIR Workshop.","Teruko Mitamura, Hideki Shima, Tetsuya Sakai, Noriko Kando, Tatsunori Mori, Koichi Takeda, Chin-Yew Lin, Ruihua Song, Chuan-Jie Lin, and Cheng-Wei Lee. 2010. Overview of the ntcir-8 aclia tasks: Advanced cross-lingual information access. In Proceedings of the 8th NTCIR Workshop.","Hajime Morita, Tetsuya Sakai, and Manabu Okumura. 2011. Query snowball: a co-occurrence-based approach to multi-document summarization for question answering. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers - Volume 2, HLT ’11, pages 223–229, Stroudsburg, PA, USA. Association for Computational Linguistics.","Wolfgang Schmidt. 1991. Greedoids and searches in directed graphs. Discrete Mathmatics, 93(1):75–88, November.","Jie Tang, Limin Yao, and Dewei Chen. 2009. Multitopic based query-oriented summarization. In Proceedings of 2009 SIAM International Conference Data Mining (SDM’2009), pages 1147–1158.","David M. Zajic, Bonnie J. Dorr, Jimmy Lin, and Richard Schwartz. 2006. Sentence compression as a component of a multi-document summarization system. In Proceedings of the 2006 Document Understanding Conference (DUC 2006) at NLT/NAACL 2006. 1032"]}],"references":[{"authors":[{"first":"Taylor","last":"Berg-Kirkpatrick"},{"first":"Dan","last":"Gillick"},{"first":"Dan","last":"Klein"}],"year":"2011","title":"Jointly learning to extract and compress"},{"authors":[{"first":"Calinescu","last":"Calinescu"},{"first":"Chandra","last":"Chekuri"},{"first":"Martin","last":"Pál"},{"first":"Jan","last":"Vondrák"}],"year":"2011","title":"Maximizing a monotone submodular function subject to a matroid constraint"},{"authors":[{"first":"Michele","last":"Conforti"},{"first":"Gérard","last":"Cornuéjols"}],"year":"1984","title":"Submodular set functions, matroids and the greedy algorithm: Tight worst-case bounds and some generalizations of the rado-edmonds theorem"},{"authors":[{"first":"Hoa","middle":"Trang","last":"Dang"}],"year":"2008","title":"Overview of the tac 2008 opinion question answering and summarization tasks"},{"authors":[{"first":"Anupam","last":"Gupta"},{"first":"Aaron","last":"Roth"},{"first":"Grant","last":"Schoenebeck"},{"first":"Kunal","last":"Talwar"}],"year":"2010","title":"Constrained non-monotone submodular maximization: offline and secretary algorithms"},{"authors":[{"first":"Sun-Yuan","last":"Hsieh"},{"first":"Ting-Yu","last":"Chou"}],"year":"2010","title":"The weight-constrained maximum-density subtree problem and related problems in trees"},{"authors":[{"first":"Daisuke","last":"Kawahara"},{"first":"Sadao","last":"Kurohashi"}],"year":"2006","title":"A fully-lexicalized probabilistic model for japanese syntactic and case structure analysis"},{"authors":[{"first":"Samir","last":"Khuller"},{"first":"Anna","last":"Moss"},{"first":"Joseph","middle":"S.","last":"Naor"}],"year":"1999","title":"The budgeted maximum coverage problem"},{"authors":[{"first":"Andreas","last":"Krause"},{"first":"Carlos","last":"Guestrin"}],"year":"2005","title":"A note on the budgeted maximization on submodular functions"},{"authors":[{"first":"Ariel","last":"Kulik"},{"first":"Hadas","last":"Shachnai"},{"first":"Tami","last":"Tamir"}],"year":"2009","title":"Maximizing submodular set functions subject to multiple linear constraints"},{"authors":[{"first":"Sadao","last":"Kurohashi"},{"first":"Daisuke","last":"Kawahara"}],"year":"2009a","title":"Japanese Morphological Analysis System JUMAN 6"},{"authors":[{"first":"Sadao","last":"Kurohashi"},{"first":"Daisuke","last":"Kawahara"}],"year":"2009b","title":"KN parser (Kurohashi-Nagao parser) 3"},{"authors":[{"first":"Jon","last":"Lee"},{"first":"Vahab","middle":"S.","last":"Mirrokni"},{"first":"Viswanath","last":"Nagarajan"},{"first":"Maxim","last":"Sviridenko"}],"year":"2009","title":"Non-monotone submodular maximization under matroid and knapsack constraints"},{"authors":[{"first":"Hui","last":"Lin"},{"first":"Jeff","last":"Bilmes"}],"year":"2010","title":"Multi-document summarization via budgeted maximization of submodular functions"},{"authors":[{"first":"Hui","last":"Lin"},{"first":"Jeff","last":"Bilmes"}],"year":"2011","title":"A class of submodular functions for document summarization"},{"authors":[{"first":"Jimmy","last":"Lin"},{"first":"Dina","last":"Demner-Fushman"}],"year":"2006","title":"Methods for automatically evaluating answers to complex questions"},{"authors":[{"first":"André","middle":"F. T.","last":"Martins"},{"first":"Noah","middle":"A.","last":"Smith"}],"year":"2009","title":"Sum-marization with a joint model for sentence extraction and compression"},{"authors":[{"first":"Ryan","last":"McDonald"}],"year":"2007","title":"A study of global inference algorithms in multi-document summarization"},{"authors":[{"first":"Teruko","last":"Mitamura"},{"first":"Eric","last":"Nyberg"},{"first":"Hideki","last":"Shima"},{"first":"Tsuneaki","last":"Kato"},{"first":"Tatsunori","last":"Mori"},{"first":"Chin-Yew","last":"Lin"},{"first":"Ruihua","last":"Song"},{"first":"Chuan-Jie","last":"Lin"},{"first":"Tetsuya","last":"Sakai"},{"first":"Donghong","last":"Ji"},{"first":"Noriko","last":"Kando"}],"year":"2008","title":"Overview of the NTCIR-7 ACLIA Tasks: Advanced Cross-Lingual Information Access"},{"authors":[{"first":"Teruko","last":"Mitamura"},{"first":"Hideki","last":"Shima"},{"first":"Tetsuya","last":"Sakai"},{"first":"Noriko","last":"Kando"},{"first":"Tatsunori","last":"Mori"},{"first":"Koichi","last":"Takeda"},{"first":"Chin-Yew","last":"Lin"},{"first":"Ruihua","last":"Song"},{"first":"Chuan-Jie","last":"Lin"},{"first":"Cheng-Wei","last":"Lee"}],"year":"2010","title":"Overview of the ntcir-8 aclia tasks: Advanced cross-lingual information access"},{"authors":[{"first":"Hajime","last":"Morita"},{"first":"Tetsuya","last":"Sakai"},{"first":"Manabu","last":"Okumura"}],"year":"2011","title":"Query snowball: a co-occurrence-based approach to multi-document summarization for question answering"},{"authors":[{"first":"Wolfgang","last":"Schmidt"}],"year":"1991","title":"Greedoids and searches in directed graphs"},{"authors":[{"first":"Jie","last":"Tang"},{"first":"Limin","last":"Yao"},{"first":"Dewei","last":"Chen"}],"year":"2009","title":"Multitopic based query-oriented summarization"},{"authors":[{"first":"David","middle":"M.","last":"Zajic"},{"first":"Bonnie","middle":"J.","last":"Dorr"},{"first":"Jimmy","last":"Lin"},{"first":"Richard","last":"Schwartz"}],"year":"2006","title":"Sentence compression as a component of a multi-document summarization system"}],"cites":[{"authors":[{"last":"Berg-Kirkpatrick"},{"last":"al."}],"year":"2011","style":0,"reference":{"authors":[{"first":"Taylor","last":"Berg-Kirkpatrick"},{"first":"Dan","last":"Gillick"},{"first":"Dan","last":"Klein"}],"year":"2011","title":"Jointly learning to extract and compress"}},{"authors":[{"last":"Martins"},{"last":"Smith"}],"year":"2009","style":0,"reference":{"authors":[{"first":"André","middle":"F. T.","last":"Martins"},{"first":"Noah","middle":"A.","last":"Smith"}],"year":"2009","title":"Sum-marization with a joint model for sentence extraction and compression"}},{"authors":[{"last":"Zajic"},{"last":"al."}],"year":"2006","style":0,"reference":{"authors":[{"first":"David","middle":"M.","last":"Zajic"},{"first":"Bonnie","middle":"J.","last":"Dorr"},{"first":"Jimmy","last":"Lin"},{"first":"Richard","last":"Schwartz"}],"year":"2006","title":"Sentence compression as a component of a multi-document summarization system"}},{"authors":[{"last":"Lin"},{"last":"Bilmes"}],"year":"2010","style":0,"reference":{"authors":[{"first":"Hui","last":"Lin"},{"first":"Jeff","last":"Bilmes"}],"year":"2010","title":"Multi-document summarization via budgeted maximization of submodular functions"}},{"authors":[{"last":"Lin"},{"last":"Bilmes"}],"year":"2011","style":0,"reference":{"authors":[{"first":"Hui","last":"Lin"},{"first":"Jeff","last":"Bilmes"}],"year":"2011","title":"A class of submodular functions for document summarization"}},{"authors":[{"last":"Morita"},{"last":"al."}],"year":"2011","style":0,"reference":{"authors":[{"first":"Hajime","last":"Morita"},{"first":"Tetsuya","last":"Sakai"},{"first":"Manabu","last":"Okumura"}],"year":"2011","title":"Query snowball: a co-occurrence-based approach to multi-document summarization for question answering"}},{"authors":[{"last":"Lee"},{"last":"al."}],"year":"2009","style":0,"reference":{"authors":[{"first":"Jon","last":"Lee"},{"first":"Vahab","middle":"S.","last":"Mirrokni"},{"first":"Viswanath","last":"Nagarajan"},{"first":"Maxim","last":"Sviridenko"}],"year":"2009","title":"Non-monotone submodular maximization under matroid and knapsack constraints"}},{"authors":[{"last":"Kulik"},{"last":"al."}],"year":"2009","style":0,"reference":{"authors":[{"first":"Ariel","last":"Kulik"},{"first":"Hadas","last":"Shachnai"},{"first":"Tami","last":"Tamir"}],"year":"2009","title":"Maximizing submodular set functions subject to multiple linear constraints"}},{"authors":[{"last":"Gupta"},{"last":"al."}],"year":"2010","style":0,"reference":{"authors":[{"first":"Anupam","last":"Gupta"},{"first":"Aaron","last":"Roth"},{"first":"Grant","last":"Schoenebeck"},{"first":"Kunal","last":"Talwar"}],"year":"2010","title":"Constrained non-monotone submodular maximization: offline and secretary algorithms"}},{"authors":[{"last":"Tang"},{"last":"al."}],"year":"2009","style":0,"reference":{"authors":[{"first":"Jie","last":"Tang"},{"first":"Limin","last":"Yao"},{"first":"Dewei","last":"Chen"}],"year":"2009","title":"Multitopic based query-oriented summarization"}},{"authors":[{"last":"Lin"},{"last":"Bilmes"}],"year":"2010","style":0,"reference":{"authors":[{"first":"Hui","last":"Lin"},{"first":"Jeff","last":"Bilmes"}],"year":"2010","title":"Multi-document summarization via budgeted maximization of submodular functions"}},{"authors":[{"last":"Lin"},{"last":"Bilmes"}],"year":"2011","style":0,"reference":{"authors":[{"first":"Hui","last":"Lin"},{"first":"Jeff","last":"Bilmes"}],"year":"2011","title":"A class of submodular functions for document summarization"}},{"authors":[{"last":"Morita"},{"last":"al."}],"year":"2011","style":0,"reference":{"authors":[{"first":"Hajime","last":"Morita"},{"first":"Tetsuya","last":"Sakai"},{"first":"Manabu","last":"Okumura"}],"year":"2011","title":"Query snowball: a co-occurrence-based approach to multi-document summarization for question answering"}},{"authors":[{"last":"Kulik"},{"last":"al."}],"year":"2009","style":0,"reference":{"authors":[{"first":"Ariel","last":"Kulik"},{"first":"Hadas","last":"Shachnai"},{"first":"Tami","last":"Tamir"}],"year":"2009","title":"Maximizing submodular set functions subject to multiple linear constraints"}},{"authors":[{"last":"McDonald"}],"year":"2007","style":0,"reference":{"authors":[{"first":"Ryan","last":"McDonald"}],"year":"2007","title":"A study of global inference algorithms in multi-document summarization"}},{"authors":[{"last":"Berg-Kirkpatrick"},{"last":"al."}],"year":"2011","style":0,"reference":{"authors":[{"first":"Taylor","last":"Berg-Kirkpatrick"},{"first":"Dan","last":"Gillick"},{"first":"Dan","last":"Klein"}],"year":"2011","title":"Jointly learning to extract and compress"}},{"authors":[{"last":"Khuller"},{"last":"al."}],"year":"1999","style":0,"reference":{"authors":[{"first":"Samir","last":"Khuller"},{"first":"Anna","last":"Moss"},{"first":"Joseph","middle":"S.","last":"Naor"}],"year":"1999","title":"The budgeted maximum coverage problem"}},{"authors":[{"last":"Krause"},{"last":"al."}],"year":"2005","style":0},{"authors":[{"last":"Lin"},{"last":"Bilmes"}],"year":"2010","style":0,"reference":{"authors":[{"first":"Hui","last":"Lin"},{"first":"Jeff","last":"Bilmes"}],"year":"2010","title":"Multi-document summarization via budgeted maximization of submodular functions"}},{"authors":[{"last":"Lin"},{"last":"Bilmes"}],"year":"2010","style":0,"reference":{"authors":[{"first":"Hui","last":"Lin"},{"first":"Jeff","last":"Bilmes"}],"year":"2010","title":"Multi-document summarization via budgeted maximization of submodular functions"}},{"authors":[{"last":"Schmidt"}],"year":"1991","style":0,"reference":{"authors":[{"first":"Wolfgang","last":"Schmidt"}],"year":"1991","title":"Greedoids and searches in directed graphs"}},{"authors":[{"last":"Conforti"},{"last":"Cornuéjols"}],"year":"1984","style":0,"reference":{"authors":[{"first":"Michele","last":"Conforti"},{"first":"Gérard","last":"Cornuéjols"}],"year":"1984","title":"Submodular set functions, matroids and the greedy algorithm: Tight worst-case bounds and some generalizations of the rado-edmonds theorem"}},{"authors":[{"last":"Calinescu"},{"last":"al."}],"year":"2011","style":0,"reference":{"authors":[{"first":"Calinescu","last":"Calinescu"},{"first":"Chandra","last":"Chekuri"},{"first":"Martin","last":"Pál"},{"first":"Jan","last":"Vondrák"}],"year":"2011","title":"Maximizing a monotone submodular function subject to a matroid constraint"}},{"authors":[{"last":"Krause"},{"last":"Guestrin"}],"year":"2005","style":0,"reference":{"authors":[{"first":"Andreas","last":"Krause"},{"first":"Carlos","last":"Guestrin"}],"year":"2005","title":"A note on the budgeted maximization on submodular functions"}},{"authors":[{"last":"Morita"},{"last":"al."}],"year":"2011","style":0,"reference":{"authors":[{"first":"Hajime","last":"Morita"},{"first":"Tetsuya","last":"Sakai"},{"first":"Manabu","last":"Okumura"}],"year":"2011","title":"Query snowball: a co-occurrence-based approach to multi-document summarization for question answering"}},{"authors":[{"last":"Hsieh"},{"last":"al."}],"year":"2010","style":0},{"authors":[{"last":"Hsieh"},{"last":"al."}],"year":"2010","style":0},{"authors":[{"last":"Mitamura"},{"last":"al."}],"year":"2008","style":0,"reference":{"authors":[{"first":"Teruko","last":"Mitamura"},{"first":"Eric","last":"Nyberg"},{"first":"Hideki","last":"Shima"},{"first":"Tsuneaki","last":"Kato"},{"first":"Tatsunori","last":"Mori"},{"first":"Chin-Yew","last":"Lin"},{"first":"Ruihua","last":"Song"},{"first":"Chuan-Jie","last":"Lin"},{"first":"Tetsuya","last":"Sakai"},{"first":"Donghong","last":"Ji"},{"first":"Noriko","last":"Kando"}],"year":"2008","title":"Overview of the NTCIR-7 ACLIA Tasks: Advanced Cross-Lingual Information Access"}},{"authors":[{"last":"Mitamura"},{"last":"al."}],"year":"2010","style":0,"reference":{"authors":[{"first":"Teruko","last":"Mitamura"},{"first":"Hideki","last":"Shima"},{"first":"Tetsuya","last":"Sakai"},{"first":"Noriko","last":"Kando"},{"first":"Tatsunori","last":"Mori"},{"first":"Koichi","last":"Takeda"},{"first":"Chin-Yew","last":"Lin"},{"first":"Ruihua","last":"Song"},{"first":"Chuan-Jie","last":"Lin"},{"first":"Cheng-Wei","last":"Lee"}],"year":"2010","title":"Overview of the ntcir-8 aclia tasks: Advanced cross-lingual information access"}},{"authors":[{"last":"Morita"},{"last":"al."}],"year":"2011","style":0,"reference":{"authors":[{"first":"Hajime","last":"Morita"},{"first":"Tetsuya","last":"Sakai"},{"first":"Manabu","last":"Okumura"}],"year":"2011","title":"Query snowball: a co-occurrence-based approach to multi-document summarization for question answering"}},{"authors":[{"last":"Kurohashi"},{"last":"Kawahara"}],"year":"2009a","style":0,"reference":{"authors":[{"first":"Sadao","last":"Kurohashi"},{"first":"Daisuke","last":"Kawahara"}],"year":"2009a","title":"Japanese Morphological Analysis System JUMAN 6"}},{"authors":[{"last":"Lin"},{"last":"Bilmes"}],"year":"2011","style":0,"reference":{"authors":[{"first":"Hui","last":"Lin"},{"first":"Jeff","last":"Bilmes"}],"year":"2011","title":"A class of submodular functions for document summarization"}},{"authors":[{"last":"Kurohashi"},{"last":"Kawahara"}],"year":"2009b","style":0,"reference":{"authors":[{"first":"Sadao","last":"Kurohashi"},{"first":"Daisuke","last":"Kawahara"}],"year":"2009b","title":"KN parser (Kurohashi-Nagao parser) 3"}},{"authors":[{"last":"Kawahara"},{"last":"Kurohashi"}],"year":"2006","style":0,"reference":{"authors":[{"first":"Daisuke","last":"Kawahara"},{"first":"Sadao","last":"Kurohashi"}],"year":"2006","title":"A fully-lexicalized probabilistic model for japanese syntactic and case structure analysis"}},{"authors":[{"last":"Dang"}],"year":"2008","style":0,"reference":{"authors":[{"first":"Hoa","middle":"Trang","last":"Dang"}],"year":"2008","title":"Overview of the tac 2008 opinion question answering and summarization tasks"}},{"authors":[{"last":"Mitamura"},{"last":"al."}],"year":"2010","style":0,"reference":{"authors":[{"first":"Teruko","last":"Mitamura"},{"first":"Hideki","last":"Shima"},{"first":"Tetsuya","last":"Sakai"},{"first":"Noriko","last":"Kando"},{"first":"Tatsunori","last":"Mori"},{"first":"Koichi","last":"Takeda"},{"first":"Chin-Yew","last":"Lin"},{"first":"Ruihua","last":"Song"},{"first":"Chuan-Jie","last":"Lin"},{"first":"Cheng-Wei","last":"Lee"}],"year":"2010","title":"Overview of the ntcir-8 aclia tasks: Advanced cross-lingual information access"}},{"authors":[{"last":"Lin"},{"last":"Demner-Fushman"}],"year":"2006","style":0,"reference":{"authors":[{"first":"Jimmy","last":"Lin"},{"first":"Dina","last":"Demner-Fushman"}],"year":"2006","title":"Methods for automatically evaluating answers to complex questions"}},{"authors":[{"last":"Mitamura"},{"last":"al."}],"year":"2010","style":0,"reference":{"authors":[{"first":"Teruko","last":"Mitamura"},{"first":"Hideki","last":"Shima"},{"first":"Tetsuya","last":"Sakai"},{"first":"Noriko","last":"Kando"},{"first":"Tatsunori","last":"Mori"},{"first":"Koichi","last":"Takeda"},{"first":"Chin-Yew","last":"Lin"},{"first":"Ruihua","last":"Song"},{"first":"Chuan-Jie","last":"Lin"},{"first":"Cheng-Wei","last":"Lee"}],"year":"2010","title":"Overview of the ntcir-8 aclia tasks: Advanced cross-lingual information access"}},{"authors":[{"last":"Lin"},{"last":"Bilmes"}],"year":"2011","style":0,"reference":{"authors":[{"first":"Hui","last":"Lin"},{"first":"Jeff","last":"Bilmes"}],"year":"2011","title":"A class of submodular functions for document summarization"}},{"authors":[{"last":"Lin"},{"last":"Bilmes"}],"year":"2011","style":0,"reference":{"authors":[{"first":"Hui","last":"Lin"},{"first":"Jeff","last":"Bilmes"}],"year":"2011","title":"A class of submodular functions for document summarization"}},{"authors":[{"last":"Morita"},{"last":"al."}],"year":"2011","style":0,"reference":{"authors":[{"first":"Hajime","last":"Morita"},{"first":"Tetsuya","last":"Sakai"},{"first":"Manabu","last":"Okumura"}],"year":"2011","title":"Query snowball: a co-occurrence-based approach to multi-document summarization for question answering"}},{"authors":[{"last":"Krause"},{"last":"Guestrin"}],"year":"2005","style":0,"reference":{"authors":[{"first":"Andreas","last":"Krause"},{"first":"Carlos","last":"Guestrin"}],"year":"2005","title":"A note on the budgeted maximization on submodular functions"}},{"authors":[{"last":"Krause"},{"last":"Guestrin"}],"year":"2005","style":0,"reference":{"authors":[{"first":"Andreas","last":"Krause"},{"first":"Carlos","last":"Guestrin"}],"year":"2005","title":"A note on the budgeted maximization on submodular functions"}}]}
