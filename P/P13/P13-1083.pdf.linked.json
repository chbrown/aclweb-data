{"sections":[{"title":"","paragraphs":["Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 841–851, Sofia, Bulgaria, August 4-9 2013. c⃝2013 Association for Computational Linguistics"]},{"title":"Part-of-Speech Induction in Dependency Trees for Statistical MachineTranslationAkihiro Tamura","paragraphs":["†,‡"]},{"title":", Taro Watanabe","paragraphs":["†"]},{"title":", Eiichiro Sumita","paragraphs":["†"]},{"title":",Hiroya Takamura","paragraphs":["‡"]},{"title":", Manabu Okumura","paragraphs":["‡"]},{"title":"† National Institute of Information and Communications Technology{akihiro.tamura, taro.watanabe, eiichiro.sumita}@nict.go.jp† Precision and Intelligence Laboratory, Tokyo Institute of Technology{takamura, oku}@pi.titech.ac.jpAbstract","paragraphs":["This paper proposes a nonparametric Bayesian method for inducing Part-of-Speech (POS) tags in dependency trees to improve the performance of statistical machine translation (SMT). In particular, we extend the monolingual infinite tree model (Finkel et al., 2007) to a bilingual scenario: each hidden state (POS tag) of a source-side dependency tree emits a source word together with its aligned target word, either jointly (joint model), or independently (independent model). Evaluations of Japanese-to-English translation on the NTCIR-9 data show that our induced Japanese POS tags for dependency trees improve the performance of a forest-to-string SMT system. Our independent model gains over 1 point in BLEU by resolving the sparseness problem introduced in the joint model."]},{"title":"1 Introduction","paragraphs":["In recent years, syntax-based SMT has made promising progress by employing either dependency parsing (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Shen et al., 2008; Mi and Liu, 2010) or constituency parsing (Huang et al., 2006; Liu et al., 2006; Galley et al., 2006; Mi and Huang, 2008; Zhang et al., 2008; Cohn and Blunsom, 2009; Liu et al., 2009; Mi and Liu, 2010; Zhang et al., 2011) on the source side, the target side, or both. However, dependency parsing, which is a popular choice for Japanese, can incorporate only shallow syntactic information, i.e., POS tags, compared with the richer syntactic phrasal categories in constituency parsing. Moreover, existing POS tagsets might not be optimal for SMT because they are constructed without considering the language in the other side. Consider the examples in Figure 1. The Japanese noun “\\u{f62}\\u{f3b}” in 私 が 利用利用利用利用 料金 を 払う あなた は インターネット が 利用利用利用利用 でき ない You can not use the Internet . I pay usage fees . noun particle particle noun noun verb auxiliary verb noun particle noun noun verb particle [Example 1] [Example 2] Japanese POS: Japanese POS: Figure 1: Examples of Existing Japanese POS Tags and Dependency Structures Example 1 corresponds to the English verb “use”, while that in Example 2 corresponds to the English noun “usage”. Thus, Japanese nouns act like verbs in English in one situation, and nouns in English in another. If we could discriminate POS tags for two cases, we might improve the performance of a Japanese-to-English SMT system.","In the face of the above situations, this paper proposes an unsupervised method for inducing POS tags for SMT, and aims to improve the performance of syntax-based SMT by utilizing the induced POS tagset. The proposed method is based on the infinite tree model proposed by Finkel et al. (2007), which is a nonparametric Bayesian method for inducing POS tags from syntactic dependency structures. In this model, hidden states represent POS tags, the observations they generate represent the words themselves, and tree structures represent syntactic dependencies between pairs of POS tags.","The proposed method builds on this model by incorporating the aligned words in the other language into the observations. We investigate two types of models: (i) a joint model and (ii) an independent model. In the joint model, each hidden state jointly emits both a source word and its aligned target word as an observation. The independent model separately emits words in two languages from hidden states. By inferring POS 841 tags based on bilingual observations, both models can induce POS tags by incorporating information from the other language. Consider, for example, inducing a POS tag for the Japanese word “ \\u{f62}\\u{f3b}” in Figure 1. Under a monolingual induction method (e.g., the infinite tree model), the “\\u{f62}\\u{f3b}” in Example 1 and 2 would both be assigned the same POS tag since they share the same observation. However, our models would assign separate tags for the two different instances since the “\\u{f62} \\u{f3b}” in Example 1 and Example 2 could be disambiguated by encoding the target-side information, either “use” or “usage”, in the observations.","Inference is efficiently carried out by beam sampling (Gael et al., 2008), which combines slice sampling and dynamic programming. Experiments are carried out on the NTCIR-9 Japanese-to-English task using a binarized forest-to-string SMT system with dependency trees as its source side. Our bilingually-induced tagset significantly outperforms the original tagset and the monolingually-induced tagset. Further, our independent model achieves a more than 1 point gain in BLEU, which resolves the sparseness problem introduced by the bi-word observations."]},{"title":"2 Related Work","paragraphs":["A number of unsupervised methods have been proposed for inducing POS tags. Early methods have the problem that the number of possible POS tags must be provided preliminarily. This limita-tion has been overcome by automatically adjust-ing the number of possible POS tags using nonparametric Bayesian methods (Finkel et al., 2007; Gael et al., 2009; Blunsom and Cohn, 2011; Sirts and Alumäe, 2012). Gael et al. (2009) applied infinite HMM (iHMM) (Beal et al., 2001; Teh et al., 2006), a nonparametric version of HMM, to POS induction. Blunsom and Cohn (2011) used a hierarchical Pitman-Yor process prior to the transition and emission distribution for sophisticated smoothing. Sirts and Alumäe (2012) built a model that combines POS induction and morphological segmentation into a single learning problem. Finkel et al. (2007) proposed the infinite tree model, which represents recursive branching structures over infinite hidden states and induces POS tags from syntactic dependency structures. In the following, we overview the infinite tree model, which is the basis of our proposed model. In particular, we will describe the independent children"]},{"title":"H φ","paragraphs":["k"]},{"title":"ππππ","paragraphs":["k"]},{"title":"ρ z","paragraphs":["1"]},{"title":"z","paragraphs":["2"]},{"title":"z","paragraphs":["3"]},{"title":"x","paragraphs":["1"]},{"title":"x","paragraphs":["2"]},{"title":"x","paragraphs":["3 k=1,...,C Hk k ~ ) ,...,(","Dirichlet~| f rr rp Figure 2: A Graphical Representation of the Finite Tree Model model (Finkel et al., 2007), where children are dependent only on their parents, used in our proposed model1",". 2.1 Finite Tree Model We first review the finite tree model, which can be graphically represented in Figure 2. Let Tt denote the tree whose root node is t. A node t has a hidden state zt (the POS tag) and an observation xt (the word). The probability of a tree Tt, pT (Tt), is recursively defined: pT (Tt) = p(xt|zt) ∏","t′","∈c(t) p(zt′ |zt)pT (Tt′), where c(t) is the set of the children of t.","Let each hidden state variable have C possible values indexed by k. For each state k, there is a parameter φk which parameterizes the observation distribution for that state: xt|zt ∼ F (φzt). φk is distributed according to a prior distribution H: φk ∼ H.","Transitions between states are governed by Markov dynamics parameterized by π, where πij = p(zc(t) = j|zt = i) and πk are the transition probabilities from the parent’s state k. πk is distributed according to a Dirichlet distribution with parameter ρ: πk|ρ ∼ Dirichlet(ρ, . . . , ρ). The hidden state of each child zt′ is distributed according to a multinomial distribution πzt specific to the parent’s state zt: zt′|zt ∼ Multinomial(πzt). 2.2 Infinite Tree Model In the infinite tree model, the number of possible hidden states is potentially infinite. The infinite model is formed by extending the finite tree model using a hierarchical Dirichlet process (HDP) (Teh et al., 2006). The reason for using an HDP rather","1","Finkel et al. (2007) originally proposed three types of models: besides the independent children model, the simultaneous children model and the markov children model. Although we could apply the other two models, we leave this for future work. 842"]},{"title":"H φ","paragraphs":["k"]},{"title":"ππππ","paragraphs":["k"]},{"title":"α","paragraphs":["0"]},{"title":"z","paragraphs":["1"]},{"title":"z","paragraphs":["2"]},{"title":"z","paragraphs":["3"]},{"title":"x","paragraphs":["1"]},{"title":"x","paragraphs":["2"]},{"title":"x","paragraphs":["3"]},{"title":"∞γ ββββ","paragraphs":["Hk k ~","),(DP~,| )(GEM~| 00","f babap ggb Figure 3: A Graphical Representation of the Infinite Tree Model than a simple Dirichlet process (DP)2","(Ferguson, 1973) is that we have to introduce coupling across transitions from different parent’s states. A similar measure was adopted in iHMM (Beal et al., 2001).","HDP is a set of DPs coupled through a shared random base measure which is itself drawn from a DP: each Gk ∼ DP(α0, G0) with a shared base measure G0, and G0 ∼ DP(γ, H) with a global base measure H. From the viewpoint of the stickbreaking construction3","(Sethuraman, 1994), the HDP is interpreted as follows: G0 = ∞ ∑ k′ =1 βk′δφk′ and Gk = ∞ ∑ k′ =1 πkk′δφk′ , where β ∼ GEM(γ), πk ∼ DP(α0, β), and φk′ ∼ H.","We regard each Gk as two coindexed distributions: πk, a distribution over the transition probabilities from the parent’s state k, and φk′, an observation distribution for the state k′",". Then, the infinite tree model is formally defined as follows: β|γ ∼ GEM(γ), πk|α0, β ∼ DP(α0, β), φk ∼ H, zt′|zt ∼ Multinomial(πzt), xt|zt ∼ F (φzt).","Figure 3 shows the graphical representation of the","infinite tree model. The primary difference be-2 DP is a measure on measures. It has two parameters, a","scaling parameter α and a base measure H: DP (α, H). 3 Sethuraman (1994) showed a definition of a measure","G ∼ DP(α0, G0). First, infinite sequences of i.i.d variables","(π′ k)∞ k=1 and (φk)∞","k=1 are generated: π′","k|α0 ∼ Beta(1, α0),","φk ∼ G0. Then, G is defined as: πk = π′","k ∑k−1","l=1 (1 − π′","l),","G = ∑∞","k=1 πkδφ","k . If π is defined by this process, then we","write π ∼ GEM(α0). H φk ππππk α0 ∞ γ ββββ"]},{"title":"z","paragraphs":["1"]},{"title":"z","paragraphs":["2"]},{"title":"z","paragraphs":["3"]},{"title":"z","paragraphs":["4"]},{"title":"z","paragraphs":["5"]},{"title":"z","paragraphs":["6 “払う +pay” “を” “料金 +fees” “利用","+usage” “私+I” “が” Figure 4: An Example of the Joint Model tween Figure 2 and Figure 3 is whether the number of copies of the state is finite or not."]},{"title":"3 Bilingual Infinite Tree Model","paragraphs":["We propose a bilingual variant of the infinite tree model, the bilingual infinite tree model, which utilizes information from the other language. Specifically, the proposed model introduces bilingual observations by embedding the aligned target words in the source-side dependency trees. This paper proposes two types of models that differ in their processes for generating observations: the joint model and the independent model. 3.1 Joint Model The joint model is a simple application of the infinite tree model under a bilingual scenario. The model is formally defined in the same way as in Section 2.2 and is graphically represented similarly to Figure 3. The only difference from the infinite tree model is the instances of observations (xt). Observations in the joint model are the combination of source words and their aligned target words4",", while observations in the monolingual infinite tree model represent only source words. For each source word, all the aligned target words are copied and sorted in alphabetical order, and then concatenated into a single observation. Therefore, a single target word may be emitted multiple times if the target word is aligned with multiple source words. Likewise, there may be target words which may not be emitted by our model, if the target words are not aligned.","Figure 4 shows the process of generating Example 2 in Figure 1 through the joint model, where aligned words are jointly emitted as observations. In Figure 4, the POS tag of “\\u{f62}\\u{f3b}” (z5) generates","4","When no target words are aligned, we simply add a NULL target word. 843 H φk ππππk α0 γ ββββ"]},{"title":"z","paragraphs":["1"]},{"title":"z","paragraphs":["2"]},{"title":"z","paragraphs":["3"]},{"title":"z","paragraphs":["4"]},{"title":"z","paragraphs":["5 H’ φ'k 払う pay I私 を 利用 料金 が NONE NONE usage fees"]},{"title":"z","paragraphs":["6 ∞","'~',~ ),(DP~,| )(GEM~|","0","0 HH k","kk ff babap ggb Figure 5: A Graphical Representation of the Independent Model the string “\\u{f62}\\u{f3b}+usage” as the observation (x5). Similarly, the POS tag of “\\u{f62}\\u{f3b}” in Example 1 would generate the string “\\u{f62}\\u{f3b}+use”. Hence, this model can assign different POS tags to the two different instances of the word “\\u{f62}\\u{f3b}”, based on the different observation distributions in inference. 3.2 Independent Model The joint model is prone to a data sparseness problem, since each observation is a combination of a source word and its aligned target word. Thus, we propose an independent model, where each hidden state generates a source word and its aligned target word separately. For the aligned target side, we introduce an observation variable x′","t for each zt and a parameter φ′","k for each state k, which parameterizes a distinct distribution over the observations x′ t for that state. φ′","k is distributed according to a prior distribution H′",". Specifically, the independent model is formally defined as follows: β|γ ∼ GEM(γ), πk|α0, β ∼ DP(α0, β),","φk ∼ H, φ′ k ∼ H′",", zt′|zt ∼ Multinomial(πzt),","xt|zt ∼ F (φzt), x′","t|zt ∼ F ′","(φ′","zt). When multiple target words are aligned to a single source word, each aligned word is generated separately from observation distribution parameterized by φ′","k.","Figure 5 graphs the process of generating Example 2 in Figure 1 using the independent model. x′ t and φ′","k are introduced for aligned target words. The state of “\\u{f62}\\u{f3b}” (z5) generates the Japanese word “\\u{f62}\\u{f3b}” as x5 and the English word “usage” as x′","5. Due to this factorization, the independent model is less subject to the sparseness problem. 3.3 Introduction of Other Factors We assumed the surface form of aligned target words as additional observations in previous sec-tions. Here, we introduce additional factors, i.e., the POS of aligned target words, in the observations. Note that POSs of target words are assigned by a POS tagger in the target language and are not inferred in the proposed model.","First, we can simply replace surface forms of target words with their POSs to overcome the sparseness problem. Second, we can incorporate both information from the target language as observations. In the joint model, two pieces of information are concatenated into a single observation. In the independent model, we introduce observation variables (e.g., x′","t and x′′","t ) and parameters (e.g., φ′","k and φ′′","k) for each information. Specifically, x′","t and φ′","k are introduced for the surface form of aligned words, and x′′","t and φ′′","k for the POS of aligned words. Consider, for example, Example 1 in Figure 1. The POS tag of “\\u{f62}\\u{f3b}” generates the string “\\u{f62}\\u{f3b}+use+verb” as the observation in the joint model, while it generates “\\u{f62}\\u{f3b}”, “use”, and “verb” independently in the independent model. 3.4 POS Refinement We have assumed a completely unsupervised way of inducing POS tags in dependency trees. An-other realistic scenario is to refine the existing POS tags (Finkel et al., 2007; Liang et al., 2007) so that each refined sub-POS tag may reflect the information from the aligned words while preserving the handcrafted distinction from original POS tagset. Major difference is that we introduce separate transition probabilities πs","k and observation distributions (φs","k, φ′","s","k ) for each existing POS tag s. Then, each node t is constrained to follow the distributions indicated by the initially assigned POS tag st, and we use the pair (st, zt) as a state representation. 3.5 Inference In inference, we find the state set that maximizes the posterior probability of state transitions given observations (i.e., P (z1:n|x1:n)). However, we cannot evaluate the probability for all possible states because the number of states is infinite. Finkel et al. (2007) presented a sampling algorithm for the infinite tree model, which is based on the Gibbs sampling in the direct assignment representation for iHMM (Teh et al., 2006). In the 844 Gibbs sampling, individual hidden state variables are resampled conditioned on all other variables. Unfortunately, its convergence is slow in HMM settings because sequential data is likely to have a strong correlation between hidden states (Gael et al., 2008).","We present an inference procedure based on beam sampling (Gael et al., 2008) for the joint model and the independent model. Beam sampling limits the number of possible state transitions for each node to a finite number using slice sampling (Neal, 2003), and then efficiently samples whole hidden state transitions using dynamic programming. Beam sampling does not suffer from slow convergence as in Gibbs sampling by sampling the whole state variables at once. In addition, Gael et al. (2008) showed that beam sampling is more robust to initialization and hyperparameter choice than Gibbs sampling.","Specifically, we introduce an auxiliary variable ut for each node in a dependency tree to limit the number of possible transitions. Our procedure alternates between sampling each of the following variables: the auxiliary variables u, the state assignments z, the transition probabilities π, the shared DP parameters β, and the hyperparameters α0 and γ. We can parallelize procedures in sampling u and z because the slice sampling for u and the dynamic programing for z are independent for each sentence. See Gael el al. (2009) for details.","The only difference between inferences in the joint model and the independent model is in computing the posterior probability of state transitions given observations (e.g., p(z1:n|x1:n) and p(z1:n|x1:n, x′","1:n)) in sampling z. In the following, we describe each sampling stage. See Teh et al., (2006) for details of sampling π, β, α0 and γ. Sampling u: Each ut is sampled from the uniform distribution on [0, πzd(t)zt], where d(t) is the parent of t: ut ∼ Uniform(0, πzd(t)zt). Note that ut is a positive number, since each transition probability πzd(t)zt is larger than zero. Sampling z: Possible values k of zt are divided into the two sets using ut: a finite set with πzd(t)k > ut and an infinite set with πzd(t)k ≤ ut. The beam sampling considers only the former set. Owing to the truncation of the latter set, we can compute the posterior probability of a state zt given observations for all t (t = 1, . . . , T ) using dynamic programming as follows: In the joint model, p(zt|xσ(t), uσ(t)) ∝ p(xt|zt) · ∑","zd(t):πz d(t)z","t>ut p(zd(t)|xσ(d(t)), uσ(d(t))), and in the independent model,","p(zt|xσ(t), x′ σ(t), uσ(t)) ∝ p(xt|zt) · p(x′","t|zt) · ∑","zd(t):πz","d(t)z","t>ut p(zd(t)|xσ(d(t)), x′","σ(d(t)), uσ(d(t))), where xσ(t) (or uσ(t)) denotes the set of xt (or ut) on the path from the root node to the node t in a tree.","In our experiments, we assume that F (φk) is Multinomial(φk) and H is Dirichlet(ρ, . . . , ρ), which is the same in Finkel et al. (2007). Under this assumption, the posterior probability of an observation is as follows: p(xt|zt) = ṅxtk + ρ","ṅ·k + N ρ , where ṅxk is the number of observations x with state k, ṅ·k is the number of hidden states whose values are k, and N is the total number of observa-","tions x. Similarly, p(x′","t|zt) = ṅx′","tk + ρ′","ṅ·k + N ′","ρ′ , where","N ′ is the total number of observations x′",". When the posterior probability of a state zt","given observations for all t can be computed,","we first sample the state of each leaf node and","then perform backtrack sampling for every other","zt where the zt is sampled given the sample","for zc(t) as follows: p(zt|zc(t), x1:T , u1:T ) ∝","p(zt|xσ(t), uσ(t)) ∏","t′","∈c(t) p(zt′ |zt, ut′). Sampling π: We introduce a count variable nij ∈ n, which is the number of observations with state j whose parent’s state is i. Then, we sample π using the Dirichlet distribution: (πk1, . . . , πkK , ∑∞","k′","=K+1 πkk′ ) ∼ Dirichlet(nk1 + α0β1, . . . , nkK + α0βK , α0 ∑∞","k′","=K+1 βk′), where K is the number of distinct states in z. Sampling β: We introduce a set of auxiliary variables m, where mij ∈ m is the number of elements of πj corresponding to βi. The conditional distribution of each variable is p(mij = m|z, β, α0) ∝ S(nij, m)(α0βj)m",", where S(n, m) are unsigned Stirling numbers of the first kind5",".","5","S(0, 0) = S(1, 1) = 1, S(n, 0) = 0 for n > 0, S(n, m) = 0 for m > n, and S(n + 1, m) = S(n, m − 1) + nS(n, m) for others. 845 The parameters β are sampled using the Dirich-","let distribution: (β1, . . . , βK , ∑∞","k′","=K+1 βk′) ∼","Dirichlet(m·1, . . . , m·K , γ), where m·k =∑K k′ =1 mk′","k. Sampling α0: α0 is parameterized by a gamma hyperprior with hyperparameters αa and αb. We introduce two types of auxiliary variables for each state (k = 1, . . . , K), wk ∈ [0, 1] and vk ∈ {0, 1}. The conditional distribution of each wk is p(wk|α0) ∝ wα0","k (1−wk)n·k−1","and that of each vk","is p(vk|α0) ∝ ( n·k","α0 )vk",", where n·k = ∑K","k′","=1 nk′","k.","The conditional distribution of α0 given wk","and vk (k = 1, . . . , K) is p(α0|w, v) ∝","ααa−1+m..−∑K k=1 vk","0 e−α0(αb−∑K","k=1 logwk)",", where","m·· = ∑K","k′","=1 ∑K","k′′","=1 mk′","k′′. Sampling γ: γ is parameterized by a gamma hyperprior with hyperparameters γa and γb. We introduce an auxiliary variable η, whose conditional distribution is p(η|γ) ∝ ηγ","(1 − η)m··−1",". The conditional distribution of γ given η is p(γ|η) ∝ γγa−1+K","e−γ(γb−logη)","."]},{"title":"4 Experiment","paragraphs":["We tested our proposed models under the NTCIR-9 Japanese-to-English patent translation task (Goto et al., 2011), consisting of approximately 3.2 million bilingual sentences. Both the development data and the test data consist of 2,000 sentences. We also used the NTCIR-7 development data consisting of 2,741 sentences for development testing purposes. 4.1 Experimental Setup We evaluated our bilingual infinite tree model for POS induction using an in-house developed syntax-based forest-to-string SMT system. In the training process, the following steps are performed sequentially: preprocessing, inducing a POS tagset for a source language, training a POS tagger and a dependency parser, and training a forest-to-string MT model. Step 1. Preprocessing We used the first 10,000 Japanese-English sentence pairs in the NTCIR-9 training data for inducing a POS tagset for Japanese6",". The Japanese sentences were segmented using MeCab7",", and the English sentences were tokenized and POS tagged using TreeTagger (Schmid, 1994), where 43 and 58 types of POS tags are included in the Japanese sentences and the English sentences, respectively. The Japanese POS tags come from the second-level POS tags in the IPA POS tagset (Asahara and Matsumoto, 2003) and the English POS tags are derived from the Penn Treebank. Note that the Japanese POS tags are used for initialization of hidden states and the English POS tags are used as observations emitted by hidden states.","Word-by-word alignments for the sentence pairs are produced by first running GIZA++ (Och and Ney, 2003) in both directions and then combining the alignments using the “grow-diag-final-and” heuristic (Koehn et al., 2003). Note that we ran GIZA++ on all of the NTCIR-9 training data in order to obtain better alignements.","The Japanese sentences are parsed using CaboCha (Kudo and Matsumoto, 2002), which generates dependency structures using a phrasal unit called a bunsetsu8",", rather than a word unit as in English or Chinese dependency parsing. Since we focus on the word-level POS induction, each bunsetsu-based dependency tree is converted into its corresponding word-based dependency tree using the following heuristic9",": first, the last function word inside each bunsetsu is identified as the head word10","; then, the remaining words are treated as dependents of the head word in the same bunsetsu; finally, a bunsetsu-based dependency structure is transformed to a word-based dependency structure by preserving the head/modifier relationships of the determined head words. Step 2. POS Induction A POS tag for each word in the Japanese sentences is inferred by our bilingual infinite tree model, ei-","6","Due to the high computational cost, we did not use all the NTCIR-9 training data. We leave scaling up to a larger dataset for future work.","7","http://mecab.googlecode.com/svn/ trunk/mecab/doc/index.html","8","A bunsetsu is the smallest meaningful sequence consisting of a content word and accompanying function words (e.g., a noun and a particle).","9","We could use other word-based dependency trees such as trees by the infinite PCFG model (Liang et al., 2007) and syntactic-head or semantic-head dependency trees in Nakazawa and Kurohashi (2012), although it is not our major focus. We leave this for future work.","10","If no function words exist in a bunsetsu, the last content word is treated as the head word. 846 ther jointly (J oint) or independently (Ind). We also performed monolingual induction of Finkel et al. (2007) for comparison (M ono). In each model, a sequence of sampling u, z, π, β, α0, and γ is repeated 10,000 times. In sampling α0 and γ, hyperparameters αa, αb, γa, and γb are set to 2, 1, 1, and 1, respectively, which is the same setting in Gael et al. (2008). In sampling z, parameters ρ, ρ′",", . . ., are set to 0.01. In the experiments, three types of factors for the aligned English words are compared: surface forms (‘s’), POS tags (‘P’), and the combination of both (‘s+P’). Further, two types of inference frameworks are compared: induction (IN D) and ref inement (REF ). In both frameworks, each hidden state zt is first initialized to the POS tags assigned by MeCab (the IPA POS tagset), and then each state is updated through the inference procedure described in Section 3.5. Note that in REF , the sampling distribution over zt is constrained to include only states that are a refinement of the initially assigned POS tag. Step 3. Training a POS Tagger and a Dependency Parser In this step, we train a Japanese dependency parser from the 10,000 Japanese dependency trees with the induced POS tags which are derived from Step 2. We employed a transition-based dependency parser which can jointly learn POS tagging and dependency parsing (Hatori et al., 2011) under an incremental framework11",". Note that the learned parser can identify dependencies between words and attach an induced POS tag for each word. Step 4. Training a Forest-to-String MT In this step, we train a forest-to-string MT model based on the learned dependency parser in Step 3. We use an in-house developed hypergraph-based toolkit, cicada, for training and decoding with a tree-to-string model, which has been successfully employed in our previous work for system combination (Watanabe and Sumita, 2011) and online learning (Watanabe, 2012). All the Japanese and English sentences in the NTCIR-9 training data are segmented in the same way as in Step 1, and then each Japanese sentence is parsed by the dependency parser learned in Step 3, which simultaneously assigns induced POS tags and word dependencies. Finally, a forest-to-string MT model is learned with Zhang et al., (2011), which extracts translation rules by a forest-based variant of","11","http://triplet.cc/software/corbit/ IN D REF BS 27.54 M ono 27.66 26.83 J oint[s] 28.00 28.00 J oint[P] 26.36 26.72 J oint[s+P] 27.99 27.82 Ind[s] 28.00 27.93 Ind[P] 28.11 28.63 Ind[s+P] 28.13 28.62 Table 1: Performance on Japanese-to-English Translation Measured by BLEU (%) the GHKM algorithm (Mi and Huang, 2008) after each parse tree is restructured into a binarized packed forest. Parameters are tuned on the development data using xBLEU (Rosti et al., 2011) as an objective and L-BFGS (Liu and Nocedal, 1989) as an optimization toolkit, since it is stable and less prone to randomness, unlike MERT (Och, 2003) or PRO (Hopkins and May, 2011). The development test data is used to set up hyperparameters, i.e., to terminate tuning iterations.","When translating Japanese sentences, a parse tree for each sentence is constructed in the same way as described earlier in this step, and then the parse trees are translated into English sentences using the learned forest-to-string MT model. 4.2 Experimental Results Table 1 shows the performance for the test data measured by case sensitive BLEU (Papineni et al., 2002). We also present the performance of our baseline forest-to-string MT system (BS) using the original IPA POS tags. In Table 1, numbers in bold indicate that the systems outperform the baselines, BS and M ono. Under the Moses phrase-based SMT system (Koehn et al., 2007) with the default settings, we achieved a 26.80% BLEU score.","Table 1 shows that the proposed systems outperform the baseline M ono. The differences between the performance of Ind[s+P] and M ono are statistically significant in the bootstrap method (Koehn, 2004), with a 1% significance level both in IN D and REF . The results indicate that integrating the aligned target-side information in POS induction makes inferred tagsets more suitable for SMT.","Table 1 also shows that the independent model is more effective for SMT than the joint model. This means that sparseness is a severe problem in 847 Model IN D REF Joint[s+P] 164 620 Ind[s+P] 102 517 IPA POS tags 42 Table 2: The Number of POS Tags POS induction when jointly encoding bilingual information into observations. Additionally, all the systems using the independent model outperform BS. The improvements are statistically significant in the bootstrap method (Koehn, 2004), with a 1% significance level. The results show that the proposed models can generate more favorable POS tagsets for SMT than an existing POS tagset.","In Table 1, REF s are at least comparable to, or better than, IN Ds except for M ono. This shows that REF achieves better performance by preserving the clues from the original POS tagset. However, REF may suffer sever overfitting problem for M ono since no bilingual information was in-corporated. Further, when the full-level IPA POS tags12","were used in BS, the system achieved a 27.49% BLEU score, which is worse than the result using the second-level IPA POS tags. This means that manual refinement without bilingual information may also cause an overfitting problem in MT."]},{"title":"5 Discussion","paragraphs":["5.1 Comparison to the IPA POS Tagset Table 2 shows the number of the IPA POS tags used in the experiments and the POS tags induced by the proposed models. This table shows that each induced tagset contains more POS tags than the IPA POS tagset. In the experimental data, some of Japanese verbs correspond to genuine English verbs, some are nominalized, and others correspond to English past participle verbs or present participle verbs which modify other words. Respective examples are “I use a card.”, “U sing the index is faster.”, and “I explain using an example.”, where all the underlined words correspond to the same Japanese word, “\\u{f3b}\\u{34d}”, whose IPA POS tag is a verb. Ind[s+P] in REF generated the POS tagset where the three types are assigned to separate POS groups.","The Japanese particle “\\u{374}” is sometimes attached to nouns to give them adverb roles. For","12","377 types of full-level IPA POS tags were included in our experimental data. Tagging Dependency IN D REF IN D REF Original 90.37 93.62 M ono 90.75 88.04 91.77 91.51 J oint[s] 89.08 86.73 91.55 91.14 J oint[P] 80.54 79.98 91.06 91.29 J oint[s+P] 87.56 84.92 91.31 91.10 Ind[s] 87.62 84.33 92.06 92.58 Ind[P] 90.21 88.50 92.85 93.03 Ind[s+P] 89.57 86.12 92.96 92.78 Table 3: Tagging and Dependency Accuracy (%) example, “\\u{aec}\\u{793} (mutual) \\u{279}\\u{374}” is translated as the adverb “mutually” in English. Other times, it is attached to words to make them the objects of verbs. For example, “\\u{d74} (he) \\u{279}\\u{374}\\u{279}\\u{f29}\\u{351}\\u{394} (give)” is translated as “give him”. The POS tags by Ind[s+P] in REF discriminated the two types.","These examples show that the proposed models can disambiguate POS tags that have different functions in English, whereas the IPA POS tagset treats them jointly. Thus, such discrimination improves the performance of a forest-to-string SMT.","5.2 Impact of Tagging and Dependency Accuracy The performance of our methods depends not only on the quality of the induced tag sets but also on the performance of the dependency parser learned in Step 3 of Section 4.1. We cannot directly evaluate the tagging accuracy of the parser trained through Step 3 because we do not have any data with induced POS tags other than the 10,000-sentence data gained through Step 2. Thus we split the 10,000 data into the first 9,000 data for training and the remaining 1,000 for testing, and then a dependency parser was learned in the same way as in Step 3.","Table 3 shows the results. Original is the performance of the parser learned from the training data with the original POS tagset. Note that the dependency accuracies are measured on the automatically parsed dependency trees, not on the syntactically correct gold standard trees. Thus Original achieved the best dependency accuracy.","In Table 3, the performance for our bilingually-induced POSs, J oint and Ind, are lower than Original and M ono. It seems performing parsing and tagging with the bilingually-induced POS tagset is too difficult when only monolingual in-848 formation is available to the parser. However, our bilingually-induced POSs, except for J oint[P ], with the lower accuracies are more effective for SMT than the monolingually-induced POSs and the original POSs, as indicated in Table 1. The tagging accuracies for J oint[P ] both in IN D and REF are significantly lower than the others, while the dependency accuracies do not differ significantly. The lower tagging accuracies may directly reflect the lower translation qualities for J oint[P ] in Table 1."]},{"title":"6 Conclusion","paragraphs":["We proposed a novel method for inducing POS tags for SMT. The proposed method is a nonparametric Bayesian method, which infers hidden states (i.e., POS tags) based on observations representing not only source words themselves but also aligned target words. Our experiments showed that a more favorable POS tagset can be induced by integrating aligned information, and further-more, the POS tagset generated by the proposed method is more effective for SMT than an existing POS tagset (the IPA POS tagset).","Even though we employed word alignment from GIZA++ with potential errors, large gains were achieved using our proposed method. We would like to investigate the influence of alignment errors in the future. In addition, we are planning to prove the effectiveness of our proposed method for language pairs other than Japanese-to-English. We are also planning to introduce our proposed method to other syntax-based SMT, such as a string-to-tree SMT and a tree-to-tree SMT."]},{"title":"Acknowledgments","paragraphs":["We thank Isao Goto for helpful discussions and anonymous reviewers for valuable comments. We also thank Jun Hatori for helping us to apply his software, Corbit, to our induced POS tagsets."]},{"title":"References","paragraphs":["Masayuki Asahara and Yuji Matsumoto. 2003. IPADIC User Manual. Technical report, Japan.","Matthew J. Beal, Zoubin Ghahramani, and Carl E. Rasmussen. 2001. The Infinite Hidden Markov Model. In Advances in Neural Information Processing Systems, pages 577–584.","Phil Blunsom and Trevor Cohn. 2011. A Hierarchical Pitman-Yor Process HMM for Unsupervised Part of Speech Induction. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 865–874.","Trevor Cohn and Phil Blunsom. 2009. A Bayesian Model of Syntax-Directed Tree to String Grammar Induction. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 352–361.","Yuan Ding and Martha Palmer. 2005. Machine Translation Using Probabilistic Synchronous Dependency Insertion Grammars. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics, pages 541–548.","Thomas S. Ferguson. 1973. A Bayesian Analysis of Some Nonparametric Problems. The Annals of Statistics, 1(2):209–230.","Jenny Rose Finkel, Trond Grenager, and Christopher D. Manning. 2007. The Infinite Tree. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 272–279.","Jurgen Van Gael, Yunus Saatci, Yee Whye Teh, and Zoubin Ghahramani. 2008. Beam Sampling for the Infinite Hidden Markov Model. In Proceedings of the 25th International Conference on Machine Learning, pages 1088–1095.","Jurgen Van Gael, Andreas Vlachos, and Zoubin Ghahramani. 2009. The infinite HMM for unsupervised PoS tagging. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 2 - Volume 2, pages 678–687.","Michel Galley, Jonathan Graehl, Kevin Knight, Daniel Marcu, Steve DeNeefe, Wei Wang, and Ignacio Thayer. 2006. Scalable Inference and Training of Context-Rich Syntactic Translation Models. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 961–968.","Isao Goto, Bin Lu, Ka Po Chow, Eiichiro Sumita, and Benjamin K. Tsou. 2011. Overview of the Patent Machine Translation Task at the NTCIR-9 Workshop. In Proceedings of the 9th NTCIR Workshop, pages 559–578.","Jun Hatori, Takuya Matsuzaki, Yusuke Miyao, and Jun’ichi Tsujii. 2011. Incremental Joint POS Tagging and Dependency Parsing in Chinese. In Proceedings of 5th International Joint Conference on Natural Language Processing, pages 1216–1224.","Mark Hopkins and Jonathan May. 2011. Tuning as Ranking. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1352–1362.","Liang Huang, Kevin Knight, and Aravind Joshi. 2006. A Syntax-Directed Translator with Extended Domain of Locality. In Proceedings of the Workshop on 849 Computationally Hard Problemsand Joint Inference in Speech and Language Processing, pages 1–8.","Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical Phrase-Based Translation. In Proceedings of the 2003 Human Language Technology Conference: North American Chapter of the Association for Computational Linguistics, pages 48–54.","Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constrantin, and Evan Herbst. 2007. Moses: Open Source Toolkit for Statistical Machine Translation. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics on In-teractive Poster and Demonstration Sessions, pages 177–180.","Philipp Koehn. 2004. Statistical Significance Tests for Machine Translation Evaluation. In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing, pages 388–395.","Taku Kudo and Yuji Matsumoto. 2002. Japanese Dependency Analysis using Cascaded Chunking. In Proceedings of the 6th Conference on Natural Language Learning, pages 63–69.","Percy Liang, Slav Petrov, Michael I. Jordan, and Dan Klein. 2007. The Infinite PCFG using Hierarchical Dirichlet Processes. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 688–697.","Dekang Lin. 2004. A Path-based Transfer Model for Machine Translation. In Proceedings of the 20th International Conference on Computational Linguistics, pages 625–630.","Dong C. Liu and Jorge Nocedal. 1989. On the limited memory BFGS method for large scale optimization. Mathematical Programming B, 45(3):503–528.","Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-String Alignment Template for Statistical Machine Translation. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 609–616.","Yang Liu, Yajuan Lü, and Qun Liu. 2009. Improv-ing Tree-to-Tree Translation with Packed Forests. In Proceedings of the 47th Annual Meeting of the Association for Computational Linguistics and the 4th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing, pages 558–566.","Haitao Mi and Liang Huang. 2008. Forest-based Translation Rule Extraction. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 206–214.","Haitao Mi and Qun Liu. 2010. Constituency to Dependency Translation with Forests. In Proceedings of the 48th Annual Conference of the Association for Computational Linguistics, pages 1433–1442.","Toshiaki Nakazawa and Sadao Kurohashi. 2012. Alignment by Bilingual Generation and Monolingual Derivation. In Proceedings of the 24th International Conference on Computational Linguistics, pages 1963–1978.","Radford M. Neal. 2003. Slice Sampling. Annals of Statistics, 31:705–767.","Franz Josef Och and Hermann Ney. 2003. A System-atic Comparison of Various Statistical Alignment Models. Computational Linguistics, 29:19–51.","Franz Josef Och. 2003. Minimum Error Rate Training in Statistical Machine Translation. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pages 160–167.","Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. BLEU: a Method for Automatic Evaluation of Machine Translation. In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics, pages 311–318.","Chris Quirk, Arul Menezes, and Colin Cherry. 2005. Dependency Treelet Translation: Syntactically In-formed Phrasal SMT. In Proceedings of the 43rd Annual Conference of the Association for Computational Linguistics, pages 271–279.","Antti-Veikko Rosti, Bing Zhang, Spyros Matsoukas, and Richard Schwartz. 2011. Expected BLEU Training for Graphs: BBN System Description for WMT11 System Combination Task. In Proceedings of the Sixth Workshop on Statistical Machine Translation, pages 159–165.","Helmut Schmid. 1994. Probabilistic Part-of-Speech Tagging Using Decision Trees. In Proceedings of the International Conference on New Methods in Language Processing, pages 44–49.","Jayaram Sethuraman. 1994. A Constructive Definition of Dirichlet Priors. Statistica Sinica, 4(2):639–650.","Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A New String-to-Dependency Machine Translation Algorithm with a Target Dependency Language Model. In Proceedings of the 46th Annual Conference of the Association for Computational Linguistics: Human Language Technologies, pages 577– 585.","Kairit Sirts and Tanel Alumäe. 2012. A Hierarchical Dirichlet Process Model for Joint Part-of-Speech and Morphology Induction. In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 407–416. 850","Yee Whye Teh, Michael I. Jordan, Matthew J. Beal, and David M. Blei. 2006. Hierarchical Dirichlet Processes. Journal of the American Statistical Association, 101(476):1566–1581.","Taro Watanabe and Eiichiro Sumita. 2011. Machine Translation System Combination by Confusion Forest. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 1249–1257.","Taro Watanabe. 2012. Optimized Online Rank Learn-ing for Machine Translation. In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 253–262.","Min Zhang, Hongfei Jiang, Aiti Aw, Haizhou Li, Chew Lim Tan, and Sheng Li. 2008. A Tree Sequence Alignment-based Tree-to-Tree Translation Model. In Proceedings of the 46th Annual Conference of the Association for Computational Linguistics: Human Language Technologies, pages 559– 567.","Hao Zhang, Licheng Fang, Peng Xu, and Xiaoyun Wu. 2011. Binarized Forest to String Translation. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 19–24. 851"]}],"references":[{"authors":[{"first":"Masayuki","last":"Asahara"},{"first":"Yuji","last":"Matsumoto"}],"year":"2003","title":"IPADIC User Manual"},{"authors":[{"first":"Matthew","middle":"J.","last":"Beal"},{"first":"Zoubin","last":"Ghahramani"},{"first":"Carl","middle":"E.","last":"Rasmussen"}],"year":"2001","title":"The Infinite Hidden Markov Model"},{"authors":[{"first":"Phil","last":"Blunsom"},{"first":"Trevor","last":"Cohn"}],"year":"2011","title":"A Hierarchical Pitman-Yor Process HMM for Unsupervised Part of Speech Induction"},{"authors":[{"first":"Trevor","last":"Cohn"},{"first":"Phil","last":"Blunsom"}],"year":"2009","title":"A Bayesian Model of Syntax-Directed Tree to String Grammar Induction"},{"authors":[{"first":"Yuan","last":"Ding"},{"first":"Martha","last":"Palmer"}],"year":"2005","title":"Machine Translation Using Probabilistic Synchronous Dependency Insertion Grammars"},{"authors":[{"first":"Thomas","middle":"S.","last":"Ferguson"}],"year":"1973","title":"A Bayesian Analysis of Some Nonparametric Problems"},{"authors":[{"first":"Jenny","middle":"Rose","last":"Finkel"},{"first":"Trond","last":"Grenager"},{"first":"Christopher","middle":"D.","last":"Manning"}],"year":"2007","title":"The Infinite Tree"},{"authors":[{"first":"Jurgen","last":"Van Gael"},{"first":"Yunus","last":"Saatci"},{"first":"Yee","middle":"Whye","last":"Teh"},{"first":"Zoubin","last":"Ghahramani"}],"year":"2008","title":"Beam Sampling for the Infinite Hidden Markov Model"},{"authors":[{"first":"Jurgen","last":"Van Gael"},{"first":"Andreas","last":"Vlachos"},{"first":"Zoubin","last":"Ghahramani"}],"year":"2009","title":"The infinite HMM for unsupervised PoS tagging"},{"authors":[{"first":"Michel","last":"Galley"},{"first":"Jonathan","last":"Graehl"},{"first":"Kevin","last":"Knight"},{"first":"Daniel","last":"Marcu"},{"first":"Steve","last":"DeNeefe"},{"first":"Wei","last":"Wang"},{"first":"Ignacio","last":"Thayer"}],"year":"2006","title":"Scalable Inference and Training of Context-Rich Syntactic Translation Models"},{"authors":[{"first":"Isao","last":"Goto"},{"first":"Bin","last":"Lu"},{"first":"Ka","middle":"Po","last":"Chow"},{"first":"Eiichiro","last":"Sumita"},{"first":"Benjamin","middle":"K.","last":"Tsou"}],"year":"2011","title":"Overview of the Patent Machine Translation Task at the NTCIR-9 Workshop"},{"authors":[{"first":"Jun","last":"Hatori"},{"first":"Takuya","last":"Matsuzaki"},{"first":"Yusuke","last":"Miyao"},{"first":"Jun’ichi","last":"Tsujii"}],"year":"2011","title":"Incremental Joint POS Tagging and Dependency Parsing in Chinese"},{"authors":[{"first":"Mark","last":"Hopkins"},{"first":"Jonathan","last":"May"}],"year":"2011","title":"Tuning as Ranking"},{"authors":[{"first":"Liang","last":"Huang"},{"first":"Kevin","last":"Knight"},{"first":"Aravind","last":"Joshi"}],"year":"2006","title":"A Syntax-Directed Translator with Extended Domain of Locality"},{"authors":[{"first":"Philipp","last":"Koehn"},{"first":"Franz","middle":"Josef","last":"Och"},{"first":"Daniel","last":"Marcu"}],"year":"2003","title":"Statistical Phrase-Based Translation"},{"authors":[{"first":"Philipp","last":"Koehn"},{"first":"Hieu","last":"Hoang"},{"first":"Alexandra","last":"Birch"},{"first":"Chris","last":"Callison-Burch"},{"first":"Marcello","last":"Federico"},{"first":"Nicola","last":"Bertoldi"},{"first":"Brooke","last":"Cowan"},{"first":"Wade","last":"Shen"},{"first":"Christine","last":"Moran"},{"first":"Richard","last":"Zens"},{"first":"Chris","last":"Dyer"},{"first":"Ondrej","last":"Bojar"},{"first":"Alexandra","last":"Constrantin"},{"first":"Evan","last":"Herbst"}],"year":"2007","title":"Moses: Open Source Toolkit for Statistical Machine Translation"},{"authors":[{"first":"Philipp","last":"Koehn"}],"year":"2004","title":"Statistical Significance Tests for Machine Translation Evaluation"},{"authors":[{"first":"Taku","last":"Kudo"},{"first":"Yuji","last":"Matsumoto"}],"year":"2002","title":"Japanese Dependency Analysis using Cascaded Chunking"},{"authors":[{"first":"Percy","last":"Liang"},{"first":"Slav","last":"Petrov"},{"first":"Michael I","middle":".","last":"Jordan"},{"first":"Dan","last":"Klein"}],"year":"2007","title":"The Infinite PCFG using Hierarchical Dirichlet Processes"},{"authors":[{"first":"Dekang","last":"Lin"}],"year":"2004","title":"A Path-based Transfer Model for Machine Translation"},{"authors":[{"first":"Dong","middle":"C.","last":"Liu"},{"first":"Jorge","last":"Nocedal"}],"year":"1989","title":"On the limited memory BFGS method for large scale optimization"},{"authors":[{"first":"Yang","last":"Liu"},{"first":"Qun","last":"Liu"},{"first":"Shouxun","last":"Lin"}],"year":"2006","title":"Tree-to-String Alignment Template for Statistical Machine Translation"},{"authors":[{"first":"Yang","last":"Liu"},{"first":"Yajuan","last":"Lü"},{"first":"Qun","last":"Liu"}],"year":"2009","title":"Improv-ing Tree-to-Tree Translation with Packed Forests"},{"authors":[{"first":"Haitao","last":"Mi"},{"first":"Liang","last":"Huang"}],"year":"2008","title":"Forest-based Translation Rule Extraction"},{"authors":[{"first":"Haitao","last":"Mi"},{"first":"Qun","last":"Liu"}],"year":"2010","title":"Constituency to Dependency Translation with Forests"},{"authors":[{"first":"Toshiaki","last":"Nakazawa"},{"first":"Sadao","last":"Kurohashi"}],"year":"2012","title":"Alignment by Bilingual Generation and Monolingual Derivation"},{"authors":[{"first":"Radford","middle":"M.","last":"Neal"}],"year":"2003","title":"Slice Sampling"},{"authors":[{"first":"Franz","middle":"Josef","last":"Och"},{"first":"Hermann","last":"Ney"}],"year":"2003","title":"A System-atic Comparison of Various Statistical Alignment Models"},{"authors":[{"first":"Franz","middle":"Josef","last":"Och"}],"year":"2003","title":"Minimum Error Rate Training in Statistical Machine Translation"},{"authors":[{"first":"Kishore","last":"Papineni"},{"first":"Salim","last":"Roukos"},{"first":"Todd","last":"Ward"},{"first":"Wei-Jing","last":"Zhu"}],"year":"2002","title":"BLEU: a Method for Automatic Evaluation of Machine Translation"},{"authors":[{"first":"Chris","last":"Quirk"},{"first":"Arul","last":"Menezes"},{"first":"Colin","last":"Cherry"}],"year":"2005","title":"Dependency Treelet Translation: Syntactically In-formed Phrasal SMT"},{"authors":[{"first":"Antti-Veikko","last":"Rosti"},{"first":"Bing","last":"Zhang"},{"first":"Spyros","last":"Matsoukas"},{"first":"Richard","last":"Schwartz"}],"year":"2011","title":"Expected BLEU Training for Graphs: BBN System Description for WMT11 System Combination Task"},{"authors":[{"first":"Helmut","last":"Schmid"}],"year":"1994","title":"Probabilistic Part-of-Speech Tagging Using Decision Trees"},{"authors":[{"first":"Jayaram","last":"Sethuraman"}],"year":"1994","title":"A Constructive Definition of Dirichlet Priors"},{"authors":[{"first":"Libin","last":"Shen"},{"first":"Jinxi","last":"Xu"},{"first":"Ralph","last":"Weischedel"}],"year":"2008","title":"A New String-to-Dependency Machine Translation Algorithm with a Target Dependency Language Model"},{"authors":[{"first":"Kairit","last":"Sirts"},{"first":"Tanel","last":"Alumäe"}],"year":"2012","title":"A Hierarchical Dirichlet Process Model for Joint Part-of-Speech and Morphology Induction"},{"authors":[{"first":"Yee","middle":"Whye","last":"Teh"},{"first":"Michael I","middle":".","last":"Jordan"},{"first":"Matthew","middle":"J.","last":"Beal"},{"first":"David","middle":"M.","last":"Blei"}],"year":"2006","title":"Hierarchical Dirichlet Processes"},{"authors":[{"first":"Taro","last":"Watanabe"},{"first":"Eiichiro","last":"Sumita"}],"year":"2011","title":"Machine Translation System Combination by Confusion Forest"},{"authors":[{"first":"Taro","last":"Watanabe"}],"year":"2012","title":"Optimized Online Rank Learn-ing for Machine Translation"},{"authors":[{"first":"Min","last":"Zhang"},{"first":"Hongfei","last":"Jiang"},{"first":"Aiti","last":"Aw"},{"first":"Haizhou","last":"Li"},{"first":"Chew","middle":"Lim","last":"Tan"},{"first":"Sheng","last":"Li"}],"year":"2008","title":"A Tree Sequence Alignment-based Tree-to-Tree Translation Model"},{"authors":[{"first":"Hao","last":"Zhang"},{"first":"Licheng","last":"Fang"},{"first":"Peng","last":"Xu"},{"first":"Xiaoyun","last":"Wu"}],"year":"2011","title":"Binarized Forest to String Translation"}],"cites":[{"authors":[{"last":"Finkel"},{"last":"al."}],"year":"2007","style":0,"reference":{"authors":[{"first":"Jenny","middle":"Rose","last":"Finkel"},{"first":"Trond","last":"Grenager"},{"first":"Christopher","middle":"D.","last":"Manning"}],"year":"2007","title":"The Infinite Tree"}},{"authors":[{"last":"Lin"}],"year":"2004","style":0,"reference":{"authors":[{"first":"Dekang","last":"Lin"}],"year":"2004","title":"A Path-based Transfer Model for Machine Translation"}},{"authors":[{"last":"Ding"},{"last":"Palmer"}],"year":"2005","style":0,"reference":{"authors":[{"first":"Yuan","last":"Ding"},{"first":"Martha","last":"Palmer"}],"year":"2005","title":"Machine Translation Using Probabilistic Synchronous Dependency Insertion Grammars"}},{"authors":[{"last":"Quirk"},{"last":"al."}],"year":"2005","style":0,"reference":{"authors":[{"first":"Chris","last":"Quirk"},{"first":"Arul","last":"Menezes"},{"first":"Colin","last":"Cherry"}],"year":"2005","title":"Dependency Treelet Translation: Syntactically In-formed Phrasal SMT"}},{"authors":[{"last":"Shen"},{"last":"al."}],"year":"2008","style":0,"reference":{"authors":[{"first":"Libin","last":"Shen"},{"first":"Jinxi","last":"Xu"},{"first":"Ralph","last":"Weischedel"}],"year":"2008","title":"A New String-to-Dependency Machine Translation Algorithm with a Target Dependency Language Model"}},{"authors":[{"last":"Mi"},{"last":"Liu"}],"year":"2010","style":0,"reference":{"authors":[{"first":"Haitao","last":"Mi"},{"first":"Qun","last":"Liu"}],"year":"2010","title":"Constituency to Dependency Translation with Forests"}},{"authors":[{"last":"Huang"},{"last":"al."}],"year":"2006","style":0,"reference":{"authors":[{"first":"Liang","last":"Huang"},{"first":"Kevin","last":"Knight"},{"first":"Aravind","last":"Joshi"}],"year":"2006","title":"A Syntax-Directed Translator with Extended Domain of Locality"}},{"authors":[{"last":"Liu"},{"last":"al."}],"year":"2006","style":0,"reference":{"authors":[{"first":"Yang","last":"Liu"},{"first":"Qun","last":"Liu"},{"first":"Shouxun","last":"Lin"}],"year":"2006","title":"Tree-to-String Alignment Template for Statistical Machine Translation"}},{"authors":[{"last":"Galley"},{"last":"al."}],"year":"2006","style":0,"reference":{"authors":[{"first":"Michel","last":"Galley"},{"first":"Jonathan","last":"Graehl"},{"first":"Kevin","last":"Knight"},{"first":"Daniel","last":"Marcu"},{"first":"Steve","last":"DeNeefe"},{"first":"Wei","last":"Wang"},{"first":"Ignacio","last":"Thayer"}],"year":"2006","title":"Scalable Inference and Training of Context-Rich Syntactic Translation Models"}},{"authors":[{"last":"Mi"},{"last":"Huang"}],"year":"2008","style":0,"reference":{"authors":[{"first":"Haitao","last":"Mi"},{"first":"Liang","last":"Huang"}],"year":"2008","title":"Forest-based Translation Rule Extraction"}},{"authors":[{"last":"Zhang"},{"last":"al."}],"year":"2008","style":0,"reference":{"authors":[{"first":"Min","last":"Zhang"},{"first":"Hongfei","last":"Jiang"},{"first":"Aiti","last":"Aw"},{"first":"Haizhou","last":"Li"},{"first":"Chew","middle":"Lim","last":"Tan"},{"first":"Sheng","last":"Li"}],"year":"2008","title":"A Tree Sequence Alignment-based Tree-to-Tree Translation Model"}},{"authors":[{"last":"Cohn"},{"last":"Blunsom"}],"year":"2009","style":0,"reference":{"authors":[{"first":"Trevor","last":"Cohn"},{"first":"Phil","last":"Blunsom"}],"year":"2009","title":"A Bayesian Model of Syntax-Directed Tree to String Grammar Induction"}},{"authors":[{"last":"Liu"},{"last":"al."}],"year":"2009","style":0,"reference":{"authors":[{"first":"Yang","last":"Liu"},{"first":"Yajuan","last":"Lü"},{"first":"Qun","last":"Liu"}],"year":"2009","title":"Improv-ing Tree-to-Tree Translation with Packed Forests"}},{"authors":[{"last":"Mi"},{"last":"Liu"}],"year":"2010","style":0,"reference":{"authors":[{"first":"Haitao","last":"Mi"},{"first":"Qun","last":"Liu"}],"year":"2010","title":"Constituency to Dependency Translation with Forests"}},{"authors":[{"last":"Zhang"},{"last":"al."}],"year":"2011","style":0,"reference":{"authors":[{"first":"Hao","last":"Zhang"},{"first":"Licheng","last":"Fang"},{"first":"Peng","last":"Xu"},{"first":"Xiaoyun","last":"Wu"}],"year":"2011","title":"Binarized Forest to String Translation"}},{"authors":[{"last":"Finkel"},{"last":"al."}],"year":"2007","style":0,"reference":{"authors":[{"first":"Jenny","middle":"Rose","last":"Finkel"},{"first":"Trond","last":"Grenager"},{"first":"Christopher","middle":"D.","last":"Manning"}],"year":"2007","title":"The Infinite Tree"}},{"authors":[{"last":"Gael"},{"last":"al."}],"year":"2008","style":0},{"authors":[{"last":"Finkel"},{"last":"al."}],"year":"2007","style":0,"reference":{"authors":[{"first":"Jenny","middle":"Rose","last":"Finkel"},{"first":"Trond","last":"Grenager"},{"first":"Christopher","middle":"D.","last":"Manning"}],"year":"2007","title":"The Infinite Tree"}},{"authors":[{"last":"Gael"},{"last":"al."}],"year":"2009","style":0},{"authors":[{"last":"Blunsom"},{"last":"Cohn"}],"year":"2011","style":0,"reference":{"authors":[{"first":"Phil","last":"Blunsom"},{"first":"Trevor","last":"Cohn"}],"year":"2011","title":"A Hierarchical Pitman-Yor Process HMM for Unsupervised Part of Speech Induction"}},{"authors":[{"last":"Sirts"},{"last":"Alumäe"}],"year":"2012","style":0,"reference":{"authors":[{"first":"Kairit","last":"Sirts"},{"first":"Tanel","last":"Alumäe"}],"year":"2012","title":"A Hierarchical Dirichlet Process Model for Joint Part-of-Speech and Morphology Induction"}},{"authors":[{"last":"Gael"},{"last":"al."}],"year":"2009","style":0},{"authors":[{"last":"Beal"},{"last":"al."}],"year":"2001","style":0,"reference":{"authors":[{"first":"Matthew","middle":"J.","last":"Beal"},{"first":"Zoubin","last":"Ghahramani"},{"first":"Carl","middle":"E.","last":"Rasmussen"}],"year":"2001","title":"The Infinite Hidden Markov Model"}},{"authors":[{"last":"Teh"},{"last":"al."}],"year":"2006","style":0,"reference":{"authors":[{"first":"Yee","middle":"Whye","last":"Teh"},{"first":"Michael I","middle":".","last":"Jordan"},{"first":"Matthew","middle":"J.","last":"Beal"},{"first":"David","middle":"M.","last":"Blei"}],"year":"2006","title":"Hierarchical Dirichlet Processes"}},{"authors":[{"last":"Blunsom"},{"last":"Cohn"}],"year":"2011","style":0,"reference":{"authors":[{"first":"Phil","last":"Blunsom"},{"first":"Trevor","last":"Cohn"}],"year":"2011","title":"A Hierarchical Pitman-Yor Process HMM for Unsupervised Part of Speech Induction"}},{"authors":[{"last":"Sirts"},{"last":"Alumäe"}],"year":"2012","style":0,"reference":{"authors":[{"first":"Kairit","last":"Sirts"},{"first":"Tanel","last":"Alumäe"}],"year":"2012","title":"A Hierarchical Dirichlet Process Model for Joint Part-of-Speech and Morphology Induction"}},{"authors":[{"last":"Finkel"},{"last":"al."}],"year":"2007","style":0,"reference":{"authors":[{"first":"Jenny","middle":"Rose","last":"Finkel"},{"first":"Trond","last":"Grenager"},{"first":"Christopher","middle":"D.","last":"Manning"}],"year":"2007","title":"The Infinite Tree"}},{"authors":[{"last":"Finkel"},{"last":"al."}],"year":"2007","style":0,"reference":{"authors":[{"first":"Jenny","middle":"Rose","last":"Finkel"},{"first":"Trond","last":"Grenager"},{"first":"Christopher","middle":"D.","last":"Manning"}],"year":"2007","title":"The Infinite Tree"}},{"authors":[{"last":"Teh"},{"last":"al."}],"year":"2006","style":0,"reference":{"authors":[{"first":"Yee","middle":"Whye","last":"Teh"},{"first":"Michael I","middle":".","last":"Jordan"},{"first":"Matthew","middle":"J.","last":"Beal"},{"first":"David","middle":"M.","last":"Blei"}],"year":"2006","title":"Hierarchical Dirichlet Processes"}},{"authors":[{"last":"Finkel"},{"last":"al."}],"year":"2007","style":0,"reference":{"authors":[{"first":"Jenny","middle":"Rose","last":"Finkel"},{"first":"Trond","last":"Grenager"},{"first":"Christopher","middle":"D.","last":"Manning"}],"year":"2007","title":"The Infinite Tree"}},{"authors":[{"last":"Ferguson"}],"year":"1973","style":0,"reference":{"authors":[{"first":"Thomas","middle":"S.","last":"Ferguson"}],"year":"1973","title":"A Bayesian Analysis of Some Nonparametric Problems"}},{"authors":[{"last":"Beal"},{"last":"al."}],"year":"2001","style":0,"reference":{"authors":[{"first":"Matthew","middle":"J.","last":"Beal"},{"first":"Zoubin","last":"Ghahramani"},{"first":"Carl","middle":"E.","last":"Rasmussen"}],"year":"2001","title":"The Infinite Hidden Markov Model"}},{"authors":[{"last":"Sethuraman"}],"year":"1994","style":0,"reference":{"authors":[{"first":"Jayaram","last":"Sethuraman"}],"year":"1994","title":"A Constructive Definition of Dirichlet Priors"}},{"authors":[{"last":"Sethuraman"}],"year":"1994","style":0,"reference":{"authors":[{"first":"Jayaram","last":"Sethuraman"}],"year":"1994","title":"A Constructive Definition of Dirichlet Priors"}},{"authors":[{"last":"Finkel"},{"last":"al."}],"year":"2007","style":0,"reference":{"authors":[{"first":"Jenny","middle":"Rose","last":"Finkel"},{"first":"Trond","last":"Grenager"},{"first":"Christopher","middle":"D.","last":"Manning"}],"year":"2007","title":"The Infinite Tree"}},{"authors":[{"last":"Liang"},{"last":"al."}],"year":"2007","style":0,"reference":{"authors":[{"first":"Percy","last":"Liang"},{"first":"Slav","last":"Petrov"},{"first":"Michael I","middle":".","last":"Jordan"},{"first":"Dan","last":"Klein"}],"year":"2007","title":"The Infinite PCFG using Hierarchical Dirichlet Processes"}},{"authors":[{"last":"Finkel"},{"last":"al."}],"year":"2007","style":0,"reference":{"authors":[{"first":"Jenny","middle":"Rose","last":"Finkel"},{"first":"Trond","last":"Grenager"},{"first":"Christopher","middle":"D.","last":"Manning"}],"year":"2007","title":"The Infinite Tree"}},{"authors":[{"last":"Teh"},{"last":"al."}],"year":"2006","style":0,"reference":{"authors":[{"first":"Yee","middle":"Whye","last":"Teh"},{"first":"Michael I","middle":".","last":"Jordan"},{"first":"Matthew","middle":"J.","last":"Beal"},{"first":"David","middle":"M.","last":"Blei"}],"year":"2006","title":"Hierarchical Dirichlet Processes"}},{"authors":[{"last":"Gael"},{"last":"al."}],"year":"2008","style":0},{"authors":[{"last":"Gael"},{"last":"al."}],"year":"2008","style":0},{"authors":[{"last":"Neal"}],"year":"2003","style":0,"reference":{"authors":[{"first":"Radford","middle":"M.","last":"Neal"}],"year":"2003","title":"Slice Sampling"}},{"authors":[{"last":"Gael"},{"last":"al."}],"year":"2008","style":0},{"authors":[{"last":"Finkel"},{"last":"al."}],"year":"2007","style":0,"reference":{"authors":[{"first":"Jenny","middle":"Rose","last":"Finkel"},{"first":"Trond","last":"Grenager"},{"first":"Christopher","middle":"D.","last":"Manning"}],"year":"2007","title":"The Infinite Tree"}},{"authors":[{"last":"Goto"},{"last":"al."}],"year":"2011","style":0,"reference":{"authors":[{"first":"Isao","last":"Goto"},{"first":"Bin","last":"Lu"},{"first":"Ka","middle":"Po","last":"Chow"},{"first":"Eiichiro","last":"Sumita"},{"first":"Benjamin","middle":"K.","last":"Tsou"}],"year":"2011","title":"Overview of the Patent Machine Translation Task at the NTCIR-9 Workshop"}},{"authors":[{"last":"Schmid"}],"year":"1994","style":0,"reference":{"authors":[{"first":"Helmut","last":"Schmid"}],"year":"1994","title":"Probabilistic Part-of-Speech Tagging Using Decision Trees"}},{"authors":[{"last":"Asahara"},{"last":"Matsumoto"}],"year":"2003","style":0,"reference":{"authors":[{"first":"Masayuki","last":"Asahara"},{"first":"Yuji","last":"Matsumoto"}],"year":"2003","title":"IPADIC User Manual"}},{"authors":[{"last":"Och"},{"last":"Ney"}],"year":"2003","style":0,"reference":{"authors":[{"first":"Franz","middle":"Josef","last":"Och"},{"first":"Hermann","last":"Ney"}],"year":"2003","title":"A System-atic Comparison of Various Statistical Alignment Models"}},{"authors":[{"last":"Koehn"},{"last":"al."}],"year":"2003","style":0,"reference":{"authors":[{"first":"Philipp","last":"Koehn"},{"first":"Franz","middle":"Josef","last":"Och"},{"first":"Daniel","last":"Marcu"}],"year":"2003","title":"Statistical Phrase-Based Translation"}},{"authors":[{"last":"Kudo"},{"last":"Matsumoto"}],"year":"2002","style":0,"reference":{"authors":[{"first":"Taku","last":"Kudo"},{"first":"Yuji","last":"Matsumoto"}],"year":"2002","title":"Japanese Dependency Analysis using Cascaded Chunking"}},{"authors":[{"last":"Liang"},{"last":"al."}],"year":"2007","style":0,"reference":{"authors":[{"first":"Percy","last":"Liang"},{"first":"Slav","last":"Petrov"},{"first":"Michael I","middle":".","last":"Jordan"},{"first":"Dan","last":"Klein"}],"year":"2007","title":"The Infinite PCFG using Hierarchical Dirichlet Processes"}},{"authors":[{"last":"Nakazawa"},{"last":"Kurohashi"}],"year":"2012","style":0,"reference":{"authors":[{"first":"Toshiaki","last":"Nakazawa"},{"first":"Sadao","last":"Kurohashi"}],"year":"2012","title":"Alignment by Bilingual Generation and Monolingual Derivation"}},{"authors":[{"last":"Finkel"},{"last":"al."}],"year":"2007","style":0,"reference":{"authors":[{"first":"Jenny","middle":"Rose","last":"Finkel"},{"first":"Trond","last":"Grenager"},{"first":"Christopher","middle":"D.","last":"Manning"}],"year":"2007","title":"The Infinite Tree"}},{"authors":[{"last":"Gael"},{"last":"al."}],"year":"2008","style":0},{"authors":[{"last":"Hatori"},{"last":"al."}],"year":"2011","style":0,"reference":{"authors":[{"first":"Jun","last":"Hatori"},{"first":"Takuya","last":"Matsuzaki"},{"first":"Yusuke","last":"Miyao"},{"first":"Jun’ichi","last":"Tsujii"}],"year":"2011","title":"Incremental Joint POS Tagging and Dependency Parsing in Chinese"}},{"authors":[{"last":"Watanabe"},{"last":"Sumita"}],"year":"2011","style":0,"reference":{"authors":[{"first":"Taro","last":"Watanabe"},{"first":"Eiichiro","last":"Sumita"}],"year":"2011","title":"Machine Translation System Combination by Confusion Forest"}},{"authors":[{"last":"Watanabe"}],"year":"2012","style":0,"reference":{"authors":[{"first":"Taro","last":"Watanabe"}],"year":"2012","title":"Optimized Online Rank Learn-ing for Machine Translation"}},{"authors":[{"last":"Mi"},{"last":"Huang"}],"year":"2008","style":0,"reference":{"authors":[{"first":"Haitao","last":"Mi"},{"first":"Liang","last":"Huang"}],"year":"2008","title":"Forest-based Translation Rule Extraction"}},{"authors":[{"last":"Rosti"},{"last":"al."}],"year":"2011","style":0,"reference":{"authors":[{"first":"Antti-Veikko","last":"Rosti"},{"first":"Bing","last":"Zhang"},{"first":"Spyros","last":"Matsoukas"},{"first":"Richard","last":"Schwartz"}],"year":"2011","title":"Expected BLEU Training for Graphs: BBN System Description for WMT11 System Combination Task"}},{"authors":[{"last":"Liu"},{"last":"Nocedal"}],"year":"1989","style":0,"reference":{"authors":[{"first":"Dong","middle":"C.","last":"Liu"},{"first":"Jorge","last":"Nocedal"}],"year":"1989","title":"On the limited memory BFGS method for large scale optimization"}},{"authors":[{"last":"Och"}],"year":"2003","style":0,"reference":{"authors":[{"first":"Franz","middle":"Josef","last":"Och"}],"year":"2003","title":"Minimum Error Rate Training in Statistical Machine Translation"}},{"authors":[{"last":"Hopkins"},{"last":"May"}],"year":"2011","style":0,"reference":{"authors":[{"first":"Mark","last":"Hopkins"},{"first":"Jonathan","last":"May"}],"year":"2011","title":"Tuning as Ranking"}},{"authors":[{"last":"Papineni"},{"last":"al."}],"year":"2002","style":0,"reference":{"authors":[{"first":"Kishore","last":"Papineni"},{"first":"Salim","last":"Roukos"},{"first":"Todd","last":"Ward"},{"first":"Wei-Jing","last":"Zhu"}],"year":"2002","title":"BLEU: a Method for Automatic Evaluation of Machine Translation"}},{"authors":[{"last":"Koehn"},{"last":"al."}],"year":"2007","style":0,"reference":{"authors":[{"first":"Philipp","last":"Koehn"},{"first":"Hieu","last":"Hoang"},{"first":"Alexandra","last":"Birch"},{"first":"Chris","last":"Callison-Burch"},{"first":"Marcello","last":"Federico"},{"first":"Nicola","last":"Bertoldi"},{"first":"Brooke","last":"Cowan"},{"first":"Wade","last":"Shen"},{"first":"Christine","last":"Moran"},{"first":"Richard","last":"Zens"},{"first":"Chris","last":"Dyer"},{"first":"Ondrej","last":"Bojar"},{"first":"Alexandra","last":"Constrantin"},{"first":"Evan","last":"Herbst"}],"year":"2007","title":"Moses: Open Source Toolkit for Statistical Machine Translation"}},{"authors":[{"last":"Koehn"}],"year":"2004","style":0,"reference":{"authors":[{"first":"Philipp","last":"Koehn"}],"year":"2004","title":"Statistical Significance Tests for Machine Translation Evaluation"}},{"authors":[{"last":"Koehn"}],"year":"2004","style":0,"reference":{"authors":[{"first":"Philipp","last":"Koehn"}],"year":"2004","title":"Statistical Significance Tests for Machine Translation Evaluation"}}]}
