{"sections":[{"title":"","paragraphs":["Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 328–333, Sofia, Bulgaria, August 4-9 2013. c⃝2013 Association for Computational Linguistics"]},{"title":"Sign Language Lexical Recognition With Propositional DynamicLogicArturo CurielUniversité Paul Sabatier118 route de Narbonne, IRIT,31062, Toulouse, Francecuriel@irit.fr Christophe ColletUniversité Paul Sabatier118 route de Narbonne, IRIT,31062, Toulouse, Francecollet@irit.frAbstract","paragraphs":["This paper explores the use of Propositional Dynamic Logic (PDL) as a suitable formal framework for describing Sign Language (SL), the language of deaf people, in the context of natural language processing. SLs are visual, complete, standalone languages which are just as expressive as oral languages. Signs in SL usually correspond to sequences of highly specific body postures interleaved with movements, which make reference to real world objects, characters or situations. Here we propose a formal representation of SL signs, that will help us with the analysis of automatically-collected hand tracking data from French Sign Language (FSL) video corpora. We further show how such a representation could help us with the design of computer aided SL verification tools, which in turn would bring us closer to the development of an automatic recognition system for these languages."]},{"title":"1 Introduction","paragraphs":["Sign languages (SL), the vernaculars of deaf people, are complete, rich, standalone communication systems which have evolved in parallel with oral languages (Valli and Lucas, 2000). However, in contrast to the last ones, research in automatic SL processing has not yet managed to build a complete, formal definition oriented to their automatic recognition (Cuxac and Dalle, 2007). In SL, both hands and nonmanual features (NMF), e.g. facial muscles, can convey information with their placements, configurations and movements. These particular conditions can difficult the construction of a formal description with common natural language processing (NLP) methods, since the existing modeling techniques are mostly designed to work with one-channel sound productions inherent to oral languages, rather than with the multi-channel partially-synchronized information induced by SLs.","Our research strives to address the formalization problem by introducing a logical language that lets us represent SL from the lowest level, so as to render the recognition task more approachable. For this, we use an instance of a formal logic, specifically Propositional Dynamic Logic (PDL), as a possible description language for SL signs.","For the rest of this section, we will present a brief introduction to current research efforts in the area. Section 2 presents a general description of our formalism, while section 3 shows how our work can be used when confronted with real world data. Finally, section 4 present our final observations and future work.","Images for the examples where taken from (DictaSign, 2012) corpus. 1.1 Current Sign Language Research Extensive efforts have been made to achieve efficient automatic capture and representation of the subtle nuances commonly present in sign language discourse (Ong and Ranganath, 2005). Research ranges from the development of hand and body trackers (Dreuw et al., 2009; Gianni and Dalle, 2009), to the design of high level SL representation models (Lejeune, 2004; Lenseigne and Dalle, 2006). Linguistic research in the area has focused on the characterization of corporal expressions into meaningful transcriptions (Dreuw et al., 2010; Stokoe, 2005) or common patterns across SL (Aronoff et al., 2005; Meir et al., 2006; Wittmann, 1991), so as to gain understanding of the un-328 derlying mechanisms of SL communication.","Works like (Losson and Vannobel, 1998) deal with the creation of a lexical description oriented to computer-based sign animation. Report (Filhol, 2009) describes a lexical specification to address the same problem. Both propose a thoroughly geometrical parametric en-coding of signs, thus leaving behind meaningful information necessary for recognition and introducing data beyond the scope of recognition. This complicates the reutilization of their formal descriptions. Besides, they don’t take in account the presence of partial information. Treating partiality is important for us, since it is often the case with automatic tools that incomplete or unrecognizable information arises. Finally, little to no work has been directed towards the unification of raw collected data from SL corpora with higher level descriptions (Dalle, 2006)."]},{"title":"2 Propositional Dynamic Logic forSL","paragraphs":["Propositional Dynamic Logic (PDL) is a multimodal logic, first defined by (Fischer and Ladner, 1979). It provides a language for describing programs, their correctness and termina-tion, by allowing them to be modal operators. We work with our own variant of this logic, the Propositional Dynamic Logic for Sign Language (PDLSL), which is just an instantiation of PDL where we take signers’ movements as programs.","Our sign formalization is based on the approach of (Liddell and Johnson, 1989) and (Filhol, 2008). They describe signs as sequences of immutable key postures and movement transitions.","In general, each key posture will be characterized by the concurrent parametric state of each body articulator over a time-interval. For us, a body articulator is any relevant body part involved in signing. The parameters taken in account can vary from articulator to articulator, but most of the time they comprise their configurations, orientations and their place-ment within one or more places of articulation. Transitions will correspond to the movements executed between fixed postures. 2.1 Syntax We need to define some primitive sets that will limit the domain of our logical language. Definition 2.1 (Sign Language primitives). Let BSL = {D, W, R, L} be the set of relevant body articulators for SL, where D, W, R and L represent the dominant, weak, right and left hands, respectively. Both D and W can be aliases for the right or left hands, but they change depending on whether the signer is right-handed or left-handed, or even depending on the context.","Let Ψ be the two-dimensional projection of a human body skeleton, seen by the front. We define the set of places of articulation for SL as ΛSL = {HEAD, CHEST, NEUTRAL, . . .}, such that for each λ ∈ ΛSL, λ is a sub-plane of Ψ, as shown graphically in figure 1.","Let CSL be the set of possible morphological configurations for a hand.","Let ∆ = {↑, ↗, →, ↘, ↓, ↙, ←, ↖} be the set of relative directions from the signer’s point of view, where each arrow represents one of eight possible two-dimensional direction vectors that share the same origin. For vector δ ∈ ∆, we define vector ←−","δ as the same as δ but with the inverted abscissa axis, such that ←−","δ ∈ ∆. Let vector ̂δ indicate movement with respect to the dominant or weak hand in the following manner:","̂δ = { δ if D ≡ R or W ≡ L ←− δ if D ≡ L or W ≡ R","Finally, let −→v1 and −→v2 be any two vectors with the same origin. We denote the rotation angle between the two as θ(−→v1, −→v2).","Now we define the set of atomic propositions that we will use to characterize fixed states, and a set of atomic actions to describe movements. Definition 2.2 (Atomic Propositions for SL Body Articulators ΦSL). The set of atomic propositions for SL articulators (ΦSL) is defined as:","ΦSL = {β1δ","β2, Ξβ1","λ , T β1","β2 , F β1","c , ∠δ","β1}","where β1, β2 ∈ BSL, δ ∈ ∆, λ ∈ ΛSL and c ∈ CSL. 329 Figure 1: Possible places of articulation in BSL.","Intuitively, β1δ","β2 indicates that articulator β1 is placed in relative direction δ with respect to articulator β2. Let the current place of articulation of β2 be the origin point of β2’s Cartesian system (Cβ2). Let vector −→","β1 describe the current place of articulation of β1 in Cβ2. Proposition β1δ","β2 holds when ∀−→v ∈ ∆,","θ(−→ β1, δ) ≤ θ(−→","β1, −→v ). Ξβ1","λ asserts that articulator β1 is located in","λ. T β1 β2 is active whenever articulator β1 physi-","cally touches articulator β2. F β1","c indicates that c is the morphological","configuration of articulator β1. Finally, ∠δ","β1 means that an articulator β1 is","oriented towards direction δ ∈ ∆. For hands,","∠δ β1 will hold whenever the vector perpendicu-","lar to the plane of the palm has the smallest","rotation angle with respect to δ. Definition 2.3 (Atomic Actions for SL Body Articulators ΠSL). The atomic actions for SL articulators ( ΠSL) are given by the following set: ΠSL = {δβ1, ↭β1} where δ ∈ ∆ and β1 ∈ BSL.","Let β1’s position before movement be the origin of β1’s Cartesian system (Cβ1) and −→","β1 be the position vector of β1 in Cβ1 after moving. Action δβ1 indicates that β1 moves in relative direction δ in Cβ1 if ∀−→v ∈ ∆, θ(−→","β1, δ) ≤ θ(−→","β1, −→v ).","Action ↭β1 occurs when articulator β1 moves rapidly and continuously (thrills) with-out changing it’s current place of articulation. Definition 2.4 (Action Language for SL Body Articulators ASL). The action language for body articulators (ASL) is given by the following rule: α ::= π | α ∩ α | α ∪ α | α; α | α∗ where π ∈ ΠSL.","Intuitively, α ∩ α indicates the concurrent execution of two actions, while α ∪ α means that at least one of two actions will be nondeterministically executed. Action α; α describes the sequential execution of two actions. Finally, action α∗","indicates the reflexive transitive closure of α. Definition 2.5 (Language PDLSL ). The formulae φ of PDLSL are given by the following rule: φ ::= ⊤ | p | ¬φ | φ ∧ φ | [α]φ where p ∈ ΦSL, α ∈ ASL. 2.2 Semantics PDLSL formulas are interpreted over labeled transition systems (LTS), in the spirit of the possible worlds model introduced by (Hintikka, 1962). Models correspond to connected graphs representing key postures and transitions: states are determined by the values of their propositions, while edges represent sets of executed movements. Here we present only a small extract of the logic semantics. Definition 2.6 (Sign Language Utterance Model USL). A sign language utterance model (USL), is a tuple USL = (S, R, J·KΠSL, J·KΦSL) where: • S is a non-empty set of states","• R is a transition relation R ⊆ S×S where,","∀s ∈ S, ∃s′","∈ S such that (s, s′",") ∈ R.","• J·KΠSL : ΠSL → R, denotes the function mapping actions to the set of binary rela-tions.","• J·KΦSL : S → 2ΦSL",", maps each state to a set of atomic propositions. 330","We also need to define a structure over sequences of states to model internal dependencies between them, nevertheless we decided to omit the rest of our semantics, alongside satisfaction conditions, for the sake of readability."]},{"title":"3 Use Case: Semi-Automatic SignRecognition","paragraphs":["We now present an example of how we can use our formalism in a semi-automatic sign recognition system. Figure 2 shows a simple module diagram exemplifying information flow in the system’s architecture. We proceed to briefly describe each of our modules and how they work together. Corpus Tracking and Segmentation Module","Key postures & transitions PDLSL Model Extraction Module PDLSL Verification Module PDLSL Graph Sign Formulæ User Input","Sign Proposals Figure 2: Information flow in a semi-automatic SL lexical recognition system.","3.1 Tracking and Segmentation Module The process starts by capturing relevant information from video corpora. We use an existing head and hand tracker expressly developed for SL research (Gonzalez and Collet, 2011). This tool analyses individual video instances, and returns the frame-by-frame positions of the tracked articulators. By using this information, the module can immediately calculate speeds and directions on the fly for each hand.","The module further employs the method proposed by the authors in (Gonzalez and Collet, 2012) to achieve sub-lexical segmentation from the previously calculated data. Like them, we use the relative velocity between hands to identify when hands either move at the same time, independently or don’t move at all. With these, we can produce a set of possible key postures and transitions that will serve as input to the modeling module. 3.2 Model Extraction Module This module calculates a propositional state for each static posture, where atomic PDLSL formulas codify the information tracked in the previous part. Detected movements are interpreted as PDLSL actions between states. . . . R↗","L ΞL TORSE","ΞR R_SIDEOFBODY ¬FR L_CONFIG","¬FL FIST_CONFIG ¬T R L . . .",".",".",". R←","L","ΞL L_SIDEOFBODY","ΞR R_SIDEOFBODY FR KEY_CONFIG FL KEY_CONFIG ¬T R L . . . ↗L ↭D ∩ ↭G . . . R← L ΞL CENTEROFBODY ΞR R_SIDEOFHEAD FR BEAK_CONFIG FL INDEX_CONFIG ¬T R L . . . ↙L",".",".",".","R←","L ΞL L_SIDEOFBODY ΞR R_SIDEOFBODY","FR OPENPALM_CONFIG","FL OPENPALM_CONFIG ¬T R L . . . ↗L Figure 3: Example of modeling over four automatically identified frames as possible key postures.","Figure 3 shows an example of the process. Here, each key posture is codified into propositions acknowledging the hand positions with respect to each other (R←","L ), their place of articulation (e.g. “left hand floats over the torse” with ΞL","TORSE), their configuration (e.g. “right hand is open” with F R","OPENPALM_CONFIG) and their movements (e.g. “left hand moves to the up-left direction” with ↗L).","This module also checks that the generated graph is correct: it will discard simple tracking errors to ensure that the resulting LTS will remain consistent. 3.3 Verification Module First of all, the verification module has to be loaded with a database of sign descriptions en-coded as PDLSL formulas. These will characterize the specific sequence of key postures that morphologically describe a sign. For example, let’s take the case for sign “route” in FSL, shown in figure 4, with the following PDLSL formulation, Example 3.1 (ROUTEFSL formula).","(ΞR","FACE ∧ ΞL","FACE ∧ L→ R ∧ F R","CLAMP ∧ F L","CLAMP ∧ T R","L ) →","[←R ∩ →L](L→","R ∧ F R","CLAMP ∧ F L","CLAMP ∧ ¬T R","L )","(1) 331 Figure 4: ROUTEFSL production.","Formula (1) describes ROUTEFSL as a sign with two key postures, connected by a two-hand simultaneous movement (represented with operator ∩). It also indicates the position of each hand, their orientation, whether they touch and their respective configurations (in this example, both hold the same CLAMP configuration).","The module can then verify whether a sign formula in the lexical database holds in any sub-sequence of states of the graph generated in the previous step. Algorithm 1 sums up the process. Algorithm 1 PDLSL Verification Algorithm Require: SL model MSL Require: connected graph GSL Require: lexical database DBSL 1: Proposals_For[state_qty] 2: for state s ∈ GSL do 3: for sign φ ∈ DBSL where s ∈ φ do 4: if MSL, s |= φ then 5: Proposals_For[s].append(φ) 6: end if 7: end for 8: end for 9: return Proposals_For","For each state, the algorithm returns a set of possible signs. Expert users (or higher level algorithms) can further refine the process by introducing additional information previously missed by the tracker."]},{"title":"4 Conclusions and Future Work","paragraphs":["We have shown how a logical language can be used to model SL signs for semi-automatic recognition, albeit with some restrictions. The traits we have chosen to represent were imposed by the limits of the tracking tools we had to our disposition, most notably working with 2D coordinates. With these in mind, we tried to design something flexible that could be easily adapted by computer scientists and linguists alike. Our primitive sets, were intentionally defined in a very general fashion due to the same reason: all of the perceived directions, articulators and places of articulation can easily change their domains, depending on the SL we are modeling or the technological constraints we have to deal with. Propositions can also be changed, or even induced, by existing written sign representation languages such as Zebedee (Filhol, 2008) or HamNoSys (Hanke, 2004), mainly for the sake of extendability.","From the application side, we still need to create an extensive sign database codified in PDLSL and try recognition on other corpora, with different tracking information. For verification and model extraction, further optimizations are expected, including the handling of data inconsistencies and repairing broken queries when verifying the graph.","Regarding our theoretical issues, future work will be centered in improving our language to better comply with SL research. This includes adding new features, like incorporating probability representation to improve recognition. We also expect to finish the definition of our formal semantics, as well as proving correction and complexity of our algorithms."]},{"title":"References","paragraphs":["Mark Aronoff, Irit Meir, and Wendy Sandler. 2005. The paradox of sign language morphology. Language, 81(2):301.","Christian Cuxac and Patrice Dalle. 2007. Problé- matique des chercheurs en traitement automatique des langues des signes, volume 48 of Traitement Automatique des Langues. Lavoisier, http://www.editions-hermes.fr/, October.","Patrice Dalle. 2006. High level models for sign language analysis by a vision system. In Workshop on the Representation and Processing of Sign Language: Lexicographic Matters and Didactic Scenarios (LREC), Italy, ELDA, page 17–20. DictaSign. 2012. http://www.dictasign.eu.","Philippe Dreuw, Daniel Stein, and Hermann Ney. 2009. Enhancing a sign language transla-tion system with vision-based features. In Miguel Sales Dias, Sylvie Gibet, Marcelo M. 332 Wanderley, and Rafael Bastos, editors, Gesture-Based Human-Computer Interaction and Simulation, number 5085 in Lecture Notes in Computer Science, pages 108–113. Springer Berlin Heidelberg, January.","Philippe Dreuw, Hermann Ney, Gregorio Martinez, Onno Crasborn, Justus Piater, Jose Miguel Moya, and Mark Wheatley. 2010. The Sign-Speak project - bridging the gap between sign-ers and speakers. In Nicoletta Calzolari (Conference Chair), Khalid Choukri, and et. al., editors, Proceedings of the Seventh International Conference on Language Resources and Evaluation (LREC’10), Valletta, Malta, May. European Language Resources Association (ELRA).","Michael Filhol. 2008. Modèle descriptif des signes pour un traitement automatique des langues des signes. Ph.D. thesis, Université Paris-sud (Paris 11).","Michael Filhol. 2009. Zebedee: a lexical description model for sign language synthesis. Internal, LIMSI.","Michael J. Fischer and Richard E. Ladner. 1979. Propositional dynamic logic of regular programs. Journal of Computer and System Sciences, 18(2):194–211, April.","Frédéric Gianni and Patrice Dalle. 2009. Robust tracking for processing of videos of communication’s gestures. Gesture-Based Human-Computer Interaction and Simulation, page 93–101.","Matilde Gonzalez and Christophe Collet. 2011. Robust body parts tracking using particle filter and dynamic template. In 2011 18th IEEE International Conference on Image Processing (ICIP), pages 529 –532, September.","Matilde Gonzalez and Christophe Collet. 2012. Sign segmentation using dynamics and hand configuration for semi-automatic annotation of sign language corpora. In Eleni Efthimiou, Georgios Kouroupetroglou, and Stavroula-Evita Fotinea, editors, Gesture and Sign Language in Human-Computer Interaction and Embodied Communication, number 7206 in Lecture Notes in Computer Science, pages 204–215. Springer Berlin Heidelberg, January.","Thomas Hanke. 2004. HamNoSys—Representing sign language data in language resources and language processing contexts. In Proceedings of the Workshop on the Representation and Processing of Sign Languages “From SignWriting to Image Processing. Information, Lisbon, Portugal, 30 May.","Jaakko Hintikka. 1962. Knowledge and Belief. Ithaca, N.Y.,Cornell University Press.","Fanch Lejeune. 2004. Analyse sémantico-cognitive d’énoncés en Langue des Signes Fran\\ccaise pour une génération automatique de séquences gestuelles. Ph.D. thesis, PhD thesis, Orsay University, France.","Boris Lenseigne and Patrice Dalle. 2006. Us-ing signing space as a representation for sign language processing. In Sylvie Gibet, Nicolas Courty, and Jean-François Kamp, editors, Gesture in Human-Computer Interaction and Simulation, number 3881 in Lecture Notes in Computer Science, pages 25–36. Springer Berlin Heidelberg, January.","S. K. Liddell and R. E. Johnson. 1989. American sign language: The phonological base. Gallaudet University Press, Washington. DC.","Olivier Losson and Jean-Marc Vannobel. 1998. Sign language formal description and synthesis. INT.JOURNAL OF VIRTUAL REALITY, 3:27—34.","Irit Meir, Carol Padden, Mark Aronoff, and Wendy Sandler. 2006. Re-thinking sign language verb classes: the body as subject. In Sign Languages: Spinning and Unraveling the Past, Present and Future. 9th Theoretical Issues in Sign Language Research Conference, Florianopolis, Brazil, volume 382.","Sylvie C. W. Ong and Surendra Ranganath. 2005. Automatic sign language analysis: a survey and the future beyond lexical meaning. IEEE Trans-actions on Pattern Analysis and Machine Intelligence, 27(6):873 – 891, June.","William C. Stokoe. 2005. Sign language structure: An outline of the visual communication systems of the american deaf. Journal of Deaf Studies and Deaf Education, 10(1):3–37, January.","Clayton Valli and Ceil Lucas. 2000. Linguistics of American Sign Language Text, 3rd Edition: An Introduction. Gallaudet University Press.","Henri Wittmann. 1991. Classification linguistique des langues signées non vocalement. Revue québécoise de linguistique théorique et appliquée, 10(1):88. 333"]}],"references":[{"authors":[{"first":"Mark","last":"Aronoff"},{"first":"Irit","last":"Meir"},{"first":"Wendy","last":"Sandler"}],"year":"2005","title":"The paradox of sign language morphology"},{"authors":[{"first":"Christian","last":"Cuxac"},{"first":"Patrice","last":"Dalle"}],"year":"2007","title":"Problé- matique des chercheurs en traitement automatique des langues des signes, volume 48 of Traitement Automatique des Langues"},{"authors":[{"first":"Patrice","last":"Dalle"}],"year":"2006","title":"High level models for sign language analysis by a vision system"},{"authors":[{"first":"Philippe","last":"Dreuw"},{"first":"Daniel","last":"Stein"},{"first":"Hermann","last":"Ney"}],"year":"2009","title":"Enhancing a sign language transla-tion system with vision-based features"},{"authors":[{"first":"Philippe","last":"Dreuw"},{"first":"Hermann","last":"Ney"},{"first":"Gregorio","last":"Martinez"},{"first":"Onno","last":"Crasborn"},{"first":"Justus","last":"Piater"},{"first":"Jose","middle":"Miguel","last":"Moya"},{"first":"Mark","last":"Wheatley"}],"year":"2010","title":"The Sign-Speak project - bridging the gap between sign-ers and speakers"},{"authors":[{"first":"Michael","last":"Filhol"}],"year":"2008","title":"Modèle descriptif des signes pour un traitement automatique des langues des signes"},{"authors":[{"first":"Michael","last":"Filhol"}],"year":"2009","title":"Zebedee: a lexical description model for sign language synthesis"},{"authors":[{"first":"Michael","middle":"J.","last":"Fischer"},{"first":"Richard","middle":"E.","last":"Ladner"}],"year":"1979","title":"Propositional dynamic logic of regular programs"},{"authors":[{"first":"Frédéric","last":"Gianni"},{"first":"Patrice","last":"Dalle"}],"year":"2009","title":"Robust tracking for processing of videos of communication’s gestures"},{"authors":[{"first":"Matilde","last":"Gonzalez"},{"first":"Christophe","last":"Collet"}],"year":"2011","title":"Robust body parts tracking using particle filter and dynamic template"},{"authors":[{"first":"Matilde","last":"Gonzalez"},{"first":"Christophe","last":"Collet"}],"year":"2012","title":"Sign segmentation using dynamics and hand configuration for semi-automatic annotation of sign language corpora"},{"authors":[{"first":"Thomas","last":"Hanke"}],"year":"2004","title":"HamNoSys—Representing sign language data in language resources and language processing contexts"},{"authors":[{"first":"Jaakko","last":"Hintikka"}],"year":"1962","title":"Knowledge and Belief"},{"authors":[{"first":"Fanch","last":"Lejeune"}],"year":"2004","title":"Analyse sémantico-cognitive d’énoncés en Langue des Signes Fran\\ccaise pour une génération automatique de séquences gestuelles"},{"authors":[{"first":"Boris","last":"Lenseigne"},{"first":"Patrice","last":"Dalle"}],"year":"2006","title":"Us-ing signing space as a representation for sign language processing"},{"authors":[{"first":"S.","middle":"K.","last":"Liddell"},{"first":"R.","middle":"E.","last":"Johnson"}],"year":"1989","title":"American sign language: The phonological base"},{"authors":[{"first":"Olivier","last":"Losson"},{"first":"Jean-Marc","last":"Vannobel"}],"year":"1998","title":"Sign language formal description and synthesis"},{"authors":[{"first":"Irit","last":"Meir"},{"first":"Carol","last":"Padden"},{"first":"Mark","last":"Aronoff"},{"first":"Wendy","last":"Sandler"}],"year":"2006","title":"Re-thinking sign language verb classes: the body as subject"},{"authors":[{"first":"Sylvie","middle":"C. W.","last":"Ong"},{"first":"Surendra","last":"Ranganath"}],"year":"2005","title":"Automatic sign language analysis: a survey and the future beyond lexical meaning"},{"authors":[{"first":"William","middle":"C.","last":"Stokoe"}],"year":"2005","title":"Sign language structure: An outline of the visual communication systems of the american deaf"},{"authors":[{"first":"Clayton","last":"Valli"},{"first":"Ceil","last":"Lucas"}],"year":"2000","title":"Linguistics of American Sign Language Text, 3rd Edition: An Introduction"},{"authors":[{"first":"Henri","last":"Wittmann"}],"year":"1991","title":"Classification linguistique des langues signées non vocalement"}],"cites":[{"authors":[{"last":"Valli"},{"last":"Lucas"}],"year":"2000","style":0,"reference":{"authors":[{"first":"Clayton","last":"Valli"},{"first":"Ceil","last":"Lucas"}],"year":"2000","title":"Linguistics of American Sign Language Text, 3rd Edition: An Introduction"}},{"authors":[{"last":"Cuxac"},{"last":"Dalle"}],"year":"2007","style":0,"reference":{"authors":[{"first":"Christian","last":"Cuxac"},{"first":"Patrice","last":"Dalle"}],"year":"2007","title":"Problé- matique des chercheurs en traitement automatique des langues des signes, volume 48 of Traitement Automatique des Langues"}},{"authors":[{"last":"DictaSign"}],"year":"2012","style":0},{"authors":[{"last":"Ong"},{"last":"Ranganath"}],"year":"2005","style":0,"reference":{"authors":[{"first":"Sylvie","middle":"C. W.","last":"Ong"},{"first":"Surendra","last":"Ranganath"}],"year":"2005","title":"Automatic sign language analysis: a survey and the future beyond lexical meaning"}},{"authors":[{"last":"Dreuw"},{"last":"al."}],"year":"2009","style":0,"reference":{"authors":[{"first":"Philippe","last":"Dreuw"},{"first":"Daniel","last":"Stein"},{"first":"Hermann","last":"Ney"}],"year":"2009","title":"Enhancing a sign language transla-tion system with vision-based features"}},{"authors":[{"last":"Gianni"},{"last":"Dalle"}],"year":"2009","style":0,"reference":{"authors":[{"first":"Frédéric","last":"Gianni"},{"first":"Patrice","last":"Dalle"}],"year":"2009","title":"Robust tracking for processing of videos of communication’s gestures"}},{"authors":[{"last":"Lejeune"}],"year":"2004","style":0,"reference":{"authors":[{"first":"Fanch","last":"Lejeune"}],"year":"2004","title":"Analyse sémantico-cognitive d’énoncés en Langue des Signes Fran\\ccaise pour une génération automatique de séquences gestuelles"}},{"authors":[{"last":"Lenseigne"},{"last":"Dalle"}],"year":"2006","style":0,"reference":{"authors":[{"first":"Boris","last":"Lenseigne"},{"first":"Patrice","last":"Dalle"}],"year":"2006","title":"Us-ing signing space as a representation for sign language processing"}},{"authors":[{"last":"Dreuw"},{"last":"al."}],"year":"2010","style":0,"reference":{"authors":[{"first":"Philippe","last":"Dreuw"},{"first":"Hermann","last":"Ney"},{"first":"Gregorio","last":"Martinez"},{"first":"Onno","last":"Crasborn"},{"first":"Justus","last":"Piater"},{"first":"Jose","middle":"Miguel","last":"Moya"},{"first":"Mark","last":"Wheatley"}],"year":"2010","title":"The Sign-Speak project - bridging the gap between sign-ers and speakers"}},{"authors":[{"last":"Stokoe"}],"year":"2005","style":0,"reference":{"authors":[{"first":"William","middle":"C.","last":"Stokoe"}],"year":"2005","title":"Sign language structure: An outline of the visual communication systems of the american deaf"}},{"authors":[{"last":"Aronoff"},{"last":"al."}],"year":"2005","style":0,"reference":{"authors":[{"first":"Mark","last":"Aronoff"},{"first":"Irit","last":"Meir"},{"first":"Wendy","last":"Sandler"}],"year":"2005","title":"The paradox of sign language morphology"}},{"authors":[{"last":"Meir"},{"last":"al."}],"year":"2006","style":0,"reference":{"authors":[{"first":"Irit","last":"Meir"},{"first":"Carol","last":"Padden"},{"first":"Mark","last":"Aronoff"},{"first":"Wendy","last":"Sandler"}],"year":"2006","title":"Re-thinking sign language verb classes: the body as subject"}},{"authors":[{"last":"Wittmann"}],"year":"1991","style":0,"reference":{"authors":[{"first":"Henri","last":"Wittmann"}],"year":"1991","title":"Classification linguistique des langues signées non vocalement"}},{"authors":[{"last":"Losson"},{"last":"Vannobel"}],"year":"1998","style":0,"reference":{"authors":[{"first":"Olivier","last":"Losson"},{"first":"Jean-Marc","last":"Vannobel"}],"year":"1998","title":"Sign language formal description and synthesis"}},{"authors":[{"last":"Filhol"}],"year":"2009","style":0,"reference":{"authors":[{"first":"Michael","last":"Filhol"}],"year":"2009","title":"Zebedee: a lexical description model for sign language synthesis"}},{"authors":[{"last":"Dalle"}],"year":"2006","style":0,"reference":{"authors":[{"first":"Patrice","last":"Dalle"}],"year":"2006","title":"High level models for sign language analysis by a vision system"}},{"authors":[{"last":"Fischer"},{"last":"Ladner"}],"year":"1979","style":0,"reference":{"authors":[{"first":"Michael","middle":"J.","last":"Fischer"},{"first":"Richard","middle":"E.","last":"Ladner"}],"year":"1979","title":"Propositional dynamic logic of regular programs"}},{"authors":[{"last":"Liddell"},{"last":"Johnson"}],"year":"1989","style":0,"reference":{"authors":[{"first":"S.","middle":"K.","last":"Liddell"},{"first":"R.","middle":"E.","last":"Johnson"}],"year":"1989","title":"American sign language: The phonological base"}},{"authors":[{"last":"Filhol"}],"year":"2008","style":0,"reference":{"authors":[{"first":"Michael","last":"Filhol"}],"year":"2008","title":"Modèle descriptif des signes pour un traitement automatique des langues des signes"}},{"authors":[{"last":"Hintikka"}],"year":"1962","style":0,"reference":{"authors":[{"first":"Jaakko","last":"Hintikka"}],"year":"1962","title":"Knowledge and Belief"}},{"authors":[{"last":"Gonzalez"},{"last":"Collet"}],"year":"2011","style":0,"reference":{"authors":[{"first":"Matilde","last":"Gonzalez"},{"first":"Christophe","last":"Collet"}],"year":"2011","title":"Robust body parts tracking using particle filter and dynamic template"}},{"authors":[{"last":"Gonzalez"},{"last":"Collet"}],"year":"2012","style":0,"reference":{"authors":[{"first":"Matilde","last":"Gonzalez"},{"first":"Christophe","last":"Collet"}],"year":"2012","title":"Sign segmentation using dynamics and hand configuration for semi-automatic annotation of sign language corpora"}},{"authors":[{"last":"Filhol"}],"year":"2008","style":0,"reference":{"authors":[{"first":"Michael","last":"Filhol"}],"year":"2008","title":"Modèle descriptif des signes pour un traitement automatique des langues des signes"}},{"authors":[{"last":"Hanke"}],"year":"2004","style":0,"reference":{"authors":[{"first":"Thomas","last":"Hanke"}],"year":"2004","title":"HamNoSys—Representing sign language data in language resources and language processing contexts"}}]}
