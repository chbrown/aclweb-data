{"sections":[{"title":"","paragraphs":["Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 87–91, Sofia, Bulgaria, August 4-9 2013. c⃝2013 Association for Computational Linguistics"]},{"title":"Learning to Order Natural Language Texts Jiwei Tan","paragraphs":["a, b"]},{"title":", Xiaojun Wan","paragraphs":["a"]},{"title":"* and Jianguo Xiao","paragraphs":["a  a"]},{"title":"Institute of Computer Science and Technology, The MOE Key Laboratory of Computational Linguistics, Peking University, China","paragraphs":["b"]},{"title":"School of Information Science and Technology, Beijing Normal University, China tanjiwei8@gmail.com, {wanxiaojun,jgxiao}@pku.edu.cn  Abstract","paragraphs":["Ordering texts is an important task for many NLP applications. Most previous works on summary sentence ordering rely on the contextual information (e.g. adjacent sentences) of each sentence in the source document. In this paper, we investigate a more challenging task of ordering a set of unordered sentences without any contextual information. We introduce a set of features to characterize the order and coherence of natural language texts, and use the learning to rank technique to determine the order of any two sentences. We also propose to use the genetic algorithm to determine the total order of all sentences. Evaluation results on a news corpus show the effectiveness of our proposed method."]},{"title":"1 Introduction","paragraphs":["Ordering texts is an important task in many natural language processing (NLP) applications. It is typically applicable in the text generation field, both for concept-to-text generation and text-to-text generation (Lapata, 2003), such as multiple document summarization (MDS), question an-swering and so on. However, ordering a set of sentences into a coherent text is still a hard and challenging problem for computers.","Previous works on sentence ordering mainly focus on the MDS task (Barzilay et al., 2002; Okazaki et al., 2004; Nie et al., 2006; Ji and Pulman, 2006; Madnani et al., 2007; Zhang et al., 2010; He et al., 2006; Bollegala et al., 2005; Bollegala et al., 2010). In this task, each summary sentence is extracted from a source document. The timestamp of the source documents and the adjacent sentences in the source documents can be used as important clues for ordering summary sentences.","In this study, we investigate a more challenging and more general task of ordering a set of unordered sentences (e.g. randomly shuffle the * Xiaojun Wan is the corresponding author. sentences in a text paragraph) without any contextual information. This task can be applied to almost all text generation applications without restriction.","In order to address this challenging task, we first introduce a few useful features to characterize the order and coherence of natural language texts, and then propose to use the learning to rank algorithm to determine the order of two sentences. Moreover, we propose to use the genetic algorithm to decide the overall text order. Evaluations are conducted on a news corpus, and the results show the prominence of our method. Each component technique or feature in our method has also been validated."]},{"title":"2 Related Work","paragraphs":["For works taking no use of source document, Lapata (2003) proposed a probabilistic model which learns constraints on sentence ordering from a corpus of texts. Experimental evaluation indicated the importance of several learned lexical and syntactic features. However, the model only works well when using single feature, but unfortunately, it becomes worse when multiple features are combined. Barzilay and Lee (2004) investigated the utility of domain-specific content model for representing topic and topic shifts and the model performed well on the five selected domains. Nahnsen (2009) employed features which were based on discourse entities, shallow syntactic analysis, and temporal precedence relations retrieved from VerbOcean. However, the model does not perform well on data-sets describing the consequences of events."]},{"title":"3 Our Proposed Method 3.1 Overview","paragraphs":["The task of text ordering can be modeled like (Cohen et al., 1998), as measuring the coherence of a text by summing the association strength of any sentence pairs. Then the objective of a text ordering model is to find a permutation which can maximize the summation. 87","Formally, we define an association strength function PREF( , ) Ruv ∈ to measure how strong it is that sentence u should be arranged before sentence v (denoted as uv; ). We then define function AGREE( ,PREF)ρ as: ,: ( ) ()AGREE( ,PREF) = PREF( , ) uv u v uv ρρ ρ >"]},{"title":"∑","paragraphs":["(1) where ρ denotes a sentence permutation and","() ()uvρρ> means uv; in the permutation ρ . Then the objective of finding an overall order of the sentences becomes finding a permutation ρ to maximize AGREE( ,PREF)ρ .","The main framework is made up of two parts: defining a pairwise order relation and determin-ing an overall order. Our study focuses on both the two parts by learning a better pairwise relation and proposing a better search strategy, as described respectively in next sections. 3.2 Pairwise Relation Learning The goal for pairwise relation learning is defining the strength function PREF for any sentence pair. In our method we define the function PREF by combining multiple features.","Method: Traditionally, there are two main methods for defining a strength function: integrating features by a linear combination (He et al., 2006; Bollegala et al., 2005) or by a binary classifier (Bollegala et al., 2010). However, the binary classification method is very coarsegrained since it considers any pair of sentences either “positive” or “negative”. Instead we propose to use a better model of learning to rank to integrate multiple features.","In this study, we use Ranking SVM implemented in the svmrank","toolkit (Joachims, 2002; Joachims, 2006) as the ranking model. The examples to be ranked in our ranking model are sequential sentence pairs like uv; . The feature values for a training example are generated by a few feature functions (,)if uv , and we will introduce the features later. We build the training examples for svmrank","as follows:","For a training query, which is a paragraph with n sequential sentences as 12... ns ss;;;, we can get","2","(1)nAnn=− training examples. For pairs like (0)aakssk+ >; the target rank values are set to nk− , which means that the longer the distance between the two sentences is, the smaller the target value is. Other pairs like ak as s+ ; are all set to 0. In order to better capture the order information of each feature, for every sentence pair uv; , we derive four feature values from each function (,)if uv , which are listed as follows:",",1 (,)iiV f uv= (2) ,2 1/ 2, if ( , ) ( , ) 0 (,)",",otherwise (,) (, ) ii i i ii fuv fvu","V fuv fuv fvu","+=⎧ ⎪","= ⎨ ⎪ +⎩ (3) ,3 1/ if ( , ) 0 (,)/ (, ), otherwise i yS yu i","ii yS yu Sfuy V fuv fuy ∈∩≠ ∈∩≠ ⎧ = ⎪ = ⎨ ⎪ ⎩"]},{"title":"∑ ∑","paragraphs":[", (4) ,4 1/ if ( , ) 0 ( , ) / ( , ), otherwise i xS xv i","ii xS xv Sfxv V fuv fxv ∈∩≠ ∈∩≠ ⎧ = ⎪= ⎨ ⎪ ⎩"]},{"title":"∑ ∑","paragraphs":[", (5) where S is the set of all sentences in a paragraph and S is the number of sentences in S . The three additional feature values of (3) (4) (5) are defined to measure the priority of uv; to vu; , uv; to {,}uySuv∀∈ −; and uv; to","{,}x Suvv∀∈ − ; respectively, by calculating the proportion of (,)if uv in respective summations.","The learned model can be used to predict target values for new examples. A paragraph of unordered sentences is viewed as a test query, and the predicted target value for uv; is set as PREF( , )uv .","Features: We select four types of features to characterize text coherence. Every type of features is quantified with several functions distinguished by i in the formulation of (,)if uv and normalized to [0,1] . The features and definitions","of (,)if uv are introduced in Table 1. Type Description","sim( , )uv Similarity","sim(latter( ),former( ))uv overlap ( , ) / min(| |,| |)j uv u v Overlap overlap (latter( ), former( )) overlap ( , ) j j uv uv","Number of coreference chains Coreference Number of","coreference words","Noun","Verb","Verb & noun dependency","Probability Model","Adjective & adverb Table 1: Features used in our model. 88","As in Table 1, function sim( , )uv denotes the cosine similarity of sentence u and v ; latter( )u and former( )v denotes the latter half part of u and the former part of v respectively, which are separated by the most centered comma (if exists) or word (if no comma exits); overlap ( , )j uv denotes the number of mutual words of u and v , for 1, 2, 3j = representing lemmatized noun, verb and adjective or adverb respectively; | |u is the number of words of sentence u . The value will be set to 0 if the denominator is 0.","For the coreference features we use the ARKref 1","tool. It can output the coreference chains containing words which represent the same entity for two sequential sentences uv; .","The probability model originates from (Lapata, 2003), and we implement the model with four features of lemmatized noun, verb, adjective or adverb, and verb and noun related dependency. 3.3 Overall Order Determination Cohen et al. (1998) proved finding a permutation ρ to maximize AGREE( ,PREF)ρ is NPcomplete. To solve this, they proposed a greedy algorithm for finding an approximately optimal order. Most later works adopted the greedy search strategy to determine the overall order.","However, a greedy algorithm does not always lead to satisfactory results, as our experiment shows in Section 4.2. Therefore, we propose to use the genetic algorithm (Holland, 1992) as the search strategy, which can lead to better results.","Genetic Algorithm: The genetic algorithm (GA) is an artificial intelligence algorithm for optimization and search problems. The key point of using GA is modeling the individual, fitness function and three operators of crossover, mutation and selection. Once a problem is modeled, the algorithm can be constructed conventionally.","In our method we set a permutation ρ as an individual encoded by a numerical path, for example a permutation 213s ss;; is encoded as (2 1 3). Then the function AGREE( ,PREF)ρ is just the fitness function. We adopt the order-based crossover operator which is described in (Davis, 1985). The mutation operator is a random inversion of two sentences. For selection operator we take a tournament selection operator which randomly selects two individuals to choose the one with the greater fitness value AGREE( ,PREF)ρ . 1 http://www.ark.cs.cmu.edu/ARKref/ After several generations of evolution, the individual with the greatest fitness value will be a close solution to the optimal result."]},{"title":"4 Experiments 4.1 Experiment Setup Data Set and Evaluation Metric:","paragraphs":["We conducted the experiments on the North American News Text Corpus2",". We trained the model on 80 thousand paragraphs and tested with 200 shuffled paragraphs. We use Kendall’s τ as the evaluation metric, which is based on the number of in-versions in the rankings.","Comparisons: It is incomparable with other methods for summary sentence ordering based on special summarization corpus, so we implemented Lapata’s probability model for comparison, which is considered the state of the art for this task. In addition, we implemented a random ordering as a baseline. We also tried to use a classification model in place of the ranking model. In the classification model, sentence pairs like","1aas s +; were viewed as positive examples and all other pairs were viewed as negative examples. When deciding the overall order for either ranking or classification model we used three search strategies: greedy, genetic and exhaustive (or brutal) algorithms. In addition, we conducted a series of experiments to evaluate the effect of each feature. For each feature, we tested in two experiments, one of which only contained the single feature and the other one contained all the other features. For comparative analysis of features, we tested with an exhaustive search algorithm to determine the overall order. 4.2 Experiment Results The comparison results in Table 2 show that our Ranking SVM based method improves the performance over the baselines and the classification based method with any of the search algorithms. We can also see the greedy search strategy does not perform well and the genetic algorithm can provide a good approximate solution to obtain optimal results.","Method Greedy Exhaustive Genetic","Baseline -0.0127","Probability 0.1859","Classification 0.5006 0.5360 0.5264","Ranking 0.5191 0.5768 0.5747","Table 2: Average τ of different methods. 2 The corpus is available from http://www.ldc.upenn.edu/Catalog/catalogEntry.jsp?catalog Id=LDC98T30 89","Ranking vs. Classification: It is not surpris-ing that the ranking model is better, because when using a classification model, an example should be labeled either positive or negative. It is not very reasonable to label a sentence pair like","(1)aakssk+ >; as a negative example, nor a positive one, because in some cases, it is easy to conclude one sentence should be arranged after another but hard to decide whether they should be adjacent. As we see in the function AGREE , the value of PREF( , )aakss+ also contributes to the summation. In a ranking model, this information can be quantified by the different priorities of sentence pairs with different distances.","Single Feature Effect: The effects of different types of features are shown in Table 3. Prob denotes Lapata’s probability model with different features.","Feature Only Removed","Similarity 0.0721 0.4614","Overlap 0.1284 0.4631","Coreference 0.0734 0.4704","Probnoun 0.3679 0.3932","Probverb 0.0615 0.4544 Probadjective&adverb 0.2650 0.4258","Probdependency 0.2687 0.4892","All 0.5768","Table 3: Effects of different features.","It can be seen in Table 3 that all these features contribute to the final result. The two features of noun probability and dependency probability play an important role as demonstrated in (Lapata, 2003). Other features also improve the final performance. A paragraph which is ordered entirely right by our method is shown in Figure 1.","","Sentences which should be arranged together tend to have a higher similarity and overlap. Like sentence (3) and (4) in Figure 1, they have a highest cosine similarity of 0.2240 and most overlap words of “Israel” and “nuclear”. However, the similarity or overlap of the two sentences does not help to decide which sentence should be arranged before another. In this case the overlap and similarity of half part of the sentences may help. For example latter((3)) and former((4)) share an overlap of “Israel” while there is no overlap for latter((4)) and former((3)).","Coreference is also an important clue for ordering natural language texts. When we use a pronoun to represent an entity, it always has occurred before. For example when conducting coreference resolution for (1) ( 2); , it will be found that “He” refers to “Vanunu”. Otherwise for (2) (1); , no coreference chain will be found. 4.3 Genetic Algorithm There are three main parameters for GA includ-ing the crossover probability (PC), the mutation probability (PM) and the population size (PS). There is no definite selection for these parameters. In our study we experimented with a wide range of parameter values to see the effect of each parameter. It is hard to traverse all possible combinations so when testing a parameter we fixed the other two parameters. The results are shown in Table 4. Value Para Avg Max Min Stddev","PS 0.5731 0.5859 0.5606 0.0046","PC 0.5733 0.5806 0.5605 0.0038","PM 0.5741 0.5803 0.5337 0.0045 Table 4: Results of GA with different parameters.","As we can see in Table 4, when adjusting the three parameters the average τ values are all close to the exhaustive result of 0.5768 and their standard deviations are low. Table 4 shows that in our case the genetic algorithm is not very sensible to the parameters. In the experiments, we set PS to 30, PC to 0.5 and PM to 0.05, and reached a value of 0.5747, which is very close to the theoretical upper bound of 0.5768."]},{"title":"5 Conclusion and Discussion","paragraphs":["In this paper we propose a method for ordering sentences which have no contextual information by making use of Ranking SVM and the genetic algorithm. Evaluation results demonstrate the good effectiveness of our method.","In future work, we will explore more features such as semantic features to further improve the performance. Acknowledgments The work was supported by NSFC (61170166), Beijing Nova Program (2008B03) and National High-Tech R&D Program (2012AA011101).","(1) Vanunu, 43, is serving an 18-year sentence for treason.","(2) He was kidnapped by Israel's Mossad spy agency in Rome in 1986 after giving The Sunday Times of London photographs of the in-side of the Dimona reactor.","(3) From the photographs, experts determined that Israel had the world's sixth largest stockpile of nuclear weapons.","(4) Israel has never confirmed or denied that it has a nuclear capability.","Figure 1: A right ordered paragraph. 90"]},{"title":"References","paragraphs":["Danushka Bollegala, Naoaki Okazaki, Mitsuru Ishizuka. 2005. A machine learning approach to sentence ordering for multi-document summarization and its evaluation. In Proceedings of the Second international joint conference on Natural Language Processing (IJCNLP '05), 624-635.","Danushka Bollegala, Naoaki Okazaki, and Mitsuru Ishizuka. 2010. A bottom-up approach to sentence ordering for multi-document summarization. Inf. Process. Manage. 46, 1 (January 2010), 89-109.","John H. Holland. 1992. Adaptation in Natural and Artificial Systems: An Introductory Analysis with Applications to Biology, Control and Artificial Intelligence. MIT Press, Cambridge, MA, USA.","Lawrence Davis. 1985. Applying adaptive algorithms to epistatic domains. In Proceedings of the 9th international joint conference on Artificial intelligence - Volume 1 (IJCAI'85), Aravind Joshi (Ed.), Vol. 1. Morgan Kaufmann Publishers Inc., San Francisco, CA, USA, 162-164.","Mirella Lapata. 2003. Probabilistic text structuring: experiments with sentence ordering. InProceedings of the 41st Annual Meeting on Association for Computational Linguistics - Volume 1(ACL '03), Vol. 1. Association for Computational Linguistics, Stroudsburg, PA, USA, 545-552.","Naoaki Okazaki, Yutaka Matsuo, and Mitsuru Ishizuka. 2004. Improving chronological sentence ordering by precedence relation. In Proceedings of the 20th international conference on Computational Linguistics (COLING '04). Association for Computational Linguistics, Stroudsburg, PA, USA, , Article 750 .","Nitin Madnani, Rebecca Passonneau, Necip Fazil Ayan, John M. Conroy, Bonnie J. Dorr, Judith L. Klavans, Dianne P. O'Leary, and Judith D. Schlesinger. 2007. Measuring variability in sentence ordering for news summarization. In Proceedings of the Eleventh European Workshop on Natural Language Generation (ENLG '07), Stephan Busemann (Ed.). Association for Computational Linguistics, Stroudsburg, PA, USA, 81-88.","Paul D. Ji and Stephen Pulman. 2006. Sentence ordering with manifold-based classification in multi-document summarization. In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP '06). Association for Computational Linguistics, Stroudsburg, PA, USA, 526-533.","Regina Barzilay, Noemie Elhadad, and Kathleen McKeown. 2002. Inferring strategies for sentence ordering in multidocument news summarization. Journal of Artificial Intelligence Research, 17:35– 55.","Regina Barzilay and Lillian Lee. 2004. Catching the drift: Probabilistic content models, with applications to generation and summarization. In HLT-NAACL2004: Proceedings of the Main Conference, pages 113–120.","Renxian Zhang, Wenjie Li, and Qin Lu. 2010. Sentence ordering with event-enriched semantics and two-layered clustering for multi-document news summarization. In Proceedings of the 23rd Interna-tional Conference on Computational Linguistics: Posters (COLING '10). Association for Computational Linguistics, Stroudsburg, PA, USA, 1489-1497.","Thade Nahnsen. 2009. Domain-independent shallow sentence ordering. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, Companion Volume: Student Research Workshop and Doctoral Consortium (SRWS '09). Association for Computational Linguistics, Stroudsburg, PA, USA, 78-83.","Thorsten Joachims. 2002. Optimizing search engines using click through data. In Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining (KDD '02). ACM, New York, NY, USA, 133-142.","Thorsten Joachims. 2006. Training linear SVMs in linear time. In Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining (KDD '06). ACM, New York, NY, USA, 217-226.","William W. Cohen, Robert E. Schapire, and Yoram Singer. 1998. Learning to order things. InProceedings of the 1997 conference on Advances in neural information processing systems 10(NIPS '97), Michael I. Jordan, Michael J. Kearns, and Sara A. Solla (Eds.). MIT Press, Cambridge, MA, USA, 451-457.","Yanxiang He, Dexi Liu, Hua Yang, Donghong Ji, Chong Teng, and Wenqing Qi. 2006. A hybrid sentence ordering strategy in multi-document summarization. In Proceedings of the 7th international conference on Web Information Systems (WISE'06), Karl Aberer, Zhiyong Peng, Elke A. Rundensteiner, Yanchun Zhang, and Xuhui Li (Eds.). Springer-Verlag, Berlin, Heidelberg, 339-349.","Yu Nie, Donghong Ji, and Lingpeng Yang. 2006. An adjacency model for sentence ordering in multi-document summarization. In Proceedings of the Third Asia conference on Information Retrieval Technology (AIRS'06), 313-322. 91"]}],"references":[{"authors":[{"first":"Danushka","last":"Bollegala"},{"first":"Naoaki","last":"Okazaki"},{"first":"Mitsuru","last":"Ishizuka"}],"year":"2005","title":"A machine learning approach to sentence ordering for multi-document summarization and its evaluation"},{"authors":[{"first":"Danushka","last":"Bollegala"},{"first":"Naoaki","last":"Okazaki"},{"first":"Mitsuru","last":"Ishizuka"}],"year":"2010","title":"A bottom-up approach to sentence ordering for multi-document summarization"},{"authors":[{"first":"John","middle":"H.","last":"Holland"}],"year":"1992","title":"Adaptation in Natural and Artificial Systems: An Introductory Analysis with Applications to Biology, Control and Artificial Intelligence"},{"authors":[{"first":"Lawrence","last":"Davis"}],"year":"1985","title":"Applying adaptive algorithms to epistatic domains"},{"authors":[{"first":"Mirella","last":"Lapata"}],"year":"2003","title":"Probabilistic text structuring: experiments with sentence ordering"},{"authors":[{"first":"Naoaki","last":"Okazaki"},{"first":"Yutaka","last":"Matsuo"},{"first":"Mitsuru","last":"Ishizuka"}],"year":"2004","title":"Improving chronological sentence ordering by precedence relation"},{"authors":[{"first":"Nitin","last":"Madnani"},{"first":"Rebecca","last":"Passonneau"},{"first":"Necip","middle":"Fazil","last":"Ayan"},{"first":"John","middle":"M.","last":"Conroy"},{"first":"Bonnie","middle":"J.","last":"Dorr"},{"first":"Judith","middle":"L.","last":"Klavans"},{"first":"Dianne","middle":"P.","last":"O'Leary"},{"first":"Judith","middle":"D.","last":"Schlesinger"}],"year":"2007","title":"Measuring variability in sentence ordering for news summarization"},{"authors":[{"first":"Paul","middle":"D.","last":"Ji"},{"first":"Stephen","last":"Pulman"}],"year":"2006","title":"Sentence ordering with manifold-based classification in multi-document summarization"},{"authors":[{"first":"Regina","last":"Barzilay"},{"first":"Noemie","last":"Elhadad"},{"first":"Kathleen","last":"McKeown"}],"year":"2002","title":"Inferring strategies for sentence ordering in multidocument news summarization"},{"authors":[{"first":"Regina","last":"Barzilay"},{"first":"Lillian","last":"Lee"}],"year":"2004","title":"Catching the drift: Probabilistic content models, with applications to generation and summarization"},{"authors":[{"first":"Renxian","last":"Zhang"},{"first":"Wenjie","last":"Li"},{"first":"Qin","last":"Lu"}],"year":"2010","title":"Sentence ordering with event-enriched semantics and two-layered clustering for multi-document news summarization"},{"authors":[{"first":"Thade","last":"Nahnsen"}],"year":"2009","title":"Domain-independent shallow sentence ordering"},{"authors":[{"first":"Thorsten","last":"Joachims"}],"year":"2002","title":"Optimizing search engines using click through data"},{"authors":[{"first":"Thorsten","last":"Joachims"}],"year":"2006","title":"Training linear SVMs in linear time"},{"authors":[{"first":"William","middle":"W.","last":"Cohen"},{"first":"Robert","middle":"E.","last":"Schapire"},{"first":"Yoram","last":"Singer"}],"year":"1998","title":"Learning to order things"},{"authors":[{"first":"Yanxiang","last":"He"},{"first":"Dexi","last":"Liu"},{"first":"Hua","last":"Yang"},{"first":"Donghong","last":"Ji"},{"first":"Chong","last":"Teng"},{"first":"Wenqing","last":"Qi"}],"year":"2006","title":"A hybrid sentence ordering strategy in multi-document summarization"},{"authors":[{"first":"Yu","last":"Nie"},{"first":"Donghong","last":"Ji"},{"first":"Lingpeng","last":"Yang"}],"year":"2006","title":"An adjacency model for sentence ordering in multi-document summarization"}],"cites":[{"authors":[{"last":"Lapata"}],"year":"2003","style":0,"reference":{"authors":[{"first":"Mirella","last":"Lapata"}],"year":"2003","title":"Probabilistic text structuring: experiments with sentence ordering"}},{"authors":[{"last":"Barzilay"},{"last":"al."}],"year":"2002","style":0,"reference":{"authors":[{"first":"Regina","last":"Barzilay"},{"first":"Noemie","last":"Elhadad"},{"first":"Kathleen","last":"McKeown"}],"year":"2002","title":"Inferring strategies for sentence ordering in multidocument news summarization"}},{"authors":[{"last":"Okazaki"},{"last":"al."}],"year":"2004","style":0,"reference":{"authors":[{"first":"Naoaki","last":"Okazaki"},{"first":"Yutaka","last":"Matsuo"},{"first":"Mitsuru","last":"Ishizuka"}],"year":"2004","title":"Improving chronological sentence ordering by precedence relation"}},{"authors":[{"last":"Nie"},{"last":"al."}],"year":"2006","style":0,"reference":{"authors":[{"first":"Yu","last":"Nie"},{"first":"Donghong","last":"Ji"},{"first":"Lingpeng","last":"Yang"}],"year":"2006","title":"An adjacency model for sentence ordering in multi-document summarization"}},{"authors":[{"last":"Ji"},{"last":"Pulman"}],"year":"2006","style":0,"reference":{"authors":[{"first":"Paul","middle":"D.","last":"Ji"},{"first":"Stephen","last":"Pulman"}],"year":"2006","title":"Sentence ordering with manifold-based classification in multi-document summarization"}},{"authors":[{"last":"Madnani"},{"last":"al."}],"year":"2007","style":0,"reference":{"authors":[{"first":"Nitin","last":"Madnani"},{"first":"Rebecca","last":"Passonneau"},{"first":"Necip","middle":"Fazil","last":"Ayan"},{"first":"John","middle":"M.","last":"Conroy"},{"first":"Bonnie","middle":"J.","last":"Dorr"},{"first":"Judith","middle":"L.","last":"Klavans"},{"first":"Dianne","middle":"P.","last":"O'Leary"},{"first":"Judith","middle":"D.","last":"Schlesinger"}],"year":"2007","title":"Measuring variability in sentence ordering for news summarization"}},{"authors":[{"last":"Zhang"},{"last":"al."}],"year":"2010","style":0,"reference":{"authors":[{"first":"Renxian","last":"Zhang"},{"first":"Wenjie","last":"Li"},{"first":"Qin","last":"Lu"}],"year":"2010","title":"Sentence ordering with event-enriched semantics and two-layered clustering for multi-document news summarization"}},{"authors":[{"last":"He"},{"last":"al."}],"year":"2006","style":0,"reference":{"authors":[{"first":"Yanxiang","last":"He"},{"first":"Dexi","last":"Liu"},{"first":"Hua","last":"Yang"},{"first":"Donghong","last":"Ji"},{"first":"Chong","last":"Teng"},{"first":"Wenqing","last":"Qi"}],"year":"2006","title":"A hybrid sentence ordering strategy in multi-document summarization"}},{"authors":[{"last":"Bollegala"},{"last":"al."}],"year":"2005","style":0,"reference":{"authors":[{"first":"Danushka","last":"Bollegala"},{"first":"Naoaki","last":"Okazaki"},{"first":"Mitsuru","last":"Ishizuka"}],"year":"2005","title":"A machine learning approach to sentence ordering for multi-document summarization and its evaluation"}},{"authors":[{"last":"Bollegala"},{"last":"al."}],"year":"2010","style":0,"reference":{"authors":[{"first":"Danushka","last":"Bollegala"},{"first":"Naoaki","last":"Okazaki"},{"first":"Mitsuru","last":"Ishizuka"}],"year":"2010","title":"A bottom-up approach to sentence ordering for multi-document summarization"}},{"authors":[{"last":"Lapata"}],"year":"2003","style":0,"reference":{"authors":[{"first":"Mirella","last":"Lapata"}],"year":"2003","title":"Probabilistic text structuring: experiments with sentence ordering"}},{"authors":[{"last":"Barzilay"},{"last":"Lee"}],"year":"2004","style":0,"reference":{"authors":[{"first":"Regina","last":"Barzilay"},{"first":"Lillian","last":"Lee"}],"year":"2004","title":"Catching the drift: Probabilistic content models, with applications to generation and summarization"}},{"authors":[{"last":"Nahnsen"}],"year":"2009","style":0,"reference":{"authors":[{"first":"Thade","last":"Nahnsen"}],"year":"2009","title":"Domain-independent shallow sentence ordering"}},{"authors":[{"last":"Cohen"},{"last":"al."}],"year":"1998","style":0,"reference":{"authors":[{"first":"William","middle":"W.","last":"Cohen"},{"first":"Robert","middle":"E.","last":"Schapire"},{"first":"Yoram","last":"Singer"}],"year":"1998","title":"Learning to order things"}},{"authors":[{"last":"He"},{"last":"al."}],"year":"2006","style":0,"reference":{"authors":[{"first":"Yanxiang","last":"He"},{"first":"Dexi","last":"Liu"},{"first":"Hua","last":"Yang"},{"first":"Donghong","last":"Ji"},{"first":"Chong","last":"Teng"},{"first":"Wenqing","last":"Qi"}],"year":"2006","title":"A hybrid sentence ordering strategy in multi-document summarization"}},{"authors":[{"last":"Bollegala"},{"last":"al."}],"year":"2005","style":0,"reference":{"authors":[{"first":"Danushka","last":"Bollegala"},{"first":"Naoaki","last":"Okazaki"},{"first":"Mitsuru","last":"Ishizuka"}],"year":"2005","title":"A machine learning approach to sentence ordering for multi-document summarization and its evaluation"}},{"authors":[{"last":"Bollegala"},{"last":"al."}],"year":"2010","style":0,"reference":{"authors":[{"first":"Danushka","last":"Bollegala"},{"first":"Naoaki","last":"Okazaki"},{"first":"Mitsuru","last":"Ishizuka"}],"year":"2010","title":"A bottom-up approach to sentence ordering for multi-document summarization"}},{"authors":[{"last":"Joachims"}],"year":"2002","style":0,"reference":{"authors":[{"first":"Thorsten","last":"Joachims"}],"year":"2002","title":"Optimizing search engines using click through data"}},{"authors":[{"last":"Joachims"}],"year":"2006","style":0,"reference":{"authors":[{"first":"Thorsten","last":"Joachims"}],"year":"2006","title":"Training linear SVMs in linear time"}},{"authors":[{"last":"Lapata"}],"year":"2003","style":0,"reference":{"authors":[{"first":"Mirella","last":"Lapata"}],"year":"2003","title":"Probabilistic text structuring: experiments with sentence ordering"}},{"authors":[{"last":"Cohen"},{"last":"al."}],"year":"1998","style":0,"reference":{"authors":[{"first":"William","middle":"W.","last":"Cohen"},{"first":"Robert","middle":"E.","last":"Schapire"},{"first":"Yoram","last":"Singer"}],"year":"1998","title":"Learning to order things"}},{"authors":[{"last":"Holland"}],"year":"1992","style":0,"reference":{"authors":[{"first":"John","middle":"H.","last":"Holland"}],"year":"1992","title":"Adaptation in Natural and Artificial Systems: An Introductory Analysis with Applications to Biology, Control and Artificial Intelligence"}},{"authors":[{"last":"Davis"}],"year":"1985","style":0,"reference":{"authors":[{"first":"Lawrence","last":"Davis"}],"year":"1985","title":"Applying adaptive algorithms to epistatic domains"}},{"authors":[{"last":"Lapata"}],"year":"2003","style":0,"reference":{"authors":[{"first":"Mirella","last":"Lapata"}],"year":"2003","title":"Probabilistic text structuring: experiments with sentence ordering"}}]}
