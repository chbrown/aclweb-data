{"sections":[{"title":"","paragraphs":["Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 777–783, Sofia, Bulgaria, August 4-9 2013. c⃝2013 Association for Computational Linguistics"]},{"title":"An Information Theoretic Approach to Bilingual Word ClusteringManaal Faruqui and Chris DyerLanguage Technologies InstituteCarnegie Mellon UniversityPittsburgh, PA, 15213, USA{mfaruqui, cdyer}@cs.cmu.eduAbstract","paragraphs":["We present an information theoretic objective for bilingual word clustering that in-corporates both monolingual distributional evidence as well as cross-lingual evidence from parallel corpora to learn high quality word clusters jointly in any number of languages. The monolingual component of our objective is the average mutual information of clusters of adjacent words in each language, while the bilingual component is the average mutual information of the aligned clusters. To evaluate our method, we use the word clusters in an NER system and demonstrate a statistically significant improvement in F1 score when using bilingual word clusters instead of monolingual clusters."]},{"title":"1 Introduction","paragraphs":["A word cluster is a group of words which ideally captures syntactic, semantic, and distributional regularities among the words belonging to the group. Word clustering is widely used to reduce the number of parameters in statistical models which leads to improved generalization (Brown et al., 1992; Kneser and Ney, 1993; Clark, 2003; Koo et al., 2008; Turian et al., 2010), and multilingual clustering has been proposed as a means to improve modeling of translational correspondences and to facilitate projection of linguistic resource across languages (Och, 1999; Täckström et al., 2012). In this paper, we argue that generally more informative clusters can be learned when evidence from multiple languages is considered while creating the clusters.","We propose a novel bilingual word clustering objective (§2). The first term deals with each language independently and ensures that the data is well-explained by the clustering in a sequence model (§2.1). The second term ensures that the cluster alignments induced by a word alignment have high mutual information across languages (§2.2). Since the objective consists of terms representing the entropy monolingual data (for each language) and parallel bilingual data, it is particularly attractive for the usual situation in which there is much more monolingual data available than parallel data. Because of its similarity to the variation of information metric (Meilǎ, 2003), we call this bilingual term in the objective the aligned variation of information."]},{"title":"2 Word Clustering","paragraphs":["A word clustering C is a partition of a vocabulary Σ = {x1, x2, . . . , x|Σ|} into K disjoint subsets, C1, C2, . . . , CK . That is, C = {C1, C2, . . . , CK }; Ci ∩ Cj = ∅ for all i ̸= j and ⋃K","k=1 Ck = Σ. 2.1 Monolingual objective We use the average surprisal in a probabilistic sequence model to define the monolingual clustering objective. Let ci denote the word class of word wi. Our objective assumes that the probability of a word sequence w = ⟨w1, w2, . . . , wM ⟩ is p(w) = M ∏ i=1 p(ci | ci−1) × p(wi | ci), (2.1) where c0 is a special start symbol. The term p(ci | ci−1) is the probability of class ci following class ci−1, and p(wi | ci) is the probability of class ci emitting word wi. Using the MLE esitmates after taking the negative logarithm, this term reduces to 777 the following as shown in (Brown et al., 1992): H(C; w) = 2 K ∑ k=1","#(Ck) M log #(Ck)","M","− ∑","i ∑ j̸=i #(Ci, Cj)","M log #(Ci, Cj)","M where #(Ck) is the count of Ck in the corpus w under the clustering C, #(Ci, Cj) is the count of the number of times that cluster Ci precedes Cj and M is the size of the corpus. Using the monolingual objective to cluster, we solve the following search problem:","Ĉ = arg min C H(C; w). (2.2) 2.2 Bilingual objective Now let us suppose we have a second language with vocabulary Ω = {y1, y2, . . . , y|Ω|}, which is clustered into K disjoint subsets D = {D1, D2, . . . , DK }, and a corpus of text in the second language, v = ⟨v1, v2, . . . , vN ⟩. Obviously we can cluster both languages using the monolingual objective above:","Ĉ, D̂ = arg min C,D H(C; w) + H(D; v). This joint minimization for the clusterings for both languages clearly has no benefit since the two terms of the objective are independent. We must alter the object by further assuming that we have a priori beliefs that some of the words in w and v have the same meaning.","To encode this belief, we introduce the notion of a weighted vocabulary alignment A, which is a function on pairs of words in vocabularies Σ and Ω to a value greater than or equal to 0, i.e., A : Σ × Ω \\u{37}→R≥0. For concreteness, A(x, y) will be the number of times that x is aligned to y in a word aligned parallel corpus. By abuse of notation, we write marginal weights A(x) = ∑","y∈Ω A(x, y) and A(y) = ∑","x∈Σ A(x, y). We also define the set marginals A(C, D) = ∑","x∈C ∑","y∈D A(x, y).","Using this weighted vocabulary alignment, we state an objective that encourages clusterings to have high average mutual information when alignment links are followed; that is, on average how much information does knowing the cluster of a word x ∈ Σ impart about the clustering of y ∈ Ω, and vice-versa?"]},{"title":"C DC","paragraphs":["Figure 1: Factor graphs of the monolingual (left) & proposed bilingual clustering problem (right).","We call this quantity the aligned variation of information (AVI). AVI(C, D; A) = EA(x,y) [− log p(cx | dy) − log p(dy | cx)] Writing out the expectation and gathering terms, we obtain","AVI(C, D; A) = − ∑","x∈Σ ∑","y∈Ω A(x, y)","A(·, ·) ×","[","2 log A(C, D)","A(·, ·) − log p(C) − log p(D)]",", where it is assumed that 0 log x = 0.","Our bilingual clustering objective can therefore be stated as the following search problem over a linear combination of the monolingual and bilingual objectives:","arg min C,D monolingual { }} { H(C; w) + H(D; v) + β×bilingual { }} { βAVI(C, D) . (2.3) Understanding AVI. Intuitively, we can imagine sampling a random alignment from the distribution obtained by normalizing A(·, ·). AVI gives us a measure of how much information do we obtain, on average, from knowing the cluster in one language about the clustering of a linked element chosen at random proportional to A(x, ·) (or conditioned the other way around). In the following sections, we denote AVI(C, D; A) by AVI(C, D). To further understand AVI, we remark that AVI reduces to the VI metric when the alignment maps words to themselves in the same language. As a proper metric, VI has a number of attractive properties, and these can be generalized to AVI (without restriction on the alignment map), namely: • Non-negativity: AVI(C, D) ≥ 0; • Symmetry: AVI(C, D) = AVI(D, C);","• Triangle inequality: AVI(C, D) + AVI(D, E) ≥ AVI(C, E); 778","• Identity of indiscernables: AVI(C, D) = 0 iff C ≡ D.1 2.3 Example Figure 2 provides an example illustrating the difference between the bilingual vs. monolingual clustering objectives. We compare two different clusterings of a two-sentence Arabic-English parallel corpus (the English half of the corpus contains the same sentence, twice, while the Arabic half has two variants with the same meaning). While English has a relatively rigid SVO word order, Arabic can alternate between the traditional VSO order and an more modern SVO order. Since our monolingual clustering objective relies exclusively on the distribution of clusters before and after each token, flexible word order alternations like this can cause unintuitive results. To further complicate matters, verbs can inflect differently depending on whether their subject precedes or follows them (Haywood and Nahmad, 1999), so a monolingual model, which knows nothing about morphology and may only rely on distributional clues, has little chance of performing well without help. This is indeed what we observe in the monolingual objective optimal solution (center), in which AwlAd (boys) and yElbwn (play+PRES + 3PL) are grouped into a single class, while yElb (play+PRES + 3SG) is in its own class. However, the AVI term (which is of course not included) has a value of 1.0, reflecting the relatively disordered clustering relative to the given alignment. On the right, we see the optimal solution that includes the AVI term in the clustering objective. This has an AVI of 0, indicating that knowing the clustering of any word is completely informative about the words it is aligned to. By including this term, a slightly worse monolingual solution is chosen, but the clustering corresponds to the reasonable intuition that words with the same meaning (i.e., the two variants of to play) should be clustered together. 2.4 Inference Figure 1 shows the factor graph representation of our clustering models. Finding the optimal clustering under both the monolingual and bilingual objectives is a computationally hard combinatorial optimization problem (Och, 1995). We use a greedy hill-climbing word exchange algorithm (Martin et al., 1995) to find a minimum","1","C ≡ D iff ∀i|{D(y)|∀(x, y) ∈ A, C(x) = i}| = 1 value for our objective. We terminate the optimization procedure when the number of words exchanged at the end of one complete iteration through both the languages is less than 0.1% of the sum of vocabulary of the two languages and at least five complete iterations have been completed.2","For every language the word clusters are initialised in a round robin order according to the token frequency."]},{"title":"3 Experiments","paragraphs":["Evaluation of clustering is not a trivial problem. One branch of work seeks to recast the problem as the of part-of-speech (POS) induction and at-tempts to match linguistic intuitions. However, hard clusters are particularly useful for downstream tasks (Turian et al., 2010). We therefore chose to focus our evaluation on the latter problem. For our evaluation, we use our word clusters as an input to a named entity recognizer which uses these clusters as a source of features. Our evaluation task is the German corpus with NER annotation that was created for the shared task at CoNLL-2003 3",". The training set contains approximately 220,000 tokens and the development set and test set contains 55,000 tokens each. We use Stanford’s Named Entity Recognition system4 which uses a linear-chain conditional random field to predict the most likely sequence of NE labels (Finkel and Manning, 2009). Corpora for Clustering: We used parallel corpora for {Arabic, English, French, Korean & Turkish}-German pairs from WIT-3 corpus (Cettolo et al., 2012) 5",", which is a collection of translated transcriptions of TED talks. Each language pair contained around 1.5 million German words. The corpus was word aligned in two directions using an unsupervised word aligner (Dyer et al., 2013), then the intersected alignment points were taken.","Monolingual Clustering: For every language","pair, we train German word clusters on the mono-","lingual German data from the parallel data. Note","that the parallel corpora are of different sizes and","hence the monolingual German data from every","parallel corpus is different. We treat the F1 score 2 In practice, the number of exchanged words drops of exponentially,","so this threshold is typically reached in not many iterations. 3 http://www.cnts.ua.ac.be/conll2003/ner/ 4 http://nlp.stanford.edu/ner/index.shtml 5 https://wit3.fbk.eu/mt.php?release=2012-03 779 The boys are playing Al- AwlAd ylEbwn Al- AwlAd ylEb ) ا\"و$د &'()*ن ( ) &'(, ا\"و$د ( ylEb ylEbwn Al-AwlAd are playing boys The Al-ylEb are playing boys The AwlAd ylEbwn H(D;v) = 4 H(C;w) = 4.56 H(C;w) = 4 H(D;v) = 3.88 H(C;w) + H(D;v)= 8.56 H(C;w) + H(D;v)= 7.88 AVI(C,D)= 0 AVI(C,D)= 1.0 Figure 2: A two-sentence English-Arabic parallel corpus (left); a 3-class clustering that maximizes the monolingual objective (β = 0; center); and a 3-class clustering that maximizes the joint monolingual and bilingual objective (any β > 0.68; right). obtained using monolingual word clusters (β = 0) as the baseline. Table 1 shows the F1 score of NER6","when trained on these monolingual German word clusters. Bilingual Clustering: While we have formulated a joint objective that enables using both monolingual and bilingual evidence, it is possible to create word clusters using the bilingual signal only by removing the first term in Eq. 2.3. Table 1 shows the performance of NER when the word clusters are obtained using only the bilingual information for different language pairs. As can be seen, these clusters are helpful for all the language pairs. For Turkish the F1 score improves by 1.0 point over when there are no distributional clusters which clearly shows that the word alignment information improves the clustering quality. We now need to supplement the bilingual information with monolingual information to see if the improvement sustains.","We varied the weight of the bilingual objective (β) from 0.05 to 0.9 and observed the effect in NER performance on English-German language pair. The F1 score is maximum for β = 0.1 and decreases monotonically when β is either increased or decreased. This indicates that bilingual information is helpful, but less valuable than monolingual information. Preliminary experiments showed that the value of β = 0.1 is fairly robust across other language pairs and hence we fix it to that for all the experiments.","We run our bilingual clustering model (β = 6 Faruqui and Padó (2010) show that for the size of our generalization","data in German-NER, K = 100 should give us the optimum value. 0.1) across all language pairs and note the F1 scores. Table 1 (unrefined) shows that except for Arabic-German & French-German, all other language pairs deliver a better F1 score than only using monolingual German data. In case of Arabic-German there is a drop in score by 0.25 points. Although, we have observed improvement in F1 score over the monolingual case, the gains do not reach significance according to McNemar’s test (Dietterich, 1998).","Thus we propose to further refine the quality of word alignment links as follows: Let x be a word in language Σ and y be a word in language Ω and let there exists an alignment link between x and y. Recall that A(x, y) is the count of the alignment links between x and y observed in the parallel data, and A(x) and A(y) are the respective marginal counts. Then we define an edge association weight e(x, y) = 2×A(x,y)","A(x)+A(y) This quantity is an association of the strength of the relationship between x and y, and we use it to remove all alignment links whose e(x, y) is below a given threshold before running the bilingual clustering model. We vary e from 0.1 to 0.7 and observe the new F1 scores on the development data. Table 1 (refined) shows the results obtained by our refined model. The values shown in bold are the highest improvements over the monolingual model.","For English and Turkish we observe a statistically significant improvement over the monolingual model (cf. Table 1) with p < 0.007 and p < 0.001 according to McNemar’s test. Arabic improves least with just an improvement of 0.02 F1 points over the monolingual baseline. We 780 Dev Test Language Pair — β = 0 β = 0.1 β = 0.1 β = 0 β = 0.1 (only bi) (only mono) (unrefined) (refined) (only mono) (refined) No clusters 68.27 72.32 En-De 68.95 70.04 70.33 70.64† 72.30 72.98† Fr-De 69.16 69.74 69.69 69.89 72.66 72.83 Ar-De 69.01 69.65 69.40 69.67 72.90 72.37 Tr-De 69.29 69.46 69.64 70.05† 72.41 72.54 Ko-De 68.95 69.70 69.78 69.95 72.71 72.54 Average 69.07 69.71 69.76 70.04† 72.59 72.65 Table 1: NER performance using different word clustering models. Bold indicates an improvement over the monolingual (β = 0) baseline; † indicates a significant improvement (McNemar’s test,p < 0.01). see that the optimal value of e changes from one language pair to another. For French and English e = 0.1 gives the best results whereas for Turkish and Arabic e = 0.5 and for Korean e = 0.7. Are these thresholds correlated with anything? We suggest that higher values of e correspond to more intrinsically noisy alignments. Since alignment models are parameterized based on the vocabularies of the languages they are aligning, larger vocabularies are more prone to degenerate solutions resulting from overfitting. So we are not surprised to see that sparser alignments (resulting from higher values of e) are required by languages like Korean, while languages like French and English make due with denser alignments. Evaluation on Test Set: We now verify our results on the test set. We take the best bilingual word clustering model obtained for every language pair (e = 0.1 for En, Fr. e = 0.5 for Ar, Tr. e = 0.7 for Ko) and train NER classifiers using these. Table 1 shows the performance of German NER classifiers on the test set. All the values shown in bold are better than the monolingual baselines. English again has a statistically significant improvement over the baseline. French and Turkish show the next best improvements. The English-German cluster model performs better than the mkcls7","tool (72.83%)."]},{"title":"4 Related Work","paragraphs":["Our monolingual clustering model is purely distributional in nature. Other extensions to word clustering have incorporated morphological and or-thographic information (Clark, 2003). The work of Snyder and Barzilay (2010), which focused on POS induction is very closely related. The earliest work on bilingual word clustering was proposed by (Och, 1999) which, like us, uses a lan-","7","http://www.statmt.org/moses/giza/mkcls.html guage modeling approach (Brown et al., 1992; Kneser and Ney, 1993) for monolingual optimization and a similarity function for bilingual similarity. Täckström et al. (2012) use cross-lingual word clusters to show transfer of linguistic structure. While their clustering method is superficially similar, the objective function is more heuristic in nature than our information-theoretic conception of the problem. Multilingual learning has been applied to a number of unsupervised and supervised learning problems, including word sense disambiguation (Diab, 2003; Guo and Diab, 2010), topic modeling (Mimno et al., 2009; Boyd-Graber and Blei, 2009), and morphological segmenta-tion (Snyder and Barzilay, 2008).","Also closely related is the technique of cross-lingual annotation projection. This has been applied to bootstrapping syntactic parsers (Hwa et al., 2005; Smith and Smith, 2007; Cohen et al., 2011), morphology (Fraser, 2009), tense (Schiehlen, 1998) and T/V pronoun us-age (Faruqui and Padó, 2012)."]},{"title":"5 Conclusions","paragraphs":["We presented a novel information theoretic model for bilingual word clustering which seeks a clustering with high average mutual information between clusters of adjacent words, and also high mutual information across observed word alignment links. We have shown that improvement in clustering can be obtained across a range of language pairs, evaluated in terms of their value as features in an extrinsic NER task. Our model can be extended for clustering any number of given languages together in a joint framework, and in-corporate both monolingual and parallel data. Acknowledgement: We woud like to thank W. Ammar, V. Chahuneau and W. Ling for valuable discussions. 781"]},{"title":"References","paragraphs":["J. Boyd-Graber and D. M. Blei. 2009. Multilingual topic models for unaligned text. In Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence, UAI ’09, pages 75–82, Arlington, Virginia, United States. AUAI Press.","P. F. Brown, P. V. deSouza, R. L. Mercer, V. J. D. Pietra, and J. C. Lai. 1992. Class-based n-gram models of natural language. Comput. Linguist., 18(4):467– 479, December.","M. Cettolo, C. Girardi, and M. Federico. 2012. Wit3",": Web inventory of transcribed and translated talks. In Proceedings of the 16th","Conference of the European Association for Machine Translation (EAMT), pages 261–268, Trento, Italy, May.","A. Clark. 2003. Combining distributional and morphological information for part of speech induction. In Proceedings of the tenth conference on European chapter of the Association for Computational Linguistics - Volume 1, EACL ’03, pages 59–66, Stroudsburg, PA, USA. Association for Computational Linguistics.","S. B. Cohen, D. Das, and N. A. Smith. 2011. Unsupervised structure prediction with non-parallel multilingual guidance. In Proceedings of the Conference on Empirical Methods in Natural Language Process-ing, EMNLP ’11, pages 50–61, Stroudsburg, PA, USA. Association for Computational Linguistics.","M. T. Diab. 2003. Word sense disambiguation within a multilingual framework. Ph.D. thesis, University of Maryland at College Park, College Park, MD, USA. AAI3115805.","T. G. Dietterich. 1998. Approximate statistical tests for comparing supervised classification learning algorithms. Neural Computation, 10:1895–1923.","C. Dyer, V. Chahuneau, and N. A. Smith. 2013. A simple, fast, and effective reparameterization of IBM Model 2. In Proc. NAACL.","M. Faruqui and S. Padó. 2010. Training and Evaluating a German Named Entity Recognizer with Semantic Generalization. In Proceedings of KON-VENS 2010, Saarbrücken, Germany.","M. Faruqui and S. Padó. 2012. Towards a model of for-mal and informal address in english. In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics. Association for Computational Linguistics.","J. R. Finkel and C. D. Manning. 2009. Nested named entity recognition. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 1 - Volume 1, EMNLP ’09, pages 141–150, Stroudsburg, PA, USA. Association for Computational Linguistics.","A. Fraser. 2009. Experiments in morphosyntactic processing for translating to and from German. In Proceedings of the Fourth Workshop on Statistical Machine Translation, pages 115–119, Athens, Greece, March. Association for Computational Linguistics.","W. Guo and M. Diab. 2010. Combining orthogonal monolingual and multilingual sources of evidence for all words wsd. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL ’10, pages 1542–1551, Stroudsburg, PA, USA. Association for Computational Linguistics.","J. A. Haywood and H. M. Nahmad. 1999. A new Arabic grammar of the written language. Lund Humphries Publishers.","R. Hwa, P. Resnik, A. Weinberg, C. I. Cabezas, and O. Kolak. 2005. Bootstrapping parsers via syntactic projection across parallel texts. Natural Language Engineering, pages 311–325.","R. Kneser and H. Ney. 1993. Forming word classes by statistical clustering for statistical language modelling. In R. Khler and B. Rieger, editors, Contributions to Quantitative Linguistics, pages 221–226. Springer Netherlands.","T. Koo, X. Carreras, and M. Collins. 2008. Simple semi-supervised dependency parsing. In Proc. of ACL.","S. Martin, J. Liermann, and H. Ney. 1995. Algorithms for bigram and trigram word clustering. In Speech Communication, pages 1253–1256.","M. Meilǎ. 2003. Comparing Clusterings by the Varia-tion of Information. In Learning Theory and Kernel Machines, pages 173–187.","D. Mimno, H. M. Wallach, J. Naradowsky, D. A. Smith, and A. McCallum. 2009. Polylingual topic models. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Process-ing: Volume 2 - Volume 2, EMNLP ’09, pages 880– 889, Stroudsburg, PA, USA. Association for Computational Linguistics.","F. J. Och. 1995. Maximum-Likelihood-Schätzung von Wortkategorien mit Verfahren der kombinatorischen Optimierung. Studienarbeit, University of Erlangen.","F. J. Och. 1999. An efficient method for determin-ing bilingual word classes. In Proceedings of the ninth conference on European chapter of the Association for Computational Linguistics, EACL ’99, pages 71–76, Stroudsburg, PA, USA. Association for Computational Linguistics.","M. Schiehlen. 1998. Learning tense translation from bilingual corpora.","D. A. Smith and N. A. Smith. 2007. Probabilistic Models of Nonprojective Dependency Trees. In Proceedings of the 2007 Joint Conference on 782 Empirical Methods in Natural Language Process-ing and Computational Natural Language Learning (EMNLP-CoNLL), pages 132–140, Prague, Czech Republic, June. Association for Computational Linguistics.","B. Snyder and R. Barzilay. 2008. Unsupervised multilingual learning for morphological segmentation. In In The Annual Conference of the Association for Computational Linguistics.","B. Snyder and R. Barzilay. 2010. Climbing the tower of babel: Unsupervised multilingual learning. In J. Frnkranz and T. Joachims, editors, Proceedings of the 27th International Conference on Machine Learning (ICML-10), June 21-24, 2010, Haifa, Israel, pages 29–36. Omnipress.","O. Täckström, R. McDonald, and J. Uszkoreit. 2012. Cross-lingual word clusters for direct transfer of linguistic structure. In The 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, volume 1, page 11. Association for Computational Linguistics.","J. Turian, L. Ratinov, and Y. Bengio. 2010. Word representations: a simple and general method for semi-supervised learning. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL ’10, pages 384–394, Stroudsburg, PA, USA. Association for Computational Linguistics. 783"]}],"references":[{"authors":[{"first":"J.","last":"Boyd-Graber"},{"first":"D.","middle":"M.","last":"Blei"}],"year":"2009","title":"Multilingual topic models for unaligned text"},{"authors":[{"first":"P.","middle":"F.","last":"Brown"},{"first":"P.","middle":"V.","last":"deSouza"},{"first":"R.","middle":"L.","last":"Mercer"},{"first":"V.","middle":"J. D.","last":"Pietra"},{"first":"J.","middle":"C.","last":"Lai"}],"year":"1992","title":"Class-based n-gram models of natural language"},{"authors":[]},{"authors":[]},{"authors":[]},{"authors":[{"first":"A.","last":"Clark"}],"year":"2003","title":"Combining distributional and morphological information for part of speech induction"},{"authors":[{"first":"S.","middle":"B.","last":"Cohen"},{"first":"D.","last":"Das"},{"first":"N.","middle":"A.","last":"Smith"}],"year":"2011","title":"Unsupervised structure prediction with non-parallel multilingual guidance"},{"authors":[{"first":"M.","middle":"T.","last":"Diab"}],"year":"2003","title":"Word sense disambiguation within a multilingual framework"},{"authors":[{"first":"T.","middle":"G.","last":"Dietterich"}],"year":"1998","title":"Approximate statistical tests for comparing supervised classification learning algorithms"},{"authors":[{"first":"C.","last":"Dyer"},{"first":"V.","last":"Chahuneau"},{"first":"N.","middle":"A.","last":"Smith"}],"year":"2013","title":"A simple, fast, and effective reparameterization of IBM Model 2"},{"authors":[{"first":"M.","last":"Faruqui"},{"first":"S.","last":"Padó"}],"year":"2010","title":"Training and Evaluating a German Named Entity Recognizer with Semantic Generalization"},{"authors":[{"first":"M.","last":"Faruqui"},{"first":"S.","last":"Padó"}],"year":"2012","title":"Towards a model of for-mal and informal address in english"},{"authors":[{"first":"J.","middle":"R.","last":"Finkel"},{"first":"C.","middle":"D.","last":"Manning"}],"year":"2009","title":"Nested named entity recognition"},{"authors":[{"first":"A.","last":"Fraser"}],"year":"2009","title":"Experiments in morphosyntactic processing for translating to and from German"},{"authors":[{"first":"W.","last":"Guo"},{"first":"M.","last":"Diab"}],"year":"2010","title":"Combining orthogonal monolingual and multilingual sources of evidence for all words wsd"},{"authors":[{"first":"J.","middle":"A.","last":"Haywood"},{"first":"H.","middle":"M.","last":"Nahmad"}],"year":"1999","title":"A new Arabic grammar of the written language"},{"authors":[{"first":"R.","last":"Hwa"},{"first":"P.","last":"Resnik"},{"first":"A.","last":"Weinberg"},{"first":"C.","middle":"I.","last":"Cabezas"},{"first":"O.","last":"Kolak"}],"year":"2005","title":"Bootstrapping parsers via syntactic projection across parallel texts"},{"authors":[{"first":"R.","last":"Kneser"},{"first":"H.","last":"Ney"}],"year":"1993","title":"Forming word classes by statistical clustering for statistical language modelling"},{"authors":[{"first":"T.","last":"Koo"},{"first":"X.","last":"Carreras"},{"first":"M.","last":"Collins"}],"year":"2008","title":"Simple semi-supervised dependency parsing"},{"authors":[{"first":"S.","last":"Martin"},{"first":"J.","last":"Liermann"},{"first":"H.","last":"Ney"}],"year":"1995","title":"Algorithms for bigram and trigram word clustering"},{"authors":[{"first":"M.","last":"Meilǎ"}],"year":"2003","title":"Comparing Clusterings by the Varia-tion of Information"},{"authors":[{"first":"D.","last":"Mimno"},{"first":"H.","middle":"M.","last":"Wallach"},{"first":"J.","last":"Naradowsky"},{"first":"D.","middle":"A.","last":"Smith"},{"first":"A.","last":"McCallum"}],"year":"2009","title":"Polylingual topic models"},{"authors":[{"first":"F.","middle":"J.","last":"Och"}],"year":"1995","title":"Maximum-Likelihood-Schätzung von Wortkategorien mit Verfahren der kombinatorischen Optimierung"},{"authors":[{"first":"F.","middle":"J.","last":"Och"}],"year":"1999","title":"An efficient method for determin-ing bilingual word classes"},{"authors":[{"first":"M.","last":"Schiehlen"}],"year":"1998","title":"Learning tense translation from bilingual corpora"},{"authors":[{"first":"D.","middle":"A.","last":"Smith"},{"first":"N.","middle":"A.","last":"Smith"}],"year":"2007","title":"Probabilistic Models of Nonprojective Dependency Trees"},{"authors":[{"first":"B.","last":"Snyder"},{"first":"R.","last":"Barzilay"}],"year":"2008","title":"Unsupervised multilingual learning for morphological segmentation"},{"authors":[{"first":"B.","last":"Snyder"},{"first":"R.","last":"Barzilay"}],"year":"2010","title":"Climbing the tower of babel: Unsupervised multilingual learning"},{"authors":[{"first":"O.","last":"Täckström"},{"first":"R.","last":"McDonald"},{"first":"J.","last":"Uszkoreit"}],"year":"2012","title":"Cross-lingual word clusters for direct transfer of linguistic structure"},{"authors":[{"first":"J.","last":"Turian"},{"first":"L.","last":"Ratinov"},{"first":"Y.","last":"Bengio"}],"year":"2010","title":"Word representations: a simple and general method for semi-supervised learning"}],"cites":[{"authors":[{"last":"Brown"},{"last":"al."}],"year":"1992","style":0,"reference":{"authors":[{"first":"P.","middle":"F.","last":"Brown"},{"first":"P.","middle":"V.","last":"deSouza"},{"first":"R.","middle":"L.","last":"Mercer"},{"first":"V.","middle":"J. D.","last":"Pietra"},{"first":"J.","middle":"C.","last":"Lai"}],"year":"1992","title":"Class-based n-gram models of natural language"}},{"authors":[{"last":"Kneser"},{"last":"Ney"}],"year":"1993","style":0,"reference":{"authors":[{"first":"R.","last":"Kneser"},{"first":"H.","last":"Ney"}],"year":"1993","title":"Forming word classes by statistical clustering for statistical language modelling"}},{"authors":[{"last":"Clark"}],"year":"2003","style":0,"reference":{"authors":[{"first":"A.","last":"Clark"}],"year":"2003","title":"Combining distributional and morphological information for part of speech induction"}},{"authors":[{"last":"Koo"},{"last":"al."}],"year":"2008","style":0,"reference":{"authors":[{"first":"T.","last":"Koo"},{"first":"X.","last":"Carreras"},{"first":"M.","last":"Collins"}],"year":"2008","title":"Simple semi-supervised dependency parsing"}},{"authors":[{"last":"Turian"},{"last":"al."}],"year":"2010","style":0,"reference":{"authors":[{"first":"J.","last":"Turian"},{"first":"L.","last":"Ratinov"},{"first":"Y.","last":"Bengio"}],"year":"2010","title":"Word representations: a simple and general method for semi-supervised learning"}},{"authors":[{"last":"Och"}],"year":"1999","style":0,"reference":{"authors":[{"first":"F.","middle":"J.","last":"Och"}],"year":"1999","title":"An efficient method for determin-ing bilingual word classes"}},{"authors":[{"last":"Täckström"},{"last":"al."}],"year":"2012","style":0,"reference":{"authors":[{"first":"O.","last":"Täckström"},{"first":"R.","last":"McDonald"},{"first":"J.","last":"Uszkoreit"}],"year":"2012","title":"Cross-lingual word clusters for direct transfer of linguistic structure"}},{"authors":[{"last":"Meilǎ"}],"year":"2003","style":0,"reference":{"authors":[{"first":"M.","last":"Meilǎ"}],"year":"2003","title":"Comparing Clusterings by the Varia-tion of Information"}},{"authors":[{"last":"Brown"},{"last":"al."}],"year":"1992","style":0,"reference":{"authors":[{"first":"P.","middle":"F.","last":"Brown"},{"first":"P.","middle":"V.","last":"deSouza"},{"first":"R.","middle":"L.","last":"Mercer"},{"first":"V.","middle":"J. D.","last":"Pietra"},{"first":"J.","middle":"C.","last":"Lai"}],"year":"1992","title":"Class-based n-gram models of natural language"}},{"authors":[{"last":"Haywood"},{"last":"Nahmad"}],"year":"1999","style":0,"reference":{"authors":[{"first":"J.","middle":"A.","last":"Haywood"},{"first":"H.","middle":"M.","last":"Nahmad"}],"year":"1999","title":"A new Arabic grammar of the written language"}},{"authors":[{"last":"Och"}],"year":"1995","style":0,"reference":{"authors":[{"first":"F.","middle":"J.","last":"Och"}],"year":"1995","title":"Maximum-Likelihood-Schätzung von Wortkategorien mit Verfahren der kombinatorischen Optimierung"}},{"authors":[{"last":"Martin"},{"last":"al."}],"year":"1995","style":0,"reference":{"authors":[{"first":"S.","last":"Martin"},{"first":"J.","last":"Liermann"},{"first":"H.","last":"Ney"}],"year":"1995","title":"Algorithms for bigram and trigram word clustering"}},{"authors":[{"last":"Turian"},{"last":"al."}],"year":"2010","style":0,"reference":{"authors":[{"first":"J.","last":"Turian"},{"first":"L.","last":"Ratinov"},{"first":"Y.","last":"Bengio"}],"year":"2010","title":"Word representations: a simple and general method for semi-supervised learning"}},{"authors":[{"last":"Finkel"},{"last":"Manning"}],"year":"2009","style":0,"reference":{"authors":[{"first":"J.","middle":"R.","last":"Finkel"},{"first":"C.","middle":"D.","last":"Manning"}],"year":"2009","title":"Nested named entity recognition"}},{"authors":[{"last":"Cettolo"},{"last":"al."}],"year":"2012","style":0},{"authors":[{"last":"Dyer"},{"last":"al."}],"year":"2013","style":0,"reference":{"authors":[{"first":"C.","last":"Dyer"},{"first":"V.","last":"Chahuneau"},{"first":"N.","middle":"A.","last":"Smith"}],"year":"2013","title":"A simple, fast, and effective reparameterization of IBM Model 2"}},{"authors":[{"last":"Faruqui"},{"last":"Padó"}],"year":"2010","style":0,"reference":{"authors":[{"first":"M.","last":"Faruqui"},{"first":"S.","last":"Padó"}],"year":"2010","title":"Training and Evaluating a German Named Entity Recognizer with Semantic Generalization"}},{"authors":[{"last":"Dietterich"}],"year":"1998","style":0,"reference":{"authors":[{"first":"T.","middle":"G.","last":"Dietterich"}],"year":"1998","title":"Approximate statistical tests for comparing supervised classification learning algorithms"}},{"authors":[{"last":"Clark"}],"year":"2003","style":0,"reference":{"authors":[{"first":"A.","last":"Clark"}],"year":"2003","title":"Combining distributional and morphological information for part of speech induction"}},{"authors":[{"last":"Snyder"},{"last":"Barzilay"}],"year":"2010","style":0,"reference":{"authors":[{"first":"B.","last":"Snyder"},{"first":"R.","last":"Barzilay"}],"year":"2010","title":"Climbing the tower of babel: Unsupervised multilingual learning"}},{"authors":[{"last":"Och"}],"year":"1999","style":0,"reference":{"authors":[{"first":"F.","middle":"J.","last":"Och"}],"year":"1999","title":"An efficient method for determin-ing bilingual word classes"}},{"authors":[{"last":"Brown"},{"last":"al."}],"year":"1992","style":0,"reference":{"authors":[{"first":"P.","middle":"F.","last":"Brown"},{"first":"P.","middle":"V.","last":"deSouza"},{"first":"R.","middle":"L.","last":"Mercer"},{"first":"V.","middle":"J. D.","last":"Pietra"},{"first":"J.","middle":"C.","last":"Lai"}],"year":"1992","title":"Class-based n-gram models of natural language"}},{"authors":[{"last":"Kneser"},{"last":"Ney"}],"year":"1993","style":0,"reference":{"authors":[{"first":"R.","last":"Kneser"},{"first":"H.","last":"Ney"}],"year":"1993","title":"Forming word classes by statistical clustering for statistical language modelling"}},{"authors":[{"last":"Täckström"},{"last":"al."}],"year":"2012","style":0,"reference":{"authors":[{"first":"O.","last":"Täckström"},{"first":"R.","last":"McDonald"},{"first":"J.","last":"Uszkoreit"}],"year":"2012","title":"Cross-lingual word clusters for direct transfer of linguistic structure"}},{"authors":[{"last":"Diab"}],"year":"2003","style":0,"reference":{"authors":[{"first":"M.","middle":"T.","last":"Diab"}],"year":"2003","title":"Word sense disambiguation within a multilingual framework"}},{"authors":[{"last":"Guo"},{"last":"Diab"}],"year":"2010","style":0,"reference":{"authors":[{"first":"W.","last":"Guo"},{"first":"M.","last":"Diab"}],"year":"2010","title":"Combining orthogonal monolingual and multilingual sources of evidence for all words wsd"}},{"authors":[{"last":"Mimno"},{"last":"al."}],"year":"2009","style":0,"reference":{"authors":[{"first":"D.","last":"Mimno"},{"first":"H.","middle":"M.","last":"Wallach"},{"first":"J.","last":"Naradowsky"},{"first":"D.","middle":"A.","last":"Smith"},{"first":"A.","last":"McCallum"}],"year":"2009","title":"Polylingual topic models"}},{"authors":[{"last":"Boyd-Graber"},{"last":"Blei"}],"year":"2009","style":0,"reference":{"authors":[{"first":"J.","last":"Boyd-Graber"},{"first":"D.","middle":"M.","last":"Blei"}],"year":"2009","title":"Multilingual topic models for unaligned text"}},{"authors":[{"last":"Snyder"},{"last":"Barzilay"}],"year":"2008","style":0,"reference":{"authors":[{"first":"B.","last":"Snyder"},{"first":"R.","last":"Barzilay"}],"year":"2008","title":"Unsupervised multilingual learning for morphological segmentation"}},{"authors":[{"last":"Hwa"},{"last":"al."}],"year":"2005","style":0,"reference":{"authors":[{"first":"R.","last":"Hwa"},{"first":"P.","last":"Resnik"},{"first":"A.","last":"Weinberg"},{"first":"C.","middle":"I.","last":"Cabezas"},{"first":"O.","last":"Kolak"}],"year":"2005","title":"Bootstrapping parsers via syntactic projection across parallel texts"}},{"authors":[{"last":"Smith"},{"last":"Smith"}],"year":"2007","style":0,"reference":{"authors":[{"first":"D.","middle":"A.","last":"Smith"},{"first":"N.","middle":"A.","last":"Smith"}],"year":"2007","title":"Probabilistic Models of Nonprojective Dependency Trees"}},{"authors":[{"last":"Cohen"},{"last":"al."}],"year":"2011","style":0,"reference":{"authors":[{"first":"S.","middle":"B.","last":"Cohen"},{"first":"D.","last":"Das"},{"first":"N.","middle":"A.","last":"Smith"}],"year":"2011","title":"Unsupervised structure prediction with non-parallel multilingual guidance"}},{"authors":[{"last":"Fraser"}],"year":"2009","style":0,"reference":{"authors":[{"first":"A.","last":"Fraser"}],"year":"2009","title":"Experiments in morphosyntactic processing for translating to and from German"}},{"authors":[{"last":"Schiehlen"}],"year":"1998","style":0,"reference":{"authors":[{"first":"M.","last":"Schiehlen"}],"year":"1998","title":"Learning tense translation from bilingual corpora"}},{"authors":[{"last":"Faruqui"},{"last":"Padó"}],"year":"2012","style":0,"reference":{"authors":[{"first":"M.","last":"Faruqui"},{"first":"S.","last":"Padó"}],"year":"2012","title":"Towards a model of for-mal and informal address in english"}}]}
