{"sections":[{"title":"","paragraphs":["Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 538–542, Sofia, Bulgaria, August 4-9 2013. c⃝2013 Association for Computational Linguistics"]},{"title":"Neighbors Help: Bilingual Unsupervised WSD Using Context Sudha Bhingardive Samiulla Shaikh Pushpak Bhattacharyya Department of Computer Science and Engineering, IIT Bombay, Powai, Mumbai, 400076. {sudha,samiulla,pb}@cse.iitb.ac.in Abstract","paragraphs":["Word Sense Disambiguation (WSD) is one of the toughest problems in NLP, and in WSD, verb disambiguation has proved to be extremely difficult, because of high degree of polysemy, too fine grained senses, absence of deep verb hierarchy and low in-ter annotator agreement in verb sense an-notation. Unsupervised WSD has received widespread attention, but has performed poorly, specially on verbs. Recently an unsupervised bilingual EM based algorithm has been proposed, which makes use only of the raw counts of the translations in comparable corpora (Marathi and Hindi). But the performance of this approach is poor on verbs with accuracy level at 25-38%. We suggest a modifica-tion to this mentioned formulation, using context and semantic relatedness of neighboring words. An improvement of 17% - 35% in the accuracy of verb WSD is obtained compared to the existing EM based approach. On a general note, the work can be looked upon as contributing to the framework of unsupervised WSD through context aware expectation maximization."]},{"title":"1 Introduction","paragraphs":["The importance of unsupervised approaches in WSD is well known, because they do not need sense tagged corpus. In multilingual unsupervised scenario, either comparable or parallel corpora have been used by past researchers for disambiguation (Dagan et al., 1991; Diab and Resnik, 2002; Kaji and Morimoto, 2002; Specia et al., 2005; Lefever and Hoste, 2010; Khapra et al., 2011). Recent work by Khapra et al., (2011) has shown that, in comparable corpora, sense distribution of a word in one language can be estimated using the raw counts of translations of the target words in the other language; such sense distributions contribute to the ranking of senses. Since translations can themselves be ambiguous, Expectation Maximization based formulation is used to determine the sense frequencies. Using this approach every instance of a word is tagged with the most probable sense according to the algorithm.","In the above formulation, no importance is given to the context. That would do, had the accuracy of disambiguation on verbs not been poor 25-35%. This motivated us to propose and investigate use of context in the formulation by Khapra et al. (2011).","For example consider the sentence in chemistry domain, “Keep the beaker on the flat table.” In this sentence, the target word ‘table’ will be tagged as ‘the tabular array’ sense since it is dominant in the chemistry domain by their algorithm. But its actual sense is ‘a piece of furniture’ which can be captured only if context is taken into consideration. In our approach we tackle this problem by taking into account the words from the context of the target word. We use semantic relatedness between translations of the target word and those of its context words to determine its sense.","Verb disambiguation has proved to be extremely difficult (Jean, 2004), because of high degree of polysemy (Khapra et al., 2010), too fine grained senses, absence of deep verb hierarchy and low in-ter annotator agreement in verb sense annotation. On the other hand, verb disambiguation is very important for NLP applications like MT and IR. Our approach has shown significant improvement in verb accuracy as compared to Khapra’s (2011) approach.","The roadmap of the paper is as follows. Section 2 presents related work. Section 3 covers the background work. Section 4 explains the modified EM formulation using context and semantic relatedness. Section 5 presents the experimental setup. 538 Results are presented in section 6. Section 7 covers phenomena study and error analysis. Conclusions and future work are given in the last section, section 8."]},{"title":"2 Related work","paragraphs":["Word Sense Disambiguation is one of the hardest problems in NLP. Successful supervised WSD approaches (Lee et al., 2004; Ng and Lee, 1996) are restricted to resource rich languages and domains. They are directly dependent on availability of good amount of sense tagged data. Creating such a costly resource for all language-domain pairs is impracticable looking at the amount of time and money required. Hence, unsupervised WSD approaches (Diab and Resnik, 2002; Kaji and Morimoto, 2002; Mihalcea et al., 2004; Jean, 2004; Khapra et al., 2011) attract most of the researchers."]},{"title":"3 Background","paragraphs":["Khapra et al. (2011) dealt with bilingual unsupervised WSD. It uses EM algorithm for estimating sense distributions in comparable corpora. Every polysemous word is disambiguated using the raw counts of its translations in different senses. Synset aligned multilingual dictionary (Mohanty et al., 2008) is used for finding its translations. In this dictionary, synsets are linked, and after that the words inside the synsets are also linked. For example, for the concept of ‘boy’, the Hindi synset {ladakaa, balak, bachhaa} is linked with the Marathi synset {mulagaa, poragaa, por}. The Marathi word ‘mulagaa’ is linked to the Hindi word ‘ladakaa’ which is its exact lexical substitution.","Suppose words u in language L1 and v in language L2 are translations of each other and their senses are required. The EM based formulation is as follows: E-Step:","P (SL1 |u) = ∑ v","P (πL 2 (SL1",")|v) · #(v) ∑ SL 1 i ∑ x","P (πL 2 (SL1","i )|x) · #(x)","where, SL1","i ∈ synsetsL 1 (u)","v ∈ crosslinksL","2 (u, SL1",")","x ∈ crosslinksL","2 (u, SL1","i ) M-Step:","P (SL2 |v) = ∑ u","P (πL 1 (SL2",")|u) · #(u) ∑ SL 2 i ∑ y","P (πL 1 (SL2","i )|y) · #(y)","where, SL2","i ∈ synsetsL 2 (v)","u ∈ crosslinksL","1 (v, SL2",")","y ∈ crosslinksL","1 (v, SL2","i ) Here, • ‘#’ indicates the raw count.","• crosslinksL","1 (a, SL2",") is the set of possible","translations of the word ‘a’ from language L1","to L2 in the sense SL2",".","• πL 2 (SL1",") means the linked synset of the","sense SL1","in L 2.","E and M steps are symmetric except for the change in language. In both the steps, we estimate sense distribution in one language using raw counts of translations in another language. But this approach has following limitations: Poor performance on verbs: This approach gives poor performance on verbs (25%-38%). See section 6. Same sense throughout the corpus: Every occurrence of a word is tagged with the single sense found by the algorithm, throughout the corpus. Closed loop of translations: This formulation does not work for some common words which have the same translations in all senses. For example, the verb ‘karna’ in Hindi has two different senses in the corpus viz., ‘to do’ (S1) and ‘to make’ (S2). In both these senses, it gets translated as ‘karne’ in Marathi. The word ‘karne’ also back translates to ‘karna’ in Hindi through both its senses. In this case, the formulation works out as follows:","The probabilities are initialized uniformly. Hence, P (S1|karna) = P (S2|karna) = 0.5. Now, in first iteration the sense of ‘karne’ will be estimated as follows (E-step): P (S1|karne) =","P (S1|karna) ∗ #(karna) #(karna) = 0.5, 539 P (S2|karne) =","P (S2|karna) ∗ #(karna) #(karna) = 0.5 Similarly, in M-step, we will get P (S1|karna) = P (S2|karna) = 0.5. Eventually, it will end up with initial probabilities and no strong decision can be made.","To address these problems we have introduced contextual clues in their formulation by using semantic relatedness."]},{"title":"4 Modified Bilingual EM approach","paragraphs":["We introduce context in the EM formulation stated above and treat the context as a bag of words. We assume that each word in the context influences the sense of the target word independently. Hence, p(S|w, C) = ∏ ci∈C p(S|w, ci) where, w is the target word, S is one of the candidate synsets of w, C is the set of words in context (sentence in our case) and ci is one of the context words.","Suppose we would have sense tagged data, p(S|w, c) could have been computed as: p(S|w, c) = #(S, w, c) #(w, c) But since the sense tagged corpus is not available, we cannot find #(S, w, c) from the corpus directly. However, we can estimate it using the comparable corpus in other language. Here, we assume that given a word and its context word in language L1, the sense distribution in L1 will be same as that in L2 given the translation of a word and the translation of its context word in L2. But these translations can be ambiguous, hence we can use Expectation Maximization approach similar to (Khapra et al., 2011) as follows: E-Step:","P (SL1 |u, a) = ∑ v,b","P (πL 2 (SL1",")|v, b) · σ(v, b) ∑ SL 1 i ∑ x,b","P (πL 2 (SL1","i )|x, b) · σ(x, b)","where, SL1 i ∈ synsetsL","1 (u)","a ∈ context(u)","v ∈ crosslinksL","2 (u, SL1",")","b ∈ crosslinksL 2 (a)","x ∈ crosslinksL 2 (u, SL1","i ) crosslinksL","1 (a) is the set of all possible translations of the word ‘a’ from L1 to L2 in all its senses.","σ(v, b) is the semantic relatedness between the senses of v and senses of b. Since, v and b go over all possible translations of u and a respectively. σ(v, b) has the effect of indirectly capturing the semantic similarity between the senses of u and a. A symetric formulation in the M-step below takes the computation back from language L2 to language L1. The semantic relatedness comes as an additional weighing factor, capturing context, in the probablistic score. M-Step:","P (SL2 |v, b) = ∑ u,a","P (πL 1 (SL2",")|u, a) · σ(u, a) ∑ SL 2 i ∑ y,b","P (πL 1 (SL2","i )|y, a) · σ(y, a)","where, SL2 i ∈ synsetsL","2 (v)","b ∈ context(v)","u ∈ crosslinksL","1 (v, SL2",")","a ∈ crosslinksL1(b)","y ∈ crosslinksL","1 (v, SL2","i )","σ(u, a) is the semantic relatedness between the senses of u and senses of a and contributes to the score like σ(v, b).","Note how the computation moves back and forth between L1 and L2 considering translations of both target words and their context words.","In the above formulation, we could have considered the term #(word, context word) (i.e., the co-occurrence count of the translations of the word and the context word) instead of σ(word, context word). But it is very unlikely that every translation of a word will co-occur with 540 Algorithm HIN-HEALTH MAR-HEALTH","NOUN ADV ADJ VERB Overall NOUN ADV ADJ VERB Overall EM-C 59.82 67.80 56.66 60.38 59.63 62.90 62.54 53.63 52.49 59.77 EM 60.68 67.48 55.54 25.29 58.16 63.88 58.88 55.71 35.60 58.03 WFS 53.49 73.24 55.16 38.64 54.46 59.35 67.32 38.12 34.91 52.57 RB 32.52 45.08 35.42 17.93 33.31 33.83 38.76 37.68 18.49 32.45 Table 1: Comparison(F-Score) of EM-C and EM for Health domain Algorithm HIN-TOURISM MAR-TOURISM","NOUN ADV ADJ VERB Overall NOUN ADV ADJ VERB Overall EM-C 62.78 65.10 54.67 55.24 60.70 59.08 63.66 58.02 55.23 58.67 EM 61.16 62.31 56.02 31.85 57.92 59.66 62.15 58.42 38.33 56.90 WFS 63.98 75.94 52.72 36.29 60.22 61.95 62.39 48.29 46.56 57.47 RB 32.46 42.56 36.35 18.29 32.68 33.93 39.30 37.49 15.99 32.65 Table 2: Comparison(F-Score) of EM-C and EM for Tourism domain every translation of its context word considerable number of times. This term may make sense only if we have arbitrarily large comparable corpus in the other language. 4.1 Computation of semantic relatedness The semantic relatedness is computed by taking the inverse of the length of the shortest path among two senses in the wordnet graph (Pedersen et al., 2005). All the semantic relations (including crosspart-of-speech links) viz., hypernymy, hyponymy, meronymy, entailment, attribute etc., are used for computing the semantic relatedness.","Sense scores thus obtained are used to disambiguate all words in the corpus. We consider all the content words from the context for disambiguation of a word. The winner sense is the one with the highest probability."]},{"title":"5 Experimental setup","paragraphs":["We have used freely available in-domain comparable corpora1","in Hindi and Marathi languages. These corpora are available for health and tourism domains. The dataset is same as that used in (Khapra et al., 2011) in order to compare the performance."]},{"title":"6 Results","paragraphs":["Table 1 and Table 2 compare the performance of the following two approaches:","1. EM-C (EM with Context): Our modified approach explained in section 4. 2. EM: Basic EM based approach by Khapra et al., (2011). 1 http://www.cfilt.iitb.ac.in/wsd/annotated corpus/ 3. WFS: Wordnet First Sense baseline. 4. RB: Random baseline. Results clearly show that EM-C outperforms EM especially in case of verbs in all language-domain pairs. In health domain, verb accuracy is increased by 35% for Hindi and 17% for Marathi, while in tourism domain, it is increased by 23% for Hindi and 17% for Marathi. The overall accuracy is increased by (1.8-2.8%) for health domain and (1.5-1.7%) for tourism domain. Since there are less number of verbs, the improved accuracy is not directly reflected in the overall performance."]},{"title":"7 Error analysis and phenomena study","paragraphs":["Our approach tags all the instances of a word depending on its context as apposed to basic EM approach. For example, consider the following sentence from the tourism domain:","। (vaha patte khel rahe the)","(They were playing cards/leaves)","Here, the word (plural form of ) has two senses viz., ‘leaf’ and ‘playing card’. In tourism domain, the ‘leaf’ sense is more dominant. Hence, basic EM will tag with ‘leaf’ sense. But it’s true sense is ‘playing card’. The true sense is captured only if context is considered. Here, the word (to play) (root form of ) endorses the ‘playing card’ sense of the word . This phenomenon is captured by our approach through semantic relatedness.","But there are certain cases where our algorithm fails. For example, consider the following sentence: 541","। (vaha ped ke niche patte khel rahe the)","(They were playing cards/leaves below the tree)","Here, two strong context words (tree) and (play) are influencing the sense of the word . Semantic relatedness between (tree) and (leaf) is more than that of (play) and (playing card). Hence, the ‘leaf sense’ is assigned to .","This problem occurred because we considered the context as a bag of words. This problem can be solved by considering the semantic structure of the sentence. In this example, the word (leaf/playing card) is the subject of the verb (to play) while (tree) is not even in the same clause with (leaf/playing cards). Thus we could consider (to play) as the stronger clue for its disambiguation."]},{"title":"8 Conclusion and Future Work","paragraphs":["We have presented a context aware EM formulation building on the framework of Khapra et al (2011). Our formulation solves the problems of “inhibited progress due to lack of translation diversity” and “uniform sense assignment, irrespective of context” that the previous EM based formulation of Khapra et al. suffers from. More importantly our accuracy on verbs is much higher and more than the state of the art, to the best of our knowledge. Improving the performance on other parts of speech is the primary future work. Future directions also point to usage of semantic role clues, investigation of familialy apart pair of languages and effect of variation of measures of semantic relatedness."]},{"title":"References","paragraphs":["Ido Dagan, Alon Itai, and Ulrike Schwall. 1991. Two languages are more informative than one. In Douglas E. Appelt, editor, ACL, pages 130–137. ACL.","Mona Diab and Philip Resnik. 2002. An unsupervised method for word sense tagging using parallel corpora. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL ’02, pages 255–262, Morristown, NJ, USA. Association for Computational Linguistics.","Véronis Jean. 2004. Hyperlex: Lexical cartography for information retrieval. In Computer Speech and Language, pages 18(3):223–252.","Hiroyuki Kaji and Yasutsugu Morimoto. 2002. Unsupervised word sense disambiguation using bilingual comparable corpora. In Proceedings of the 19th in-ternational conference on Computational linguistics - Volume 1, COLING ’02, pages 1–7, Stroudsburg, PA, USA. Association for Computational Linguistics.","Mitesh M. Khapra, Anup Kulkarni, Saurabh Sohoney, and Pushpak Bhattacharyya. 2010. All words domain adapted wsd: Finding a middle ground between supervision and unsupervision. In Jan Hajic, Sandra Carberry, and Stephen Clark, editors, ACL, pages 1532–1541. The Association for Computer Linguistics.","Mitesh M Khapra, Salil Joshi, and Pushpak Bhattacharyya. 2011. It takes two to tango: A bilingual unsupervised approach for estimating sense distributions using expectation maximization. In Proceedings of 5th International Joint Conference on Natural Language Processing, pages 695–704, Chiang Mai, Thailand, November. Asian Federation of Natural Language Processing.","K. Yoong Lee, Hwee T. Ng, and Tee K. Chia. 2004. Supervised word sense disambiguation with support vector machines and multiple knowledge sources. In Proceedings of Senseval-3: Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text, pages 137–140.","Els Lefever and Veronique Hoste. 2010. Semeval-2010 task 3: cross-lingual word sense disambiguation. In Katrin Erk and Carlo Strapparava, editors, SemEval 2010 : 5th International workshop on Semantic Evaluation : proceedings of the workshop, pages 15–20. ACL.","Rada Mihalcea, Paul Tarau, and Elizabeth Figa. 2004. Pagerank on semantic networks, with application to word sense disambiguation. In COLING.","Rajat Mohanty, Pushpak Bhattacharyya, Prabhakar Pande, Shraddha Kalele, Mitesh Khapra, and Aditya Sharma. 2008. Synset based multilingual dictionary: Insights, applications and challenges. In Global Wordnet Conference.","Hwee Tou Ng and Hian Beng Lee. 1996. Integrating multiple knowledge sources to disambiguate word sense: an exemplar-based approach. In Proceedings of the 34th annual meeting on Association for Computational Linguistics, pages 40–47, Morristown, NJ, USA. ACL.","T. Pedersen, S. Banerjee, and S. Patwardhan. 2005. Maximizing Semantic Relatedness to Perform Word Sense Disambiguation. Research Report UMSI 2005/25, University of Minnesota Supercomputing Institute, March.","Lucia Specia, Maria Das Graca̧s, Volpe Nunes, and Mark Stevenson. 2005. Exploiting parallel texts to produce a multilingual sense tagged corpus for word sense disambiguation. In In Proceedings of RANLP-05, Borovets, pages 525–531. 542"]}],"references":[{"authors":[{"first":"Ido","last":"Dagan"},{"first":"Alon","last":"Itai"},{"first":"Ulrike","last":"Schwall"}],"year":"1991","title":"Two languages are more informative than one"},{"authors":[{"first":"Mona","last":"Diab"},{"first":"Philip","last":"Resnik"}],"year":"2002","title":"An unsupervised method for word sense tagging using parallel corpora"},{"authors":[{"first":"Véronis","last":"Jean"}],"year":"2004","title":"Hyperlex: Lexical cartography for information retrieval"},{"authors":[{"first":"Hiroyuki","last":"Kaji"},{"first":"Yasutsugu","last":"Morimoto"}],"year":"2002","title":"Unsupervised word sense disambiguation using bilingual comparable corpora"},{"authors":[{"first":"Mitesh","middle":"M.","last":"Khapra"},{"first":"Anup","last":"Kulkarni"},{"first":"Saurabh","last":"Sohoney"},{"first":"Pushpak","last":"Bhattacharyya"}],"year":"2010","title":"All words domain adapted wsd: Finding a middle ground between supervision and unsupervision"},{"authors":[{"first":"Mitesh","middle":"M","last":"Khapra"},{"first":"Salil","last":"Joshi"},{"first":"Pushpak","last":"Bhattacharyya"}],"year":"2011","title":"It takes two to tango: A bilingual unsupervised approach for estimating sense distributions using expectation maximization"},{"authors":[{"first":"K.","middle":"Yoong","last":"Lee"},{"first":"Hwee","middle":"T.","last":"Ng"},{"first":"Tee","middle":"K.","last":"Chia"}],"year":"2004","title":"Supervised word sense disambiguation with support vector machines and multiple knowledge sources"},{"authors":[{"first":"Els","last":"Lefever"},{"first":"Veronique","last":"Hoste"}],"year":"2010","title":"Semeval-2010 task 3: cross-lingual word sense disambiguation"},{"authors":[{"first":"Rada","last":"Mihalcea"},{"first":"Paul","last":"Tarau"},{"first":"Elizabeth","last":"Figa"}],"year":"2004","title":"Pagerank on semantic networks, with application to word sense disambiguation"},{"authors":[{"first":"Rajat","last":"Mohanty"},{"first":"Pushpak","last":"Bhattacharyya"},{"first":"Prabhakar","last":"Pande"},{"first":"Shraddha","last":"Kalele"},{"first":"Mitesh","last":"Khapra"},{"first":"Aditya","last":"Sharma"}],"year":"2008","title":"Synset based multilingual dictionary: Insights, applications and challenges"},{"authors":[{"first":"Hwee","middle":"Tou","last":"Ng"},{"first":"Hian","middle":"Beng","last":"Lee"}],"year":"1996","title":"Integrating multiple knowledge sources to disambiguate word sense: an exemplar-based approach"},{"authors":[{"first":"T.","last":"Pedersen"},{"first":"S.","last":"Banerjee"},{"first":"S.","last":"Patwardhan"}],"year":"2005","title":"Maximizing Semantic Relatedness to Perform Word Sense Disambiguation"},{"authors":[{"first":"Lucia","last":"Specia"},{"first":"Maria","middle":"Das","last":"Graca̧s"},{"first":"Volpe","last":"Nunes"},{"first":"Mark","last":"Stevenson"}],"year":"2005","title":"Exploiting parallel texts to produce a multilingual sense tagged corpus for word sense disambiguation"}],"cites":[{"authors":[{"last":"Dagan"},{"last":"al."}],"year":"1991","style":0,"reference":{"authors":[{"first":"Ido","last":"Dagan"},{"first":"Alon","last":"Itai"},{"first":"Ulrike","last":"Schwall"}],"year":"1991","title":"Two languages are more informative than one"}},{"authors":[{"last":"Diab"},{"last":"Resnik"}],"year":"2002","style":0,"reference":{"authors":[{"first":"Mona","last":"Diab"},{"first":"Philip","last":"Resnik"}],"year":"2002","title":"An unsupervised method for word sense tagging using parallel corpora"}},{"authors":[{"last":"Kaji"},{"last":"Morimoto"}],"year":"2002","style":0,"reference":{"authors":[{"first":"Hiroyuki","last":"Kaji"},{"first":"Yasutsugu","last":"Morimoto"}],"year":"2002","title":"Unsupervised word sense disambiguation using bilingual comparable corpora"}},{"authors":[{"last":"Specia"},{"last":"al."}],"year":"2005","style":0,"reference":{"authors":[{"first":"Lucia","last":"Specia"},{"first":"Maria","middle":"Das","last":"Graca̧s"},{"first":"Volpe","last":"Nunes"},{"first":"Mark","last":"Stevenson"}],"year":"2005","title":"Exploiting parallel texts to produce a multilingual sense tagged corpus for word sense disambiguation"}},{"authors":[{"last":"Lefever"},{"last":"Hoste"}],"year":"2010","style":0,"reference":{"authors":[{"first":"Els","last":"Lefever"},{"first":"Veronique","last":"Hoste"}],"year":"2010","title":"Semeval-2010 task 3: cross-lingual word sense disambiguation"}},{"authors":[{"last":"Khapra"},{"last":"al."}],"year":"2011","style":0,"reference":{"authors":[{"first":"Mitesh","middle":"M","last":"Khapra"},{"first":"Salil","last":"Joshi"},{"first":"Pushpak","last":"Bhattacharyya"}],"year":"2011","title":"It takes two to tango: A bilingual unsupervised approach for estimating sense distributions using expectation maximization"}},{"authors":[{"last":"Khapra"},{"last":"al."}],"year":"2011","style":0,"reference":{"authors":[{"first":"Mitesh","middle":"M","last":"Khapra"},{"first":"Salil","last":"Joshi"},{"first":"Pushpak","last":"Bhattacharyya"}],"year":"2011","title":"It takes two to tango: A bilingual unsupervised approach for estimating sense distributions using expectation maximization"}},{"authors":[{"last":"Jean"}],"year":"2004","style":0,"reference":{"authors":[{"first":"Véronis","last":"Jean"}],"year":"2004","title":"Hyperlex: Lexical cartography for information retrieval"}},{"authors":[{"last":"Khapra"},{"last":"al."}],"year":"2010","style":0,"reference":{"authors":[{"first":"Mitesh","middle":"M.","last":"Khapra"},{"first":"Anup","last":"Kulkarni"},{"first":"Saurabh","last":"Sohoney"},{"first":"Pushpak","last":"Bhattacharyya"}],"year":"2010","title":"All words domain adapted wsd: Finding a middle ground between supervision and unsupervision"}},{"authors":[{"last":"Khapra’s"}],"year":"2011","style":0},{"authors":[{"last":"Lee"},{"last":"al."}],"year":"2004","style":0,"reference":{"authors":[{"first":"K.","middle":"Yoong","last":"Lee"},{"first":"Hwee","middle":"T.","last":"Ng"},{"first":"Tee","middle":"K.","last":"Chia"}],"year":"2004","title":"Supervised word sense disambiguation with support vector machines and multiple knowledge sources"}},{"authors":[{"last":"Ng"},{"last":"Lee"}],"year":"1996","style":0,"reference":{"authors":[{"first":"Hwee","middle":"Tou","last":"Ng"},{"first":"Hian","middle":"Beng","last":"Lee"}],"year":"1996","title":"Integrating multiple knowledge sources to disambiguate word sense: an exemplar-based approach"}},{"authors":[{"last":"Diab"},{"last":"Resnik"}],"year":"2002","style":0,"reference":{"authors":[{"first":"Mona","last":"Diab"},{"first":"Philip","last":"Resnik"}],"year":"2002","title":"An unsupervised method for word sense tagging using parallel corpora"}},{"authors":[{"last":"Kaji"},{"last":"Morimoto"}],"year":"2002","style":0,"reference":{"authors":[{"first":"Hiroyuki","last":"Kaji"},{"first":"Yasutsugu","last":"Morimoto"}],"year":"2002","title":"Unsupervised word sense disambiguation using bilingual comparable corpora"}},{"authors":[{"last":"Mihalcea"},{"last":"al."}],"year":"2004","style":0,"reference":{"authors":[{"first":"Rada","last":"Mihalcea"},{"first":"Paul","last":"Tarau"},{"first":"Elizabeth","last":"Figa"}],"year":"2004","title":"Pagerank on semantic networks, with application to word sense disambiguation"}},{"authors":[{"last":"Jean"}],"year":"2004","style":0,"reference":{"authors":[{"first":"Véronis","last":"Jean"}],"year":"2004","title":"Hyperlex: Lexical cartography for information retrieval"}},{"authors":[{"last":"Khapra"},{"last":"al."}],"year":"2011","style":0,"reference":{"authors":[{"first":"Mitesh","middle":"M","last":"Khapra"},{"first":"Salil","last":"Joshi"},{"first":"Pushpak","last":"Bhattacharyya"}],"year":"2011","title":"It takes two to tango: A bilingual unsupervised approach for estimating sense distributions using expectation maximization"}},{"authors":[{"last":"Khapra"},{"last":"al."}],"year":"2011","style":0,"reference":{"authors":[{"first":"Mitesh","middle":"M","last":"Khapra"},{"first":"Salil","last":"Joshi"},{"first":"Pushpak","last":"Bhattacharyya"}],"year":"2011","title":"It takes two to tango: A bilingual unsupervised approach for estimating sense distributions using expectation maximization"}},{"authors":[{"last":"Mohanty"},{"last":"al."}],"year":"2008","style":0,"reference":{"authors":[{"first":"Rajat","last":"Mohanty"},{"first":"Pushpak","last":"Bhattacharyya"},{"first":"Prabhakar","last":"Pande"},{"first":"Shraddha","last":"Kalele"},{"first":"Mitesh","last":"Khapra"},{"first":"Aditya","last":"Sharma"}],"year":"2008","title":"Synset based multilingual dictionary: Insights, applications and challenges"}},{"authors":[{"last":"Khapra"},{"last":"al."}],"year":"2011","style":0,"reference":{"authors":[{"first":"Mitesh","middle":"M","last":"Khapra"},{"first":"Salil","last":"Joshi"},{"first":"Pushpak","last":"Bhattacharyya"}],"year":"2011","title":"It takes two to tango: A bilingual unsupervised approach for estimating sense distributions using expectation maximization"}},{"authors":[{"last":"Pedersen"},{"last":"al."}],"year":"2005","style":0,"reference":{"authors":[{"first":"T.","last":"Pedersen"},{"first":"S.","last":"Banerjee"},{"first":"S.","last":"Patwardhan"}],"year":"2005","title":"Maximizing Semantic Relatedness to Perform Word Sense Disambiguation"}},{"authors":[{"last":"Khapra"},{"last":"al."}],"year":"2011","style":0,"reference":{"authors":[{"first":"Mitesh","middle":"M","last":"Khapra"},{"first":"Salil","last":"Joshi"},{"first":"Pushpak","last":"Bhattacharyya"}],"year":"2011","title":"It takes two to tango: A bilingual unsupervised approach for estimating sense distributions using expectation maximization"}}]}
