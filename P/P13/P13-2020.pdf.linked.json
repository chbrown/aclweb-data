{"sections":[{"title":"","paragraphs":["Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 110–114, Sofia, Bulgaria, August 4-9 2013. c⃝2013 Association for Computational Linguistics"]},{"title":"Easy-First POS Tagging and Dependency Parsing with Beam Search Ji Ma","paragraphs":["† "]},{"title":"JingboZhu","paragraphs":["† "]},{"title":"Tong Xiao","paragraphs":["† "]},{"title":"Nan Yang","paragraphs":["‡  †"]},{"title":"Natrual Language Processing Lab., Northeastern University, Shenyang, China","paragraphs":["‡"]},{"title":"MOE-MS Key Lab of MCC, University of Science and Technology of China, Hefei, China majineu@outlook.com {zhujingbo, xiaotong}@mail.neu.edu.cn nyang.ustc@gmail.com  Abstract","paragraphs":["In this paper, we combine easy-first dependency parsing and POS tagging algorithms with beam search and structured perceptron. We propose a simple variant of “early-update” to ensure valid update in the training process. The proposed solution can also be applied to combine beam search and structured perceptron with other systems that exhibit spurious ambiguity. On CTB, we achieve 94.01% tagging accuracy and 86.33% unlabeled attachment score with a relatively small beam width. On PTB, we also achieve state-of-the-art performance."]},{"title":"1 Introduction","paragraphs":["The easy-first dependency parsing algorithm (Goldberg and Elhadad, 2010) is attractive due to its good accuracy, fast speed and simplicity. The easy-first parser has been applied to many applications (Seeker et al., 2012; Søggard and Wulff, 2012). By processing the input tokens in an easy-to-hard order, the algorithm could make use of structured information on both sides of the hard token thus making more indicative predictions. However, rich structured information also causes exhaustive inference intractable. As an alternative, greedy search which only explores a tiny fraction of the search space is adopted (Goldberg and Elhadad, 2010).","To enlarge the search space, a natural extension to greedy search is beam search. Recent work also shows that beam search together with perceptron-based global learning (Collins, 2002) enable the use of non-local features that are helpful to improve parsing performance without overfitting (Zhang and Nivre, 2012). Due to the-se advantages, beam search and global learning has been applied to many NLP tasks (Collins and Roark 2004; Zhang and Clark, 2007). However, to the best of our knowledge, no work in the literature has ever applied the two techniques to easy-first dependency parsing.","While applying beam-search is relatively straightforward, the main difficulty comes from combining easy-first dependency parsing with perceptron-based global learning. In particular, one needs to guarantee that each parameter update is valid, i.e., the correct action sequence has lower model score than the predicted one1",". The difficulty in ensuring validity of parameter update for the easy-first algorithm is caused by its spurious ambiguity, i.e., the same result might be derived by more than one action sequences.","For algorithms which do not exhibit spurious ambiguity, “early update” (Collins and Roark 2004) is always valid: at the k-th step when the single correct action sequence falls off the beam, 1 As shown by (Huang et al., 2012), only valid update guarantees the convergence of any perceptron-based training. Invalid update may lead to bad learning or even make the learning not converge at all. Figure 1: Example of cases without/with spurious ambiguity. The 3 × 1 table denotes a beam. “C/P” denotes correct/predicted action sequence. The numbers following C/P are model scores."," 110 its model score must be lower than those still in the beam (as illustrated in figure 1, also see the proof in (Huang et al., 2012)). While for easy-first dependency parsing, there could be multiple action sequences that yield the gold result (C1 and C2 in figure 1). When all correct sequences fall off the beam, some may indeed have higher model score than those still in the beam (C2 in figure 1), causing invalid update.","For the purpose of valid update, we present a simple solution which is based on early update. The basic idea is to use one of the correct action sequences that were pruned right at the k-th step (C1 in figure 1) for parameter update.","The proposed solution is general and can also be applied to other algorithms that exhibit spurious ambiguity, such as easy-first POS tagging (Ma et al., 2012) and transition-based dependency parsing with dynamic oracle (Goldberg and Nivre, 2012). In this paper, we report experimental results on both easy-first dependency parsing and POS tagging (Ma et al., 2012). We show that both easy-first POS tagging and dependency parsing can be improved significantly from beam search and global learning. Specifically, on CTB we achieve 94.01% tagging accuracy which is the best result to date","2","for a single tagging model. With a relatively small beam, we achieve 86.33% unlabeled score (assume gold tags), better than state-of-the-art transition-based parsers (Huang and Sagae, 2010; Zhang and Nivre, 2011). On PTB, we also achieve good results that are comparable to the state-of-the-art."]},{"title":"2 Easy-first dependency parsing","paragraphs":["The easy-first dependency parsing algorithm (Goldberg and Elhadad, 2010) builds a dependency tree by performing two types of actions LEFT(i) and RIGHT(i) to a list of sub-tree structures p1,..., pr. pi is initialized with the i-th word 2 Joint tagging-parsing models achieve higher accuracy, but those models are not directly comparable to ours. Algorithm 1: Easy-first with beam search Input: sentence of n words, beam width s Output: one best dependency tree","( )","( ) ","( ) // top s extensions from the beam 1 // initially, empty beam 2 for 1 1 do 3 ( ) 4 return ( ) // tree built by the best sequence  of the input sentence. Action LEFT(i)/RIGHT(i) attaches pi to its left/right neighbor and then removes pi from the sub-tree list. The algorithm proceeds until only one sub-tree left which is the dependency tree of the input sentence (see the example in figure 2). Each step, the algorithm chooses the highest score action to perform according to the linear model:","( ) ( ) Here, is the weight vector and is the feature representation. In particular, ( ( ) ( )) denotes features extracted from pi.","The parsing algorithm is greedy which explores a tiny fraction of the search space. Once an incorrect action is selected, it can never yield the correct dependency tree. To enlarge the search space, we introduce the beam-search extension in the next section."]},{"title":"3 Easy-first with beam search","paragraphs":["In this section, we introduce easy-first with beam search in our own notations that will be used throughout the rest of this paper.","For a sentence x of n words, let be the action (sub-)sequence that can be applied, in sequence, to x and the result sub-tree list is denoted by ( ) For example, suppose x is “I am valid” and y is [RIGHT(1)], then y(x) yields figure 2(b). Let to be LEFT(i)/RIGHT(i) actions where 1 . Thus, the set of all possible one-action extension of is: ( ) ( ) Here, ‘ ’ means insert to the end of . Following (Huang et al., 2012), in order to formalize beam search, we also use the","( ) operation which returns the top s action sequences in according to ( ). Here, denotes a set of action sequences, ( ) denotes the sum of feature vectors of each action in","Pseudo-code of easy-first with beam search is shown in algorithm 1. Beam search grows s (beam width) action sequences in parallel using a Figure 2: An example of parsing “I am valid”. Spurious ambiguity: (d) can be derived by both [RIGHT(1), LEFT(2)] and [LEFT(3), RIGHT(1)]. 111 Algorithm 2: Perceptron-based training over one training sample ( ) Input: ( ), s, parameter Output: new parameter ( ) ( ( ))"," ( ) // top correct extension from the beam 1 2 for 1 1 do 3 ̂ ( ) 4 ( ) 5 if // all correct seq. falls off the beam 6 ( ̂) ( ) 7 break 8 if ( ) // full update 9 ( ̂) ( ) 10 return"," beam , (sequences in are sorted in terms of model score, i.e., ( ) ( 1 ) ). At each step, the sequences in are expanded in all possible ways and then is filled up with the top s newly expanded sequences (line 2 ~ line 3). Finally, it returns the dependency tree built by the top action sequence in ."]},{"title":"4 Training","paragraphs":["To learn the weight vector , we use the perceptron-based global learning3","(Collins, 2002) which updates by rewarding the feature weights fired in the correct action sequence and punish those fired in the predicted incorrect action sequence. Current work (Huang et al., 2012) rigorously explained that only valid update ensures convergence of any perceptron variants. They also justified that the popular “early update” (Collins and Roark, 2004) is valid for the systems that do not exhibit spurious ambiguity4",".","However, for the easy-first algorithm or more generally, systems that exhibit spurious ambiguity, even “early update” could fail to ensure validity of update (see the example in figure 1). For validity of update, we propose a simple solution which is based on “early update” and which can accommodate spurious ambiguity. The basic idea is to use the correct action sequence which was 3 Following (Zhang and Nivre, 2012), we say the training algorithm is global if it optimizes the score of an entire action sequence. A local learner trains a classifier which distinguishes between single actions. 4 As shown in (Goldberg and Nivre 2012), most transition-based dependency parsers (Nivre et al., 2003; Huang and Sagae 2010;Zhang and Clark 2008) ignores spurious ambiguity by using a static oracle which maps a dependency tree to a single action sequence. Features of (Goldberg and Elhadad, 2010) for p in pi-1, pi, pi+1 wp-vlp, wp-vrp, tp-vlp,","tp-vrp, tlcp, trcp, wlcp, wlcp for p in pi-2, pi-1, pi, pi+1, pi+2 tp-tlcp, tp-trcp, tp-tlcp-trcp for p, q, r in (pi-2, pi-1, pi), (pi-1, pi+1, pi), (pi+1, pi+2 ,pi) tp-tq-tr, tp-tq-wr","for p, q in (pi-1, pi) tp-tlcp-tq, tp-trcp-tq, ,tp-tlcp-wq,, tp-trcp-wq, tp-wq-tlcq, tp-wq-trcq  Table 1: Feature templates for English dependency parsing. wp denotes the head word of p, tp denotes the POS tag of wp. vlp/vrp denotes the number p’s of left/right child. lcp/rcp denotes p’s leftmost/rightmost child. pi denotes partial tree being considered."," pruned right at the step when all correct sequence falls off the beam (as C1 in figure 1).","Algorithm 2 shows the pseudo-code of the training procedure over one training sample ( ), a sentence-tree pair. Here we assume to be the set of all correct action sequences/sub-sequences. At step k, the algorithm constructs a correct action sequence ̂ of length k by extending those in (line 3). It also checks whether no longer contains any correct sequence. If so, ̂ together with are used for parameter update (line 5 ~ line 6). It can be easily verified that each update in line 6 is valid. Note that both ‘TOPC’ and the operation in line 5 use to check whether an action sequence y is correct or not. This can be efficiently implemented (without explicitly enumerating ) by checking if each LEFT(i)/RIGHT(i) in y are compatible with ( ): pi already collected all its dependents according to t; pi is attached to the correct neighbor suggested by t."]},{"title":"5 Experiments","paragraphs":["For English, we use PTB as our data set. We use the standard split for dependency parsing and the split used by (Ratnaparkhi, 1996) for POS tagging. Penn2Malt5","is used to convert the bracketed structure into dependencies. For dependency parsing, POS tags of the training set are generated using 10-fold jack-knifing.","For Chinese, we use CTB 5.1 and the split suggested by (Duan et al., 2007) for both tagging and dependency parsing. We also use Penn2Malt and the head-finding rules of (Zhang and Clark 2008) to convert constituency trees into dependencies. For dependency parsing, we assume gold segmentation and POS tags for the input. 5 http://w3.msi.vxu.se/~nivre/research/Penn2Malt.html 112","Features used in English dependency parsing are listed in table 1. Besides the features in (Goldberg and Elhadad, 2010), we also include some trigram features and valency features which are useful for transition-based dependency parsing (Zhang and Nivre, 2011). For English POS tagging, we use the same features as in (Shen et al., 2007). For Chinese POS tagging and dependency parsing, we use the same features as (Ma et al., 2012). All of our experiments are conducted on a Core i7 (2.93GHz) machine, both the tagger and parser are implemented using C++. 5.1 Effect of beam width Tagging/parsing performances with different beam widths on the development set are listed in table 2 and table 3. We can see that Chinese POS tagging, dependency parsing as well as English dependency parsing greatly benefit from beam search. While tagging accuracy on English only slightly improved. This may because that the accuracy of the greedy baseline tagger is already very high and it is hard to get further improvement. Table 2 and table 3 also show that the speed of both tagging and dependency parsing drops linearly with the growth of beam width. 5.2 Final results Tagging results on the test set together with some previous results are listed in table 4. Dependency parsing results on CTB and PTB are listed in table 5 and table 6, respectively.","On CTB, tagging accuracy of our greedy baseline is already comparable to the state-of-the-art. As the beam size grows to 5, tagging accuracy increases to 94.01% which is 2.3% error reduc-tion. This is also the best tagging accuracy comparing with previous single tagging models (For limited space, we do not list the performance of joint tagging-parsing models).","Parsing performances on both PTB and CTB are significantly improved with a relatively small beam width (s = 8). In particular, we achieve 86.33% uas on CTB which is 1.54% uas improvement over the greedy baseline parser. Moreover, the performance is better than the best transition-based parser (Zhang and Nivre, 2011) which adopts a much larger beam width (s = 64)."]},{"title":"6 Conclusion and related work","paragraphs":["This work directly extends (Goldberg and Elhadad, 2010) with beam search and global learning. We show that both the easy-first POS tagger and dependency parser can be significantly impr-s PTB CTB speed 1 97.17 93.91 1350 3 97.20 94.15 560 5 97.22 94.17 385","","Table 2: Tagging accuracy vs beam width vs. Speed is","evaluated using the number of sentences that can be","processed in one second  s PTB CTB speed","uas compl uas compl 1 91.77 45.29 84.54 33.75 221 2 92.29 46.28 85.11 34.62 124 4 92.50 46.82 85.62 37.11 71 8 92.74 48.12 86.00 35.87 39"," Table 3: Parsing accuracy vs beam width. ‘uas’ and ‘compl’ denote unlabeled score and complete match rate respectively (all excluding punctuations).","","PTB CTB (Collins, 2002) 97.11 (Hatori et al., 2012) 93.82 (Shen et al., 2007) 97.33 (Li et al., 2012) 93.88","(Huang et al., 2012) 97.35 (Ma et al., 2012) 93.84 this work 1 97.22 this work 1 93.87 this work 97.28 this work 94.01†","  Table 4: Tagging results on the test set. ‘†","’ denotes statistically significant over the greedy baseline by McNemar’s test ( )  Systems s uas compl","(Huang and Sagae, 2010) 8 85.20 33.72","(Zhang and Nivre, 2011) 64 86.00 36.90","(Li et al., 2012) - 86.55 - this work 1 84.79 32.98 this work 8 86.33†","36.13"," Table 5: Parsing results on CTB test set.  Systems s uas compl","(Huang and Sagae, 2010) 8 92.10 -","(Zhang and Nivre, 2011) 64 92.90 48.50","(Koo and Collins, 2010) - 93.04 - this work 1 91.72 44.04 this work 8 92.47†","46.07"," Table 6: Parsing results on PTB test set.  oved using beam search and global learning.","This work can also be considered as applying (Huang et al., 2012) to the systems that exhibit spurious ambiguity. One future direction might be to apply the training method to transition-based parsers with dynamic oracle (Goldberg and Nivre, 2012) and potentially further advance performances of state-of-the-art transition-based parsers. 113","Shen et al., (2007) and (Shen and Joshi, 2008) also proposed bi-directional sequential classifica-tion with beam search for POS tagging and LTAG dependency parsing, respectively. The main difference is that their training method aims to learn a classifier which distinguishes between each local action while our training method aims to distinguish between action sequences. Our method can also be applied to their framework. Acknowledgments We would like to thank Yue Zhang, Yoav Goldberg and Zhenghua Li for discussions and suggestions on earlier drift of this paper. We would also like to thank the three anonymous reviewers for their suggestions. This work was supported in part by the National Science Foundation of China (61073140; 61272376), Specialized Research Fund for the Doctoral Program of Higher Educa-tion (20100042110031) and the Fundamental Research Funds for the Central Universities (N100204002)."]},{"title":"References","paragraphs":["Collins, M. 2002. Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms. In Proceedings of EMNLP.","Duan, X., Zhao, J., , and Xu, B. 2007. Probabilistic models for action-based Chinese dependency parsing. In Proceedings of ECML/ECPPKDD.","Goldberg, Y. and Elhadad, M. 2010 An Efficient Algorithm for Eash-First Non-Directional Dependency Parsing. In Proceedings of NAACL","Huang, L. and Sagae, K. 2010. Dynamic programming for linear-time incremental parsing. In Proceedings of ACL.","Huang, L. Fayong, S. and Guo, Y. 2012. Structured Perceptron with Inexact Search. In Proceedings of NAACL.","Koo, T. and Collins, M. 2010. Efficient third-order dependency parsers. In Proceedings of ACL.","Li, Z., Zhang, M., Che, W., Liu, T. and Chen, W. 2012. A Separately Passive-Aggressive Training Algorithm for Joint POS Tagging and Dependency Parsing. In Proceedings of COLING","Ma, J., Xiao, T., Zhu, J. and Ren, F. 2012. Easy-First Chinese POS Tagging and Dependency Parsing. In Proceedings of COLING","Rataparkhi, A. (1996) A Maximum Entropy Part-Of-Speech Tagger. In Proceedings of EMNLP","Shen, L., Satt, G. and Joshi, A. K. (2007) Guided Learning for Bidirectional Sequence Classification. In Proceedings of ACL.","Shen, L. and Josh, A. K. 2008. LTAG Dependency Parsing with Bidirectional Incremental Construc-tion. In Proceedings of EMNLP.","Seeker, W., Farkas, R. and Bohnet, B. 2012 Datadriven Dependency Parsing With Empty Heads. In Proceedings of COLING","Søggard, A. and Wulff, J. 2012. An Empirical Study of Non-lexical Extensions to Delexicalized Transfer. In Proceedings of COLING","Yue Zhang and Stephen Clark. 2007 Chinese Segmentation Using a Word-based Perceptron Algorithm. In Proceedings of ACL.","Zhang, Y. and Clark, S. 2008. Joint word segmentation and POS tagging using a single perceptron. In Proceedings of ACL.","Zhang, Y. and Nivre, J. 2011. Transition-based dependency parsing with rich non-local features. In Proceedings of ACL.","Zhang, Y. and Nivre, J. 2012. Analyzing the Effect of Global Learning and Beam-Search for Transition-Based Dependency Parsing. In Proceedings of COLING. 114"]}],"references":[{"authors":[{"last":"Collins"},{"last":"M"}],"year":"2002","title":"Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms"},{"authors":[{"first":"X.","last":"Duan"},{"first":"J.","last":"Zhao"},{"last":"Xu"},{"last":"B"}],"year":"2007","title":"Probabilistic models for action-based Chinese dependency parsing"},{"authors":[]},{"authors":[{"first":"L.","last":"Huang"},{"last":"Sagae"},{"last":"K"}],"year":"2010","title":"Dynamic programming for linear-time incremental parsing"},{"authors":[{"first":"L.","last":"Huang"},{"first":"S.","last":"Fayong"},{"last":"Guo"},{"last":"Y"}],"year":"2012","title":"Structured Perceptron with Inexact Search"},{"authors":[{"first":"T.","last":"Koo"},{"last":"Collins"},{"last":"M"}],"year":"2010","title":"Efficient third-order dependency parsers"},{"authors":[{"first":"Z.","last":"Li"},{"first":"M.","last":"Zhang"},{"first":"W.","last":"Che"},{"first":"T.","last":"Liu"},{"last":"Chen"},{"last":"W"}],"year":"2012","title":"A Separately Passive-Aggressive Training Algorithm for Joint POS Tagging and Dependency Parsing"},{"authors":[{"first":"J.","last":"Ma"},{"first":"T.","last":"Xiao"},{"first":"J.","last":"Zhu"},{"last":"Ren"},{"last":"F"}],"year":"2012","title":"Easy-First Chinese POS Tagging and Dependency Parsing"},{"authors":[]},{"authors":[]},{"authors":[{"first":"L.","last":"Shen"},{"first":"A.","last":"Josh"},{"last":"K"}],"year":"2008","title":"LTAG Dependency Parsing with Bidirectional Incremental Construc-tion"},{"authors":[]},{"authors":[{"first":"A.","last":"Søggard"},{"last":"Wulff"},{"last":"J"}],"year":"2012","title":"An Empirical Study of Non-lexical Extensions to Delexicalized Transfer"},{"authors":[]},{"authors":[{"first":"Y.","last":"Zhang"},{"last":"Clark"},{"last":"S"}],"year":"2008","title":"Joint word segmentation and POS tagging using a single perceptron"},{"authors":[{"first":"Y.","last":"Zhang"},{"last":"Nivre"},{"last":"J"}],"year":"2011","title":"Transition-based dependency parsing with rich non-local features"},{"authors":[{"first":"Y.","last":"Zhang"},{"last":"Nivre"},{"last":"J"}],"year":"2012","title":"Analyzing the Effect of Global Learning and Beam-Search for Transition-Based Dependency Parsing"}],"cites":[{"authors":[{"last":"Goldberg"},{"last":"Elhadad"}],"year":"2010","style":0},{"authors":[{"last":"Seeker"},{"last":"al."}],"year":"2012","style":0},{"authors":[{"last":"Søggard"},{"last":"Wulff"}],"year":"2012","style":0},{"authors":[{"last":"Goldberg"},{"last":"Elhadad"}],"year":"2010","style":0},{"authors":[{"last":"Collins"}],"year":"2002","style":0},{"authors":[{"last":"Zhang"},{"last":"Nivre"}],"year":"2012","style":0},{"authors":[{"last":"Zhang"},{"last":"Clark"}],"year":"2007","style":0},{"authors":[{"last":"Huang"},{"last":"al."}],"year":"2012","style":0,"reference":{"authors":[{"first":"L.","last":"Huang"},{"first":"S.","last":"Fayong"},{"last":"Guo"},{"last":"Y"}],"year":"2012","title":"Structured Perceptron with Inexact Search"}},{"authors":[{"last":"Huang"},{"last":"al."}],"year":"2012","style":0,"reference":{"authors":[{"first":"L.","last":"Huang"},{"first":"S.","last":"Fayong"},{"last":"Guo"},{"last":"Y"}],"year":"2012","title":"Structured Perceptron with Inexact Search"}},{"authors":[{"last":"Ma"},{"last":"al."}],"year":"2012","style":0,"reference":{"authors":[{"first":"J.","last":"Ma"},{"first":"T.","last":"Xiao"},{"first":"J.","last":"Zhu"},{"last":"Ren"},{"last":"F"}],"year":"2012","title":"Easy-First Chinese POS Tagging and Dependency Parsing"}},{"authors":[{"last":"Goldberg"},{"last":"Nivre"}],"year":"2012","style":0},{"authors":[{"last":"Ma"},{"last":"al."}],"year":"2012","style":0,"reference":{"authors":[{"first":"J.","last":"Ma"},{"first":"T.","last":"Xiao"},{"first":"J.","last":"Zhu"},{"last":"Ren"},{"last":"F"}],"year":"2012","title":"Easy-First Chinese POS Tagging and Dependency Parsing"}},{"authors":[{"last":"Huang"},{"last":"Sagae"}],"year":"2010","style":0},{"authors":[{"last":"Zhang"},{"last":"Nivre"}],"year":"2011","style":0},{"authors":[{"last":"Goldberg"},{"last":"Elhadad"}],"year":"2010","style":0},{"authors":[{"last":"Huang"},{"last":"al."}],"year":"2012","style":0,"reference":{"authors":[{"first":"L.","last":"Huang"},{"first":"S.","last":"Fayong"},{"last":"Guo"},{"last":"Y"}],"year":"2012","title":"Structured Perceptron with Inexact Search"}},{"authors":[{"last":"Collins"}],"year":"2002","style":0},{"authors":[{"last":"Huang"},{"last":"al."}],"year":"2012","style":0,"reference":{"authors":[{"first":"L.","last":"Huang"},{"first":"S.","last":"Fayong"},{"last":"Guo"},{"last":"Y"}],"year":"2012","title":"Structured Perceptron with Inexact Search"}},{"authors":[{"last":"Collins"},{"last":"Roark"}],"year":"2004","style":0},{"authors":[{"last":"Zhang"},{"last":"Nivre"}],"year":"2012","style":0},{"authors":[{"last":"Nivre"},{"last":"al."}],"year":"2003","style":0},{"authors":[{"last":"Goldberg"},{"last":"Elhadad"}],"year":"2010","style":0},{"authors":[{"last":"Ratnaparkhi"}],"year":"1996","style":0},{"authors":[{"last":"Duan"},{"last":"al."}],"year":"2007","style":0,"reference":{"authors":[{"first":"X.","last":"Duan"},{"first":"J.","last":"Zhao"},{"last":"Xu"},{"last":"B"}],"year":"2007","title":"Probabilistic models for action-based Chinese dependency parsing"}},{"authors":[{"last":"Goldberg"},{"last":"Elhadad"}],"year":"2010","style":0},{"authors":[{"last":"Zhang"},{"last":"Nivre"}],"year":"2011","style":0},{"authors":[{"last":"Shen"},{"last":"al."}],"year":"2007","style":0},{"authors":[{"last":"Ma"},{"last":"al."}],"year":"2012","style":0,"reference":{"authors":[{"first":"J.","last":"Ma"},{"first":"T.","last":"Xiao"},{"first":"J.","last":"Zhu"},{"last":"Ren"},{"last":"F"}],"year":"2012","title":"Easy-First Chinese POS Tagging and Dependency Parsing"}},{"authors":[{"last":"Zhang"},{"last":"Nivre"}],"year":"2011","style":0},{"authors":[{"last":"Goldberg"},{"last":"Elhadad"}],"year":"2010","style":0},{"authors":[{"last":"Collins"}],"year":"2002","style":0},{"authors":[{"last":"Hatori"},{"last":"al."}],"year":"2012","style":0},{"authors":[{"last":"Shen"},{"last":"al."}],"year":"2007","style":0},{"authors":[{"last":"Li"},{"last":"al."}],"year":"2012","style":0,"reference":{"authors":[{"first":"Z.","last":"Li"},{"first":"M.","last":"Zhang"},{"first":"W.","last":"Che"},{"first":"T.","last":"Liu"},{"last":"Chen"},{"last":"W"}],"year":"2012","title":"A Separately Passive-Aggressive Training Algorithm for Joint POS Tagging and Dependency Parsing"}},{"authors":[{"last":"Huang"},{"last":"al."}],"year":"2012","style":0,"reference":{"authors":[{"first":"L.","last":"Huang"},{"first":"S.","last":"Fayong"},{"last":"Guo"},{"last":"Y"}],"year":"2012","title":"Structured Perceptron with Inexact Search"}},{"authors":[{"last":"Ma"},{"last":"al."}],"year":"2012","style":0,"reference":{"authors":[{"first":"J.","last":"Ma"},{"first":"T.","last":"Xiao"},{"first":"J.","last":"Zhu"},{"last":"Ren"},{"last":"F"}],"year":"2012","title":"Easy-First Chinese POS Tagging and Dependency Parsing"}},{"authors":[{"last":"Huang"},{"last":"Sagae"}],"year":"2010","style":0},{"authors":[{"last":"Zhang"},{"last":"Nivre"}],"year":"2011","style":0},{"authors":[{"last":"Li"},{"last":"al."}],"year":"2012","style":0,"reference":{"authors":[{"first":"Z.","last":"Li"},{"first":"M.","last":"Zhang"},{"first":"W.","last":"Che"},{"first":"T.","last":"Liu"},{"last":"Chen"},{"last":"W"}],"year":"2012","title":"A Separately Passive-Aggressive Training Algorithm for Joint POS Tagging and Dependency Parsing"}},{"authors":[{"last":"Huang"},{"last":"Sagae"}],"year":"2010","style":0},{"authors":[{"last":"Zhang"},{"last":"Nivre"}],"year":"2011","style":0},{"authors":[{"last":"Koo"},{"last":"Collins"}],"year":"2010","style":0},{"authors":[{"last":"Huang"},{"last":"al."}],"year":"2012","style":0,"reference":{"authors":[{"first":"L.","last":"Huang"},{"first":"S.","last":"Fayong"},{"last":"Guo"},{"last":"Y"}],"year":"2012","title":"Structured Perceptron with Inexact Search"}},{"authors":[{"last":"Goldberg"},{"last":"Nivre"}],"year":"2012","style":0},{"authors":[{"last":"Shen"},{"last":"Joshi"}],"year":"2008","style":0}]}
