{"sections":[{"title":"A Syllable Based Word Recognition Model for Korean Noun Extraction Do-Gil Lee and Hae-Chang Rim Dept. of Computer Science & Engineering Korea University 1, 5-ka, Anam-dong, Seongbuk-ku Seoul 136-701, Korea  dglee, rim @nlp.korea.ac.kr Heui-Seok Lim Dept. of Information & Communications Chonan University 115 AnSeo-dong CheonAn 330-704, Korea limhs@infocom.chonan.ac.kr Abstract","paragraphs":["Noun extraction is very important for many NLP applications such as information retrieval, automatic text classification, and information extraction. Most of the previous Korean noun extraction systems use a morphological analyzer or a Part-of-Speech (POS) tagger. Therefore, they require much of the linguistic knowledge such as morpheme dictionaries and rules (e.g. morphosyntactic rules and morphological rules). This paper proposes a new noun extraction method that uses the syllable based word recognition model. It finds the most probable syllable-tag sequence of the input sentence by using automatically acquired statistical information from the POS tagged corpus and extracts nouns by detecting word boundaries. Furthermore, it does not require any labor for constructing and maintaining linguistic knowledge. We have performed various experiments with a wide range of variables influencing the performance. The experimental results show that without morphological analysis or POS tagging, the proposed method achieves comparable performance with the previous methods."]},{"title":"1 Introduction","paragraphs":["Noun extraction is a process to find every noun in a document (Lee et al., 2001). In Korean, Nouns are used as the most important terms (features) that express the document in NLP applications such as information retrieval, document categorization, text summarization, information extraction, and etc.","Korean is a highly agglutinative language and nouns are included in Eojeols. An Eojeol is a surface level form consisting of more than one combined morpheme. Therefore, morphological analysis or POS tagging is required to extract Korean nouns.","The previous Korean noun extraction methods are classified into two categories: morphological analysis based method (Kim and Seo, 1999; Lee et al., 1999a; An, 1999) and POS tagging based method (Shim et al., 1999; Kwon et al., 1999). The morphological analysis based method tries to generate all possible interpretations for a given Eojeol by implementing a morphological analyzer or a simpler method using lexical dictionaries. It may over-generate or extract inaccurate nouns due to lexical ambiguity and shows a low precision rate. Although several studies have been proposed to reduce the over-generated results of the morphological analysis by using exclusive information (Lim et al., 1995; Lee et al., 2001), they cannot completely resolve the ambiguity.","The POS tagging based method chooses the most probable analysis among the results produced by the morphological analyzer. Due to the resolution of the ambiguities, it can obtain relatively accurate results. But it also suffers from errors not only produced by a POS tagger but also triggered by the preceding morphological analyzer.","Furthermore, both methods have serious deficien- (Cheol-Su-neun) (sa-lam-deul-eul) (bwass-da) (Cheol-Su) (neun) (sa-lam-deul) (eul) (bwass-da) (Cheol-Su) (sa-lam) (deul) (eul) (bo) (ass) (da) eojeol word morpheme proper noun : per son name pos tposition noun : person noun suffi x: plural postposition verb : seeprefinal endingending (neun)","Figure 1: Constitution of the sentence “","","","","","","","","","   ","","","","","","","","","","","(Cheol-Su saw the persons)” cies in that they require considerable manual labor to construct and maintain linguistic knowledge and suffer from the unknown word problem. If a morphological analyzer fails to recognize an unknown noun in an unknown Eojeol, the POS tagger would never extract the unknown noun. Although the morphological analyzer properly recognizes the unknown noun, it would not be extracted due to the sparse data problem.","This paper proposes a new noun extraction method that uses a syllable based word recognition model. The proposed method does not require labor for constructing and maintaining linguistic knowledge and it can also alleviate the unknown word problem or the sparse data problem. It finds the most probable syllable-tag sequence of the input sentence by using statistical information and extracts nouns by detecting the word boundaries. The statistical information is automatically acquired from a POS annotated corpus and the word boundary can be detected by using an additional tag to represent the boundary of a word.","This paper is organized as follows. In Section 2, the notion of word is defined. Section 3 presents the syllable based word recognition model. Section 4 describes the method of constructing the training data from existing POS tagged corpora. Section 5 discusses experimental results. Finally, Section 6 concludes the paper."]},{"title":"2 A new definition of word","paragraphs":["Korean spacing unit is an Eojeol, which is delimited by whitespace, as with word in English. In Korean, an Eojeol is made up of one or more words, and a word is made up of one or more morphemes. Figure 1 represents the relationships among morphemes, words, and Eojeols with an example sentence. Syllables are delimited by a hyphen in the figure.","All of the previous noun extraction methods regard a morpheme as a processing unit. In order to extract nouns, nouns in a given Eojeol should be segmented. To do this, the morphological analysis has been used, but it requires complicated processes because of the surface forms caused by various morphological phenomena such as irregular conjugation of verbs, contraction, and elision. Most of the morphological phenomena occur at the inside of a morpheme or the boundaries between morphemes, not a word. We have also observed that a noun belongs to a morpheme as well as a word. Thus, we do not have to do morphological analysis in the noun extraction point of view.","In Korean linguistics, a word is defined as a morpheme or a sequence of morphemes that can be used independently. Even though a postposition is not used independently, it is regarded as a word because it is easily segmented from the preceding word. This definition is rather vague for computational processing. If we follow the definition of the word in linguistics, it would be difficult to analyze a word like the morphological analysis. For this reason, we define a different notion of a word.","According to our definition of a word, each uninflected morpheme or a sequence of successive inflected morphemes is regarded as an individual","word. 1","By virtue of the new definition of a word,","we need not consider mismatches between the sur-","face level form and the lexical level one in recogniz-","ing words.","The example sentence “  ","","","","","","","","","","","","","","","","","","","","","(Cheol-Su saw the persons)” represented in Fig-","ure 1 includes six words such as “","","","","","(Cheol-Su)”,","“","","","(neun)”, “","","","","","(sa-lam)”, “","","","(deul)”, “","","","(eul)”,","and “","","","","","(bwass-da)”. Unlike the Korean linguis-","tics, a noun suffix such as “   (nim)”, “","","","(deul)”, or","“","","","(jeog)” is also regarded as a word because it is","an uninflected morpheme."]},{"title":"3 Syllable based word recognition model","paragraphs":["A Korean syllable consists of an obligatory onset","(initial-grapheme, consonant), an obligatory peak","(nuclear grapheme, vowel), and an optional coda","(final-grapheme, consonant). In theory, the number","of syllables that can be used in Korean is the same as","the number of every combination of the graphemes.","2 Fortunately, only a fixed number of syllables is","frequently used in practice. 3","The amount of in-","formation that a Korean syllable has is larger than","that of an alphabet in English. In addition, there are","particular characteristics in Korean syllables. The","fact that words do not start with certain syllables","is one of such examples. Several attempts have","been made to use characteristics of Korean sylla-","bles. Kang (1995) used syllable information to re-","duce the over-generated results in analyzing conju-","gated forms of verbs. Syllable statistics have been","also used for automatic word spacing (Shim, 1996;","Kang and Woo, 2001; Lee et al., 2002). The syllable based word recognition model is rep-","resented as a function","like the following equations.","It is to find the most probable syllable-tag sequence","  ","","","","","","","","","",", for a given sentence","consist-","ing of a sequence of","syllables","","","","","","","","","","","",". 1 Korean morphemes can be classified into two types: un-","inflected morphemes having fixed word forms (such as noun,","unconjugated adjective, postposition, adverb, interjection, etc.)","and inflected morphemes having conjugated word forms (such","as a morpheme with declined or conjugated endings, predicative","postposition, etc.) 2 ","","","(","","","","","",") of pure Korean syllables are pos-","sible 3 Actually,","","","of syllables are used in the training data,","including Korean characters and non-Korean characters (e.g. al-","phabets, digits, Chinese characters, symbols).","","","  ","","","   ","","","","","  ","","","","(1)","        ","","","","","","","","   ","","","","","","","","(2)","Two Markov assumptions are applied in Equation 2. One is that the probability of a current syllable tag  ","conditionally depends on only the previous syllable tag. The other is that the probability of a current syllable","","conditionally depends on the current tag. In order to reflect word spacing information in Equation 2, which is very useful in Korean POS tagging, Equation 2 is changed to Equation 3 which can consider the word spacing information by calculat-ing the transition probabilities like the equation used in Kim et al. (1998).","","  ","","","       ","","","","","","","","","  ","","","","","","","","","(3) In the equation,","becomes zero if the transition occurs in the inside of an Eojeol; otherwise","is one.","Word boundaries can be detected by an additional tag. This method has been used in some tasks such as text chunking and named entity recognition to represent a boundary of an element (e.g. individual phrase or named entity). There are several possible representation schemes to do this. The simplest one is the BIO representation scheme (Ramshaw and Marcus, 1995), where a “B” denotes the first item of an element and an “I” any non-initial item, and a syllable with tag “O” is not a part of any element. Because every syllable corresponds to one syllable tag, “O” is not used in our task. The representation schemes used in this paper are described in detail in Section 4.","The probabilities in Equation 3 are estimated by the maximum likelihood estimator (MLE) using relative frequencies in the training data. 4","The most probable sequence of syllable tags in a sentence (a sequence of syllables) can be efficiently computed by using the Viterbi algorithm.","4","Since the MLE suffers from zero probability, to avoid zero","probability, we just assign a very low value such as   ","","","","for an unseen event in the training data.","Table 1: Examples of syllable tagging by BI, BIS, IE, and IES representation schemes","surface level lexical level","BI BIS IE IES (syllable) (morpheme/POS tag)   ","(yak)","","","","","","","(yak-sok)/nc B-nc B-nc I-nc I-nc   ","(sok) I-nc I-nc E-nc E-nc   ","(jang)     ","(jang-so)/nc","B-nc B-nc I-nc I-nc  ","(so) I-nc I-nc E-nc E-nc   ","(in)","","(i)/co+","(n)/etm B-co etm S-co etm E-co etm S-co etm   ","(Sin)","","","   ","","","","(Sin-la-ho-tel)/nc","B-nc B-nc I-nc I-nc  ","(la) I-nc I-nc I-nc I-nc  ","(ho) I-nc I-nc I-nc I-nc   ","(tel) I-nc I-nc E-nc E-nc  ","(keo)","","","   ","(keo-pi-syob)/nc","B-nc B-nc I-nc I-nc  ","(pi) I-nc I-nc I-nc I-nc   ","(syob) I-nc I-nc E-nc E-nc  ","(e)","","(e)/jc B-jc S-jc E-jc S-jc  ","(Jai)","","   ","(Jai-Ok)/nc","B-nc B-nc I-nc I-nc","",""," (Ok) I-nc I-nc E-nc E-nc",""," (i)","","(i)/jc B-jc S-jc E-jc S-jc","",""," (meon)","","","","","","(meon-jeo)/mag B-mag B-mag I-mag I-mag",""," (jeo) I-mag I-mag E-mag E-mag",""," (wa)","","(o)/pv+","","(a)/ec B-pv ec S-pv ec E-pv ec S-pv ec",""," (gi)","",""," ","(gi-da-li)/pv+","","(go)/ec","B-pv ec B-pv ec I-pv ec I-pv ec  ","(da) I-pv ec I-pv ec I-pv ec I-pv ec  ","(li) I-pv ec I-pv ec I-pv ec I-pv ec  ","(go) I-pv ec I-pv ec E-pv ec E-pv ec   ","(iss)","","","","(iss)/px+  ","(eoss)/ep+","","(da)/ef","B-px ef B-px ef I-px ef I-px ef   ","(eoss) I-px ef I-px ef I-px ef I-px ef  ","(da) I-px ef I-px ef E-px ef E-px ef . ./s B-s S-s E-s S-s","Given a sequence of syllables and syllable tags, it is straightforward to obtain the corresponding sequence of words and word tags. Among the words recognized through this process, we can extract nouns by just selecting words tagged as nouns. 5"]},{"title":"4 Constructing training data","paragraphs":["Our model is a supervised learning approach, so it requires a training data. Because the existing Korean POS tagged corpora are annotated by a morpheme level, we cannot use them as a training data without converting the data suitable for the word recognition model. The corpus can be modified through the following steps:","Step 1 For a given Eojeol, segment word boundaries and assign word tags to each word.","Step 2 For each separated word, assign the word tag","to each syllable in the word according to one of","the representations. 5 For the purpose of noun extraction, we only select com-","mon nouns here (tagged as “nc” or “NC”) among other kinds of","nouns.","In step 1, word boundaries are identified by using","the information of an uninflected morpheme and a","sequence of successive inflected morphemes. An","uninflected morpheme becomes one word and its","tag is assigned to the morpheme’s tag. Successive","inflected morphemes form a word and the combined","form of the first and the last morpheme’s tag repre-","sents its tag. For example, the morpheme-unit POS","tagged form of the Eojeol “","","","","","","","","(gass-eoss-da)”","is “","","(ga)/pv+","","","(ass)/ep+","","","(eoss)/ep+","","(da)/ef”,","and all of them are inflected morphemes. Hence,","the Eojeol “  ","","","","","","(gass-eoss-da)” becomes one","word and its tag is represented as “pv ef” by using","the first morpheme’s tag (“pv”) and the last one’s","(“ef”).","In step 2, a syllable tag is assigned to each of syl-","lables forming a word. The syllable tag should ex-","press not only POS tag but also the boundary of the","word. In order to detect the word boundaries, we use","the following four representation schemes:","BI representation scheme Assign “B” tag to the first syllable of a word, and “I” tag to the others.","BIS representation scheme Assign “S” tag to a syllable which forms a word, and other tags (“B” and “I”) are the same as “BI” representation scheme.","IE representation scheme Assign “E” tag to the last syllable of a word, and “I” tag to the others.","IES representation scheme Assign “S” tag to a syllable which forms a word, and other tags (“I” and “E”) are the same as “IE” representation scheme.","Table 1 shows an example of assigning word tag by syllable unit to the morpheme unit POS tagged corpus. Table 2: Description of Tagset 2 and Tagset 3 Tag Description Tagset 2 Tagset 3 symbol s S foreign word f F common noun nc NC bound noun nb NB pronoun np NP numeral nn NN verb pv V adjective pa A auxiliary predicate px VX copula co CO general adverb mag","MA conjunctive adverb maj adnoun mm MM interjection ii IC prefix xp XPN noun-derivational suffix xsn XSN verb-derivational suffix xsv","XSV adjective-derivational suffix xsm case particle jc J auxilary particle jx conjunctive particle jj adnominal case particle jm prefinal ending ep EP final ending ef EF conjunctive ending ec EC nominalizing ending etn ETN adnominalizing ending etm ETM"]},{"title":"5 Experiments 5.1 Experimental environment","paragraphs":["We used ETRI POS tagged corpus of 288,269 Eojoels for testing and the 21st Century Sejong Project’s POS tagged corpus (Sejong corpus, for short) for training. The Sejong corpus consists of three different corpora acquired from 1999 to 2001. The Sejong corpus of 1999 consists of 1.5 million Eojeols and other two corpora have 2 million Eojeols respectively. The evaluation measures for the noun extraction task are recall, precision, and F-measure. They measure the performance by document and are averaged over all the test documents. This is because noun extractors are usually used in the fields of applications such as information retrieval (IR) and document categorization. We also consider the frequency of nouns; that is, if the noun frequency is not considered, a noun occurring twice or more in a document is treated as other nouns occurring once. From IR point of view, this takes into account of the fact that even if a noun is extracted just once as an index term, the document including the term can also be retrieved.","The performance considerably depends on the following factors: the representation schemes for word boundary detection, the tagset, the amount of training data, and the difference between training data and test data.","First, we compare four different representation schemes (BI, BIS, IE, IES) in word boundary detection as explained in Section 4. We try to use the following three kinds of tagsets in order to select the most optimal tagset through the experiments:","Tagset 1 Simply use two tags (e.g. noun and non-noun). This is intended to examine the syllable characteristics; that is, which syllables tend to belong to nouns or not.","Tagset 2 Use the tagset used in the training data without modification. ETRI tagset used for training is relatively smaller than that of other tagsets. This tagset is changeable according to the POS tagged corpus used in training.","Tagset 3 Use a simplified tagset for the purpose of noun extraction. This tagset is simplified by combining postpositions, adverbs, and verbal suffixes into one tag, respectively. This tagset is always fixed even in a different training corpus.","Tagset 2 used in Section 5.2 and Tagset 3 are represented in Table 2. 5.2 Experimental results with similar data We divided the test data into ten parts. The performances of the model are measured by averaging over","Table 3: Experimental results of the ten-fold cross validation","without considering frequency with considering frequency","Precision Recall F-measure Precision Recall F-measure BI-1 72.37 83.61 77.58 74.61 82.47 78.34 BI-2 85.99 92.30 89.03 88.96 90.42 89.69 BI-3 84.85 91.20 87.90 87.56 89.55 88.54 BIS-1 78.50 83.53 80.93 80.36 83.99 82.13 BIS-2 88.15 92.34 90.19 90.65 91.58 91.11 BIS-3 86.92 91.07 88.94 89.27 90.62 89.94 IE-1 73.21 81.38 77.07 75.11 81.04 77.96 IE-2 85.12 91.54 88.21 88.37 90.34 89.34 IE-3 83.28 89.70 86.37 86.54 88.80 87.65 IES-1 78.07 82.69 80.31 79.54 83.08 81.27 IES-2 87.30 92.18 89.67 90.05 91.48 90.76 IES-3 85.80 90.79 88.22 88.46 90.47 89.45            Figure 2: Changes of F-measure according to tagsets and representation schemes  "," ","     Figure 3: Changes of F-measure according to the size of training data the ten test sets in the 10-fold cross-validation experiment. Table 3 shows experimental results according to each representation scheme and tagset. In the first column, each number denotes the tagset used. When it comes to the issue of frequency, the cases of considering frequency are better for precision but worse for recall, and better for F-measure. The representation schemes using single syllable information (e.g. “BIS”, “IES”) are better than other representation schemes (e.g. “BI”, “IE”). Contrary to our expectation, the results of Tagset 2 consistently outper-form other tagsets. The results of Tagset 1 are not as good as other tagsets because of the lack of the syntactic context. Nevertheless, the results reflect the usefulness of the syllable based processing. The changes of the F-measure according to the tagsets and the representation schemes reflecting frequency are shown in Figure 2. 5.3 Experimental results with different data To show the influence of the difference between the training data and the test data, we have performed the experiments on the Sejong corpus as a training data and the entire ETRI corpus as a test data. Table 4 shows the experimental results on all of the three training data. Although more training data are used in this experiment, the results of Table 3 shows better outcomes. Like other POS tagging models, this indicates that our model is dependent on the text do-main.","Table 4: Experimental results of Sejong corpus (from 1999 to 2001)","without considering frequency with considering frequency","Precision Recall F-measure Precision Recall F-measure BI-1 71.91 83.92 77.45 73.57 82.95 77.98 BI-2 85.38 89.96 87.61 87.19 88.26 87.72 BI-3 83.36 89.17 86.17 85.12 87.39 86.24 BIS-1 76.77 82.60 79.58 78.40 83.16 80.71 BIS-2 87.66 90.41 89.01 88.75 89.75 89.25 BIS-3 86.02 88.89 87.43 87.10 88.41 87.75 IE-1 70.82 79.97 75.12 72.67 79.64 75.99 IE-2 84.18 89.23 86.63 85.99 87.83 86.90 IE-3 82.01 87.67 84.74 83.79 86.57 85.16 IES-1 76.19 81.84 78.91 77.31 82.32 79.74 IES-2 86.41 89.33 87.85 87.66 88.75 88.20 IES-3 84.45 88.28 86.33 85.89 87.96 86.91","Table 5: Performances of other systems","without considering frequency with considering frequency","Precision Recall F-measure Precision Recall F-measure","NE2001 84.08 91.34 87.56 87.02 89.86 88.42","KOMA 60.10 93.12 73.06 58.07 93.67 71.70","HanTag 90.54 88.68 89.60 91.77 88.58 90.15","Figure 3 shows the changes of the F-measure according to the size of the training data. In this figure, “99-2000” means 1999 corpus and 2000 corpus are used, and “99-2001” means all corpora are used as the training data. The more training data are used, the better performance we obtained. However, the improvement is insignificant in considering the amount of increase of the training data.","Results reported by Lee et al. (2001) are presented in Table 5. The experiments were performed on the same condition as that of our experiments. NE2001, which is a system designed only to extract nouns, improves efficiency of the general morphological analyzer by using positive and negative information about occurrences of nouns. KOMA (Lee et al., 1999b) is a general-purpose morphological analyzer. HanTag (Kim et al., 1998) is a POS tagger, which takes the result of KOMA as input. According to Table 5, HanTag, which is a POS tagger, is an optimal tool in performing noun extraction in terms of the precision and the F-measure. Although the best performance of our proposed model (BIS-2) is worse than HanTag, it is better than NE2001 and KOMA. 5.4 Limitation","As mentioned earlier, we assume that morphologi-","cal variations do not occur at any inflected words.","However, some exceptions might occur in a col-","loquial text. For example, the lexical level forms","of two Eojeols “","","(ddai)+","","","(neun)” and “","","","(go-","gai)+  ","(leul)” are changed into the surface level","forms by contractions such as “","","","(ddain)” and “","","","","","(go-gail)”, respectively. Our models alone cannot","deal with these cases. Such exceptions, however, are","very rare. 6","In these experiments, we do not perform","any post-processing step to deal with such excep-","tions."]},{"title":"6 Conclusion","paragraphs":["We have presented a word recognition model for extracting nouns. While the previous noun extraction","6","Actually, about 0.145% of nouns in the test data belong to these cases. methods require morphological analysis or POS tagging, our noun extraction method only uses the syllable information without using any additional morphological analyzer. This means that our method does not require any dictionary or linguistic knowledge. Therefore, without manual labor to construct and maintain those resources, our method can extract nouns by using only the statistics, which can be automatically extracted from a POS tagged corpus.","The previous noun extraction methods take a morpheme as a processing unit, but we take a new notion of word as a processing unit by considering the fact that nouns belong to uninflected morphemes in Korean. By virtue of the new definition of a word, we need not consider mismatches between the surface level form and the lexical level one in recognizing words.","We have performed various experiments with a wide range of variables influencing the performance such as the representation schemes for the word boundary detection, the tag set, the amount of training data, and the difference between the training data and the test data. Without morphological analysis or POS tagging, the proposed method achieves comparable performance compared with the previous ones. In the future, we plan to extend the context to improve the performance.","Although the word recognition model is designed to extract nouns in this paper, the model itself is meaningful and it can be applied to other fields such as language modeling and automatic word spacing. Furthermore, our study make some contributions in the area of POS tagging research."]},{"title":"References","paragraphs":["D.-U. An. 1999. A noun extractor using connectivity information. In Proceedings of the Morphological An-alyzer and Tagger Evaluation Contest (MATEC 99), pages 173–178.","S.-S. Kang and C.-W. Woo. 2001. Automatic segmenta-tion of words using syllable bigram statistics. In Proceedings of the 6th Natural Language Processing Pacific Rim Symposium, pages 729–732.","S.-S. Kang. 1995. Morphological analysis of Korean irregular verbs using syllable characteristics. Journal of the Korea Information Science Society, 22(10):1480– 1487.","N.-C. Kim and Y.-H. Seo. 1999. A Korean morphological analyzer CBKMA and a index word extractor CBKMA/IX. In Proceedings of the MATEC 99, pages 50–59.","J.-D. Kim, H.-S. Lim, S.-Z. Lee, and H.-C. Rim. 1998. Twoply hidden Markov model: A Korean pos tagging model based on morpheme-unit with word-unit context. Computer Processing of Oriental Languages, 11(3):277–290.","O.-W. Kwon, M.-Y. Chung, D.-W. Ryu, M.-K. Lee, and J.-H. Lee. 1999. Korean morphological analyzer and part-of-speech tagger based on CYK algorithm using syllable information. In Proceedings of the MATEC 99.","J.-Y. Lee, B.-H. Shin, K.-J. Lee, J.-E. Kim, and S.- G. Ahn. 1999a. Noun extractor based on a multi-purpose Korean morphological engine implemented with COM. In Proceedings of the MATEC 99, pages 167–172.","S.-Z. Lee, B.-R. Park, J.-D. Kim, W.-H. Ryu, D.-G. Lee, and H.-C. Rim. 1999b. A predictive morphological analyzer, a part-of-speech tagger based on joint independence model, and a fast noun extractor. In Proceedings of the MATEC 99, pages 145–150.","D.-G. Lee, S.-Z. Lee, and H.-C. Rim. 2001. An efficient method for Korean noun extraction using noun occurrence characteristics. In Proceedings of the 6th Natural Language Processing Pacific Rim Symposium, pages 237–244.","D.-G. Lee, S.-Z. Lee, H.-C. Rim, and H.-S. Lim. 2002. Automatic word spacing using hidden Markov model for refining Korean text corpora. In Proceedings of the 3rd Workshop on Asian Language Resources and International Standardization, pages 51–57.","H.-S. Lim, S.-Z. Lee, and H.-C. Rim. 1995. An ef-ficient Korean mophological analysis using exclusive information. In Proceedings of the 1995 International Conference on Computer Processing of Oriental Languages, pages 225–258.","Lance A. Ramshaw and Mitchell P. Marcus. 1995. Text chunking using transformation-based learning. In Proceedings of the Third Workshop on Very Large Corpora, pages 82–94.","J.-H. Shim, J.-S. Kim, J.-W. Cha, and G.-B. Lee. 1999. Robust part-of-speech tagger using statistical and rule-based approach. In Proceedings of the MATEC 99, pages 60–75.","K.-S. Shim. 1996. Automated word-segmentation for Korean using mutual information of syllables. Journal of the Korea Information Science Society, 23(9):991– 1000."]}],"references":[{"authors":[{"first":"D.","middle":"-U.","last":"An"}],"year":"1999","title":"A noun extractor using connectivity information","source":"D.-U. An. 1999. A noun extractor using connectivity information. In Proceedings of the Morphological An-alyzer and Tagger Evaluation Contest (MATEC 99), pages 173–178."},{"authors":[{"first":"S.","middle":"-S.","last":"Kang"},{"first":"C.","middle":"-W.","last":"Woo"}],"year":"2001","title":"Automatic segmenta-tion of words using syllable bigram statistics","source":"S.-S. Kang and C.-W. Woo. 2001. Automatic segmenta-tion of words using syllable bigram statistics. In Proceedings of the 6th Natural Language Processing Pacific Rim Symposium, pages 729–732."},{"authors":[{"first":"S.","middle":"-S.","last":"Kang"}],"year":"1995","title":"Morphological analysis of Korean irregular verbs using syllable characteristics","source":"S.-S. Kang. 1995. Morphological analysis of Korean irregular verbs using syllable characteristics. Journal of the Korea Information Science Society, 22(10):1480– 1487."},{"authors":[{"first":"N.","middle":"-C.","last":"Kim"},{"first":"Y.","middle":"-H.","last":"Seo"}],"year":"1999","title":"A Korean morphological analyzer CBKMA and a index word extractor CBKMA/IX","source":"N.-C. Kim and Y.-H. Seo. 1999. A Korean morphological analyzer CBKMA and a index word extractor CBKMA/IX. In Proceedings of the MATEC 99, pages 50–59."},{"authors":[{"first":"J.","middle":"-D.","last":"Kim"},{"first":"H.","middle":"-S.","last":"Lim"},{"first":"S.","middle":"-Z.","last":"Lee"},{"first":"H.","middle":"-C.","last":"Rim"}],"year":"1998","title":"Twoply hidden Markov model: A Korean pos tagging model based on morpheme-unit with word-unit context","source":"J.-D. Kim, H.-S. Lim, S.-Z. Lee, and H.-C. Rim. 1998. Twoply hidden Markov model: A Korean pos tagging model based on morpheme-unit with word-unit context. Computer Processing of Oriental Languages, 11(3):277–290."},{"authors":[{"first":"O.","middle":"-W.","last":"Kwon"},{"first":"M.","middle":"-Y.","last":"Chung"},{"first":"D.","middle":"-W.","last":"Ryu"},{"first":"M.","middle":"-K.","last":"Lee"},{"first":"J.","middle":"-H.","last":"Lee"}],"year":"1999","title":"Korean morphological analyzer and part-of-speech tagger based on CYK algorithm using syllable information","source":"O.-W. Kwon, M.-Y. Chung, D.-W. Ryu, M.-K. Lee, and J.-H. Lee. 1999. Korean morphological analyzer and part-of-speech tagger based on CYK algorithm using syllable information. In Proceedings of the MATEC 99."},{"authors":[{"first":"J.","middle":"-Y.","last":"Lee"},{"first":"B.","middle":"-H.","last":"Shin"},{"first":"K.","middle":"-J.","last":"Lee"},{"first":"J.","middle":"-E.","last":"Kim"},{"first":"S.","middle":"- G.","last":"Ahn"}],"year":"1999a","title":"Noun extractor based on a multi-purpose Korean morphological engine implemented with COM","source":"J.-Y. Lee, B.-H. Shin, K.-J. Lee, J.-E. Kim, and S.- G. Ahn. 1999a. Noun extractor based on a multi-purpose Korean morphological engine implemented with COM. In Proceedings of the MATEC 99, pages 167–172."},{"authors":[{"first":"S.","middle":"-Z.","last":"Lee"},{"first":"B.","middle":"-R.","last":"Park"},{"first":"J.","middle":"-D.","last":"Kim"},{"first":"W.","middle":"-H.","last":"Ryu"},{"first":"D.","middle":"-G.","last":"Lee"},{"first":"H.","middle":"-C.","last":"Rim"}],"year":"1999b","title":"A predictive morphological analyzer, a part-of-speech tagger based on joint independence model, and a fast noun extractor","source":"S.-Z. Lee, B.-R. Park, J.-D. Kim, W.-H. Ryu, D.-G. Lee, and H.-C. Rim. 1999b. A predictive morphological analyzer, a part-of-speech tagger based on joint independence model, and a fast noun extractor. In Proceedings of the MATEC 99, pages 145–150."},{"authors":[{"first":"D.","middle":"-G.","last":"Lee"},{"first":"S.","middle":"-Z.","last":"Lee"},{"first":"H.","middle":"-C.","last":"Rim"}],"year":"2001","title":"An efficient method for Korean noun extraction using noun occurrence characteristics","source":"D.-G. Lee, S.-Z. Lee, and H.-C. Rim. 2001. An efficient method for Korean noun extraction using noun occurrence characteristics. In Proceedings of the 6th Natural Language Processing Pacific Rim Symposium, pages 237–244."},{"authors":[{"first":"D.","middle":"-G.","last":"Lee"},{"first":"S.","middle":"-Z.","last":"Lee"},{"first":"H.","middle":"-C.","last":"Rim"},{"first":"H.","middle":"-S.","last":"Lim"}],"year":"2002","title":"Automatic word spacing using hidden Markov model for refining Korean text corpora","source":"D.-G. Lee, S.-Z. Lee, H.-C. Rim, and H.-S. Lim. 2002. Automatic word spacing using hidden Markov model for refining Korean text corpora. In Proceedings of the 3rd Workshop on Asian Language Resources and International Standardization, pages 51–57."},{"authors":[{"first":"H.","middle":"-S.","last":"Lim"},{"first":"S.","middle":"-Z.","last":"Lee"},{"first":"H.","middle":"-C.","last":"Rim"}],"year":"1995","title":"An ef-ficient Korean mophological analysis using exclusive information","source":"H.-S. Lim, S.-Z. Lee, and H.-C. Rim. 1995. An ef-ficient Korean mophological analysis using exclusive information. In Proceedings of the 1995 International Conference on Computer Processing of Oriental Languages, pages 225–258."},{"authors":[{"first":"Lance","middle":"A.","last":"Ramshaw"},{"first":"Mitchell","middle":"P.","last":"Marcus"}],"year":"1995","title":"Text chunking using transformation-based learning","source":"Lance A. Ramshaw and Mitchell P. Marcus. 1995. Text chunking using transformation-based learning. In Proceedings of the Third Workshop on Very Large Corpora, pages 82–94."},{"authors":[{"first":"J.","middle":"-H.","last":"Shim"},{"first":"J.","middle":"-S.","last":"Kim"},{"first":"J.","middle":"-W.","last":"Cha"},{"first":"G.","middle":"-B.","last":"Lee"}],"year":"1999","title":"Robust part-of-speech tagger using statistical and rule-based approach","source":"J.-H. Shim, J.-S. Kim, J.-W. Cha, and G.-B. Lee. 1999. Robust part-of-speech tagger using statistical and rule-based approach. In Proceedings of the MATEC 99, pages 60–75."},{"authors":[{"first":"K.","middle":"-S.","last":"Shim"}],"year":"1996","title":"Automated word-segmentation for Korean using mutual information of syllables","source":"K.-S. Shim. 1996. Automated word-segmentation for Korean using mutual information of syllables. Journal of the Korea Information Science Society, 23(9):991– 1000."}],"cites":[{"style":0,"text":"Lee et al., 2001","origin":{"pointer":"/sections/1/paragraphs/0","offset":63,"length":16},"authors":[{"last":"Lee"},{"last":"al."}],"year":"2001","references":["/references/8"]},{"style":0,"text":"Kim and Seo, 1999","origin":{"pointer":"/sections/1/paragraphs/2","offset":117,"length":17},"authors":[{"last":"Kim"},{"last":"Seo"}],"year":"1999","references":["/references/3"]},{"style":0,"text":"Lee et al., 1999a","origin":{"pointer":"/sections/1/paragraphs/2","offset":136,"length":17},"authors":[{"last":"Lee"},{"last":"al."}],"year":"1999a","references":["/references/6"]},{"style":0,"text":"An, 1999","origin":{"pointer":"/sections/1/paragraphs/2","offset":155,"length":8},"authors":[{"last":"An"}],"year":"1999","references":["/references/0"]},{"style":0,"text":"Shim et al., 1999","origin":{"pointer":"/sections/1/paragraphs/2","offset":195,"length":17},"authors":[{"last":"Shim"},{"last":"al."}],"year":"1999","references":["/references/12"]},{"style":0,"text":"Kwon et al., 1999","origin":{"pointer":"/sections/1/paragraphs/2","offset":214,"length":17},"authors":[{"last":"Kwon"},{"last":"al."}],"year":"1999","references":["/references/5"]},{"style":0,"text":"Lim et al., 1995","origin":{"pointer":"/sections/1/paragraphs/2","offset":678,"length":16},"authors":[{"last":"Lim"},{"last":"al."}],"year":"1995","references":["/references/10"]},{"style":0,"text":"Lee et al., 2001","origin":{"pointer":"/sections/1/paragraphs/2","offset":696,"length":16},"authors":[{"last":"Lee"},{"last":"al."}],"year":"2001","references":["/references/8"]},{"style":0,"text":"Kang (1995)","origin":{"pointer":"/sections/3/paragraphs/15","offset":6,"length":11},"authors":[{"last":"Kang"}],"year":"1995","references":["/references/2"]},{"style":0,"text":"Shim, 1996","origin":{"pointer":"/sections/3/paragraphs/18","offset":38,"length":10},"authors":[{"last":"Shim"}],"year":"1996","references":["/references/13"]},{"style":0,"text":"Kang and Woo, 2001","origin":{"pointer":"/sections/3/paragraphs/19","offset":0,"length":18},"authors":[{"last":"Kang"},{"last":"Woo"}],"year":"2001","references":["/references/1"]},{"style":0,"text":"Lee et al., 2002","origin":{"pointer":"/sections/3/paragraphs/19","offset":20,"length":16},"authors":[{"last":"Lee"},{"last":"al."}],"year":"2002","references":["/references/9"]},{"style":0,"text":"Kim et al. (1998)","origin":{"pointer":"/sections/3/paragraphs/104","offset":301,"length":17},"authors":[{"last":"Kim"},{"last":"al."}],"year":"1998","references":["/references/4"]},{"style":0,"text":"Ramshaw and Marcus, 1995","origin":{"pointer":"/sections/3/paragraphs/130","offset":338,"length":24},"authors":[{"last":"Ramshaw"},{"last":"Marcus"}],"year":"1995","references":["/references/11"]},{"style":0,"text":"Lee et al. (2001)","origin":{"pointer":"/sections/5/paragraphs/22","offset":20,"length":17},"authors":[{"last":"Lee"},{"last":"al."}],"year":"2001","references":["/references/8"]},{"style":0,"text":"Lee et al., 1999b","origin":{"pointer":"/sections/5/paragraphs/22","offset":338,"length":17},"authors":[{"last":"Lee"},{"last":"al."}],"year":"1999b","references":["/references/7"]},{"style":0,"text":"Kim et al., 1998","origin":{"pointer":"/sections/5/paragraphs/22","offset":410,"length":16},"authors":[{"last":"Kim"},{"last":"al."}],"year":"1998","references":["/references/4"]}]}
