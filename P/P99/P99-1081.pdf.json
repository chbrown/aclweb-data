{"sections":[{"title":"An Unsupervised Model for Statistically Determining Coordinate Phrase Attachment","paragraphs":["Miriam Goldberg","Central High School &","Dept. of Computer and Information Science","200 South 33rd Street Philadelphia, PA 19104-6389 University of Pennsylvania","miriamgOunagi, cis. upenn, edu Abstract This paper examines the use of an unsupervised statistical model for determining the attachment of ambiguous coordinate phrases (CP) of the form nl p n2 cc n3. The model presented here is based on JAR98], an unsupervised model for determining prepositional phrase attachment. After training on unannotated 1988 Wall Street Journal text, the model performs at 72% accuracy on a development set from sections 14 through 19 of the WSJ TreeBank [MSM93]. 1 Introduction The coordinate phrase (CP) is a source of structural ambiguity in natural language. For example, take the phrase: box of chocolates and roses","'Roses' attaches either high to 'box' or low to 'chocolates'. In this case, attachment is high, yielding: H-attach: ((box (of chocolates)) (and roses)) Consider, then, the phrase: salad of lettuce and tomatoes 'Lettuce' attaches low to 'tomatoes', giving:","L-attach: (salad (of ((lettuce) and (tomatoes))) [AR98] models. In addition to these, a corpus-based model for PP-attachment [SN97] has been reported that uses information from a semantic dictionary.","Sparse data can be a major concern in corpus-based disambiguation. Supervised models are limited by the amount of annotated data available for training. Such a model is useful only for languages in which annotated corpora are available. Because an unsupervised model does not rely on such corpora it may be modified for use in multiple languages as in [AR98].","The unsupervised model presented here trains from an unannotated version of the 1988 Wall Street Journal. After tagging and chunk-ing the text, a rough heuristic is then employed to pick out training examples. This results in a training set that is less accurate, but much larger, than currently existing annotated corpora. It is the goal, then, of unsupervised training data to be abundant in order to offset its noisiness. 2 Background The statistical model must determine the probability of a given CP attaching either high (H) or low (L), p( attachment I phrase). Results shown come from a development corpus of 500 phrases of extracted head word tuples from the WSJ TreeBank [MSM93]. 64% of these phrases attach low and 36% attach high. After further development, final testing will be done on a separate corpus.","The phrase:","Previous work has used corpus-based approaches to solve the similar problem of prepositional phrase attachment. These have in-cluded backed-off [CB 95], maximum entropy [RRR94], rule-based [HR94], and unsupervised (busloads (of ((executives) and (their wives))) gives the 6-tuple: L busloads of executives and wives 610","where, a = L, nl = busloads, p = of, n2 = executives, cc = and, n3 = wives. The CP attachment model must determine a for all (nl p n2 cc n3) sets. The attachment decision is correct if it is the same as the corresponding decision in the TreeBank set.","The probability of a CP attaching high is conditional on the 5-tuple. The algorithm presented in this paper estimates the probability: regular expressions that replace noun and quantifier phrases with their head words. These head words were then passed through a set of heuristics to extract the unambiguous phrases. The heuristics to find an unambiguous CP are:","• wn is a coordinating conjunction (cc) if it is tagged cc. • w,~_~ is the leftmost noun (nl) if: I5 = (a l nl,p, n2, cc, n3)","The parts of the CP are analogous to those of the prepositional phrase (PP) such that {nl,n2} - {n,v} and n3 - p. JAR98] determines the probability p(v,n,p,a). To be consistent, here we determine the probability p(nl, n2, n3, a). 3 Training Data Extraction A statistical learning model must train from unambiguous data. In annotated corpora ambiguous data are made unambiguous through classifications made by human annotators. In unannotated corpora the data themselves must be unambiguous. Therefore, while this model disambiguates CPs of the form (nl p n2 cc n3), it trains from implicitly unambiguous CPs of the form (n ccn). For example:","- Wn-x is the first noun to occur within 4 words to the left of cc. -no","preposition occurs between this noun and cc.","- no preposition occurs within 4 words to the left of this noun. • wn+x is the rightmost noun (n2) if:","- it is the first noun to occur within 4 words to the right of cc.","- No preposition occurs between cc and this noun.","The first noun to occur within 4 words to the right of cc is always extracted. This is ncc. Such nouns are also used in the statistical model. For example, the we process the sentence below as follows: dog and cat","Because there are only two nouns in the unambiguous CP, we must redefine its components. The first noun will be referred to as nl. It is analogous to nl and n2 in the ambiguous CP. The second, terminal noun will be referred to as n3. It is analogous to the third noun in the ambiguous CP. Hence nl -- dog, cc --- and, n3 = cat. In addition to the unambiguous CPs, the model also uses any noun that follows acc. Such nouns are classified, ncc.","We extracted 119629 unambiguous CPs and 325261 nccs from the unannotated 1988 Wall Street Journal. First the raw text was fed into the part-of-speech tagger described in [AR96] 1. This was then passed to a simple chunker as used in [AR98], implemented with two small","IBecause this tagger trained on annotated data, one may argue that the model presented here is not purely unsupervised. Several firms have also launched business subsidiaries and consulting arms specializing in trade, lobbying and other areas. First it is annotated with parts of speech: Several_JJ firms__NNS have_VBP also_RB launched_VBN business.aNN subsidiaries_NNS and_CC consulting_VBG armsANNS specializing_VBG in_IN tradeANN ,_, lobbying_NN and_CC other_JJ areas_NNS ._.","From there, it is passed to the chunker yield-ing: firmsANNS have_VBP also_RB launched_VBN subsidiaries_NNS and_CC consulting_VBG armsANNS specializing_VBG in_IN tradeANN ,_, Iobbying_.NN and_CC areas_NNS ._. 611","Noun phrase heads of ambiguous and unambiguous CPs are then extracted according to the heuristic, giving: subsidiaries and arms and areas","where the extracted unambiguous CP is {nl"]},{"title":"= subsidiaries, cc = and, n3 = arms}","paragraphs":["and"]},{"title":"areas","paragraphs":["is extracted as a"]},{"title":"ncc","paragraphs":["because, although it is not part of an unambiguous CP, it occurs within four words after a conjunction. 4 The Statistical Model First, we can factor"]},{"title":"p(a, nl, n2,","paragraphs":["n3) as follows:"]},{"title":"p(a, nl,n2, n3)","paragraphs":["="]},{"title":"p(nl)p(n2) , p(alnl ,n2) , p(n3 I a, nl,n2)","paragraphs":["The terms p(nl) and p(n2) are independent of the attachment and need not be computed. The other two terms are more problematic. Because the training phrases are unambiguous and of the form (nl"]},{"title":"cc","paragraphs":["n2), nl and n2 of the CP in question never appear together in the training data. To compensate we use the following heuristic as in JAR98]. Let the random variable ¢ range over"]},{"title":"(true, false}","paragraphs":["and let it denote the presence or absence of any n3 that unambiguously attaches to the nl or n2 in question. If"]},{"title":"¢ = true","paragraphs":["when any n3 unambiguously attaches to nl, then p(¢"]},{"title":"= true","paragraphs":["[ nl) is the conditional probability that a particular nl occurs with an unambiguously attached n3. Now"]},{"title":"p(a I nl,n2)","paragraphs":["can be approximated as:"]},{"title":"p(a","paragraphs":["= H lnl, n2)"]},{"title":"p(true l nl) Z(nl,n2) p(a","paragraphs":["= L [nl,n2) ~"]},{"title":"p(true","paragraphs":["In2)"]},{"title":"\" Z(nl,","paragraphs":["n2) where the normalization factor,"]},{"title":"Z(nl,n2)","paragraphs":["="]},{"title":"p(true","paragraphs":["I nl) +"]},{"title":"p(true","paragraphs":["I n2). The reasoning behind this approximation is that the tendency of a CP to attach high (low) is related to the tendency of the nl (n2) in question to appear in an unambiguous CP in the training data.","We approximate"]},{"title":"p(n3la, nl,","paragraphs":["n2) as follows:"]},{"title":"p(n3","paragraphs":["I a = H, nl, n2) ~"]},{"title":"p(n3 I true,","paragraphs":["nl)"]},{"title":"p(n3","paragraphs":["I a = L, nl, n2) ~"]},{"title":"p(n3 I true,","paragraphs":["n2)","The reasoning behind this approximation is that when generating n3 given high (low) attachment, the only counts from the training data that matter are those which unambiguously attach to nl (n2), i.e.,"]},{"title":"¢ = true.","paragraphs":["Word statistics from the extracted CPs are used to formulate these probabilities. 4.1","Generate ¢ The conditional probabilities"]},{"title":"p(truelnl) and p(true I","paragraphs":["n2) denote the probability of whether a noun will appear attached unambiguously to some n3. These probabilities are estimated as:"]},{"title":"{ $(.~1,true) iff(nl,true) >0","paragraphs":["f(nl)"]},{"title":"p(truelnl)","paragraphs":["= .5 otherwise { /(n2,~r~,e) if"]},{"title":"f(n2, true)> 0","paragraphs":["/(n2)"]},{"title":"p(true[n2)","paragraphs":["= .5 otherwise where f(n2,"]},{"title":"true)","paragraphs":["is the number of times n2 appears in an unambiguously attached CP in the training data and f(n2) is the number of times this noun has appeared as either nl, n3, or ncc in the training data. 4.2","Generate n3 The terms"]},{"title":"p(n3 I nl, true)","paragraphs":["and"]},{"title":"p(n3 I n2, true)","paragraphs":["denote the probabilies that the noun n3 appears attached unambiguously to nl and n2 respectively. Bigram counts axe used to compute these as follows: f(nl,n3,true)"]},{"title":"p(n3 [ true,","paragraphs":["nl) ="]},{"title":"l](nl,","paragraphs":["TM) if"]},{"title":"I(nl,n3,true)>O","paragraphs":["otherwise f(n2,n3,true)"]},{"title":"p(n3 l true,","paragraphs":["n2) ="]},{"title":"11(n2,","paragraphs":["TM) if"]},{"title":"f(n2,n3,true)>O","paragraphs":["otherwise","where N is the set of all n3s and nets that occur in the training data. 5 Results Decisions were deemed correct if they agreed with the decision in the corresponding TreeBank data. The correct attachment was chosen 612 72% of the time on the 500-phrase development corpus from the WSJ TreeBank. Because it is a forced binary decision, there are no measurements for recall or precision. If low attachment is always chosen, the accuracy is 64%. After further development the model will be tested on a testing corpus.","When evaluating the effectiveness of an unsupervised model, it is helpful to compare its performance to that of an analogous supervised model. The smaller the error reduction when going from unsupervised to supervised models, the more comparable the unsupervised model is to its supervised counterpart. To our knowledge there has been very little if any work in the area of ambiguous CPs. In addition to develop-ing an unsupervised CP disambiguation model, In [MG, in prep] we have developed two supervised models (one backed-off and one maximum entropy) for determining CP attachment. The backed-off model, closely based on [CB95] performs at 75.6% accuracy. The reduction error from the unsupervised model presented here to the backed-off model is 13%. This is comparable to the 14.3% error reduction found when going from JAR98] to [CB95].","It is interesting to note that after reducing the volume of training data by half there was no drop in accuracy. In fact, accuracy remained exactly the same as the volume of data was in-creased from half to full. The backed-off model in [MG, in prep] trained on only 1380 training phrases. The training corpus used in the study presented here consisted of 119629 training phrases. Reducing this figure by half is not overly significant. 6 Discussion In an effort to make the heuristic concise and portable, we may have oversimplified it thereby negatively affecting the performance of the model. For example, when the heuristic came upon a noun phrase consisting of more than one consecutive noun the noun closest to the cc was extracted. In a phrase like coffee and rhubarb apple pie the heuristic would chose rhubarb as the n3 when clearly pie should have been chosen. Also, the heuristic did not check if a preposition occurred between either nl and cc or cc and n3. Such cases make the CP ambiguous thereby invalidating it as an unambiguous training example. By including annotated training data from the TreeBank set, this model could be modified to become a partially-unsupervised classifier.","Because the model presented here is basically a straight reimplementation of [AR98] it fails to take into account attributes that are specific to the CP. For example, whereas (nl ce n3) -- (n3 cc nl), (v p n) ~ (n p v). In other words, there is no reason to make the distinction between \"dog and cat\" and \"cat and dog.\" Modifying the model accordingly may greatly increase the usefulness of the training data. 7 Acknowledgements We thank Mitch Marcus and Dennis Erlick for making this research possible, Mike Col]in.~ for his guidance, and Adwait Ratnaparkhi and Jason Eisner for their helpful insights. References","~[CB95] M. Collins, J. Brooks. 1995. Prepositional Phrase Attachment through a Backed-Off Model, A CL 3rd Workshop on Very Large Corpora, Pages 27-38, Cambridge, Massachusetts, June.","[MG, in prep] M. Goldberg. in preparation. Three Models for Statistically Determining Coordinate Phrase Attachment.","[HR93] D. Hindle, M. Rooth. 1993. Structural Ambiguity and Lexical Relations. Computational Linguistics, 19(1):103-120.","[MSM93] M. Marcus, B. Santorini and M. Marcinkiewicz. 1993. Building a Large Annotated Corpus of English: the Penn Treebank, Computational Linguistics, 19(2):313-330.","[RRR94] A. Ratnaparkhi, J. Reynar and S. Roukos. 1994. A Maximum Entropy Model for Prepositional Phrase Attachment, In Proceedings of the ARPA Workshop on Human Language Technology, 1994.","[AR96] A. Ratnaparkhi. 1996. A Maximum Entropy Part-Of-Speech Tagger, In Proceedings of the Empirical Methods in Natural Language Processing Conference, May 17-18.","[AR98] A. Ratnaparkhi. 1998. Unsupervised Statistical Models for Prepositional Phrase Attachment, In Proceedings of the Seventeenth International Conference on Computational Linguistics, Aug. 10-14, Montreal, Canada. 613","[SN97] J. Stetina, M. Nagao. 1997. Corpus Based PP Attachment Ambiguity Resolution with a Semantic Dictionary. In Jou Shou and Kenneth Church, editors,"]},{"title":"Proceedings o] the Fifth Workshop on Very Large Corpora,","paragraphs":["pages 66-80, Beijing and Hong Kong, Aug. 18-20. 614"]}]}
