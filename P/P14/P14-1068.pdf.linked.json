{"sections":[{"title":"","paragraphs":["Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 721–732, Baltimore, Maryland, USA, June 23-25 2014. c⃝2014 Association for Computational Linguistics"]},{"title":"Learning Grounded Meaning Representations with AutoencodersCarina Silberer and Mirella LapataInstitute for Language, Cognition and ComputationSchool of Informatics, University of Edinburgh10 Crichton Street, Edinburgh EH8 9ABc.silberer@ed.ac.uk, mlap@inf.ed.ac.ukAbstract","paragraphs":["In this paper we address the problem of grounding distributional representations of lexical meaning. We introduce a new model which uses stacked autoencoders to learn higher-level embeddings from textual and visual input. The two modalities are encoded as vectors of attributes and are obtained automatically from text and images, respectively. We evaluate our model on its ability to simulate similarity judgments and concept categorization. On both tasks, our approach outperforms baselines and related models."]},{"title":"1 Introduction","paragraphs":["Recent years have seen a surge of interest in single word vector spaces (Turney and Pantel, 2010; Collobert et al., 2011; Mikolov et al., 2013) and their successful use in many natural language applications. Examples include information retrieval (Manning et al., 2008), search query expansions (Jones et al., 2006), document classification (Sebastiani, 2002), and question answering (Yih et al., 2013). Vector spaces have been also popular in cognitive science figuring prominently in simula-tions of human behavior involving semantic prim-ing, deep dyslexia, text comprehension, synonym selection, and similarity judgments (see Griffiths et al., 2007). In general, these models specify mechanisms for constructing semantic representations from text corpora based on the distributional hypothesis (Harris, 1970): words that appear in similar linguistic contexts are likely to have related meanings.","Word meaning, however, is also tied to the physical world. Words are grounded in the external environment and relate to sensorimotor experience (Regier, 1996; Landau et al., 1998; Barsalou, 2008). To account for this, new types of perceptually grounded distributional models have emerged. These models learn the meaning of words based on textual and perceptual input. The latter is approximated by feature norms elicited from humans (Andrews et al., 2009; Steyvers, 2010; Silberer and Lapata, 2012), visual information extracted automatically from images, (Feng and Lapata, 2010; Bruni et al., 2012a; Silberer et al., 2013) or a combination of both (Roller and Schulte im Walde, 2013). Despite differences in formulation, most existing models conceptualize the problem of meaning representation as one of learning from multiple views corresponding to different modalities. These models still represent words as vectors resulting from the combination of representations with different statistical properties that do not necessarily have a natural correspondence (e.g., text and images).","In this work, we introduce a model, illustrated in Figure 1, which learns grounded meaning representations by mapping words and images into a common embedding space. Our model uses stacked autoencoders (Bengio et al., 2007) to induce semantic representations integrating visual and textual information. The literature describes several successful approaches to multimodal learning using different variants of deep networks (Ngiam et al., 2011; Srivastava and Salakhutdinov, 2012) and data sources including text, images, audio, and video. Unlike most previous work, our model is defined at a finer level of granularity — it computes meaning representations for individual words and is unique in its use of attributes as a means of representing the textual and visual modalities. We follow Silberer et al. (2013) in arguing that an attribute-centric representation is expedient for several reasons.","Firstly, attributes provide a natural way of expressing salient properties of word meaning as demonstrated in norming studies (e.g., McRae et al., 2005) where humans often employ attributes when asked to describe a concept. Secondly, from 721 a modeling perspective, attributes allow for easier integration of different modalities, since these are rendered in the same medium, namely, language. Thirdly, attributes are well-suited to describing visual phenomena (e.g., objects, scenes, actions). They allow to generalize to new in-stances for which there are no training examples available and to transcend category and task boundaries whilst offering a generic description of visual data (Farhadi et al., 2009).","Our model learns multimodal representations from attributes which are automatically inferred from text and images. We evaluate the embeddings it produces on two tasks, namely word similarity and categorization. In the first task, model estimates of word similarity (e.g., gem–jewel are similar but glass–magician are not) are compared against elicited similarity ratings. We performed a large-scale evaluation on a new dataset consisting of human similarity judgments for 7,576 word pairs. Unlike previous efforts such as the widely used WordSim353 collection (Finkelstein et al., 2002), our dataset contains ratings for visual and textual similarity, thus allowing to study the two modalities (and their contribution to meaning representation) together and in isolation. We also assess whether the learnt representations are appropriate for categorization, i.e., grouping a set of objects into meaningful semantic categories (e.g., peach and apple are members of FRUIT, whereas chair and table are FURNITURE). On both tasks, our model outperforms baselines and related models."]},{"title":"2 Related Work","paragraphs":["The presented model has connections to several lines of work in NLP, computer vision research, and more generally multimodal learning. We review related work in these areas below. Grounded Semantic Spaces Grounded semantic spaces are essentially distributional models augmented with perceptual information. A model akin to Latent Semantic Analysis (Landauer and Dumais, 1997) is proposed in Bruni et al. (2012b) who concatenate two independently constructed textual and visual spaces and subsequently project them onto a lower-dimensional space using Singular Value Decomposition.","Several other models have been extensions of Latent Dirichlet Allocation (Blei et al., 2003) where topic distributions are learned from words and other perceptual units. Feng and Lapata (2010) use visual words which they extract from a corpus of multimodal documents (i.e., BBC news articles and their associated images), whereas others (Steyvers, 2010; Andrews et al., 2009; Silberer and Lapata, 2012) use feature norms obtained in longitudinal elicitation studies (see McRae et al. (2005) for an example) as an approximation of the visual environment. More recently, topic models which combine both feature norms and visual words have also been introduced (Roller and Schulte im Walde, 2013). Drawing inspiration from the successful application of attribute classifiers in object recognition, Silberer et al. (2013) show that automatically predicted visual attributes act as substitutes for feature norms without any critical information loss.","The visual and textual modalities on which our model is trained are decoupled in that they are not derived from the same corpus (we would expect co-occurring images and text to correlate to some extent) but unified in their representation by natural language attributes. The use of stacked autoencoders to extract a shared lexical meaning representation is new to our knowledge, although, as we explain below related to a large body of work on deep learning. Multimodal Deep Learning Our work employs deep learning (a.k.a deep networks) to project linguistic and visual information onto a unified representation that fuses the two modalities together. The goal of deep learning is to learn multiple lev-els of representations through a hierarchy of network architectures, where higher-level representations are expected to help define higher-level concepts.","A large body of work has focused on projecting words and images into a common space using a variety of deep learning methods ranging from deep and restricted Boltzman machines (Srivastava and Salakhutdinov, 2012; Feng et al., 2013), to autoencoders (Wu et al., 2013), and recursive neural networks (Socher et al., 2013b). Similar methods have been employed to combine other modalities such as speech and video (Ngiam et al., 2011) or images (Huang and Kingsbury, 2013). Although our model is conceptually similar to these studies (especially those applying stacked autoencoders), it differs considerably from them in at least two aspects. Firstly, most of these approaches aim to learn a shared representation between modalities 722 so as to infer some missing modality from others (e.g., to infer text from images and vice versa); in contrast, we aim to learn an optimal representation for each modality and their optimal combination. Secondly, our problem setting is different from the former studies, which usually deal with classification tasks and fine-tune the deep neural networks using training data with explicit class labels; in contrast we fine-tune our autoencoders using a semi-supervised criterion. That is, we use indirect supervision in the form of object classification in addition to the objective of reconstructing the attribute-centric input representation."]},{"title":"3 Autoencoders for Grounded Semantics","paragraphs":["3.1 Background Our model learns higher-level meaning representations for single words from textual and visual input in a joint fashion. We first briefly review autoencoders in Section 3.1 with emphasis on as-pects relevant to our model which we then describe in Section 3.2.","Autoencoders An autoencoder is an unsuper-","vised neural network which is trained to recon-","struct a given input from its latent representation","(Bengio, 2009). It consists of an encoder fθ which","maps an input vector x(i)","to a latent representa-","tion y(i)","= fθ(x(i)",") = s(Wx(i)","+ b), with s being","a non-linear activation function, such as a sig-","moid function. A decoder gθ′ then aims to recon-","struct input x(i)","from y(i)",", i.e., x̂(i)","= gθ′(y(i)",") =","s(W′ y(i)","+ b′","). The training objective is the de-","termination of parameters θ̂ = {W, b} and θ̂′","=","{W′ , b′","} that minimize the average reconstruction","error over a set of input vectors {x(1)",", ..., x(n)","}:","θ̂, θ̂′","= arg min","θ,θ′ 1","n n"]},{"title":"∑","paragraphs":["i=1 L(x(i) , gθ′( fθ(x(i)","))), (1) where L is a loss function, such as cross-entropy. Parameters θ and θ′","can be optimized by gradient descent methods.","Autoencoders are a means to learn representations of some input by retaining useful features in the encoding phase which help to reconstruct the input, whilst discarding useless or noisy ones. To this end, different strategies have been employed to guide parameter learning and constrain the hidden representation. Examples include imposing a bottleneck to produce an under-complete representation of the input, using sparse representations, or denoising. Denoising Autoencoders The training criterion with denoising autoencoders is the reconstruction of clean input x(i)","given a corrupted version x̃(i) (Vincent et al., 2010). The underlying idea is that the learned latent representation is good if the autoencoder is capable of reconstructing the actual input from its corruption. The reconstruction error for an input x(i)","with loss function L then is:","L(x(i) , gθ′( fθ(x̃(i)","))) (2) One possible corruption process is masking noise, where the corrupted version x̃(i)","results from randomly setting a fraction v of x(i)","to 0. Stacked Autoencoders Several (denoising) autoencoders can be used as building blocks to form a deep neural network (Bengio et al., 2007; Vincent et al., 2010). For that purpose, the autoencoders are pre-trained layer by layer, with the current layer being fed the latent representation of the previous autoencoder as input. Using this unsupervised pre-training procedure, initial parameters are found which approximate a good solution. Subsequently, the original input layer and hidden representations of all the autoencoders are stacked and all network parameters are fine-tuned with backpropagation.","To further optimize the parameters of the network, a supervised criterion can be imposed on top of the last hidden layer such as the minimization of a prediction error on a supervised task (Bengio, 2009). Another approach is to unfold the stacked autoencoders and fine-tune them with respect to the minimization of the global reconstruction error (Hinton and Salakhutdinov, 2006). Alternatively, a semi-supervised criterion can be used (Ranzato and Szummer, 2008; Socher et al., 2011) through combination of the unsupervised training criterion (global reconstruction) with a supervised criterion (prediction of some target given the latent representation). 3.2 Semantic Representations To learn meaning representations of single words from textual and visual input, we employ stacked (denoising) autoencoders (SAEs). Both input modalities are vector-based representations of words, or, more precisely, the objects they refer to (e.g., canary, trolley). The vector dimensions correspond to textual and visual attributes, examples of which are shown in Table 1. We explain how these representations are obtained in more detail 723 ... ... ... input x TEXT W (1) W (3) ... ... ... IMAGES W (2) W (4) ... bimodal coding y̆","W (5′ ) W (5) ...softmax t̂ W (6) ... ...","W (3′ ) ... ...","W (4′ ) ... reconstruction x̂","W (1′ ) ...","W (2′ ) Figure 1: Stacked autoencoder trained with semi-supervised objective. Input to the model are single-word vector representations obtained from text and images. Vector dimensions correspond to textual and visual attributes, respectively (see Table 1). in Section 4.1. We first train SAEs with two hidden layers (codings) for each modality separately. Then, we join these two SAEs by feeding their respective second coding simultaneously to another autoencoder, whose hidden layer thus yields the fused meaning representation. Finally, we stack all layers and unfold them in order to fine-tune the SAE. Figure 1 illustrates the model. Unimodal Autoencoders For both modalities, we use the hyperbolic tangent function as activation function for encoder fθ and decoder gθ′ and an entropic loss function for L. The weights of each autoencoder are tied, i.e., W′","= WT",". We employ denoising autoencoders (DAEs) for pre-training the textual modality. Regarding the visual autoencoder, we derive a new (‘denoised’) target vector to be reconstructed for each input vector x(i)",", and treat x(i)","itself as corrupted input. The unimodal autoencoder is thus trained to denoise a given input. The target vector is derived as follows: each object o in our data is represented by multiple images, and each image is in turn represented by a visual attribute vector x(i)",". The target vector is the sum of x(i)","and the centroid x(j)","of the remaining attribute vectors representing object o. Bimodal Autoencoder The bimodal autoencoder is fed with the concatenated final hidden codings of the visual and textual modalities as input and maps these inputs to a joint hidden layer y̆ with B units. We normalize both unimodal input codings to unit length. Again, we use tied weights for the bimodal autoencoder. We also encourage the autoencoder to detect dependencies between the two modalities while learning the mapping to the bimodal hidden layer. We therefore apply masking noise to one modality with a masking factor v (see Section 3.1), so that the corrupted modality optimally has to rely on the other modality in order to reconstruct its missing input features. Stacked Bimodal Autoencoder We finally build a stacked bimodal autoencoder (SAE) with all pre-trained layers and fine-tune them with respect to a semi-supervised criterion. That is, we unfold the stacked autoencoder and furthermore add a softmax output layer on top of the bimodal layer y̆ that outputs predictions t̂ with respect to the inputs’ object labels (e.g., boat ):","t̂(i) =","exp(W(6) y̆(i) + b(6)",")","∑O","k=1 exp(W(6) k. y̆(i)","+ b(6)","k ) , (3) with weights W(6)","∈ RO×B",", b(6)","∈ RO×1",", where O is the number of unique object labels. The overall objective to be minimized is therefore the weighted sum of the reconstruction error Lr and the classification errorLc:","L = 1 n n"]},{"title":"∑","paragraphs":["i=1 (","δrLr(x(i)",", x̂(i) )+δcLc(t(i)",", t̂(i)","))","+λR (4) where δr and δc are weighting parameters that give different importance to the partial objectives, 724","eats seeds has beak has claws has handlebar has wheels has wings is yellow made of wood canary 0.05 0.24 0.15 0.00 –0.10 0.19 0.34 0.00","Visual trolley 0.00 0.00 0.00 0.30 0.32 0.00 0.00 0.25 bird:n breed:v cage:n chirp:v fly:v track:n ride:v run:v rail:n wheel:n canary 0.16 0.19 0.39 0.13 0.13 0.00 0.00 0.00 0.00 –0.05 T e xtual trolley –0.40 0.00 0.00 0.00 0.00 0.14 0.16 0.33 0.17 0.20 Table 1: Examples of attribute-based representations provided as input to our autoencoders. Lc and Lr are entropic loss functions, and R is a regularization term with R = ∑5","j=1 2||W(j)","||2","+ ||W(6)","||2",". Finally, t̂(i)","is the object label vector predicted by the softmax layer for input vector x(i)",", and t(i)","is the correct object label, represented as a O-dimensional one-hot vector1",".","The additional supervised criterion drives the learning towards a representation capable of discriminating between different objects. Further-more, the semi-supervised setting affords flexibility, allowing to adapt the architecture to specific tasks. For example, by setting the corruption parameter v for the textual modality to one and δr to zero, a standard object classification model for images can be trained. Setting v close to one for either modality enables the model to infer the other (missing) modality. As our input consists of natural language attributes, the model would infer textual attributes given visual attributes and vice versa."]},{"title":"4 Experimental Setup","paragraphs":["In this section we present our experimental setup for assessing the performance of our model. We give details on the tasks and datasets used for evaluation, we explain how the textual and visual inputs were constructed, how the SAE model was trained, and describe the approaches used for comparison with our own work. 4.1 Data We learn meaning representations for the nouns contained in McRae et al.’s (2005) feature norms. These are 541 concrete animate and inanimate objects (e.g., animals, clothing, vehicles, utensils, fruits, and vegetables). The norms were elicited by asking participants to list properties (e.g., barks, an animal, has legs) describing the nouns they were presented with. 1 In a one-hot vector, the element corresponding to the ob-","ject label is one and the others are zero.","As shown in Figure 1, our model takes as input two (real-valued) vectors representing the visual and textual modalities. Vector dimensions correspond to textual and visual attributes, respectively. Textual attributes were extracted by running Strudel (Baroni et al., 2010) on a 2009 dump of the English Wikipedia.2","Strudel is a fully automatic method for extracting weighted word-attribute pairs (e.g., bat –species:n, bat –bite:v) from a lemmatized and POS-tagged corpus. Weights are log-likelihood ratio scores expressing how strongly an attribute and a word are associated. We only retained the ten highest scored attributes for each target word. This returned a total of 2,362 dimensions for the textual vectors. Association scores were scaled to the [−1, 1] range.","To obtain visual vectors, we followed the methodology put forward in Silberer et al. (2013). Specifically, we used an updated version of their dataset to train SVM-based attribute classifiers that predict visual attributes for images (Farhadi et al., 2009). The dataset is a taxonomy of 636 visual attributes (e.g., has wings, made of wood) and nearly 700K images from ImageNet (Deng et al., 2009) describing more than 500 of McRae et al.’s (2005) nouns. The classifiers perform reasonably well with an interpolated average precision of 0.52. We only considered attributes assigned to at least two nouns in the dataset, obtaining a 414 dimensional vector for each noun. Analogously to the textual representations, visual vectors were scaled to the [−1, 1] range.","We follow Silberer et al.’s (2013) partition of the dataset into training, validation, and test set and acquire visual vectors for each of the sets. We use the visual vectors of the training and development set for training the autoencoders, and the vectors for the test set for evaluation. 2 The corpus is downloadable from http://wacky.","sslmit.unibo.it/doku.php?id=corpora. 725 4.2 Model Architecture Model parameters were optimized on a subset of the word association norms collected by Nelson et al. (1998).3","These were established by present-ing participants with a cue word (e.g., canary) and asking them to name an associate word in response (e.g., bird, sing, yellow). For each cue, the norms provide a set of associates and the frequencies with which they were named. The dataset contains a very large number of cue-associate pairs (63,619 in total) some of which luckily are covered in McRae et al. (2005).4","During training we used correlation analysis (Spearman’s ρ) to monitor the degree of linear relationship between model cue-associate (cosine) similarities and human probabilities.","The best autoencoder on the word association task obtained a correlation coefficient of 0.33. This performance is superior to the results reported in Silberer et al. (2013) (their correlation coefficients range from 0.16 to 0.28). This model has the following architecture: the textual autoencoder (see Figure 1, left-hand side) consists of 700 hidden units which are then mapped to the second hidden layer with 500 units (the corruption parameter was set to v = 0.1); the visual autoencoder (see Figure 1, right-hand side) has 170 and 100 hidden units, in the first and second layer, respectively. The 500 textual and 100 visual hidden units were fed to a bimodal autoencoder contain-ing 500 latent units, and masking noise was applied to the textual modality with v = 0.2. The weighting parameters for the joint training objective of the stacked autoencoder were set to δr = 0.8 and δc = 1 (see Equation (4)).","We used the model described above and the meaning representations obtained from the output of the bimodal latent layer for all the evaluation tasks detailed below. Some performance gains could be expected if parameter optimization took place separately for each task. However, we wanted to avoid overfitting, and show that our parameters are robust across tasks and datasets. 4.3 Evaluation Tasks Word Similarity We first evaluated how well our model predicts word similarity ratings. Although several relevant datasets exist, such as 3 http://w3.usf.edu/Freeassociation. 4 435 word pairs constitute the overlap between Nelson et","al.’s norms (1998) and McRae et al.’s (2005) nouns. the widely used WordSim353 (Finkelstein et al., 2002) or the more recent Rel-122 norms (Szumlanski et al., 2013), they contain many abstract words, (e.g., love–sex or arrest –detention) which are not covered in McRae et al. (2005). This is for a good reason, as most abstract words do not have discernible attributes, or at least attributes that participants would agree upon. We thus created a new dataset consisting exclusively of McRae et al. (2005) nouns which we hope will be useful for the development and evaluation of grounded semantic space models.5","Initially, we created all possible pairings over McRae et al.’s (2005) nouns and computed their semantic relatedness using Patwardhan and Pedersen (2006)’s WordNet-based measure. We opted for this specific measure as it achieves high correlation with human ratings and has a high coverage on our nouns. Next, for each word we randomly selected 30 pairs under the assumption that they are representative of the full variation of semantic similarity. This resulted in 7,576 word pairs for which we obtained similarity ratings using Amazon Mechanical Turk (AMT). Participants were asked to rate a pair on two dimensions, visual and semantic similarity using a Likert scale of 1 (highly dissimilar) to 5 (highly similar). Each task consisted of 32 pairs covering examples of weak to very strong semantic relatedness. Two control pairs from Miller and Charles (1991) were in-cluded in each task to potentially help identify and eliminate data from participants who assigned random scores. Examples of the stimuli and mean ratings are shown in Table 2.","The elicitation study comprised overall 255 tasks, each task was completed by five volunteers. The similarity data was post-processed so as to identify and remove outliers. We considered an outlier to be any individual whose mean pairwise correlation fell outside two standard deviations from the mean correlation. 11.5% of the annotations were detected as outliers and removed. After outlier removal, we further examined how well the participants agreed in their similarity judgments. We measured inter-subject agreement as the average pairwise correlation coefficient (Spearman’sρ) between the ratings of all annotators for each task. For semantic similarity, the mean correlation was 0.76 (Min =0.34, Max 5 Available from http://homepages.inf.ed.ac.uk/","mlap/index.php?page=resources. 726 Word Pairs Semantic Visual football–pillow 1.0 1.2 dagger–pencil 1.0 2.2 motorcycle–wheel 2.4 1.8 orange–pumpkin 2.5 3.0 cherry–pineapple 3.6 1.2 pickle–zucchini 3.6 4.0 canary–owl 4.0 2.4 jeans–sweater 4.5 2.2 pan–pot 4.7 4.0 hornet –wasp 4.8 4.8 airplane–jet 5.0 5.0 Table 2: Mean semantic and visual similarity ratings for the McRae et al. (2005) nouns using a scale of 1 (highly dissimilar) to 5 (highly similar). =0.97, StD =0.11) and for visual similarity 0.63 (Min =0.19, Max =0.90, SD =0.14). These results indicate that the participants found the task relatively straightforward and produced similarity ratings with a reasonable level of consistency. For comparison, Patwardhan and Pedersen’s (2006) measure achieved a coefficient of 0.56 on the dataset for semantic similarity and 0.48 for visual similarity. The correlation between the average ratings of the AMT annotators and the Miller and Charles (1991) dataset was ρ = 0.91. In our experiments (see Section 5), we correlate model-based cosine similarities with mean similarity ratings (again using Spearman’s ρ). Categorization The task of categorization (i.e., grouping objects into meaningful categories) is a classic problem in the field of cognitive science, central to perception, learning, and the use of language. We evaluated model output against a gold standard set of categories created by Fountain and Lapata (2010). The dataset contains a classification, produced by human participants, of McRae et al.’s (2005) nouns into (possibly multiple) semantic categories (40 in total).6","To obtain a clustering of nouns, we used Chinese Whispers (Biemann, 2006), a randomized graph-clustering algorithm. In the categorization setting, Chinese Whispers (CW) produces a hard clustering over a weighted graph whose nodes cor-6 The dataset can be downloaded from http:","//homepages.inf.ed.ac.uk/s0897549/data/. respond to words and edges to cosine similarity scores between vectors representing their meaning. CW is a non-parametric model, it induces the number of clusters (i.e., categories) from the data as well as which nouns belong to these clusters. In our experiments, we initialized Chinese Whispers with different graphs resulting from different vector-based representations of the McRae et al. (2005) nouns. We also transformed the dataset into hard categorizations by assigning each noun to its most typical category as extrapolated from human typicality ratings (for details see Fountain and Lapata, 2010). CW can optionally apply a minimum weight threshold which we optimized using the categorization dataset from Baroni et al. (2010). The latter contains a classification of 82 McRae et al. (2005) nouns into 10 categories. These nouns were excluded from the gold standard (Fountain and Lapata, 2010) in our final evaluation.","We evaluated the clusters produced by CW using the F-score measure introduced in the SemEval 2007 task (Agirre and Soroa, 2007); it is the harmonic mean of precision and recall defined as the number of correct members of a cluster divided by the number of items in the cluster and the number of items in the gold-standard class, respectively. 4.4 Comparison with Other Models Throughout our experiments we compare a bimodal stacked autoencoder against unimodal autoencoders based solely on textual and visual input (left- and right-hand sides in Figure 1, respectively). We also compare our model against two approaches that differ in their fusion mechanisms. The first one is based on kernelized canonical correlation (kCCA, Hardoon et al., 2004) with a linear kernel which was the best performing model in Silberer et al. (2013). The second one emulates Bruni et al.’s (2014) fusion mechanism. Specifically, we concatenate the textual and visual vectors and project them onto a lower dimensional latent space using SVD (Golub and Reinsch, 1970). All these models run on the same datasets/items and are given input identical to our model, namely attribute-based textual and visual representations.","We furthermore report results obtained with Bruni et al.’s (2014) bimodal distributional model, which employs SVD to integrate co-occurrence-based textual representations with visual repre-727 Semantic Visual Models T V T+V T V T+V McRae 0.71 0.49 0.68 0.58 0.52 0.62 Attributes 0.58 0.61 0.68 0.46 0.56 0.58 SAE 0.65 0.60 0.70 0.52 0.60 0.64 SVD — — 0.67 — — 0.57 kCCA — — 0.57 — — 0.55 Bruni — — 0.52 — — 0.46 RNN-640 0.41 — — 0.34 — — Table 3: Correlation of model predictions against similarity ratings for McRae et al. (2005) noun pairs (using Spearman’s ρ). sentations constructed from low-level image features. In their model, the textual modality is represented by the 30K-dimensional vectors extracted from UKWaC and WaCkypedia.7","The visual modality is represented by bag-of-visual-words histograms built on the basis of clustered SIFT features (Lowe, 2004). We rebuilt their model on the ESP image dataset (von Ahn and Dabbish, 2004) using Bruni et al.’s (2013) publicly available system.","Finally, we also compare to the word embeddings obtained using Mikolov et al.’s (2011) recurrent neural network based language model. These were pre-trained on Broadcast news data (400M words) using the word2vec tool.8","We report results with the 640-dimensional embeddings as they performed best."]},{"title":"5 Results","paragraphs":["Table 3 presents our results on the word similarity task. We report correlation coefficients of model predictions against similarity ratings. As an indicator to how well automatically extracted attributes can approach the performance of clean human generated attributes, we also report results of a distributional model induced from McRae et al.’s (2005) norms (see the row labeled McRae in the table). Each noun is represented as a vector with dimensions corresponding to attributes elicited by participants of the norming study. Vector components are set to the (normalized) frequency with which participants generated the corresponding attribute. We show results for three models, using all attributes except those classified as visual (T), only 7 We thank Elia Bruni for providing us with their data. 8 Available from http://www.rnnlm.org/. # Pair # Pair 1 pliers–tongs 11 cello–violin 2 cathedral–church 12 cottage–house 3 cathedral–chapel 13 horse–pony 4 pistol–revolver 14 gun–rifle 5 chapel–church 15 cedar–oak 6 airplane–helicopter 16 bull–ox 7 dagger–sword 17 dress–gown 8 pistol–rifle 18 bolts–screws 9 cloak–robe 19 salmon–trout 10 nylons–trousers 20 oven–stove Table 4: Word pairs with highest semantic and visual similarity according to SAE model. Pairs are ranked from highest to lowest similarity. visual attributes (V), and all available attributes (V+T).9","As baselines, we also report the performance of a model based solely on textual attributes (which we obtain from Strudel), visual attributes (obtained from our classifiers), and their concatenation (see row Attributes in Table 3, and columns T, V, and T+V, respectively). The automatically obtained textual and visual attribute vectors serve as input to SVD, kCCA, and our stacked autoencoder (SAE). The third row in the table presents three variants of our model trained on textual and visual attributes only (T and V, respectively) and on both modalities jointly (T+V).","Recall that participants were asked to provide ratings on two dimensions, namely semantic and visual similarity. We would expect the textual modality to be more dominant when modeling semantic similarity and conversely the perceptual modality to be stronger with respect to visual similarity. This is borne out in our unimodal SAEs. The textual SAE correlates better with semantic similarity judgments (ρ = 0.65) than its visual equivalent (ρ = 0.60). And the visual SAE correlates better with visual similarity judgments (ρ = 0.60) compared to the textual SAE (ρ = 0.52). Interestingly, the bimodal SAE is better than the unimodal variants on both types of similarity judgments, semantic and visual. This suggests that both modalities contribute complementary information and that the SAE model is able to extract a shared representation which improves generalization performance across tasks by learning them 9 Classification of attributes into categories is provided by","McRae et al. (2005) in their dataset. 728 Models T V T+V McRae 0.52 0.31 0.42 Attributes 0.35 0.37 0.33 SAE 0.36 0.35 0.43 SVD — — 0.39 kCCA — — 0.37 Bruni — — 0.34 RNN-640 0.32 — — Table 5: F-score results on concept categorization. jointly. The bimodal autoencoder (SAE, T+V) outperforms all other bimodal models on both similarity tasks. It yields a correlation coefficient of ρ = 0.70 on semantic similarity and ρ = 0.64 on visual similarity. Human agreement on the former task is 0.76 and 0.63 on the latter. Table 4 shows examples of word pairs with highest semantic and visual similarity according to the SAE model.","We also observe that simply concatenating textual and visual attributes (Attributes, T+V) performs competitively with SVD and better than kCCA. This indicates that the attribute-based representation is a powerful predictor on its own. Interestingly, both Bruni et al. (2013) and Mikolov et al. (2011) which do not make use of attributes are out-performed by all other attribute-based systems (see columns T and T+V in Table 3).","Our results on the categorization task are given in Table 5. In this task, simple concatenation of visual and textual attributes does not yield improved performance over the individual modalities (see row Attributes in Table 5). In contrast, all bimodal models (SVD, kCCA, and SAE) are better than their unimodal equivalents and RNN-640. The SAE outperforms both kCCA and SVD by a large margin delivering clustering performance similar to the McRae et al.’s (2005) norms. Table 6 shows examples of clusters produced by Chinese Whispers when using vector representations provided by the SAE model.","In sum, our experiments show that the bimodal SAE model delivers superior performance across the board when compared against competitive baselines and related models. It is interesting to note that the unimodal SAEs are in most cases better than the raw textual or visual attributes. This indicates that higher level embeddings may be beneficial to NLP tasks in general, not only to those requiring multimodal information. STICK-LIKE UTENSILS baton, ladle, peg, spatula, spoon","RELIGIOUS BUILDINGS cathedral, chapel, church","WIND INSTRUMENTS clarinet, flute, saxophone, trombone, trumpet, tuba","AXES axe, hatchet, machete, tomahawk","FURNITURE W/ LEGS bed, bench, chair, couch, desk, rocker, sofa, stool, table","FURNITURE W/O LEGS bookcase, bureau, cabinet, closet, cupboard, dishwasher, dresser","LIGHTINGS candle, chandelier, lamp, lantern","ENTRY POINTS door, elevator, gate","UNGULATES bison, buffalo, bull, calf, camel, cow, donkey, elephant, goat, horse, lamb, ox, pig, pony, sheep","BIRDS crow, dove, eagle, falcon, hawk, ostrich, owl, penguin, pigeon, raven, stork, vulture, woodpecker Table 6: Examples of clusters produced by CW using the representations obtained from the SAE model."]},{"title":"6 Conclusions","paragraphs":["In this paper, we presented a model that uses stacked autoencoders to learn grounded meaning representations by simultaneously combining textual and visual modalities. The two modalities are encoded as vectors of natural language attributes and are obtained automatically from decoupled text and image data. To the best of our knowledge, our model is novel in its use of attribute-based input in a deep neural network. Experimental results in two tasks, namely simulation of word similarity and word categorization, show that our model outperforms competitive baselines and related models trained on the same attribute-based input. Our evaluation also reveals that the bimodal models are superior to their unimodal counterparts and that higher-level unimodal representations are better than the raw input. In the future, we would like to apply our model to other tasks, such as image and text retrieval (Hodosh et al., 2013; Socher et al., 2013b), zero-shot learning (Socher et al., 2013a), and word learning (Yu and Ballard, 2007). Acknowledgment We would like to thank Vittorio Ferrari, Iain Murray and members of the ILCC at the School of Informatics for their valuable feedback. We acknowledge the support of EPSRC through project grant EP/I037415/1. 729"]},{"title":"References","paragraphs":["Agirre, Eneko and Aitor Soroa. 2007. SemEval-2007 Task 02: Evaluating Word Sense Induc-tion and Discrimination Systems. In Proceedings of the Fourth International Workshop on Semantic Evaluations. Prague, Czech Republic, pages 7–12.","Andrews, M., G. Vigliocco, and D. Vinson. 2009. Integrating Experiential and Distributional Data to Learn Semantic Representations. Psychological Review 116(3):463–498.","Baroni, M., B. Murphy, E. Barbu, and M. Poesio. 2010. Strudel: A Corpus-Based Semantic Model Based on Properties and Types. Cognitive Science 34(2):222–254.","Barsalou, Lawrence W. 2008. Grounded Cogni-tion. Annual Review of Psychology 59:617–845.","Bengio, Y., P. Lamblin, D. Popovici, and H. Larochelle. 2007. Greedy Layer-Wise Train-ing of Deep Networks. In Bernhard Schölkopf, John Platt, and Thomas Hoffman, editors, Advances in Neural Information Processing Systems 19. MIT Press, pages 153–160.","Bengio, Yoshua. 2009. Learning Deep Architectures for AI. Foundations and Trends in Machine Learning 2(1):1–127.","Biemann, Chris. 2006. Chinese Whispers – an Efficient Graph Clustering Algorithm and its Application to Natural Language Processing Problems. In Proceedings of TextGraphs: the 1st Workshop on Graph Based Methods for Natural Language Processing. New York, NY, pages 73–80.","Blei, D. M., A. Y. Ng, and M. I. Jordan. 2003. Latent Dirichlet Allocation. Journal of Machine Learning Research 3:993–1022.","Bruni, E., G. Boleda, M. Baroni, and N. Tran. 2012a. Distributional Semantics in Technicolor. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics. Jeju Island, Korea, pages 136–145.","Bruni, E., U. Bordignon, A. Liska, J. Uijlings, and I. Sergienya. 2013. Vsem: An open library for visual semantics representation. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics: System Demonstrations. Sofia, Bulgaria, pages 187–192.","Bruni, E., N. Tran, and M. Baroni. 2014. Multimodal distributional semantics. J. Artif. Intell. Res. (JAIR) 49:1–47.","Bruni, E., J. Uijlings, M. Baroni, and N. Sebe. 2012b. Distributional Semantics with Eyes: Using Image Analysis to Improve Computational Representations of Word Meaning. In Proceedings of the 20th ACM International Conference on Multimedia. Nara, Japan, pages 1219–1228.","Collobert, R., J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu, and P. Kuksa. 2011. Natural Language Processing (almost) from Scratch. Journal of Machine Learning Research 12:2493–2537.","Deng, J., W. Dong, R. Socher, L. Li, K. Li, and L. Fei-Fei. 2009. ImageNet: A Large-Scale Hierarchical Image Database. In Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition. Miami, Florida, pages 248–255.","Farhadi, A., I. Endres, D. Hoiem, and D. Forsyth. 2009. Describing Objects by their Attributes. In Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition. Miami Beach, Florida, pages 1778–1785.","Feng, Fangxiang, Ruifan Li, and Xiaojie Wang. 2013. Constructing Hierarchical Image-tags Bimodal Representations for Word Tags Alternative Choice. In Proceedings of the ICML 2013 Workshop on Challenges in Representation Learning. Atlanta, Georgia.","Feng, Yansong and Mirella Lapata. 2010. Visual Information in Semantic Representation. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics. Los Angeles, California, pages 91–99.","Finkelstein, L., E. Gabrilovich, Y. Matias, E. Rivlin, Z. Solan, G. Wolfman, and E. Ruppin. 2002. Placing Search in Context: The Concept Revisited. ACM Transactions on Information Systems 20(1):116–131.","Fountain, Trevor and Mirella Lapata. 2010. Mean-ing Representation in Natural Language Categorization. In Proceedings of the 31st Annual Conference of the Cognitive Science Society. Amsterdam, The Netherlands, pages 1916– 1921. 730","Golub, Gene and Christian Reinsch. 1970. Singular Value Decomposition and Least Squares Solutions. Numerische Mathematik 14(5):403– 420.","Griffiths, T. L., M. Steyvers, and J. B. Tenenbaum. 2007. Topics in Semantic Representation. Psychological Review 114(2):211–244.","Hardoon, D. R., S. R. Szedmak, and J. R. Shawe-Taylor. 2004. Canonical Correlation Analysis: An Overview with Application to Learning Methods. Neural Computation 16(12):2639– 2664.","Harris, Zellig. 1970. Distributional Structure. In Papers in Structural and Transformational Linguistics, pages 775–794.","Hinton, Geoffrey E. and Ruslan R. Salakhutdinov. 2006. Reducing the Dimensionality of Data with Neural Networks. Science 313(5786):504– 507.","Hodosh, Micah, Peter Young, and Julia Hockenmaier. 2013. Framing Image Description as a Ranking Task: Data, Models and Evaluation Metrics. Journal of Artificial Intelligence Research 47:853–899.","Huang, Jing and Brian Kingsbury. 2013. Audio-visual Deep Learning for Noise Robust Speech Recognition. In Proceedings of the 38th International Conference on Acoustics, Speech, and Signal Processing. Vancouver, Canada, pages 7596–7599.","Jones, R., B. Rey, O. Madani, and W. Greiner. 2006. Generating Query Substititions. In Proceedings of the 15th International Conference on the World-Wide Web. Edinburgh, Scotland, pages 387–396.","Landau, B., L. Smith, and S. Jones. 1998. Object Perception and Object Naming in Early Development. Trends in Cognitive Science 27:19–24.","Landauer, Thomas and Susan T. Dumais. 1997. A Solution to Plato’s Problem: the Latent Semantic Analysis Theory of Acquisition, Induction, and Representation of Knowledge. Psychological Review 104(2):211–240.","Lowe, D. 2004. Distinctive Image Features from Scale-invariant Keypoints. International Journal of Computer Vision 60(2):91–110.","Manning, C. D., P. Raghavan, and H. Schütze. 2008. Introduction to Information Retrieval. Cambridge University Press, New York, NY.","McRae, K., G. S. Cree, M. S. Seidenberg, and C. McNorgan. 2005. Semantic Feature Production Norms for a Large Set of Living and Nonliving Things. Behavior Research Methods 37(4):547–559.","Mikolov, T., S. Kombrink, L. Burget, J. Černocký, and S. Khudanpur. 2011. Extensions of Recurrent Neural Network Language Model. In Proceedings of the 2011 IEEE International Conference on Acoustics, Speech, and Signal Processing. Prague, Czech Republic, pages 5528– 5531.","Mikolov, T., Wen-tau Yih, and G. Zweig. 2013. Linguistic Regularities in Continuous Space Word Representations. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Atlanta, Georgia, pages 746–751.","Miller, George A. and Walter G. Charles. 1991. Contextual Correlates of Semantic Similarity. Language and Cognitive Processes 6(1).","Nelson, D. L., C. L. McEvoy, and T. A. Schreiber. 1998. The University of South Florida Word Association, Rhyme, and Word Fragment Norms.","Ngiam, Jiquan, Aditya Khosla, Mingyu Kim, Juhan Nam, Honglak Lee, and Andrew Ng. 2011. Multimodal Deep Learning. In Proceedings of the 28th International Conference on Machine Learning. Bellevue, Washington, pages 689–696.","Patwardhan, Siddharth and Ted Pedersen. 2006. Using WordNet-based Context Vectors to Estimate the Semantic Relatedness of Concepts. In Proceedings of the EACL 2006 Workshop on Making Sense of Sense: Bringing Computational Linguistics and Psycholinguistics To-gether. Trento, Italy, pages 1–8.","Ranzato, Marc’Aurelio and Martin Szummer. 2008. Semi-supervised Learning of Compact Document Representations with Deep Networks. In Proceedings of the 25th International Conference on Machine Learning. Helsinki, Finland, pages 792–799.","Regier, Terry. 1996. The Human Semantic Potential. MIT Press, Cambridge, Massachusetts.","Roller, Stephen and Sabine Schulte im Walde. 2013. A Multimodal LDA Model integrating 731 Textual, Cognitive and Visual Modalities. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing. Seattle, Washington, pages 1146–1157.","Sebastiani, Fabrizio. 2002. Machine Learning in Automated Text Categorization. ACM Computing Surveys 34:1–47.","Silberer, C., V. Ferrari, and M. Lapata. 2013. Models of Semantic Representation with Visual Attributes. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics. Sofia, Bulgaria, pages 572–582.","Silberer, Carina and Mirella Lapata. 2012. Grounded Models of Semantic Representation. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning. Jeju Island, Korea, pages 1423–1433.","Socher, R., M. Ganjoo, C. D. Manning, and A. Y. Ng. 2013a. Zero-shot learning through cross-modal transfer. In Advances in Neural Information Processing Systems 26, pages 935–943.","Socher, R., Quoc V. Le, C. D. Manning, and A. Y. Ng. 2013b. Grounded Compositional Semantics for Finding and Describing Images with Sentences. In Proceedings of the NIPS Deep Learning Workshop.","Socher, R., J. Pennington, E. H. Huang, A. Y. Ng, and C. D. Manning. 2011. Semi-Supervised Recursive Autoencoders for Predicting Sentiment Distributions. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing. Edinburgh, Scotland, pages 151–161.","Srivastava, Nitish and Ruslan Salakhutdinov. 2012. Multimodal Learning with Deep Boltzmann Machines. In Advances in Neural Information Processing Systems 25, pages 2231– 2239.","Steyvers, Mark. 2010. Combining Feature Norms and Text Data with Topic Models. Acta Psychologica 133(3):234–342.","Szumlanski, S. R., F. Gomez, and V. K. Sims. 2013. A New Set of Norms for Semantic Relatedness Measures. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics. Sofia, Bulgaria, pages 890– 895.","Turney, Peter D. and Patrick Pantel. 2010. From Frequency to Meaning: Vector Space Models of Semantics. Journal of Artificial Intelligence Research 37(1):141–188.","Vincent, P., H. Larochelle, I. Lajoie, Y. Bengio, and P. Manzagol. 2010. Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion. Journal of Machine Learning Research 11:3371–3408.","von Ahn, Luis and Laura Dabbish. 2004. Labeling Images with a Computer Game. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. Vienna, Austria, pages 319–326.","Wu, Pengcheng, Steven C. H. Hoi, Hao Xia, Peilin Zhao, Dayong Wang, and Chunyan Miao. 2013. Online Multimodal Deep Similarity Learning with Application to Image Retrieval. In Proceedings of the 21st ACM International Conference on Multimedia. Barcelona, Spain, pages 153–162.","Yih, Wen-tau, Ming-Wei Chang, Christopher Meek, and Andrzej Pastusiak. 2013. Question Answering Using Enhanced Lexical Semantic Models. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics. Sofia, Bulgaria, pages 1744–1753.","Yu, C. and D. H. Ballard. 2007. A Unified Model of Early Word Learning Integrating Statistical and Social Cues. Neurocomputing 70:2149– 2165. 732"]}],"references":[{"authors":[{"last":"Agirre"},{"last":"Eneko"},{"first":"Aitor","last":"Soroa"}],"year":"2007","title":"SemEval-2007 Task 02: Evaluating Word Sense Induc-tion and Discrimination Systems"},{"authors":[{"first":"M.","last":"Andrews"},{"first":"G.","last":"Vigliocco"},{"first":"D.","last":"Vinson"}],"year":"2009","title":"Integrating Experiential and Distributional Data to Learn Semantic Representations"},{"authors":[{"first":"M.","last":"Baroni"},{"first":"B.","last":"Murphy"},{"first":"E.","last":"Barbu"},{"first":"M.","last":"Poesio"}],"year":"2010","title":"Strudel: A Corpus-Based Semantic Model Based on Properties and Types"},{"authors":[{"last":"Barsalou"},{"first":"Lawrence","last":"W"}],"year":"2008","title":"Grounded Cogni-tion"},{"authors":[{"first":"Y.","last":"Bengio"},{"first":"P.","last":"Lamblin"},{"first":"D.","last":"Popovici"},{"first":"H.","last":"Larochelle"}],"year":"2007","title":"Greedy Layer-Wise Train-ing of Deep Networks"},{"authors":[{"last":"Bengio"},{"last":"Yoshua"}],"year":"2009","title":"Learning Deep Architectures for AI"},{"authors":[{"last":"Biemann"},{"last":"Chris"}],"year":"2006","title":"Chinese Whispers – an Efficient Graph Clustering Algorithm and its Application to Natural Language Processing Problems"},{"authors":[{"first":"D.","middle":"M.","last":"Blei"},{"first":"A.","middle":"Y.","last":"Ng"},{"first":"M.","middle":"I.","last":"Jordan"}],"year":"2003","title":"Latent Dirichlet Allocation"},{"authors":[{"first":"E.","last":"Bruni"},{"first":"G.","last":"Boleda"},{"first":"M.","last":"Baroni"},{"first":"N.","last":"Tran"}],"year":"2012a","title":"Distributional Semantics in Technicolor"},{"authors":[{"first":"E.","last":"Bruni"},{"first":"U.","last":"Bordignon"},{"first":"A.","last":"Liska"},{"first":"J.","last":"Uijlings"},{"first":"I.","last":"Sergienya"}],"year":"2013","title":"Vsem: An open library for visual semantics representation"},{"authors":[{"first":"E.","last":"Bruni"},{"first":"N.","last":"Tran"},{"first":"M.","last":"Baroni"}],"year":"2014","title":"Multimodal distributional semantics"},{"authors":[{"first":"E.","last":"Bruni"},{"first":"J.","last":"Uijlings"},{"first":"M.","last":"Baroni"},{"first":"N.","last":"Sebe"}],"year":"2012b","title":"Distributional Semantics with Eyes: Using Image Analysis to Improve Computational Representations of Word Meaning"},{"authors":[{"first":"R.","last":"Collobert"},{"first":"J.","last":"Weston"},{"first":"L.","last":"Bottou"},{"first":"M.","last":"Karlen"},{"first":"K.","last":"Kavukcuoglu"},{"first":"P.","last":"Kuksa"}],"year":"2011","title":"Natural Language Processing (almost) from Scratch"},{"authors":[{"first":"J.","last":"Deng"},{"first":"W.","last":"Dong"},{"first":"R.","last":"Socher"},{"first":"L.","last":"Li"},{"first":"K.","last":"Li"},{"first":"L.","last":"Fei-Fei"}],"year":"2009","title":"ImageNet: A Large-Scale Hierarchical Image Database"},{"authors":[{"first":"A.","last":"Farhadi"},{"first":"I.","last":"Endres"},{"first":"D.","last":"Hoiem"},{"first":"D.","last":"Forsyth"}],"year":"2009","title":"Describing Objects by their Attributes"},{"authors":[{"last":"Feng"},{"last":"Fangxiang"},{"first":"Ruifan","last":"Li"},{"first":"Xiaojie","last":"Wang"}],"year":"2013","title":"Constructing Hierarchical Image-tags Bimodal Representations for Word Tags Alternative Choice"},{"authors":[{"last":"Feng"},{"last":"Yansong"},{"first":"Mirella","last":"Lapata"}],"year":"2010","title":"Visual Information in Semantic Representation"},{"authors":[{"first":"L.","last":"Finkelstein"},{"first":"E.","last":"Gabrilovich"},{"first":"Y.","last":"Matias"},{"first":"E.","last":"Rivlin"},{"first":"Z.","last":"Solan"},{"first":"G.","last":"Wolfman"},{"first":"E.","last":"Ruppin"}],"year":"2002","title":"Placing Search in Context: The Concept Revisited"},{"authors":[{"last":"Fountain"},{"last":"Trevor"},{"first":"Mirella","last":"Lapata"}],"year":"2010","title":"Mean-ing Representation in Natural Language Categorization"},{"authors":[{"last":"Golub"},{"last":"Gene"},{"first":"Christian","last":"Reinsch"}],"year":"1970","title":"Singular Value Decomposition and Least Squares Solutions"},{"authors":[{"first":"T.","middle":"L.","last":"Griffiths"},{"first":"M.","last":"Steyvers"},{"first":"J.","middle":"B.","last":"Tenenbaum"}],"year":"2007","title":"Topics in Semantic Representation"},{"authors":[{"first":"D.","middle":"R.","last":"Hardoon"},{"first":"S.","middle":"R.","last":"Szedmak"},{"first":"J.","middle":"R.","last":"Shawe-Taylor"}],"year":"2004","title":"Canonical Correlation Analysis: An Overview with Application to Learning Methods"},{"authors":[{"last":"Harris"},{"last":"Zellig"}],"year":"1970","title":"Distributional Structure"},{"authors":[{"last":"Hinton"},{"first":"Geoffrey","last":"E."},{"first":"Ruslan","middle":"R.","last":"Salakhutdinov"}],"year":"2006","title":"Reducing the Dimensionality of Data with Neural Networks"},{"authors":[{"last":"Hodosh"},{"last":"Micah"},{"first":"Peter","last":"Young"},{"first":"Julia","last":"Hockenmaier"}],"year":"2013","title":"Framing Image Description as a Ranking Task: Data, Models and Evaluation Metrics"},{"authors":[{"last":"Huang"},{"last":"Jing"},{"first":"Brian","last":"Kingsbury"}],"year":"2013","title":"Audio-visual Deep Learning for Noise Robust Speech Recognition"},{"authors":[{"first":"R.","last":"Jones"},{"first":"B.","last":"Rey"},{"first":"O.","last":"Madani"},{"first":"W.","last":"Greiner"}],"year":"2006","title":"Generating Query Substititions"},{"authors":[{"first":"B.","last":"Landau"},{"first":"L.","last":"Smith"},{"first":"S.","last":"Jones"}],"year":"1998","title":"Object Perception and Object Naming in Early Development"},{"authors":[{"last":"Landauer"},{"last":"Thomas"},{"first":"Susan","middle":"T.","last":"Dumais"}],"year":"1997","title":"A Solution to Plato’s Problem: the Latent Semantic Analysis Theory of Acquisition, Induction, and Representation of Knowledge"},{"authors":[{"last":"Lowe"},{"last":"D"}],"year":"2004","title":"Distinctive Image Features from Scale-invariant Keypoints"},{"authors":[{"first":"C.","middle":"D.","last":"Manning"},{"first":"P.","last":"Raghavan"},{"first":"H.","last":"Schütze"}],"year":"2008","title":"Introduction to Information Retrieval"},{"authors":[{"first":"K.","last":"McRae"},{"first":"G.","middle":"S.","last":"Cree"},{"first":"M.","middle":"S.","last":"Seidenberg"},{"first":"C.","last":"McNorgan"}],"year":"2005","title":"Semantic Feature Production Norms for a Large Set of Living and Nonliving Things"},{"authors":[{"first":"T.","last":"Mikolov"},{"first":"S.","last":"Kombrink"},{"first":"L.","last":"Burget"},{"first":"J.","last":"Černocký"},{"first":"S.","last":"Khudanpur"}],"year":"2011","title":"Extensions of Recurrent Neural Network Language Model"},{"authors":[{"first":"T.","last":"Mikolov"},{"first":"Wen-tau","last":"Yih"},{"first":"G.","last":"Zweig"}],"year":"2013","title":"Linguistic Regularities in Continuous Space Word Representations"},{"authors":[{"last":"Miller"},{"first":"George","last":"A."},{"first":"Walter","middle":"G.","last":"Charles"}],"year":"1991","title":"Contextual Correlates of Semantic Similarity"},{"authors":[{"first":"D.","middle":"L.","last":"Nelson"},{"first":"C.","middle":"L.","last":"McEvoy"},{"first":"T.","middle":"A.","last":"Schreiber"}],"year":"1998","title":"The University of South Florida Word Association, Rhyme, and Word Fragment Norms"},{"authors":[{"last":"Ngiam"},{"last":"Jiquan"},{"first":"Aditya","last":"Khosla"},{"first":"Mingyu","last":"Kim"},{"first":"Juhan","last":"Nam"},{"first":"Honglak","last":"Lee"},{"first":"Andrew","last":"Ng"}],"year":"2011","title":"Multimodal Deep Learning"},{"authors":[{"last":"Patwardhan"},{"last":"Siddharth"},{"first":"Ted","last":"Pedersen"}],"year":"2006","title":"Using WordNet-based Context Vectors to Estimate the Semantic Relatedness of Concepts"},{"authors":[{"last":"Ranzato"},{"last":"Marc’Aurelio"},{"first":"Martin","last":"Szummer"}],"year":"2008","title":"Semi-supervised Learning of Compact Document Representations with Deep Networks"},{"authors":[{"last":"Regier"},{"last":"Terry"}],"year":"1996","title":"The Human Semantic Potential"},{"authors":[{"last":"Roller"},{"last":"Stephen"},{"first":"Sabine","middle":"Schulte im","last":"Walde"}],"year":"2013","title":"A Multimodal LDA Model integrating 731 Textual, Cognitive and Visual Modalities"},{"authors":[{"last":"Sebastiani"},{"last":"Fabrizio"}],"year":"2002","title":"Machine Learning in Automated Text Categorization"},{"authors":[{"first":"C.","last":"Silberer"},{"first":"V.","last":"Ferrari"},{"first":"M.","last":"Lapata"}],"year":"2013","title":"Models of Semantic Representation with Visual Attributes"},{"authors":[{"last":"Silberer"},{"last":"Carina"},{"first":"Mirella","last":"Lapata"}],"year":"2012","title":"Grounded Models of Semantic Representation"},{"authors":[{"first":"R.","last":"Socher"},{"first":"M.","last":"Ganjoo"},{"first":"C.","middle":"D.","last":"Manning"},{"first":"A.","middle":"Y.","last":"Ng"}],"year":"2013a","title":"Zero-shot learning through cross-modal transfer"},{"authors":[{"first":"R.","last":"Socher"},{"first":"Quoc V","middle":".","last":"Le"},{"first":"C.","middle":"D.","last":"Manning"},{"first":"A.","middle":"Y.","last":"Ng"}],"year":"2013b","title":"Grounded Compositional Semantics for Finding and Describing Images with Sentences"},{"authors":[{"first":"R.","last":"Socher"},{"first":"J.","last":"Pennington"},{"first":"E.","middle":"H.","last":"Huang"},{"first":"A.","middle":"Y.","last":"Ng"},{"first":"C.","middle":"D.","last":"Manning"}],"year":"2011","title":"Semi-Supervised Recursive Autoencoders for Predicting Sentiment Distributions"},{"authors":[{"last":"Srivastava"},{"last":"Nitish"},{"first":"Ruslan","last":"Salakhutdinov"}],"year":"2012","title":"Multimodal Learning with Deep Boltzmann Machines"},{"authors":[{"last":"Steyvers"},{"last":"Mark"}],"year":"2010","title":"Combining Feature Norms and Text Data with Topic Models"},{"authors":[{"first":"S.","middle":"R.","last":"Szumlanski"},{"first":"F.","last":"Gomez"},{"first":"V.","middle":"K.","last":"Sims"}],"year":"2013","title":"A New Set of Norms for Semantic Relatedness Measures"},{"authors":[{"last":"Turney"},{"first":"Peter","last":"D."},{"first":"Patrick","last":"Pantel"}],"year":"2010","title":"From Frequency to Meaning: Vector Space Models of Semantics"},{"authors":[{"first":"P.","last":"Vincent"},{"first":"H.","last":"Larochelle"},{"first":"I.","last":"Lajoie"},{"first":"Y.","last":"Bengio"},{"first":"P.","last":"Manzagol"}],"year":"2010","title":"Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion"},{"authors":[{"last":"von Ahn"},{"last":"Luis"},{"first":"Laura","last":"Dabbish"}],"year":"2004","title":"Labeling Images with a Computer Game"},{"authors":[{"last":"Wu"},{"last":"Pengcheng"},{"first":"Steven","middle":"C. H.","last":"Hoi"},{"first":"Hao","last":"Xia"},{"first":"Peilin","last":"Zhao"},{"first":"Dayong","last":"Wang"},{"first":"Chunyan","last":"Miao"}],"year":"2013","title":"Online Multimodal Deep Similarity Learning with Application to Image Retrieval"},{"authors":[{"last":"Yih"},{"last":"Wen-tau"},{"first":"Ming-Wei","last":"Chang"},{"first":"Christopher","last":"Meek"},{"first":"Andrzej","last":"Pastusiak"}],"year":"2013","title":"Question Answering Using Enhanced Lexical Semantic Models"},{"authors":[{"first":"C.","last":"Yu"},{"first":"D.","middle":"H.","last":"Ballard"}],"year":"2007","title":"A Unified Model of Early Word Learning Integrating Statistical and Social Cues"}],"cites":[{"authors":[{"last":"Turney"},{"last":"Pantel"}],"year":"2010","style":0},{"authors":[{"last":"Collobert"},{"last":"al."}],"year":"2011","style":0,"reference":{"authors":[{"first":"R.","last":"Collobert"},{"first":"J.","last":"Weston"},{"first":"L.","last":"Bottou"},{"first":"M.","last":"Karlen"},{"first":"K.","last":"Kavukcuoglu"},{"first":"P.","last":"Kuksa"}],"year":"2011","title":"Natural Language Processing (almost) from Scratch"}},{"authors":[{"last":"Mikolov"},{"last":"al."}],"year":"2013","style":0,"reference":{"authors":[{"first":"T.","last":"Mikolov"},{"first":"Wen-tau","last":"Yih"},{"first":"G.","last":"Zweig"}],"year":"2013","title":"Linguistic Regularities in Continuous Space Word Representations"}},{"authors":[{"last":"Manning"},{"last":"al."}],"year":"2008","style":0,"reference":{"authors":[{"first":"C.","middle":"D.","last":"Manning"},{"first":"P.","last":"Raghavan"},{"first":"H.","last":"Schütze"}],"year":"2008","title":"Introduction to Information Retrieval"}},{"authors":[{"last":"Jones"},{"last":"al."}],"year":"2006","style":0,"reference":{"authors":[{"first":"R.","last":"Jones"},{"first":"B.","last":"Rey"},{"first":"O.","last":"Madani"},{"first":"W.","last":"Greiner"}],"year":"2006","title":"Generating Query Substititions"}},{"authors":[{"last":"Sebastiani"}],"year":"2002","style":0},{"authors":[{"last":"Yih"},{"last":"al."}],"year":"2013","style":0,"reference":{"authors":[{"last":"Yih"},{"last":"Wen-tau"},{"first":"Ming-Wei","last":"Chang"},{"first":"Christopher","last":"Meek"},{"first":"Andrzej","last":"Pastusiak"}],"year":"2013","title":"Question Answering Using Enhanced Lexical Semantic Models"}},{"authors":[{"last":"Griffiths"},{"last":"al."}],"year":"2007","style":0,"reference":{"authors":[{"first":"T.","middle":"L.","last":"Griffiths"},{"first":"M.","last":"Steyvers"},{"first":"J.","middle":"B.","last":"Tenenbaum"}],"year":"2007","title":"Topics in Semantic Representation"}},{"authors":[{"last":"Harris"}],"year":"1970","style":0},{"authors":[{"last":"Regier"}],"year":"1996","style":0},{"authors":[{"last":"Landau"},{"last":"al."}],"year":"1998","style":0,"reference":{"authors":[{"first":"B.","last":"Landau"},{"first":"L.","last":"Smith"},{"first":"S.","last":"Jones"}],"year":"1998","title":"Object Perception and Object Naming in Early Development"}},{"authors":[{"last":"Barsalou"}],"year":"2008","style":0},{"authors":[{"last":"Andrews"},{"last":"al."}],"year":"2009","style":0,"reference":{"authors":[{"first":"M.","last":"Andrews"},{"first":"G.","last":"Vigliocco"},{"first":"D.","last":"Vinson"}],"year":"2009","title":"Integrating Experiential and Distributional Data to Learn Semantic Representations"}},{"authors":[{"last":"Steyvers"}],"year":"2010","style":0},{"authors":[{"last":"Silberer"},{"last":"Lapata"}],"year":"2012","style":0},{"authors":[{"last":"Feng"},{"last":"Lapata"}],"year":"2010","style":0},{"authors":[{"last":"Bruni"},{"last":"al."}],"year":"2012a","style":0,"reference":{"authors":[{"first":"E.","last":"Bruni"},{"first":"G.","last":"Boleda"},{"first":"M.","last":"Baroni"},{"first":"N.","last":"Tran"}],"year":"2012a","title":"Distributional Semantics in Technicolor"}},{"authors":[{"last":"Silberer"},{"last":"al."}],"year":"2013","style":0,"reference":{"authors":[{"first":"C.","last":"Silberer"},{"first":"V.","last":"Ferrari"},{"first":"M.","last":"Lapata"}],"year":"2013","title":"Models of Semantic Representation with Visual Attributes"}},{"authors":[{"last":"Walde"}],"year":"2013","style":0},{"authors":[{"last":"Bengio"},{"last":"al."}],"year":"2007","style":0,"reference":{"authors":[{"first":"Y.","last":"Bengio"},{"first":"P.","last":"Lamblin"},{"first":"D.","last":"Popovici"},{"first":"H.","last":"Larochelle"}],"year":"2007","title":"Greedy Layer-Wise Train-ing of Deep Networks"}},{"authors":[{"last":"Ngiam"},{"last":"al."}],"year":"2011","style":0,"reference":{"authors":[{"last":"Ngiam"},{"last":"Jiquan"},{"first":"Aditya","last":"Khosla"},{"first":"Mingyu","last":"Kim"},{"first":"Juhan","last":"Nam"},{"first":"Honglak","last":"Lee"},{"first":"Andrew","last":"Ng"}],"year":"2011","title":"Multimodal Deep Learning"}},{"authors":[{"last":"Srivastava"},{"last":"Salakhutdinov"}],"year":"2012","style":0},{"authors":[{"last":"Silberer"},{"last":"al."}],"year":"2013","style":0,"reference":{"authors":[{"first":"C.","last":"Silberer"},{"first":"V.","last":"Ferrari"},{"first":"M.","last":"Lapata"}],"year":"2013","title":"Models of Semantic Representation with Visual Attributes"}},{"authors":[{"last":"McRae"},{"last":"al."}],"year":"2005","style":0,"reference":{"authors":[{"first":"K.","last":"McRae"},{"first":"G.","middle":"S.","last":"Cree"},{"first":"M.","middle":"S.","last":"Seidenberg"},{"first":"C.","last":"McNorgan"}],"year":"2005","title":"Semantic Feature Production Norms for a Large Set of Living and Nonliving Things"}},{"authors":[{"last":"Farhadi"},{"last":"al."}],"year":"2009","style":0,"reference":{"authors":[{"first":"A.","last":"Farhadi"},{"first":"I.","last":"Endres"},{"first":"D.","last":"Hoiem"},{"first":"D.","last":"Forsyth"}],"year":"2009","title":"Describing Objects by their Attributes"}},{"authors":[{"last":"Finkelstein"},{"last":"al."}],"year":"2002","style":0,"reference":{"authors":[{"first":"L.","last":"Finkelstein"},{"first":"E.","last":"Gabrilovich"},{"first":"Y.","last":"Matias"},{"first":"E.","last":"Rivlin"},{"first":"Z.","last":"Solan"},{"first":"G.","last":"Wolfman"},{"first":"E.","last":"Ruppin"}],"year":"2002","title":"Placing Search in Context: The Concept Revisited"}},{"authors":[{"last":"Landauer"},{"last":"Dumais"}],"year":"1997","style":0},{"authors":[{"last":"Bruni"},{"last":"al."}],"year":"2012b","style":0,"reference":{"authors":[{"first":"E.","last":"Bruni"},{"first":"J.","last":"Uijlings"},{"first":"M.","last":"Baroni"},{"first":"N.","last":"Sebe"}],"year":"2012b","title":"Distributional Semantics with Eyes: Using Image Analysis to Improve Computational Representations of Word Meaning"}},{"authors":[{"last":"Blei"},{"last":"al."}],"year":"2003","style":0,"reference":{"authors":[{"first":"D.","middle":"M.","last":"Blei"},{"first":"A.","middle":"Y.","last":"Ng"},{"first":"M.","middle":"I.","last":"Jordan"}],"year":"2003","title":"Latent Dirichlet Allocation"}},{"authors":[{"last":"Feng"},{"last":"Lapata"}],"year":"2010","style":0},{"authors":[{"last":"Steyvers"}],"year":"2010","style":0},{"authors":[{"last":"Andrews"},{"last":"al."}],"year":"2009","style":0,"reference":{"authors":[{"first":"M.","last":"Andrews"},{"first":"G.","last":"Vigliocco"},{"first":"D.","last":"Vinson"}],"year":"2009","title":"Integrating Experiential and Distributional Data to Learn Semantic Representations"}},{"authors":[{"last":"Silberer"},{"last":"Lapata"}],"year":"2012","style":0},{"authors":[{"last":"McRae"},{"last":"al."}],"year":"2005","style":0,"reference":{"authors":[{"first":"K.","last":"McRae"},{"first":"G.","middle":"S.","last":"Cree"},{"first":"M.","middle":"S.","last":"Seidenberg"},{"first":"C.","last":"McNorgan"}],"year":"2005","title":"Semantic Feature Production Norms for a Large Set of Living and Nonliving Things"}},{"authors":[{"last":"Walde"}],"year":"2013","style":0},{"authors":[{"last":"Silberer"},{"last":"al."}],"year":"2013","style":0,"reference":{"authors":[{"first":"C.","last":"Silberer"},{"first":"V.","last":"Ferrari"},{"first":"M.","last":"Lapata"}],"year":"2013","title":"Models of Semantic Representation with Visual Attributes"}},{"authors":[{"last":"Srivastava"},{"last":"Salakhutdinov"}],"year":"2012","style":0},{"authors":[{"last":"Feng"},{"last":"al."}],"year":"2013","style":0,"reference":{"authors":[{"last":"Feng"},{"last":"Fangxiang"},{"first":"Ruifan","last":"Li"},{"first":"Xiaojie","last":"Wang"}],"year":"2013","title":"Constructing Hierarchical Image-tags Bimodal Representations for Word Tags Alternative Choice"}},{"authors":[{"last":"Wu"},{"last":"al."}],"year":"2013","style":0,"reference":{"authors":[{"last":"Wu"},{"last":"Pengcheng"},{"first":"Steven","middle":"C. H.","last":"Hoi"},{"first":"Hao","last":"Xia"},{"first":"Peilin","last":"Zhao"},{"first":"Dayong","last":"Wang"},{"first":"Chunyan","last":"Miao"}],"year":"2013","title":"Online Multimodal Deep Similarity Learning with Application to Image Retrieval"}},{"authors":[{"last":"Socher"},{"last":"al."}],"year":"2013b","style":0,"reference":{"authors":[{"first":"R.","last":"Socher"},{"first":"Quoc V","middle":".","last":"Le"},{"first":"C.","middle":"D.","last":"Manning"},{"first":"A.","middle":"Y.","last":"Ng"}],"year":"2013b","title":"Grounded Compositional Semantics for Finding and Describing Images with Sentences"}},{"authors":[{"last":"Ngiam"},{"last":"al."}],"year":"2011","style":0,"reference":{"authors":[{"last":"Ngiam"},{"last":"Jiquan"},{"first":"Aditya","last":"Khosla"},{"first":"Mingyu","last":"Kim"},{"first":"Juhan","last":"Nam"},{"first":"Honglak","last":"Lee"},{"first":"Andrew","last":"Ng"}],"year":"2011","title":"Multimodal Deep Learning"}},{"authors":[{"last":"Huang"},{"last":"Kingsbury"}],"year":"2013","style":0},{"authors":[{"last":"Bengio"}],"year":"2009","style":0},{"authors":[{"last":"Vincent"},{"last":"al."}],"year":"2010","style":0,"reference":{"authors":[{"first":"P.","last":"Vincent"},{"first":"H.","last":"Larochelle"},{"first":"I.","last":"Lajoie"},{"first":"Y.","last":"Bengio"},{"first":"P.","last":"Manzagol"}],"year":"2010","title":"Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion"}},{"authors":[{"last":"Bengio"},{"last":"al."}],"year":"2007","style":0,"reference":{"authors":[{"first":"Y.","last":"Bengio"},{"first":"P.","last":"Lamblin"},{"first":"D.","last":"Popovici"},{"first":"H.","last":"Larochelle"}],"year":"2007","title":"Greedy Layer-Wise Train-ing of Deep Networks"}},{"authors":[{"last":"Vincent"},{"last":"al."}],"year":"2010","style":0,"reference":{"authors":[{"first":"P.","last":"Vincent"},{"first":"H.","last":"Larochelle"},{"first":"I.","last":"Lajoie"},{"first":"Y.","last":"Bengio"},{"first":"P.","last":"Manzagol"}],"year":"2010","title":"Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion"}},{"authors":[{"last":"Bengio"}],"year":"2009","style":0},{"authors":[{"last":"Hinton"},{"last":"Salakhutdinov"}],"year":"2006","style":0},{"authors":[{"last":"Ranzato"},{"last":"Szummer"}],"year":"2008","style":0},{"authors":[{"last":"Socher"},{"last":"al."}],"year":"2011","style":0,"reference":{"authors":[{"first":"R.","last":"Socher"},{"first":"J.","last":"Pennington"},{"first":"E.","middle":"H.","last":"Huang"},{"first":"A.","middle":"Y.","last":"Ng"},{"first":"C.","middle":"D.","last":"Manning"}],"year":"2011","title":"Semi-Supervised Recursive Autoencoders for Predicting Sentiment Distributions"}},{"authors":[{"last":"Baroni"},{"last":"al."}],"year":"2010","style":0,"reference":{"authors":[{"first":"M.","last":"Baroni"},{"first":"B.","last":"Murphy"},{"first":"E.","last":"Barbu"},{"first":"M.","last":"Poesio"}],"year":"2010","title":"Strudel: A Corpus-Based Semantic Model Based on Properties and Types"}},{"authors":[{"last":"Silberer"},{"last":"al."}],"year":"2013","style":0,"reference":{"authors":[{"first":"C.","last":"Silberer"},{"first":"V.","last":"Ferrari"},{"first":"M.","last":"Lapata"}],"year":"2013","title":"Models of Semantic Representation with Visual Attributes"}},{"authors":[{"last":"Farhadi"},{"last":"al."}],"year":"2009","style":0,"reference":{"authors":[{"first":"A.","last":"Farhadi"},{"first":"I.","last":"Endres"},{"first":"D.","last":"Hoiem"},{"first":"D.","last":"Forsyth"}],"year":"2009","title":"Describing Objects by their Attributes"}},{"authors":[{"last":"Deng"},{"last":"al."}],"year":"2009","style":0,"reference":{"authors":[{"first":"J.","last":"Deng"},{"first":"W.","last":"Dong"},{"first":"R.","last":"Socher"},{"first":"L.","last":"Li"},{"first":"K.","last":"Li"},{"first":"L.","last":"Fei-Fei"}],"year":"2009","title":"ImageNet: A Large-Scale Hierarchical Image Database"}},{"authors":[{"last":"Nelson"},{"last":"al."}],"year":"1998","style":0,"reference":{"authors":[{"first":"D.","middle":"L.","last":"Nelson"},{"first":"C.","middle":"L.","last":"McEvoy"},{"first":"T.","middle":"A.","last":"Schreiber"}],"year":"1998","title":"The University of South Florida Word Association, Rhyme, and Word Fragment Norms"}},{"authors":[{"last":"McRae"},{"last":"al."}],"year":"2005","style":0,"reference":{"authors":[{"first":"K.","last":"McRae"},{"first":"G.","middle":"S.","last":"Cree"},{"first":"M.","middle":"S.","last":"Seidenberg"},{"first":"C.","last":"McNorgan"}],"year":"2005","title":"Semantic Feature Production Norms for a Large Set of Living and Nonliving Things"}},{"authors":[{"last":"Silberer"},{"last":"al."}],"year":"2013","style":0,"reference":{"authors":[{"first":"C.","last":"Silberer"},{"first":"V.","last":"Ferrari"},{"first":"M.","last":"Lapata"}],"year":"2013","title":"Models of Semantic Representation with Visual Attributes"}},{"authors":[{"last":"Finkelstein"},{"last":"al."}],"year":"2002","style":0,"reference":{"authors":[{"first":"L.","last":"Finkelstein"},{"first":"E.","last":"Gabrilovich"},{"first":"Y.","last":"Matias"},{"first":"E.","last":"Rivlin"},{"first":"Z.","last":"Solan"},{"first":"G.","last":"Wolfman"},{"first":"E.","last":"Ruppin"}],"year":"2002","title":"Placing Search in Context: The Concept Revisited"}},{"authors":[{"last":"Szumlanski"},{"last":"al."}],"year":"2013","style":0,"reference":{"authors":[{"first":"S.","middle":"R.","last":"Szumlanski"},{"first":"F.","last":"Gomez"},{"first":"V.","middle":"K.","last":"Sims"}],"year":"2013","title":"A New Set of Norms for Semantic Relatedness Measures"}},{"authors":[{"last":"McRae"},{"last":"al."}],"year":"2005","style":0,"reference":{"authors":[{"first":"K.","last":"McRae"},{"first":"G.","middle":"S.","last":"Cree"},{"first":"M.","middle":"S.","last":"Seidenberg"},{"first":"C.","last":"McNorgan"}],"year":"2005","title":"Semantic Feature Production Norms for a Large Set of Living and Nonliving Things"}},{"authors":[{"last":"McRae"},{"last":"al."}],"year":"2005","style":0,"reference":{"authors":[{"first":"K.","last":"McRae"},{"first":"G.","middle":"S.","last":"Cree"},{"first":"M.","middle":"S.","last":"Seidenberg"},{"first":"C.","last":"McNorgan"}],"year":"2005","title":"Semantic Feature Production Norms for a Large Set of Living and Nonliving Things"}},{"authors":[{"last":"Patwardhan"},{"last":"Pedersen"}],"year":"2006","style":0},{"authors":[{"last":"Miller"},{"last":"Charles"}],"year":"1991","style":0},{"authors":[{"last":"McRae"},{"last":"al."}],"year":"2005","style":0,"reference":{"authors":[{"first":"K.","last":"McRae"},{"first":"G.","middle":"S.","last":"Cree"},{"first":"M.","middle":"S.","last":"Seidenberg"},{"first":"C.","last":"McNorgan"}],"year":"2005","title":"Semantic Feature Production Norms for a Large Set of Living and Nonliving Things"}},{"authors":[{"last":"Patwardhan"},{"last":"Pedersen’s"}],"year":"2006","style":0},{"authors":[{"last":"Miller"},{"last":"Charles"}],"year":"1991","style":0},{"authors":[{"last":"Fountain"},{"last":"Lapata"}],"year":"2010","style":0},{"authors":[{"last":"Biemann"}],"year":"2006","style":0},{"authors":[{"last":"McRae"},{"last":"al."}],"year":"2005","style":0,"reference":{"authors":[{"first":"K.","last":"McRae"},{"first":"G.","middle":"S.","last":"Cree"},{"first":"M.","middle":"S.","last":"Seidenberg"},{"first":"C.","last":"McNorgan"}],"year":"2005","title":"Semantic Feature Production Norms for a Large Set of Living and Nonliving Things"}},{"authors":[{"last":"Fountain"},{"last":"Lapata"}],"year":"2010","style":0},{"authors":[{"last":"Baroni"},{"last":"al."}],"year":"2010","style":0,"reference":{"authors":[{"first":"M.","last":"Baroni"},{"first":"B.","last":"Murphy"},{"first":"E.","last":"Barbu"},{"first":"M.","last":"Poesio"}],"year":"2010","title":"Strudel: A Corpus-Based Semantic Model Based on Properties and Types"}},{"authors":[{"last":"McRae"},{"last":"al."}],"year":"2005","style":0,"reference":{"authors":[{"first":"K.","last":"McRae"},{"first":"G.","middle":"S.","last":"Cree"},{"first":"M.","middle":"S.","last":"Seidenberg"},{"first":"C.","last":"McNorgan"}],"year":"2005","title":"Semantic Feature Production Norms for a Large Set of Living and Nonliving Things"}},{"authors":[{"last":"Fountain"},{"last":"Lapata"}],"year":"2010","style":0},{"authors":[{"last":"Agirre"},{"last":"Soroa"}],"year":"2007","style":0},{"authors":[{"last":"Hardoon"},{"last":"al."}],"year":"2004","style":0,"reference":{"authors":[{"first":"D.","middle":"R.","last":"Hardoon"},{"first":"S.","middle":"R.","last":"Szedmak"},{"first":"J.","middle":"R.","last":"Shawe-Taylor"}],"year":"2004","title":"Canonical Correlation Analysis: An Overview with Application to Learning Methods"}},{"authors":[{"last":"Silberer"},{"last":"al."}],"year":"2013","style":0,"reference":{"authors":[{"first":"C.","last":"Silberer"},{"first":"V.","last":"Ferrari"},{"first":"M.","last":"Lapata"}],"year":"2013","title":"Models of Semantic Representation with Visual Attributes"}},{"authors":[{"last":"Golub"},{"last":"Reinsch"}],"year":"1970","style":0},{"authors":[{"last":"McRae"},{"last":"al."}],"year":"2005","style":0,"reference":{"authors":[{"first":"K.","last":"McRae"},{"first":"G.","middle":"S.","last":"Cree"},{"first":"M.","middle":"S.","last":"Seidenberg"},{"first":"C.","last":"McNorgan"}],"year":"2005","title":"Semantic Feature Production Norms for a Large Set of Living and Nonliving Things"}},{"authors":[{"last":"Lowe"}],"year":"2004","style":0},{"authors":[{"last":"Ahn"},{"last":"Dabbish"}],"year":"2004","style":0},{"authors":[{"last":"McRae"},{"last":"al."}],"year":"2005","style":0,"reference":{"authors":[{"first":"K.","last":"McRae"},{"first":"G.","middle":"S.","last":"Cree"},{"first":"M.","middle":"S.","last":"Seidenberg"},{"first":"C.","last":"McNorgan"}],"year":"2005","title":"Semantic Feature Production Norms for a Large Set of Living and Nonliving Things"}},{"authors":[{"last":"Bruni"},{"last":"al."}],"year":"2013","style":0,"reference":{"authors":[{"first":"E.","last":"Bruni"},{"first":"U.","last":"Bordignon"},{"first":"A.","last":"Liska"},{"first":"J.","last":"Uijlings"},{"first":"I.","last":"Sergienya"}],"year":"2013","title":"Vsem: An open library for visual semantics representation"}},{"authors":[{"last":"Mikolov"},{"last":"al."}],"year":"2011","style":0,"reference":{"authors":[{"first":"T.","last":"Mikolov"},{"first":"S.","last":"Kombrink"},{"first":"L.","last":"Burget"},{"first":"J.","last":"Černocký"},{"first":"S.","last":"Khudanpur"}],"year":"2011","title":"Extensions of Recurrent Neural Network Language Model"}},{"authors":[{"last":"Hodosh"},{"last":"al."}],"year":"2013","style":0,"reference":{"authors":[{"last":"Hodosh"},{"last":"Micah"},{"first":"Peter","last":"Young"},{"first":"Julia","last":"Hockenmaier"}],"year":"2013","title":"Framing Image Description as a Ranking Task: Data, Models and Evaluation Metrics"}},{"authors":[{"last":"Socher"},{"last":"al."}],"year":"2013b","style":0,"reference":{"authors":[{"first":"R.","last":"Socher"},{"first":"Quoc V","middle":".","last":"Le"},{"first":"C.","middle":"D.","last":"Manning"},{"first":"A.","middle":"Y.","last":"Ng"}],"year":"2013b","title":"Grounded Compositional Semantics for Finding and Describing Images with Sentences"}},{"authors":[{"last":"Socher"},{"last":"al."}],"year":"2013a","style":0,"reference":{"authors":[{"first":"R.","last":"Socher"},{"first":"M.","last":"Ganjoo"},{"first":"C.","middle":"D.","last":"Manning"},{"first":"A.","middle":"Y.","last":"Ng"}],"year":"2013a","title":"Zero-shot learning through cross-modal transfer"}},{"authors":[{"last":"Yu"},{"last":"Ballard"}],"year":"2007","style":0,"reference":{"authors":[{"first":"C.","last":"Yu"},{"first":"D.","middle":"H.","last":"Ballard"}],"year":"2007","title":"A Unified Model of Early Word Learning Integrating Statistical and Social Cues"}}]}
