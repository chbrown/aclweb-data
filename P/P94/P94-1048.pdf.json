{"sections":[{"title":"DUAL-CODING THEORY AND CONNECTIONIST LEXICAL SELECTION Ye-Yi Wang*","paragraphs":["Computational Linguistics Program Carnegie Mellon University","Pittsburgh, PA 15232 Internet: yyw@cs.cmu.edu"]},{"title":"Abstract","paragraphs":["We introduce the bilingual dual-coding theory as a model for bilingual mental representation. Based on this model, lexical selection neural networks are implemented for a connectionist transfer project in machine translation."]},{"title":"Introduction","paragraphs":["Psycholinguistic knowledge would be greatly helpful, as we believe, in constructing an artificial language processing system. As for machine translation, we should take advantage of our understandings of (1) how the languages are represented in human mind; (2) how the representation is mapped from one language to another; (3) how the representation and mapping are acquired by human.","The bilingual dual-coding theory (Paivio, 1986) partially answers the above questions. It depicts the verbal representations for two different languages as two separate but connected logogen systems, characterizes the translation process as the activation along the connections between the logogen systems, and attributes the acquisition of the representation to some unspecified statistical processes.","We have explored an information theoretical neural network (Gorin and Levinson, 1989) that can acquire the verbal associations in the dual-coding theory. It provides a learnable lexical selection sub-system for a conneetionist transfer project in machine translation."]},{"title":"Dual-Coding Theory","paragraphs":["There is a well-known debate in psycholinguistics concerning the bilingual mental representation: independence position assumes that bilingual memory is represented by two functionally independent storage and retrieval systems, whereas interdependence position hypothesizes that all information of languages exists in a common memory store. Studies on cross-language transfer and cross-language priming have","*This work was partly supported by ARPA and ATR In-terpreting Telephony Research Laboratorie. provided evidence for both hypotheses (de Groot and Nas, 1991; Lambert, 1958).","Dual-coding theory explains the coexistence of independent and interdependent phenomena with separate but connected structures. The general dual-coding theory hypothesizes that human represents language with dual systems -- the verbal system and the imagery system. The elements of the verbal system are logogens for words in a language. The elements of the imagery system, called \"imagens\", are connected to the logogens in the verbal systems via referential connections. Logogens in a verbal system are also interconnected with associative connections. The bilingual dual-coding theory proposes an architecture in which a common imagery system is connected to two verbal systems, and the two verbal systems are interconnected to each other via associative connections [Figure 1]. Unlike the within-language associations, which are rich and diverse, these between-language associations involve primarily translation equivalent terms that are experienced together frequently. The interconnections among the three systems explain the interdependent functional behavior. On the other hand, the different characteristics of within-language and between-language associations account for the independent functional behavior.","Based on the above structural assumption, dual-\" coding theory proposes a parallel set of processing assumptions. Activation of connections between referentially related imagens and logogens is called referential processing. Naming objects and imaging to words are prototypical examples. Activation of associative connections between logogens is called associative processing. Lexical translation is an example of associative processing between two languages."]},{"title":"Connectionist Lexical Selection Lexical Selection","paragraphs":["Lexical selection is the task of choosing target language words that accurately reflect the meaning of the corresponding source language words. It plays an important role in machine translation (Pustejovsky and"]},{"title":"325 L1 Verbal System","paragraphs":["f.. -~ V I Association Network L2 Verbal System f V 2 Association Nelwork VI - I Connections V 2 - I Connections Imagery System Figure 1: Bilingual Dual-Coding Representation Nirenburg, 1987).","A common lexical selection practice involves an intermediate representation. It disambiguates the source language words to entities in the intermediate representation, then maps from the entities to the target lexical entries. This intermediate representation may be Lexical Concept Structure (Dorr, 1989) or interlingua (Nirenberg, 1987). This engineering approach requires great effort in designing the representation and the mapping rules.","Currently, there are some efforts in statistical lexical selection. A target language word W t can be selected with the posterior probability Pr(Wt I Ws) given the source language word Ws. Several target language lexicai entries may be selected for a single source language word. Then the correct selections can be identiffed by the language model of the target language (Brown, 1990). This approach is learnable. However, the accuracy is low. One reason is that it does not use any structural information of a language.","In next subsections, we propose information-theoretical networks based on the bilingual dual-coding theory for lexical selection. Information-Theoretical Networks Information-theoretical network is a neural network formalism that is capable of doing associations between two layers of representations. The associations can be obtained statistically according to the network's experiences.","An information-theoretical network has two layers. Each unit of a layer represents an element in the input or output of a training pattern, which might be a logogen or a word. Units in different layers are connected. The weight of the connection between unit i in one layer and unit j in the other layer is assigned with the mutual information between the elements represenled by the two units (1) wij = l(vi, vj) = log(Pr(vjvi)/er(vi)) l","Each layer also contains a bias unit, which is always activated. The weight of the connection between the bias unit in one layer and unitj in the other layer is (2) woj = loger(vj)","Both the information-theoretical network and the back-propagation network compute the posterior probabilities for an association task (Gorin and Levinson, 1989; Robinson, 1992). However, only the information-theoretical network is isomorphic to the directly interconnected verbal systems in the dual-coding theory. Besides, an information-theoretical network has the following advantages: (1) it learns fast. The network can learn in a single pass without gradient decent. (2) it is adaptive. It can incrementally adapt to new experiences simply by adding new data to the training samples and modifying the associations according to the changed statistics. These make the network more psychologically plausible. Lexical Selection as an Associative Process We tried to map source language f-structures to target language f-structure in a connectionist transfer project (Wang, 1994). Functionally, there were two sub-tasks: 1. finding the target sub-structures, their phrasal categories and their corresponding source structures; 2. finding the head of a target structure. The second sub-task is a problem of lexical selection. It was first implemented with a back-propagation network.","We replaced the back-propagation networks for lexical selection with information-theoretical networks simulating the associative process in the dual-coding theory. The networks have two layers of units. Each source (target) language lexical item is represented by a unit in the input (output) layer. One network is constructed for each phrasal category (NP, VP, AP, etc.).","The networks works in the following way: for a target-language f-structure to be generated, the transfer system knows its phrasal category and its corresponding source-language f-structure from the networks that perform the sub-task 1. It then activates the lexical selection network for that phrasal category with the input units that correspond to the heads of the source language f-structure and its sub-structures. Through the connections between the two layers, the output units are activated, and the lexical item that corresponds to the most active output unit is selected as the head of the target f-structure. The following example illustrates how the system selects the head anmelden for 1Where vi means the event that unit i is activated."]},{"title":"326","paragraphs":["the German XCOMP sub-structure when it does the transfer from [sentence [subj i]"]},{"title":"would","paragraphs":["[xcomp [subj ]]"]},{"title":"like","paragraphs":["[xeomp [subj"]},{"title":"I] register [pp-adjfor the conference]]]]","paragraphs":["to"]},{"title":"[sentence [subj Ich] werde","paragraphs":["[xcomp [subj"]},{"title":"Ich]","paragraphs":["[adj"]},{"title":"gerne]","paragraphs":["anmelden"]},{"title":"[pp-aajfuer der Konferenz]]] 2.","paragraphs":["Since the structure networks find that there is a VP sub-structure of XCOMP in the target structure whose corresponding input structure is [xcomp"]},{"title":"[subj to register [pp-adjfor the conference]]],","paragraphs":["it activates the VP lexical selection network's input units for"]},{"title":"I, register and conference.","paragraphs":["By propagating the activation via the associative connections, the unit for"]},{"title":"anmelden","paragraphs":["is the most active output. Therefore,"]},{"title":"anmelden","paragraphs":["is chosen as the head of the"]},{"title":"xcomp","paragraphs":["sub-structure."]},{"title":"Preliminary Result","paragraphs":["The domain of our work was the Conference Registration Telephony Conversations. The lexicon for the task contained about 500 English and 500 German words. There were 300 English/German f-structurepairs available from other research tasks (Osterholtz, 1992). A separate set of 154 sentential f-structures was used to test the generalization performance of the system. The testing data was collected for an independent task (Jain, 1991).","From the 300 sentential f-structure pairs, every German VP sub-structure is extracted and labeled with its English counterpart. The English counterpart's head and its immediate sub-structures' heads serve as the input in a sample of VP association, and the German f-structure's head become the output of the association. For the above example, the association"]},{"title":"(]input I, regis- ter, conference] [output anmelden])","paragraphs":["is a sample drawn from the f-structures for the VP network. The training samples for all the other networks are created in the same way.","The accuracy of our system with information-theoretical network lexical selection is lower than the one with back-propagation networks (around 84% versus around 92%) for the training data. However, the generalization performance on the unseen inputs is better (around 70% versus around 62%). The information-theoretical networks do not over-learn as the back-propagation networks. This is partially due to the reduced number of free parameters in the information-theoretical networks."]},{"title":"Summary","paragraphs":["The lexical selection approach discussed here has two advantages. First, it is learnable. Little human effort on knowledge engineering is required. Secondly, it is psycholinguisticaUy well-founded in that the approach","2The f-structures are simplified here for the sake of conciseness. adopts a local activation processing model instead of relies upon symbol passing, as symbolic systems usually do."]},{"title":"References","paragraphs":["P. F. Brown and et al. A statistical approach to machine translation."]},{"title":"ComputationalLinguistics,","paragraphs":["16(2):73-85, 1990.","A. M. de Groot and G. L. Nas. Lexical representation of cognates and noncognates in compound bilingums."]},{"title":"Journal of Memory and Language,","paragraphs":["30(1), 1991.","B. J. Dorr. Conceptual basis of the lexicon in machine translation. Technical Report A.I. Memo No. 1166, Artificial Intelligence Laboratory, MIT, August, 1989.","A. L. Gorin and S. E. Levinson. Adaptive acquisition of language. Technical report, Speech Research Department, AT&T Bell Laboratories, Murray Hill, 1989.","A. N. Jain. Parsec: A connectionist learning architecture for parsing spoken language. Technical Report CMU-CS-91-208, Carnegie Mellon University, 1991.","W. E. Lambert, J. Havelka and C. Crosby. The influence of language acquisition contexts on bilingual-ism."]},{"title":"Journal of Abnormal and Social Psychology,","paragraphs":["56, 1958.","S. Nirenberg, V. Raskin and A. B. Tucker. The structure of interlingua in translator. In S. Nirenburg, editor,"]},{"title":"Machine Translation: Theoretical andMethodologicallssues.","paragraphs":["Cambridge University Press, Cambridge, England, 1987.","L. Osterholtz and et al. Janus: a multi-lingual speech to speech translation system. In"]},{"title":"Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing,","paragraphs":["volume 1, pages 209-212. IEEE, 1992. A. Paivio."]},{"title":"Mental Representations ~ A Dual Coding Approach.","paragraphs":["Oxford University Press, New York, 1986.","J. Pustejovsky and S. Nirenburg. Lexical selection in the process of language generation. In"]},{"title":"Proceed- ings of the 25th Annual Conference of the Associ- ation for Computational Linguistics,","paragraphs":["pages 201-206, Standford University, Standford, CA, 1987.","A. Robinson. Practical network design and implementation. In"]},{"title":"Cambridge Neural Network Summer School,","paragraphs":["1992.","Y. Wang and A. Waibel. Connectionist transfer in machine translation."]},{"title":"Inprepare,","paragraphs":["1994."]},{"title":"327","paragraphs":[]}]}
