{"sections":[{"title":"Towards a Self-Extending Lexicon*","paragraphs":["Uri Zernik Michael G. Dyer Artificial Intelligence Laboratory Computer Science Department","3531 Boelt~r Hall","University of tMifomis Los Angeles, California 90024 Abstract","The problem of manually modifying the lexicon appears with any natural language processing program. Ideally, a program should be able to acquire new lexieal entries from context, the way people learn. We address the problem of acquiring entire phrases, specifically"]},{"title":"Jigurative phr~es,","paragraphs":["through augmenting a"]},{"title":"phr~al lezico~","paragraphs":["Facilitating such a self-extending lexicon involves (a)"]},{"title":"disambiguation~se|ection","paragraphs":["of the intended phrase from a set of matching phrases, (b)"]},{"title":"robust parsin~-comprehension","paragraphs":["of partially-matching phrases, and (c)"]},{"title":"error analysis---use","paragraphs":["of errors in forming hypotheses about new phrases. We have designed and implemented a program called RINA which uses"]},{"title":"demons","paragraphs":["to implement"]},{"title":"funetional-~rammar","paragraphs":["principles. RINA receives new figurative phrases in context and through the application of a sequence of"]},{"title":"failure-driven","paragraphs":["rules, creates and refines both the patterns and the concepts which hold syntactic and semantic information about phrases. David vs. Goliath Native: Learner: Native: Learner: Native: Learner: Native: Remember the s~ory of David and Goliath? David took on Goliath. David took GoltLth sons,here? No. David took on Goliath. He took on him. He yon the fight? No. He took him on. David attacked him. He ~ok him on. He accepted She challenge? Right. Native: Learner: Here in annt,her story. John took on the third exam question. He took on a hard problem. Another dialogue involves put one's foot do~-a. Again, the phrase is unknown while its constituents are known: Going Punk 1. Introduction A language understanding program should be able to acquire new lexical items from context, forming for novel phrases their linguistic patterns and figuring out their conceptual meanings. The lexicon of a learning program should satisfy three requirements: Each lexical entry should (1) be learnable, (2) facilitate conceptual analysis, and (3) facilitate generation. In this paper we focus on the first two aspects. 1.1 The Task Domain","Two examples, which will be used throughout this paper, are given below. In the first dialogue the learner is introduced to an unknown phrase: take on. The words take and on are familiar to the learner, who also remembers the biblical story of David and Goliath. The program, modeling a language learner, interacts with a native speaker, as follows: * This work w~s made possible in part by s grant from the Keck Foundation. Native: Learner: Native: Learner: Jenny vant,ed ~o go punk, but, her father put, his toot dovu. He moved his foot dora? It, doen not, mike sense. No. He put his foot, dora. He put his foot dovu. He refused to let her go punk. A"]},{"title":"figurative phrase","paragraphs":["such as put one's fooc down is a linguistic pattern whose associated meaning cannot be produced from the composition of its constituents. Indeed, an interpretation of the phrase based on the meanings of its constituents often exists, but it carries a different meaning. The fact that this literal interpretation of the figurative phrase exists is a misleading clue in learning. Furthermore, the learner may not even notice that a novel phrase has been introduced since she is familiar with dram as well as with foot. Becker [Becker?5] has described a space of phrases ranging in generality from fixed proverbs such as charity begsns at, home through idioms such as Xay dove t,he tar and phrasal verbs such as put, up rich one's spouse and look up the name, to literal verb phrases such as sit, on she chair. He suggested employing a phrasal lexicon to capture this entire range o( language structures."]},{"title":"284","paragraphs":["1.2 Issues in Phrase AequLsition","Three issues must be addressed when learning phrases in context.","(I) Detecting failures: What are the indications that the initial interpretation of the phrase take him on as \"to take a person to a location\" is incorrect? Since all the words in the sentence are known, the problem is detected both as a conceptual discrepancy (why would he take his enemy anywhere?) and as a syntactic failure (the expected location of the assunied physical transfer is missing).","(2) Determining scope and generality of patterns: The linguistic pattern of a phrase may be perceived by the learner at various levels of generalit~l. For example, in the second dialogue, incorrect generalizations could yield patterns accepting sentences such as: Her boss put his left foot down. He moved his foot dora. He put down his foot. He put dovn his leg."]},{"title":"(3)","paragraphs":["A decision is also required about the scope of the pattern (i.e., the tokens included in the pattern). For instance, the scope of the pattern in John put up with Mary could be (I) ?x:persoa put:verb up where with is associated with l'lmry or (2)"]},{"title":"?x:persos","paragraphs":["put:verb up with ?y:persou, where with is associated with put up. Finding appropriate meanings: The conceptual meaning of the phrase must be extracted from the context which contains many concepts, both appropriate and inappropriate for hypothesis formation. Thus there must be strategies for focusing on appropriate elements in the context."]},{"title":"1.3","paragraphs":["The Program","RINA [Dyer85] is a computer program designed to learn English phrases. It takes as input English sentences which may include unknown phrases and conveys as out-put its hypotheses about novel phrases. The pro~am consists of four components:","(l) Phrasal lexicon: This is a list of phrases where each phrase is a declarative pattern-concept pair [WilenskySl]."]},{"title":"(2) Case-frame parser:","paragraphs":["In the parsing process, case-frame expectations are handled by spawning demons [Dyer83]. The parser detects comprehension failures which are used in learning."]},{"title":"(3) Pattern Constructor:","paragraphs":["Learning of phrase patterns is accomplished by analyzing parsing failures. Each failure situation is associated with a pattern-modification action.","(4) Concept Constructor: Learning of phrase concepts is accomplished by a set of strategies which are selected according to the context. Schematically, the program receives a sequence of sentence/contezt pairs from which it refines its current pattern/concept pair. The pattern is derived from the sentence and the concept is derived from the coLtext. However, the two processes are not independent since the context influences construction of patterns while linguistic clues in the sentence influence formation of concepts. 2. Phrasal Representation of the Lexicon","Parsing in RINA is central since learning is evaluated in terms of parsing ability before and after phrases are acquired. Moreover, learning is accomplished through parsing. 2.1 The Background","RINA combines elements of the following two approaches to language processing:","Phra~-bued pattern matching: In the implementation of UC [Wilensky84], an intelligent help system for UNIX users, both PHRAN [AJ'ens82 l, the conceptual analyzer, and PHRED [Jacobs85] the generator, share a phrasal lepton. As outlined by Wilensky {Wilensky81] this lexicon provides a declarative database, being modularly separated from the control part of the system which carries out parsing and generation. This development in representation of linguistic knowledge is paralleled by theories of functional grammars {Kay79[, and lezicalfunctional grammars [Bresnan78].","Ca~,-b,,-,,ed demon pmming: Boris [DyerS3 I modeled reading and understanding stories in depth. Its conceptual analyzer employed demon-based templates for parsing and for generation. Demons are used in parsing for two purposes: (1) to implement syntactic and semantic expectations [Riesbeck74] and (2) to implement memory operations such as search, match and update. This approach implements Schank's [Schank77] theory of representation of concepts, and follows case-grammar [Fillmore681 principles.","RINA uses a declarative phrasal lexicon as suggested by Wilensky [Wilensky82], where a lexical phrase is a pattern-concept pair. The pattern notation is described below and the concept notation is Dyer's"]},{"title":"[Dyer83]","paragraphs":["i-link notation."]},{"title":"285","paragraphs":["2.2 The Pattern Notation To span English sentences, RINA uses two kinds of patterns:"]},{"title":"lezical patterns","paragraphs":["and"]},{"title":"ordering patterns","paragraphs":["[Arens82]. In Figure I we show sample lexical patterns (patterns of lexical phrases). Such patterns are viewed as the generic linguistic forms of their corresponding phrases. I. ?x: (animate.a~ent) nibble :verb <on ?y: food> 2. ?z: Cpernou.Lgent) tLke:verb on ?y:p,tlent 3. ?x: (person.a~ent) <put:verb foot:body-part do~m> Figure h The Pattern Notation The notation is explained below:","(t) A token is a literal unless otherwise specified. For example, on is a literal in the patterns above.","(2) ?x:sort denotes a variable called .~x of a semantic type sort. ?y:food above is a variable which stands for references to objects of the semantic class food.","(3) Act.verb denotes any form of the verb s!lntactic class with the root act. nibble:vet6 above stands for expressions such as: nibbled, hms never nibbled, etc.","(4) By default, a pattern sequence does not specify the order of its tokens.","(5) Tokens delimited by < and > are restricted to their specified order. In Pattern I above, on must directly precede ?y:food."]},{"title":"Ordering patterns","paragraphs":["pertain to language word-order conventions in general. Some sample ordering patterns are: active: <?x:agenr. ?y: (verb.~tive)> passive: <?x:pattent ?y: (verb.p~,.s£ve)>","*<by ?Z : agent> infinitive:<to ?x: verb. active> \"?y: Iq~ent Figure 2: Ordering Patterns The additional notation introduced here is:","(6) An * preceding a term, such as *<by ?z:~ent> in the first pattern above indicates that the term is optional.","(7) * denotes an omitted term. The concept for Ty in the third example above is extracted from the agent of the pattern including the current pattern."]},{"title":"(8)","paragraphs":["By convention, the agent is the case-frame which precedes the verb in the lexical pattern. Notice that the notion of"]},{"title":"agent is","paragraphs":["necessary since (a) the agent is not necessarily the subject (i.e., she vu taken) and {b) the agent is not necessarily the"]},{"title":"actor","paragraphs":["{i.e., she received the book, he took a blo~), and (c) in the infinitive form, the agent must be referred to since the agent is omitted from the pattern in the lexicon.","(9) Uni/ieation [Kay79] accounts for the interaction of lexical patterns with ordering patterns in matching input sentences. So far, we have given a declarative definition of our grammar, a definition which is neutral with respect to either parsing or generation. The parsing procedure which is derived from the definitions above still has to be given."]},{"title":"2.3 Parsing Objectives","paragraphs":["Three main tasks in phrasal parsing may be identified, ordered by degree of difficulty."]},{"title":"(1)","paragraphs":["Phrase dlaambiguation: When more than one lexicat phrase matches the input sentence, the parser must select the phrase intended by the speaker. For example, the input the vorkeru took to the streets could mean either \"they demonstrated\" or \"they were fond of the streets'. In this case, the first phrase is selected according to the principle of pattern"]},{"title":"speci]icit 9","paragraphs":["[Arens821. The pattern ?X: person taXe:verb <to the streets> is more specific then ?x:person take:verb <to ?y:thing> However, in terms of our pattern notation, how do we define pattern specificity?"]},{"title":"{2)","paragraphs":["Ill-formed input comprehension: Even when an input sentence is not well phrased according to text-book grammar, it may be comprehensible by people and so must be comprehensible to the parser. For example, John took Nary school is telegraphic, but comprehensible, while John took Nzry to conveys only a partial concept. Partially matching sentences (or \"near misses') are not handled well by syntax-driven pattern matehers. A deviation in a function word (such as the word to above) might inhibit the detection of the phrase which could be detected by a semantics-driven parser."]},{"title":"(3)","paragraphs":["Error-detection: when the hypothesized phrase does not match the input sentence/context pair, the parser is required to detect the failure and return with an indication of its nature. Error analysis requires that pattern tokens be assigned a case-significance, as shown in Section 4.","Compounding requirements--disambiguation plus error-analysis capability-- complicate the design of the parser. On one hand, analysis of \"near misses\" (they bury a hatchet instead of they buried the hatchet) can 288"]},{"title":"be performed through a rigorous analysis--assuming the presence of a single phrase only. On the other hand, in the presence of multiple candidate phrases, disambigua- finn could be made efficient by organizing sequences of pattern tokens into a discrimination net. However, at- tempting to perform both disambiguation and \"near miss\" recognition and analysis simultaneously presents a difficult problem. The discrimination net organization would not enable comparing the input sentence, the \"near miss\", with existing phrases.","paragraphs":["The solution is to organize the discrimination sequence by order of generality from the general to the specific. According to this principle, verb phrases are matched by conceptual features first and by syntactic features only later on. For example, consider three initial erroneous hypotheses: (a) bury a hatchet (b) bury the gun, and (c) bury the hash. On hearing the words \"bury the hatchet', the first hypothesis would be the easiest to analyze (it differs only by a function word while the second differs by a content-holding word) and the third one would be the hardest (as opposed to the second, huh does not have a common concept with hlttchet). 2.4 Case-Frames","Since these requirements are not facilitated by the representation of patterns as given above, we slightly modify our view of patterns. An entire pattern is constructed from a set of"]},{"title":"case-/tames","paragraphs":["where each case-frame is constructed of single tokens: words and concepts. Each frame has several slots containing information about the case and pertaining to: (a) its syntactic appearance (b) its semantic concept and (c) its phrase role: agent, patient. Variable identifiers (e.g., ?x. ?y) are used for unification of phrase patterns with their corresponding phrase concepts. Two example patterns are given below:","The first example pattern denotes a simple literal verb phrase:"]},{"title":"{id:?x class:person role:agent} (take:verb) (id:?y class:person role:patient} {id:?z class:location marker:to}","paragraphs":["Figure 3: Cue Frmmes for \"He took her to school\" Both the agent and the patient are of the class person; the indirect object is a location marked by the preposition co. The second phrase is figurative:"]},{"title":"{id:?x","paragraphs":["class:person"]},{"title":"role:agent) {take:verb} (marker:to determiner:the word:streets}","paragraphs":["Figure 4: Case Frames for \"He took to the streets\" The third case frame in Figure 4 above, the indirect object, does not have any corresponding concept. Rather it is represented as a sequence of words. However the words in the sequence are designated as the"]},{"title":"marker,","paragraphs":["the"]},{"title":"determiner","paragraphs":["and the word itself. Using this view of patterns enables the recognition of \"near misses\" and facilitate error-analysis in parsing. 3. Demons Make Patterns Operational","So far, we have described only the linguistic notation and indicated that unification [Kay79] accounts for production of sentences from patterns. However, it is not obvious how to make pattern unification operational in parsing. One approach [Arens82] is to generate word sequences and to compare generated sequences with the input sentence. Another approach IPereiraS01 is to implement unification using PROLOG. Since our task is to provide"]},{"title":"lenient parsing,","paragraphs":["namely also ill-formed sentences must be handled by the parser, these two approaches are not suitable. In our approach, parsing is carried out by converting patterns into demons.","Conceptual analysis is the process which involves reading input words left to right, matching them with existing linguistic patterns and instantiating or modifying in memory the associated conceptual meanings. For example, assume that these are the phrases for take: in the lexicon:","?x:person take:verb ?y:person ?z:locale John took"]},{"title":"her","paragraphs":["to Boston.","?x:person take:verb ?y:phys-obj He took the book.","?x:person take:verb off ?y:attire He took off his coaL.","?x:person take:verb on ?y:person David took on Goliath.","?x:person take:verb a bow The actor took a boy.","?x:thing take:verb a blow The vail took a blov.","?x:person take:verb ~to the streets~ The vorkern ~ok t,o the streets. The juvenile took t,o the e~reeCs. Figure 5: A Variety of Phrases for TAKE where variables ?x, :y and ?z also appear in correspond-in& concepts (not shown here). How are these patterns"]},{"title":"287","paragraphs":["actually applied in conceptual analysis? 3.1 Interaction of Lexlcal and Ordering Patterns","Token order in the lexical patterns themselves (Figure 5) supports the derivation of simple active-voice sentences only. Sentences such as: Msry vas ~,zken on by John. A veak contender David might, have left, alone,","bu~ Goliath he book on. David dec£ded to take on Gol'tath. Figure 6: A Variety of Word Orders cannot be derived directly by the given hxical patterns. These sentences deviate from the order given by the corresponding lexical patterns and require interaction with language conventions such as passive voice and infinitive."]},{"title":"Ordering patterns are","paragraphs":["used to span a wider range of sentences in the language. Ordering patterns such as the one's given in Figure 2 depict the word order involving verb phrases. In each pattern the case-frame preceding the verb is specified. (In active voice, the agent appears imediately before the verb, while in the passive it is the patient that precedes the verb.) 3.2 How Does It All Work?","Ordering patterns are compiled into demons. For example, DAGENT, the demon anticipating the agent of the phrase is generated by the patterns in Figure 2. rt has three clauses: If the verb is in active form then the agent is immediately"]},{"title":"be/ore","paragraphs":["the verb If the verb is in passive form then the agent"]},{"title":"may","paragraphs":["appear,"]},{"title":"preceded","paragraphs":["by by. If the verb is in infinitive then the agent is omitted. Its concept is obtained from the function verb. Figure T: The Conatruction of D_AGENT In parsing, this demon is spawned when a verb is encountered. For example, consider the process in parsing the sentence","Da.v~.d dec'ideal ~ bake on ~,o].£ath. Through identifying the verbs and their forms, the protess is: decided (active, simple) Search for the agent before the verb, anticipate an infinitive form. talc, (active, infinitive) Do not anticipate the agent. The actor of the \"take on\" concept which is the agent, is extracted from the agent of \"decide'. 4. Failure-Driven Pattern Construction","Learning of phrases in RINA is an iterative protess. The input is a sequence of sentence-context pairs, through which the program refines its current hypothesis about the new phrase. The hypothesis pertains to both the pattern and the concept of the phrase. 4.2 The Learning Cycle The basic cycle in the process is:","(a) A sentence is parsed on the background of a conceptual context.","(b) Using the current hypothesis, either the sentence is comprehended smoothly, or a failure is detected.","(c) If a failure is detected then the current hypothesis is updated. The crucial point in this scheme is to obtain from the parser an intelligible analysis of failures. As an example, consider this part of the first dialog:. 1 Program: tie took on him. He von ~he fight? 2 User:. No. He took him on. Dav'[d Lt, ta, cked him. 3 Program: He took him on.","He accepted the challenge? The first hypothesis is shown in Figure 8. pattern: concept: ?x:person take:verb don ?y:person~ ?x win the conflict with ?y Figure 8: First Hypothesis Notice that the preposition on is attached to the object ?y, thus assuming that the phrase is similar to He looked at Iqaar7 which cannot produce the following sentence: H. look.d her at. This hypothesis underlies Sentence 1 which is erroneous in both its form and its meaning. Two observations should be made by comparing this pattern to Sentence 2: The object is not preceded by the preposition on. The preposition on does not precede any object. These comments direct the construction of the new hypothesis:"]},{"title":"288","paragraphs":["pattern: concept: ?x:person take:verb on ?y:person ?x win the conflict with ?y Figure 9: Second Hypothesis where the preposition on is taken as a modifier of the verb itself, thus correctly generating Sentence 3. In Figure 9 the conceptual hypothesis is still incorrect and must itself be modified. 4.3 Learning Strategies","A subset of RINA's learning strategies, the ones used for the David and OoliaCh Dialog (Section 1.1) are described in this section. In our exposition of failures and actions we will illustrate the situations involved in the dialogues above, where each situation is specified by the following five ingredients: (1) the input sentence (Sentence), (2) the context (not shown explicitly here),","(3} the active pattern: either the pattern under construction, or the best matching pattern if this is the first sentence in the dialogue (Patternl).","(4) the failures detected in the current situation (Failures),","(5) the pattern resulting from the application of the ac-tion to the current pattern (Pattern2). Creating a New Phrase","A case.role mismatch occurs when the input sentence can only be partially matched by the active pattern. A 9oal mismatch occurs when the concept instantinted by the selected pattern does not match the goal situation in the context. Sentence: Patternt: Failures: Pattern2: David took on Goliath.","?x:person take:verb ?y:person ?z:location","Pattern and goal mismatch","?x:person take:verb","David's physically transferring Goliath to a location fails since {1) a location is not found and (2) the ac-tion does not match David's goals. If these two failures are encountered, then a new phrase is created. In absence of a better alternative, RINA initially generates David Cook him somevhere. Discriminating a Pattern by Freezing a Prepoab tional Phrase","A prepoMtional mismatch occurs when a preposition P matches in neither the active pattern nor in one of the lexical prepositional phrases, such as: <on ?x:platform> (indicating a spatial relation) <on ?x:time-unit> (indicating a time of action) <on ?x:location> (indicating a place) Sentence: Patternl: Failures: Pattern2: David took on Goliath.","?x:person take:verb","Prepositional mismatch","?x:person take:verb <on ?y:person>","The preposition on is not part of the active pattern. Neither does it match any of the prepositional phrases which currently exist for on. Therefore, since it cannot be interpreted in any other way, the ordering of the sub-expression <on ?y,:peraoa> is frozen in the larger pattern, using < and >.","Two-word verbs present a di~culty to language learners [Ulm75] who tend to ignore the separated verb-particle form, generating: take on him instead of cake him o,s. In the situation above, the learner produced this typical error. Relaxing an Undergeneralized Pattern Two failures involving on: (1) case-role mismatch (on ?y:p,r6oa is not found)and (2) prepositional mismatch (on appears unmatched at the end of the sentence) are encountered in the situation below: Sentence: Patte~at: Failures: Pattern2: David took him on. ?x:person take:verb <on ?y'person Prepositional and case-role mismatch. ?x:person take:verb on ?y:person","The combination of these two failures indicate that the pattern is too restrictive. Therefore, the < and > freezing delimiters are removed, and the pattern may now account for two-word verbs. In this case on can be separated from ¢,&ke. Generaiising a Semantic Restriction A semantic mismatch is marked when the semantic class of a variable in the pattern does not subsume the class of the corresponding concept in the sentence. Sentence: Patternt: Failures: Pattern2: John took on the third question. ?x:person take:verb on ?y:person Semantic mismatch ?x:person take:verb on ?y:task As a result, the type of ?y in the pattern is generalized to include both cases. 289 Freezing a Reference Which Relates to a Metaphor An unrelated reference is marked when a reference in the sentence does not relate to the context, but rather it relates to a metaphor (see elaboration in [Zernik85] ). The reference his fooc cannot be resolved in the context, rather it is resolved by a metaphoric gesture. Sentence: Pattern1: Failures: Pattern2: Her father put his foot down. ?x:person put:verb down ?y:phys-obj Goal mismatch and unrelated reference ?x:person put:verb down foot:body-part","Since, (I) putting his foot on the floor does not match any of the goals of Jenny's father and (2) the reference his foot is related to the domain of metaphoric gestures rather than to the context. Therefore, foot becomes frozen in the pattern. This method is similar to a method suggested by Fuss and Wilks [Fuss83]. In their method, a metaphor is analyzed when an apparently ill-formed input is detected, e.g.: the car drank ffi lot of gas. 4.4 Concept Constructor","Each pattern has an associated concept which is specified using Dyer's [Dyer83] i-link notation. The concept of a new phrase is extracted from the context, which may contain more than one element. For example, in the first dialogue above, the given context contains some salient sto W points [Wilensky82] which are indexed in episodic memory as two violated expectations: • David won the fight in spite of Goliath's physical superiority. • David accepted the challenge in spite of the risk involved. The program extracts meanings from the given set of points. Concept hypothesis construction is further discussed in [Zernik85]. 5. Previous Work in Language Learning In RINA, the stimulus for learning is comprehension failure. In previous models language learning was ,~lso driven by detection of failures.","PST [Reeker76] learned grammar by acting upon dilfercnces detected between the input sentence and internally generated sentences. Six types of differences were classified, and the detection of a difference which belonged to a class caused the associated alteration of the grammar.","FOUL-UP [Granger771 learned meanings of single words when an unknown word was encountered. The meaning was extracted from the script [Schank77] which was given as the context. A typical learning situation was The cffir vas driving on Hvy 66, vhen it careened off the road. The meaning of the unknown verb care.ned was guessed from the SACCIDENT script. POLITICS [CarbonellTO], which modeled comprehension of text involving political concepts, initiated learning when semantic constraints were violated. Constraints were generalized by analyzing underlying metaphors.","AMBER [Langley82] modeled learning of basic sentence structure. The process of learning was directed by mismatches between input sentences and sentences generated by the program. Learning involved recovery from both errors of omission (omitting a function word such as the or is in daddy bouncing ball) and errors of commission (producing daddy is liking dinner). Thus, some programs acquired linguistic patterns and some programs acquired meanings from context, but none of the above programs acquired new phrases. Acquisition of phrases involves two parallel processes: the formation of the pattern from the given set of example sentences, and the construction of the meaning from the context. These two processes are not independent since the construction of the conceptual meaning utilizes linguistic clues while the selection of pattern elements of new figurative phrases bears on concepts in the context. 6. Current and Future Work","Currently, RINA can learn a variety of phrasal verbs and idioms. For example, RINA implements the behavior of the learner in vffivtd vs. c, oliffich and in Go- £ng Punk in Section 1. Modifications of lexicM entries are driven by analysis of failures. This analysis is similar to analysis of ill-formed input, however, detection of failures may result in the augmentation of the lexicon. Failures appear as semantic discrepancies (e.g., goal-plan mismatch}, or syntactic discrepancies (e.g., case-role mismatch). Finally, references in figurative phrases are resolved by metaphor mapping.","Currently our efforts are focussed on learning the conceptual elements of phrases. We attempt to develop strategies for generalizing and refining acquired concepts. For example, it is desirable to refine the concept for \"take on\" by this sequence of examples: David toak on Goliath. The [t, kers took on ~he Celtics. I took on a, bard ~ffi,,.k. I took on a, hey Job. In selecting ~he naae °TQvard8 a. Self-EzCending","LeXiCOne. Ye t,43olc OU in old nKme. 29O The first three examples \"deciding to fight someone', \"playing against someone\" and \"accepting a challenge\" could be generalized into the same concept, but the last two examples deviate in their meanings from that developed concept. The problem is to determine the desired level of generality. Clearly, the phrases in the following examples: ~sdce on am enemy Lake os an old name ~a~e on the shape of a essdce deserve separate entries in the phrasal lexicon. The question is, at what stage is the advantage of further generalization diminished? Acknowledgments We wish to thank Erik Muelhr and Mike Gasser for their incisive comments on drafts of this paper."]},{"title":"References","paragraphs":["{ArensS2J"]},{"title":"[Becker75]","paragraphs":["[Bresnan78] [Carbonel179]"]},{"title":"Areas, Y., \"The Context Model: Language Understanding in a Con-","paragraphs":["text,\" in"]},{"title":"Proceedings Fourth Annual Conference of the Cofnitive Science So- ciety,","paragraphs":["Ann"]},{"title":"Arbor,","paragraphs":["Michigan (1982}. Bucker, Joseph D., \"The Phrasal Lexicon,\" pp. 70-73 in"]},{"title":"Proceedings Interdis- ciplinary Workshop on Theoretical Is. sues in Natural Lanfaage Processing,","paragraphs":["Cambridge, Massachusets (June 1975). Bresnan, Joan, \"A Realistic Transformational Grammar,\" pp. 1-59 in"]},{"title":"Linguistic Theory and Psychological Reality,","paragraphs":["ed. M. Halle J. Bresnan G. Miller, MIT Press, Harvard, Massachusets (1978). Carbonell, J. G., \"Towards a Sell'- Extending Parser,\" pp. 3-7 in"]},{"title":"Proceed- ings 17th Annual Meeting of the Associ- ation for Computational Linfaistics,","paragraphs":["La Jolla, California (1070). [Dyer83] [Dyer8S] Dyer, Michael G.,"]},{"title":"In-Depth Under- standing: A Computer Model of In- tegrated Processing for Narrative Comprehension,","paragraphs":["MIT Press, Cambridge, MA"]},{"title":"(1983).","paragraphs":["Dyer, Michael G. and Uri Zernik, \"Parsing Paradignm and Language Learning,\" in"]},{"title":"Proceedings AI-85,","paragraphs":["Long Beach, California (May 1085)."]},{"title":"[Fasss3l","paragraphs":["[Fillmore681 [Granger77] [Jacobs85] [Kay791 [Langley82[ [PereiraS01"]},{"title":"[Reeker76] [Riesbeck74[ [Schank77]","paragraphs":["Fans, Dan and Yorick Wilks, \"Preference Semantics, IlbFormedness and Metaphor,\""]},{"title":"American Journal of Com- putational Linguistics","paragraphs":["0(3-4), pp.178-"]},{"title":"1s7 (zoo).","paragraphs":["Fillmore, C., \"The Case for Case,\" pp. l-g0 in"]},{"title":"Universals in Linguistic Theory,","paragraphs":["ed. E. Bach R. Harms, Holt, Reinhart and Winston, Chicago (1988). Granger, R. H., \"FOUL-UP: A Program That Figures Out Meanings of Words from Context,\" pp. 172-178 in"]},{"title":"Proceedings Fifth [JCAI,","paragraphs":["Cambridge, Massachusets (August 1977). Jaeobs, Paul S., \"PHRED: A Generator for Natural Language Interfaces,\" UCB/CSD 85/108,. Computer Science Division, University of California Berkeley, Berkeley, California (January 1985). Kay, Martin, \"Functional Grammar.\" pp. 142-158 in"]},{"title":"Proceedings 5th Annual Meeting of the Berkeley Linguistic So- ciety,","paragraphs":["Berkeley, California (1979). Langley, Pat, \"Language Acquisition Through Error Recovery,\""]},{"title":"Cognition","paragraphs":["and Brain Theory"]},{"title":"~;(3),","paragraphs":["pp.211-255 {I082). Pereira,"]},{"title":"F.","paragraphs":["C. N. and David H."]},{"title":"D.","paragraphs":["Warren, \"Definite Clause Grammars for"]},{"title":"Language Analysis- A Survey of the Formalism and a Comparison with Augmented Transition Networks.\" Artificial Intelligence","paragraphs":["13, pp.231-278"]},{"title":"(i~o).","paragraphs":["Reeker, L. H., \"The Computational Study of Language Learning,\" in .Ad-"]},{"title":"vances in Computers,","paragraphs":["ed. M. Yovits M. Rubinoff, Academic Press, New York"]},{"title":"(1976).","paragraphs":["Riesbeck, C. K., \"Computational Understanding: Analysis of Sentences and Context,\" Memo 238, AI Laboratory (1974). Schank, Roger and Robert AbeLson,"]},{"title":"Scripts Plans Goals and Understanding,","paragraphs":["Lawrence Erlbaum Associates, Hillsdale, New Jersey (1977)."]},{"title":"291","paragraphs":["\""]},{"title":"{Ulm751 [Wilensky81] [Wilensky82] [Wilensky84] [Zernik85] Ulm, Susan C., \"The Separation Phenomenon in English Phrasal Verbs, Double trouble,\" 601, University of California Los Angeles (1975). M.A. Thesis. Wilensky, R., \"A Knowledge-Ba~ed Approach to Natural Language Pro- eessing:. A progress Report,\" in Proceedings Seventh International Joint Conference on Artificial Intelligence, Vancouver, Canada (1981). Wilensky, R., \"Points: A Theory of Structure of Stories in Memory,\" pp. 345-375 in Strategies for Natural Lanfaage Processing, ed. W. G. Lehnert M. H. Ringle, Laurence Erl- banm Associates, New Jersey (1982). Wilensky, R., Y. Arens, and D. Chin, \"Talking to UNIX in English: an Over- view of UC,\" Communications of the ACM 2T(6), pp.574.-593 (June 1984). Zernik, Uri and Michael G. Dyer, Failure-Driven Aquisition of Fifarative Phrasea by Second Language Speakers, 1985. (submitted to publication). 292","paragraphs":[]}]}
