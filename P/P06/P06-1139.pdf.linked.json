{"sections":[{"title":"","paragraphs":["Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 1105–1112, Sydney, July 2006. c⃝2006 Association for Computational Linguistics"]},{"title":"Stochastic Language Generation Using WIDL-expressions and its Application in Machine Translation and Summarization Radu Soricut Information Sciences Institute University of Southern California 4676 Admiralty Way, Suite 1001 Marina del Rey, CA 90292 radu@isi.edu Daniel Marcu Information Sciences Institute University of Southern California 4676 Admiralty Way, Suite 1001 Marina del Rey, CA 90292 marcu@isi.edu Abstract","paragraphs":["We propose WIDL-expressions as a fle xible formalism that facilitates the integration of a generic sentence realization system within end-to-end language processing applications. WIDL-expressions represent compactly probability distributions over finite sets of candidate realizations, and have optimal algorithms for realization via interpolation with language model probability distributions. We show the effectiveness of a WIDL-based NLG system in two sentence realization tasks: automatic translation and headline generation."]},{"title":"1 Introduction","paragraphs":["The Natural Language Generation (NLG) community has produced over the years a considerable number of generic sentence realization systems: Penman (Matthiessen and Bateman, 1991), FUF (Elhadad, 1991), Nitrogen (Knight and Hatzivassiloglou, 1995), Fergus (Bangalore and Rambow, 2000), HALogen (Langkilde-Geary, 2002), Amalgam (Corston-Oliver et al., 2002), etc. However, when it comes to end-to-end, text-to-text applications – Machine Translation, Summarization, Question Answering – these generic systems either cannot be employed, or, in instances where they can be, the results are significantly below that of state-of-the-art, application-specific systems (Hajic et al., 2002; Habash, 2003). We believe two reasons explain this state of affairs.","First, these generic NLG systems use input representation languages with complex syntax and semantics. These languages involve deep, semantic-based subject-verb or verb-object relations (such as ACTOR, AGENT, PATIENT, etc., for Penman and FUF), syntactic relations (such as subject, object, premod, etc., for HALogen), or lexical dependencies (Fergus, Amalgam). Such inputs cannot be accurately produced by state-of-the-art analysis components from arbitrary textual input in the context of text-to-text applications.","Second, most of the recent systems (starting with Nitrogen) have adopted a hybrid approach to generation, which has increased their robustness. These hybrid systems use, in a first phase, symbolic knowledge to (over)generate a large set of candidate realizations, and, in a second phase, statistical knowledge about the target language (such as stochastic language models) to rank the candidate realizations and find the best scoring one. The disadvantage of the hybrid approach – from the perspective of integrating these systems within end-to-end applications – is that the two generation phases cannot be tightly coupled. More precisely, input-driven preferences and target language–driven preferences cannot be integrated in a true probabilistic model that can be trained and tuned for maximum performance.","In this paper, we propose WIDL-expressions (WIDL stands for Weighted Interleave, Disjunc-tion, and Lock, after the names of the main operators) as a representation formalism that facilitates the integration of a generic sentence realization system within end-to-end language applications. The WIDL formalism, an extension of the IDL-expressions formalism of Nederhof and Satta (2004), has several crucial properties that differentiate it from previously-proposed NLG representation formalisms. First, it has a simple syntax (expressions are built using four operators) and a simple, formal semantics (probability distributions over finite sets of strings). Second, it is a compact representation that grows linearly 1105 in the number of words available for generation (see Section 2). (In contrast, representations such as word lattices (Knight and Hatzivassiloglou, 1995) or non-recursive CFGs (Langkilde-Geary, 2002) require exponential space in the number of words available for generation (Nederhof and Satta, 2004).) Third, it has good computational properties, such as optimal algorithms for intersection with","-gram language models (Section 3). Fourth, it is fle xible with respect to the amount of linguistic processing required to produce WIDL-expressions directly from text (Sections 4 and 5). Fifth, it allows for a tight integration of input-specific preferences and target-language preferences via interpolation of probability distributions using log-linear models. We show the effectiveness of our proposal by directly employing a generic WIDL-based generation system in two end-to-end tasks: machine translation and automatic headline generation."]},{"title":"2 The WIDL Representation Language 2.1 WIDL-expressions","paragraphs":["In this section, we introduce WIDL-expressions, a formal language used to compactly represent probability distributions over finitesets of strings.","Given a finite alphabet of symbols ",", atomic","WIDL-expressions are of the form , with ",". For a WIDL-expression",", its semantics is a probability distribution  ",", where  ","and  . Complex WIDL-expressions are created from other WIDL-expressions, by employing the following four operators, as well as operator distribution functions","from an alphabet",". Weighted Disjunction. If "," are","WIDL-expressions, then  "," ,","with "," ","    , specified such that   "," ",", is a WIDL-","expression. Its semantics is a probability","distribution ","   , where","","",""," ","","","",", and the probabil-","ity values are induced by","and",",","",". For example, if    ,","","","","",", its semantics is a proba-","bility distribution over","","   ,","defined by"," ","","","",""," ","","","and","","  ","","","","   ","",". Precedence. If ","","are WIDL-expressions,","then","","  ","is a WIDL-expression. Its semantics is a probability distribution   ",", where","is the set of all","strings that obey the precedence imposed over","the arguments, and the probability values are in-","duced by","and",". For example, if","   ,","  ",""," , and    ,","","","","",", then","","","represents a probability distribution","over the set","       , defined","by","  ","","","   ","","","","","  ,","","","","     ","","   ","",", etc. Weighted Interleave. If "," are WIDL-expressions, then   "," , with  ","",", , specified such that","     ",", is a","WIDL-expression. Its semantics is a probability","distribution ","  ",", where",""," consists of all the possible interleavings of","strings from","",","," ",", and the proba-","bility values are induced by","and",". The","distribution function ","is definedeither explicitly,","over","(the set of all permutations of","elements), or implicitly, as",". Be-","cause the set of argument permutations is a sub-","set of all possible interleavings,","also needs to","specify the probability mass for the strings that","are not argument permutations,",". For","example, if"," ","",""," "," ,  ","","","",""," "," ","","","","","","",", its","semantics is a probability distribution",",","with domain","          , defined","by","  ","","","","","","","  ,","","",""," ","",""," ",",",""," ",""," ","",". Lock. If","is a WIDL-expression, then ","","is a WIDL-expression. The semantic map-","ping","is the same as","","",", except","that  contains strings in which no addi-","tional symbol can be interleaved. For exam-","ple, if "," ","    "," ,  ",""," ",", its semantics is a proba-","bility distribution",", with domain","","","   ","",", defined by","  ","","","","  ","","","",",  ",""," "," ","","","",".","In Figure 1, we show a more complex WIDL-","expression. The probability distribution","","associ-","ated with the operator assigns probability 0.2","to the argument order ","; from a probability mass of 0.7, it assigns uniformly, for each of the remaining  ","argument permutations, a","permutation probability value of      . The 1106  ","","","","","          ","","","","   ","","","","  ","","","","","","","","","","",""," ","                                         ","    Figure 1: An example of a WIDL-expression.","remaining probability mass of 0.1 is left for the","12 shuffles associated with the unlocked expres-","sion    ",", for a shuffle probability of ","    . The list below enumerates some of the   ","  ","","","","","","pairs that belong to the proba-","bility distribution definedby our example:","rebels fightingturkish government in iraq 0.130","in iraq attacked rebels turkish goverment 0.049","in turkish goverment iraq rebels fighting 0.005","The following result characterizes an important","representation property for WIDL-expressions.","Theorem 1 A WIDL-expression over ","and","using","atomic expressions has space complexity","O( ), if the operator distribution functions of","have space complexity at most O(","). For proofs and more details regarding WIDL-expressions, we refer the interested reader to (Soricut, 2006). Theorem 1 ensures that high-complexity hypothesis spaces can be represented efficientlyby WIDL-expressions (Section 5).","2.2 WIDL-graphs and Probabilistic Finite-State Acceptors","WIDL-graphs. Equivalent at the representation","level with WIDL-expressions, WIDL-graphs al-","low for formulations of algorithms that process","them. For each WIDL-expression",", there exists","an equivalent WIDL-graph","",". As an example,","we illustrate in Figure 2(a) the WIDL-graph cor-","responding to the WIDL-expression in Figure 1.","WIDL-graphs have an initial vertex","and a final","vertex",". Vertices","",",",", and","","with in-going","edges labeled    ,   ",", and","","",", respectively, and","vertices","",",","","",", and   with out-going edges labeled   , "," , and","",", respectively, result from","the expansion of the","operator. Vertices","","and","  with in-going edges labeled "," , ","",", re-","spectively, and vertices","","and  ","with out-going","edges labeled "," , ","",", respectively, result from the","expansion of the","operator.","With each WIDL-graph","",", we associate a","probability distribution. The domain of this dis-","tribution is the finite collection of strings that can","be generated from the paths of a WIDL-specific","traversal of","",", starting from","and ending in",".","Each path (and its associated string) has a proba-","bility value induced by the probability distribution","functions associated with the edge labels of","",". A","WIDL-expression","and its corresponding WIDL-","graph"," are said to be equivalent because they","represent the same distribution",". WIDL-graphs and Probabilistic FSA. Probabilistic finite-state acceptors (pFSA) are a wellknown formalism for representing probability distributions (Mohri et al., 2002). For a WIDL-expression",", we define a mapping, called UNFOLD, between the WIDL-graph","","and a pFSA","",". A state","in","","is created for each set of WIDL-graph vertices that can be reached simultaneously when traversing the graph. State ","records, in what we call a","-stack (interleave stack), the order in which   , ","","–bordered sub-","graphs are traversed. Consider Figure 2(b), in","which state              (at the bottom) cor-","responds to reaching vertices  "," , and"," ","(see","the WIDL-graph in Figure 2(a)), by first reach-","ing vertex"," ","(inside the",""," ,  ","–bordered sub-","graph), and then reaching vertex","","(inside the "," ,  ","","–bordered sub-graph).","A transition labeled","between two","","states","","","and  in","","exists if there exists a vertex","in the description of","","and a vertex ","in the de-","scription of ","such that there exists a path in","","between and ",", and is the only ","-labeled transitions in this path. For example, transition             ","                  (Fig-","ure 2(b)) results from unfolding the path","  "," ","   ","","","        ","(Figure 2(a)). A tran-","sition labeled","between two","","states","","and","","in","","","exists if there exists a vertex","in the descrip-","tion of","","and vertices       in the descrip-","tion of ",", such that      ","","","",","," "," (see transition                  ), or if there exists vertices     "," in the description","of  and vertex ","in the description of  , such that        ","","","",","," ","",". The -transitions 1107        ","","","","        ","","","","","","","","        ","","","","            ","","","","","","","","","","","","","","","","        ","","","","","","","","    ","","","","","","","","","","","","","","","","                                                  ","attacked attacked attacked attacked attacked rebels rebels rebels fighting rebels rebels rebels rebels rebels fighting","fighting fighting fighting turkish turkish turkish turkish turkish turkish","turkish government government government government","government government in iraq in in in in in iraq iraq iraq iraq iraq e e d1 government turkish :0.3 attacked :0.1 :0.3 :1 :1 rebels :0.2 :1 fighting",":1 rebels :1 d1 :0.18 :0.18",":1 rebels",":1 rebels :1 e 0 6 21","0","6","0","23","9","23","9 0 21 11 0 20 9 0 15 20 6 20 2 0 21 s e (b)(a) rebels rebels fighting (","( )2 d1 d1 d1 d1 d1 d1 d2","d2 d2 1 2 3 2 11 3 )1 d2attacked in iraq","e e","e","e e","e","e e","e e turkish government1 1 1 1 1 1 1 1 1 1 1","2 v v v v v v v v v v v v v v v v v v v v v v v v1 v","s e","0","1 2","3","4 v 6","7","8 9 10","11","12","13","14 15 16","17","18 19","20","23","22","21 5 0 6 20","0 d1 23 190 23 19 [v , ]","0","d1 23 190 23 19","[v v v ,<32][v v v ,<32] [v v v ,<3] [v v v ,<3] [v v v ,<32] [v v v ,<0] [v v v ,<32] [v v v ,<2] [v v v ,<2] [v , ] [v v v ,<1 ] e e d1","d1 d1","d1 d1","d1 d1 d1 d1 d1","0.1 }shuffles 0.7,d1= {2 1 3 0.2, other perms","d2 = {1 0.35 }0.65, 2 d1[v v v ,< > ] d1 [v v v ,< 0 > ] [v v v ,< 321 > ]d1","d1 Figure 2: The WIDL-graph corresponding to the WIDL-expression in Figure 1 is shown in (a). The probabilistic finite-stateacceptor (pFSA) that corresponds to the WIDL-graph is shown in (b). are responsible for adding and removing, respectively, the  ",",","","symbols in the","-stack. The prob-","abilities associated with","","transitions are com-","puted using the vertex set and the","-stack of each","","","state, together with the distribution functions","of the and","operators. For a detailed presen-","tation of the UNFOLD relation we refer the reader","to (Soricut, 2006)."]},{"title":"3 Stochastic Language Generation from WIDL-expressions 3.1 Interpolating Probability Distributions in a Log-linear Framework","paragraphs":["Let us assume a finite set  of strings over a finite alphabet ",", representing the set of possible sentence realizations. In a log-linear frame-work, we have a vector of feature functions"," ","  "," ","  , and a vector of parameters","      ","","  . For any"," ",", the interpolated","probability   ","can be written under a log-linear","model as in Equation 1:    ","","","","","","  ","","","","","  ","","","","","","","  ","","","","","",""," (1) We can formulate the search problem of finding the most probable realization","under this model as shown in Equation 2, and therefore we do not need to be concerned about computing expensive normalization factors.","",""," ","      ","","","","","","","","  ","","","","","  (2)","For a given WIDL-expression over  , the set ","is definedby ",", and feature function   is taken to be",". Any language model we want to employ may be added in Equation 2 as a feature function","",",    .","3.2 Algorithms for Intersecting WIDL-expressions with Language Models","Algorithm WIDL-NGLM-A","(Figure 3) solves","the search problem defined by Equation 2 for a","WIDL-expression","(which provides feature func-","tion  ) and","","-gram language models (which","provide feature functions"," ","","","",". It does","so by incrementally computing UNFOLD for","","(i.e., on-demand computation of the correspond-","ing pFSA","","), by keeping track of a set of active","states, called",". The set of newly UNFOLDed","states is called",". Using Equation 1 (unnor-","malized), we EVALUATE the current","","","","scores","for the","states. Additionally, EVALUATE","uses an admissible heuristic function to compute","future (admissible) scores for the","states.","The algorithm PUSHes each state from the cur-","rent","into a priority queue",", which sorts","the states according to their total score (current","admissible). In the next iteration,","is a sin-","gleton set containing the state POPed out from the","top of . The admissible heuristic function we use","is the one defined in (Soricut and Marcu, 2005),","using Equation 1 (unnormalized) for computing","the event costs. Given the existence of the ad-","missible heuristic and the monotonicity property","of the unfolding provided by the priority queue",",","the proof for A","optimality (Russell and Norvig,","1995) guarantees that WIDL-NGLM-A","finds a","path in ","that provides an optimal solution. 1108","WIDL-NGLM-A  ","     ","1 ","       2     3 while   ","4 do"," UNFOLD","","   ","5 EVALUATE      ","6 if        7 then    ","8 for each  ","","in","do PUSH"," ","  ","","","","","","POP",""," 9 return","Figure 3: A","algorithm for interpolating WIDL-","expressions with","-gram language models.","An important property of the WIDL-NGLM-A","algorithm is that the UNFOLD relation (and, implicitly, the","","acceptor) is computed only partially, for those states for which the total cost is less than the cost of the optimal path. This results in important savings, both in space and time, over simply running a single-source shortest-path algorithm for directed acyclic graphs (Cormen et al., 2001) over the full acceptor","","(Soricut and Marcu, 2005)."]},{"title":"4 Headline Generation using WIDL-expressions","paragraphs":["We employ the WIDL formalism (Section 2) and the WIDL-NGLM-A","algorithm (Section 3) in a summarization application that aims at producing both informative and fluent headlines. Our headlines are generated in an abstractive, bottom-up manner, starting from words and phrases. A more common, extractive approach operates top-down, by starting from an extracted sentence that is compressed (Dorr et al., 2003) and annotated with additional information (Zajic et al., 2004). Automatic Creation of WIDL-expressions for Headline Generation. We generate WIDL-expressions starting from an input document. First, we extract a weighted list of topic keywords from the input document using the algorithm of Zhou and Hovy (2003). This list is enriched with phrases created from the lexical dependencies the topic keywords have in the input document. We associate probability distributions with these phrases using their frequency (we assume","Keywords","iraq 0.32, syria 0.25, rebels 0.22, kurdish 0.17, turkish 0.14, attack 0.10 Phrases iraq","in iraq 0.4, northern iraq 0.5,iraq and iran 0.1",", syria","into syria 0.6, and syria 0.4 rebels","attacked rebels 0.7,rebels fighting0.3 . . .","WIDL-expression & trigram interpolation TURKISH GOVERNMENT ATTACKED REBELS IN IRAQ AND SYRIA Figure 4: Input and output for our automatic headline generation system. that higher frequency is indicative of increased importance) and their position in the document (we assume that proximity to the beginning of the document is also indicative of importance). In Figure 4, we present an example of input keywords and lexical-dependency phrases automatically extracted from a document describing incidents at the Turkey-Iraq border.","The algorithm for producing WIDL-expressions combines the lexical-dependency phrases for each keyword using a","operator with the associated probability values for each phrase multiplied with the probability value of each topic keyword. It then combines all the","-headed expressions into a single WIDL-expression using a","operator with uniform probability. The WIDL-expression in Figure 1 is a (scaled-down) example of the expressions created by this algorithm. On average, a WIDL-expression created by this algorithm, using","  keywords and an average of ","lexical-dependency phrases per keyword,","compactly encodes a candidate set of about 3","million possible realizations. As the specification","of the","operator takes space"," ","","for uniform",",","Theorem 1 guarantees that the space complexity","of these expressions is","","","",". Finally, we generate headlines from WIDL-","expressions using the WIDL-NGLM-A","algo-","rithm, which interpolates the probability distribu-","tions represented by the WIDL-expressions with"," -gram language model distributions. The output","presented in Figure 4 is the most likely headline","realization produced by our system. Headline Generation Evaluation. To evaluate the accuracy of our headline generation system, we use the documents from the DUC 2003 evaluation competition. Half of these documents are used as development set (283 documents), 1109","ALG","(uni)","(bi) Len. Rouge","Rouge Extractive Lead10 458 114 9.9 20.8 11.1","HedgeTrimmer","399 104 7.4 18.1 9.9 Topiary","576 115 9.9 26.2 12.5 Abstractive Keywords 585 22 9.9 26.6 5.5 Webcl 311 76 7.3 14.1 7.5 WIDL-A","562 126 10.0 25.5 12.9 Table 1: Headline generation evaluation. We compare extractive algorithms against abstractive algorithms, including our WIDL-based algorithm. and the other half is used as test set (273 documents). We automatically measure performance by comparing the produced headlines against one reference headline produced by a human using ROUGE","(Lin, 2004).","For each input document, we train two language models, using the SRI Language Model Toolkit (with modified Kneser-Ney smoothing). A general trigram language model, trained on 170M English words from the Wall Street Journal, is used to model fluenc y. A document-specific trigram language model, trained on-the-fly for each input document, accounts for both fluenc y and content validity. We also employ a word-count model (which counts the number of words in a proposed realization) and a phrase-count model (which counts the number of phrases in a proposed realization), which allow us to learn to produce headlines that have restrictions in the number of words allowed (10, in our case). The interpolation weights","(Equation 2) are trained using discriminative training (Och, 2003) using ROUGE","as the objective function, on the development set.","The results are presented in Table 1. We compare the performance of several extractive algorithms (which operate on an extracted sentence to arrive at a headline) against several abstractive algorithms (which create headlines starting from scratch). For the extractive algorithms, Lead10 is a baseline which simply proposes as headline the lead sentence, cut after the first 10 words. HedgeTrimmer ","is our implementation of the Hedge Trimer system (Dorr et al., 2003), and Topiary","is our implementation of the Topiary system (Zajic et al., 2004). For the abstractive algorithms, Keywords is a baseline that proposes as headline the sequence of topic keywords, Webcl is the system THREE GORGES PROJECT IN CHINA HAS WON APPROVAL WATER IS LINK BETWEEN CLUSTER OF E. COLI CASES SRI LANKA ’S JOINT VENTURE TO EXPAND EXPORTS OPPOSITION TO EUROPEAN UNION SINGLE CURRENCY EURO OF INDIA AND BANGLADESH WATER BARRAGE Figure 5: Headlines generated automatically using a WIDL-based sentence realization system. described in (Zhou and Hovy, 2003), and WIDL-A","is the algorithm described in this paper.","This evaluation shows that our WIDL-based approach to generation is capable of obtaining headlines that compare favorably, in both content and fluenc y, with extractive, state-of-the-art results (Zajic et al., 2004), while it outperforms a previously-proposed abstractive system by a wide margin (Zhou and Hovy, 2003). Also note that our evaluation makes these results directly comparable, as they use the same parsing and topic identificationalgorithms. In Figure 5, we present a sample of headlines produced by our system, which includes both good and not-so-good outputs."]},{"title":"5 Machine Translation using WIDL-expressions","paragraphs":["We also employ our WIDL-based realization engine in a machine translation application that uses a two-phase generation approach: in a firstphase, WIDL-expressions representing large sets of possible translations are created from input foreign-language sentences. In a second phase, we use our generic, WIDL-based sentence realization engine to intersect WIDL-expressions with an","- gram language model. In the experiments reported here, we translate between Chinese (source language) and English (target language). Automatic Creation of WIDL-expressions for MT. We generate WIDL-expressions from Chinese strings by exploiting a phrase-based translation table (Koehn et al., 2003). We use an algorithm resembling probabilistic bottom-up parsing to build a WIDL-expression for an input Chinese string: each contiguous span  ","","over a","Chinese string  ","","is considered a possible “con-","stituent”, and the “non-terminals” associated with","each constituent are the English phrase transla-","tions  ","",""," that correspond in the translation ta-","ble to the Chinese string","","","",". Multiple-word En-","glish phrases, such as",", are represented","as WIDL-expressions using the precedence (",") and 1110        "," ","","   ","","","","   ","","","","   ","  ","","      ","  ","","","","","","","         ","    "," ","      ","  ","                       ","","  ","","","","  "," ","    ","  ",""," ","","    ","    ","      ","  "," ","WIDL-expression & trigram interpolation gunman was killed by police . Figure 6: A Chinese string is converted into a WIDL-expression, which provides a translation as the best scoring hypothesis under the interpolation with a trigram language model. lock (  ) operators, as ","","","","   ","","","",". To limit","the number of possible translations  ","","","","corre-","sponding to a Chinese span","","","",", we use a prob-","abilistic beam and a histogram beam","to beam","out low probability translation alternatives. At this","point, each","",""," span is “tiled” with likely transla-","tions     ","taken from the translation table.","Tiles that are adjacent are joined together in","a larger tile by a","operator, where","","","","  ","","","  ","",". That is, reordering of the component tiles are permitted by the","operators (assigned non-zero probability), but the longer the movement from the original order of the tiles, the lower the probability. (This distor-tion model is similar with the one used in (Koehn, 2004).) When multiple tiles are available for the same span  ","",", they are joined by a","operator, where","is specified by the probability distributions specified in the translation table. Usually, statistical phrase-based translation tables specify not only one, but multiple distributions that account for context preferences. In our experiments, we consider four probability distributions: ","        ","","   ","","  ","","",", and    ","","","","","",", where","and","are Chinese-English phrase translations as","they appear in the translation table. In Figure 6,","we show an example of WIDL-expression created","by this algorithm1",".","On average, a WIDL-expression created by this","algorithm, using an average of","  ","tiles per sentence (for an average input sentence length of 30 words) and an average of","","possible translations per tile, encodes a candidate set of about 10  ","possible translations. As the specification","of the","operators takes space","   , Theorem 1 1 English reference: the gunman was shot dead by the police. guarantees that these WIDL-expressions encode compactly these huge spaces in","","","",". In the second phase, we employ our WIDL-based realization engine to interpolate the distribution probabilities of WIDL-expressions with a trigram language model. In the notation of Equation 2, we use four feature functions","   ","for the WIDL-expression distributions (one for each probability distribution encoded); a feature function","","for a trigram language model; a feature function","","for a word-count model, and a feature function","","for a phrase-count model. As acknowledged in the Machine Translation literature (Germann et al., 2003), full A","search is not usually possible, due to the large size of the search spaces. We therefore use an approxima-tion algorithm, called WIDL-NGLM-A ",", which considers for unfolding only the nodes extracted from the priority queue","which already unfolded a path of length greater than or equal to the maximum length already unfolded minus","(we used ","in the experiments reported here). MT Performance Evaluation. When evaluated against the state-of-the-art, phrase-based decoder Pharaoh (Koehn, 2004), using the same experimental conditions – translation table trained on the FBIS corpus (7.2M Chinese words and 9.2M English words of parallel text), trigram language model trained on 155M words of English newswire, interpolation weights","(Equation 2) trained using discriminative training (Och, 2003) (on the 2002 NIST MT evaluation set), probabilistic beam","set to 0.01, histogram beam","set to 10 – and BLEU (Papineni et al., 2002) as our metric, the WIDL-NGLM-A","","algorithm produces translations that have a BLEU score of 0.2570, while Pharaoh translations have a BLEU score of 0.2635. The difference is not statistically significant at 95% confidencelevel.","These results show that the WIDL-based approach to machine translation is powerful enough to achieve translation accuracy comparable with state-of-the-art systems in machine translation."]},{"title":"6 Conclusions","paragraphs":["The approach to sentence realization we advocate in this paper relies on WIDL-expressions, a formal language with convenient theoretical properties that can accommodate a wide range of generation scenarios. In the worst case, one can work with simple bags of words that encode no context 1111 preferences (Soricut and Marcu, 2005). One can also work with bags of words and phrases that encode context preferences, a scenario that applies to current approaches in statistical machine translation (Section 5). And one can also encode context and ordering preferences typically used in summarization (Section 4).","The generation engine we describe enables a tight coupling of content selection with sentence realization preferences. Its algorithm comes with theoretical guarantees about its optimality. Because the requirements for producing WIDL-expressions are minimal, our WIDL-based generation engine can be employed, with state-of-the-art results, in a variety of text-to-text applications. Acknowledgments This work was partially supported under the GALE program of the Defense Advanced Research Projects Agency, Contract No. HR0011-06-C-0022."]},{"title":"References","paragraphs":["Srinivas Bangalore and Owen Rambow. 2000. Using TAG, a tree model, and a language model for generation. In Proceedings of the Fifth International Workshop on Tree-Adjoining Grammars (TAG+).","Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein. 2001. Introduction to Algorithms. The MIT Press and McGraw-Hill.","Simon Corston-Oliver, Michael Gamon, Eric K. Ringger, and Robert Moore. 2002. An overview of Amalgam: A machine-learned generation module. In Proceedings of the INLG.","Bonnie Dorr, David Zajic, and Richard Schwartz. 2003. Hedge trimmer: a parse-and-trim approach to headline generation. In Proceedings of the HLT-NAACL Text Summarization Workshop, pages 1–8.","Michael Elhadad. 1991. FUF User manual — version 5.0. Technical Report CUCS-038-91, Department of Computer Science, Columbia University.","Ulrich Germann, Mike Jahr, Kevin Knight, Daniel Marcu, and Kenji Yamada. 2003. Fast decoding and optimal decoding for machine translation. Artificial Intelligence, 154(1–2):127-143.","Nizar Habash. 2003. Matador: A large-scale Spanish-English GHMT system. In Proceedings of AMTA.","J. Hajic, M. Cmejrek, B. Dorr, Y. Ding, J. Eisner, D. Gildea, T. Koo, K. Parton, G. Penn, D. Radev, and O. Rambow. 2002. Natural language generation in the context of machine translation. Summer workshop final report, Johns Hopkins University.","K. Knight and V. Hatzivassiloglou. 1995. Two level, many-path generation. In Proceedings of the ACL.","Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003. Statistical phrase based translation. In Proceedings of the HLT-NAACL, pages 127–133.","Philipp Koehn. 2004. Pharaoh: a beam search decoder for phrase-based statistical machine transltion models. In Proceedings of the AMTA, pages 115–124.","I. Langkilde-Geary. 2002. A foundation for generalpurpose natural language generation: sentence realization using probabilistic models of language. Ph.D. thesis, University of Southern California.","Chin-Yew Lin. 2004. ROUGE: a package for automatic evaluation of summaries. In Proceedings of the Workshop on Text Summarization Branches Out (WAS 2004).","Christian Matthiessen and John Bateman. 1991. Text Generation and Systemic-Functional Linguistic. Pinter Publishers, London.","Mehryar Mohri, Fernando Pereira, and Michael Riley. 2002. Weighted finite-state transducers in speech recognition. Computer Speech and Language, 16(1):69–88.","Mark-Jan Nederhof and Giorgio Satta. 2004. IDL-expressions: a formalism for representing and parsing finite languages in natural language processing. Journal of Artificial Intelligence Research, pages 287–317.","Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of the ACL, pages 160–167.","Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In In Proceedings of the ACL, pages 311–318.","Stuart Russell and Peter Norvig. 1995. ArtificialIntelligence. A Modern Approach. Prentice Hall.","Radu Soricut and Daniel Marcu. 2005. Towards developing generation algorithms for text-to-text applications. In Proceedings of the ACL, pages 66–74.","Radu Soricut. 2006. Natural Language Generation for Text-to-Text Applications Using an Information-Slim Representation. Ph.D. thesis, University of Southern California.","David Zajic, Bonnie J. Dorr, and Richard Schwartz. 2004. BBN/UMD at DUC-2004: Topiary. In Proceedings of the NAACL Workshop on Document Understanding, pages 112–119.","Liang Zhou and Eduard Hovy. 2003. Headline summarization at ISI. In Proceedings of the NAACL Workshop on Document Understanding. 1112"]}],"references":[{"authors":[{"first":"Srinivas","last":"Bangalore"},{"first":"Owen","last":"Rambow"}],"year":"2000","title":"Using TAG, a tree model, and a language model for generation"},{"authors":[{"first":"Thomas","middle":"H.","last":"Cormen"},{"first":"Charles","middle":"E.","last":"Leiserson"},{"first":"Ronald","middle":"L.","last":"Rivest"},{"first":"Clifford","last":"Stein"}],"year":"2001","title":"Introduction to Algorithms"},{"authors":[{"first":"Simon","last":"Corston-Oliver"},{"first":"Michael","last":"Gamon"},{"first":"Eric","middle":"K.","last":"Ringger"},{"first":"Robert","last":"Moore"}],"year":"2002","title":"An overview of Amalgam: A machine-learned generation module"},{"authors":[{"first":"Bonnie","last":"Dorr"},{"first":"David","last":"Zajic"},{"first":"Richard","last":"Schwartz"}],"year":"2003","title":"Hedge trimmer: a parse-and-trim approach to headline generation"},{"authors":[{"first":"Michael","last":"Elhadad"}],"year":"1991","title":"FUF User manual — version 5"},{"authors":[{"first":"Ulrich","last":"Germann"},{"first":"Mike","last":"Jahr"},{"first":"Kevin","last":"Knight"},{"first":"Daniel","last":"Marcu"},{"first":"Kenji","last":"Yamada"}],"year":"2003","title":"Fast decoding and optimal decoding for machine translation"},{"authors":[{"first":"Nizar","last":"Habash"}],"year":"2003","title":"Matador: A large-scale Spanish-English GHMT system"},{"authors":[{"first":"J.","last":"Hajic"},{"first":"M.","last":"Cmejrek"},{"first":"B.","last":"Dorr"},{"first":"Y.","last":"Ding"},{"first":"J.","last":"Eisner"},{"first":"D.","last":"Gildea"},{"first":"T.","last":"Koo"},{"first":"K.","last":"Parton"},{"first":"G.","last":"Penn"},{"first":"D.","last":"Radev"},{"first":"O.","last":"Rambow"}],"year":"2002","title":"Natural language generation in the context of machine translation"},{"authors":[{"first":"K.","last":"Knight"},{"first":"V.","last":"Hatzivassiloglou"}],"year":"1995","title":"Two level, many-path generation"},{"authors":[{"first":"Philipp","last":"Koehn"},{"first":"Franz","middle":"J.","last":"Och"},{"first":"Daniel","last":"Marcu"}],"year":"2003","title":"Statistical phrase based translation"},{"authors":[{"first":"Philipp","last":"Koehn"}],"year":"2004","title":"Pharaoh: a beam search decoder for phrase-based statistical machine transltion models"},{"authors":[{"first":"I.","last":"Langkilde-Geary"}],"year":"2002","title":"A foundation for generalpurpose natural language generation: sentence realization using probabilistic models of language"},{"authors":[{"first":"Chin-Yew","last":"Lin"}],"year":"2004","title":"ROUGE: a package for automatic evaluation of summaries"},{"authors":[{"first":"Christian","last":"Matthiessen"},{"first":"John","last":"Bateman"}],"year":"1991","title":"Text Generation and Systemic-Functional Linguistic"},{"authors":[{"first":"Mehryar","last":"Mohri"},{"first":"Fernando","last":"Pereira"},{"first":"Michael","last":"Riley"}],"year":"2002","title":"Weighted finite-state transducers in speech recognition"},{"authors":[{"first":"Mark-Jan","last":"Nederhof"},{"first":"Giorgio","last":"Satta"}],"year":"2004","title":"IDL-expressions: a formalism for representing and parsing finite languages in natural language processing"},{"authors":[{"first":"Franz","middle":"Josef","last":"Och"}],"year":"2003","title":"Minimum error rate training in statistical machine translation"},{"authors":[{"first":"Kishore","last":"Papineni"},{"first":"Salim","last":"Roukos"},{"first":"Todd","last":"Ward"},{"first":"Wei-Jing","last":"Zhu"}],"year":"2002","title":"BLEU: a method for automatic evaluation of machine translation"},{"authors":[{"first":"Stuart","last":"Russell"},{"first":"Peter","last":"Norvig"}],"year":"1995","title":"ArtificialIntelligence"},{"authors":[{"first":"Radu","last":"Soricut"},{"first":"Daniel","last":"Marcu"}],"year":"2005","title":"Towards developing generation algorithms for text-to-text applications"},{"authors":[{"first":"Radu","last":"Soricut"}],"year":"2006","title":"Natural Language Generation for Text-to-Text Applications Using an Information-Slim Representation"},{"authors":[{"first":"David","last":"Zajic"},{"first":"Bonnie","middle":"J.","last":"Dorr"},{"first":"Richard","last":"Schwartz"}],"year":"2004","title":"BBN/UMD at DUC-2004: Topiary"},{"authors":[{"first":"Liang","last":"Zhou"},{"first":"Eduard","last":"Hovy"}],"year":"2003","title":"Headline summarization at ISI"}],"cites":[{"authors":[{"last":"Matthiessen"},{"last":"Bateman"}],"year":"1991","style":0,"reference":{"authors":[{"first":"Christian","last":"Matthiessen"},{"first":"John","last":"Bateman"}],"year":"1991","title":"Text Generation and Systemic-Functional Linguistic"}},{"authors":[{"last":"Elhadad"}],"year":"1991","style":0,"reference":{"authors":[{"first":"Michael","last":"Elhadad"}],"year":"1991","title":"FUF User manual — version 5"}},{"authors":[{"last":"Knight"},{"last":"Hatzivassiloglou"}],"year":"1995","style":0,"reference":{"authors":[{"first":"K.","last":"Knight"},{"first":"V.","last":"Hatzivassiloglou"}],"year":"1995","title":"Two level, many-path generation"}},{"authors":[{"last":"Bangalore"},{"last":"Rambow"}],"year":"2000","style":0,"reference":{"authors":[{"first":"Srinivas","last":"Bangalore"},{"first":"Owen","last":"Rambow"}],"year":"2000","title":"Using TAG, a tree model, and a language model for generation"}},{"authors":[{"last":"Langkilde-Geary"}],"year":"2002","style":0,"reference":{"authors":[{"first":"I.","last":"Langkilde-Geary"}],"year":"2002","title":"A foundation for generalpurpose natural language generation: sentence realization using probabilistic models of language"}},{"authors":[{"last":"Corston-Oliver"},{"last":"al."}],"year":"2002","style":0,"reference":{"authors":[{"first":"Simon","last":"Corston-Oliver"},{"first":"Michael","last":"Gamon"},{"first":"Eric","middle":"K.","last":"Ringger"},{"first":"Robert","last":"Moore"}],"year":"2002","title":"An overview of Amalgam: A machine-learned generation module"}},{"authors":[{"last":"Hajic"},{"last":"al."}],"year":"2002","style":0,"reference":{"authors":[{"first":"J.","last":"Hajic"},{"first":"M.","last":"Cmejrek"},{"first":"B.","last":"Dorr"},{"first":"Y.","last":"Ding"},{"first":"J.","last":"Eisner"},{"first":"D.","last":"Gildea"},{"first":"T.","last":"Koo"},{"first":"K.","last":"Parton"},{"first":"G.","last":"Penn"},{"first":"D.","last":"Radev"},{"first":"O.","last":"Rambow"}],"year":"2002","title":"Natural language generation in the context of machine translation"}},{"authors":[{"last":"Habash"}],"year":"2003","style":0,"reference":{"authors":[{"first":"Nizar","last":"Habash"}],"year":"2003","title":"Matador: A large-scale Spanish-English GHMT system"}},{"authors":[{"last":"Nederhof"},{"last":"Satta"}],"year":"2004","style":0,"reference":{"authors":[{"first":"Mark-Jan","last":"Nederhof"},{"first":"Giorgio","last":"Satta"}],"year":"2004","title":"IDL-expressions: a formalism for representing and parsing finite languages in natural language processing"}},{"authors":[{"last":"Knight"},{"last":"Hatzivassiloglou"}],"year":"1995","style":0,"reference":{"authors":[{"first":"K.","last":"Knight"},{"first":"V.","last":"Hatzivassiloglou"}],"year":"1995","title":"Two level, many-path generation"}},{"authors":[{"last":"Langkilde-Geary"}],"year":"2002","style":0,"reference":{"authors":[{"first":"I.","last":"Langkilde-Geary"}],"year":"2002","title":"A foundation for generalpurpose natural language generation: sentence realization using probabilistic models of language"}},{"authors":[{"last":"Nederhof"},{"last":"Satta"}],"year":"2004","style":0,"reference":{"authors":[{"first":"Mark-Jan","last":"Nederhof"},{"first":"Giorgio","last":"Satta"}],"year":"2004","title":"IDL-expressions: a formalism for representing and parsing finite languages in natural language processing"}},{"authors":[{"last":"Soricut"}],"year":"2006","style":0,"reference":{"authors":[{"first":"Radu","last":"Soricut"}],"year":"2006","title":"Natural Language Generation for Text-to-Text Applications Using an Information-Slim Representation"}},{"authors":[{"last":"Mohri"},{"last":"al."}],"year":"2002","style":0,"reference":{"authors":[{"first":"Mehryar","last":"Mohri"},{"first":"Fernando","last":"Pereira"},{"first":"Michael","last":"Riley"}],"year":"2002","title":"Weighted finite-state transducers in speech recognition"}},{"authors":[{"last":"Soricut"}],"year":"2006","style":0,"reference":{"authors":[{"first":"Radu","last":"Soricut"}],"year":"2006","title":"Natural Language Generation for Text-to-Text Applications Using an Information-Slim Representation"}},{"authors":[{"last":"Soricut"},{"last":"Marcu"}],"year":"2005","style":0,"reference":{"authors":[{"first":"Radu","last":"Soricut"},{"first":"Daniel","last":"Marcu"}],"year":"2005","title":"Towards developing generation algorithms for text-to-text applications"}},{"authors":[{"last":"Russell"},{"last":"Norvig"}],"year":"1995","style":0,"reference":{"authors":[{"first":"Stuart","last":"Russell"},{"first":"Peter","last":"Norvig"}],"year":"1995","title":"ArtificialIntelligence"}},{"authors":[{"last":"Cormen"},{"last":"al."}],"year":"2001","style":0,"reference":{"authors":[{"first":"Thomas","middle":"H.","last":"Cormen"},{"first":"Charles","middle":"E.","last":"Leiserson"},{"first":"Ronald","middle":"L.","last":"Rivest"},{"first":"Clifford","last":"Stein"}],"year":"2001","title":"Introduction to Algorithms"}},{"authors":[{"last":"Soricut"},{"last":"Marcu"}],"year":"2005","style":0,"reference":{"authors":[{"first":"Radu","last":"Soricut"},{"first":"Daniel","last":"Marcu"}],"year":"2005","title":"Towards developing generation algorithms for text-to-text applications"}},{"authors":[{"last":"Dorr"},{"last":"al."}],"year":"2003","style":0,"reference":{"authors":[{"first":"Bonnie","last":"Dorr"},{"first":"David","last":"Zajic"},{"first":"Richard","last":"Schwartz"}],"year":"2003","title":"Hedge trimmer: a parse-and-trim approach to headline generation"}},{"authors":[{"last":"Zajic"},{"last":"al."}],"year":"2004","style":0,"reference":{"authors":[{"first":"David","last":"Zajic"},{"first":"Bonnie","middle":"J.","last":"Dorr"},{"first":"Richard","last":"Schwartz"}],"year":"2004","title":"BBN/UMD at DUC-2004: Topiary"}},{"authors":[{"last":"Zhou"},{"last":"Hovy"}],"year":"2003","style":0,"reference":{"authors":[{"first":"Liang","last":"Zhou"},{"first":"Eduard","last":"Hovy"}],"year":"2003","title":"Headline summarization at ISI"}},{"authors":[{"last":"Lin"}],"year":"2004","style":0,"reference":{"authors":[{"first":"Chin-Yew","last":"Lin"}],"year":"2004","title":"ROUGE: a package for automatic evaluation of summaries"}},{"authors":[{"last":"Och"}],"year":"2003","style":0,"reference":{"authors":[{"first":"Franz","middle":"Josef","last":"Och"}],"year":"2003","title":"Minimum error rate training in statistical machine translation"}},{"authors":[{"last":"Dorr"},{"last":"al."}],"year":"2003","style":0,"reference":{"authors":[{"first":"Bonnie","last":"Dorr"},{"first":"David","last":"Zajic"},{"first":"Richard","last":"Schwartz"}],"year":"2003","title":"Hedge trimmer: a parse-and-trim approach to headline generation"}},{"authors":[{"last":"Zajic"},{"last":"al."}],"year":"2004","style":0,"reference":{"authors":[{"first":"David","last":"Zajic"},{"first":"Bonnie","middle":"J.","last":"Dorr"},{"first":"Richard","last":"Schwartz"}],"year":"2004","title":"BBN/UMD at DUC-2004: Topiary"}},{"authors":[{"last":"Zhou"},{"last":"Hovy"}],"year":"2003","style":0,"reference":{"authors":[{"first":"Liang","last":"Zhou"},{"first":"Eduard","last":"Hovy"}],"year":"2003","title":"Headline summarization at ISI"}},{"authors":[{"last":"Zajic"},{"last":"al."}],"year":"2004","style":0,"reference":{"authors":[{"first":"David","last":"Zajic"},{"first":"Bonnie","middle":"J.","last":"Dorr"},{"first":"Richard","last":"Schwartz"}],"year":"2004","title":"BBN/UMD at DUC-2004: Topiary"}},{"authors":[{"last":"Zhou"},{"last":"Hovy"}],"year":"2003","style":0,"reference":{"authors":[{"first":"Liang","last":"Zhou"},{"first":"Eduard","last":"Hovy"}],"year":"2003","title":"Headline summarization at ISI"}},{"authors":[{"last":"Koehn"},{"last":"al."}],"year":"2003","style":0,"reference":{"authors":[{"first":"Philipp","last":"Koehn"},{"first":"Franz","middle":"J.","last":"Och"},{"first":"Daniel","last":"Marcu"}],"year":"2003","title":"Statistical phrase based translation"}},{"authors":[{"last":"Koehn"}],"year":"2004","style":0,"reference":{"authors":[{"first":"Philipp","last":"Koehn"}],"year":"2004","title":"Pharaoh: a beam search decoder for phrase-based statistical machine transltion models"}},{"authors":[{"last":"Germann"},{"last":"al."}],"year":"2003","style":0,"reference":{"authors":[{"first":"Ulrich","last":"Germann"},{"first":"Mike","last":"Jahr"},{"first":"Kevin","last":"Knight"},{"first":"Daniel","last":"Marcu"},{"first":"Kenji","last":"Yamada"}],"year":"2003","title":"Fast decoding and optimal decoding for machine translation"}},{"authors":[{"last":"Koehn"}],"year":"2004","style":0,"reference":{"authors":[{"first":"Philipp","last":"Koehn"}],"year":"2004","title":"Pharaoh: a beam search decoder for phrase-based statistical machine transltion models"}},{"authors":[{"last":"Och"}],"year":"2003","style":0,"reference":{"authors":[{"first":"Franz","middle":"Josef","last":"Och"}],"year":"2003","title":"Minimum error rate training in statistical machine translation"}},{"authors":[{"last":"Papineni"},{"last":"al."}],"year":"2002","style":0,"reference":{"authors":[{"first":"Kishore","last":"Papineni"},{"first":"Salim","last":"Roukos"},{"first":"Todd","last":"Ward"},{"first":"Wei-Jing","last":"Zhu"}],"year":"2002","title":"BLEU: a method for automatic evaluation of machine translation"}},{"authors":[{"last":"Soricut"},{"last":"Marcu"}],"year":"2005","style":0,"reference":{"authors":[{"first":"Radu","last":"Soricut"},{"first":"Daniel","last":"Marcu"}],"year":"2005","title":"Towards developing generation algorithms for text-to-text applications"}}]}
