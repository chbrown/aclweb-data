{"sections":[{"title":"DISTRIBUTIONAL CLUSTERING OF ENGLISH WORDS Fernando Pereira AT&T Bell Laboratories 600 Mountain Ave. Murray Hill, NJ 07974, USA pereira@research, att. com Naftali Tishby Dept. of Computer Science Hebrew University Jerusalem 91904, Israel tishby@cs, hu]i. ac. il","paragraphs":["Lillian"]},{"title":"Lee","paragraphs":["Dept. of Computer Science","Cornell University Ithaca, NY 14850, USA"]},{"title":"llee~cs, cornell, edu Abstract","paragraphs":["We describe and evaluate experimentally a method for clustering words according to their distribution in particular syntactic contexts. Words"]},{"title":"are","paragraphs":["represented by the relative frequency distributions of contexts in which they appear, and relative entropy between those distributions is used as the similarity measure for clustering. Clusters are represented by average context distributions derived from the given words according to their probabilities of cluster membership. In many cases, the clusters can be thought of as encoding coarse sense distinctions. Deterministic annealing is used to find lowest distortion sets of clusters: as the annealing parameter increases, existing clusters be-come unstable and subdivide, yielding a hierarchical \"soft\" clustering of the data. Clusters are used as the basis for class models of word coocurrence, and the models evaluated with respect to held-out test data. INTRODUCTION Methods for automatically classifying words according to their contexts of use have both scientific and practical interest. The scientific questions arise in connection to distributional views of linguistic (particularly lexical) structure and also in relation to the question of lexical acquisition both from psychological and computational learning perspectives. From the practical point of view, word classification addresses questions of data sparseness and generalization in statistical language models, particularly models for deciding among alternative analyses proposed by a grammar.","It is well known that a simple tabulation of frequencies of certain words participating in certain configurations, for example of frequencies of pairs of a transitive main verb and the head noun of its direct object, cannot be reliably used for comparing the likelihoods of different alternative configurations. The problemis that for large enough corpora the number of possible joint events is much larger than the number of event occurrences in the corpus, so many events are seen rarely or never, making their frequency counts unreliable estimates of their probabilities.","Hindle (1990) proposed dealing with the sparseness problem by estimating the likelihood of unseen events from that of \"similar\" events that have been seen. For instance, one may estimate the likelihood of a particular direct object for a verb from the likelihoods of that direct object for similar verbs. This requires a reasonable definition of verb similarity and a similarity estimation method. In Hindle's proposal, words are similar if we have strong statistical evidence that they tend to participate in the same events. His notion of similarity seems to agree with our intuitions in many cases, but it is not clear how it can be used directly to construct word classes and corresponding models of association.","Our research addresses some of the same questions and uses similar raw data, but we investigate how to factor word association tendencies into associations of words to certain hidden"]},{"title":"senses classes","paragraphs":["and associations between the classes themselves. While it may be worth basing such a model on preexisting sense classes (Resnik, 1992), in the work described here we look at how to derive the classes directly from distributional data. More specifically, we model senses as probabilistic concepts or"]},{"title":"clusters","paragraphs":["c with corresponding cluster membership probabilities"]},{"title":"p(clw )","paragraphs":["for each word w. Most other class-based modeling techniques for natural language rely instead on \"hard\" Boolean classes (Brown et al., 1990). Class construction is then combinatorially very demanding and depends on frequency counts for joint events involving particular words, a potentially unreliable source of information as noted above. Our approach avoids both problems."]},{"title":"Problem Setting","paragraphs":["In what follows, we will consider two major word classes, 12 and Af, for the verbs and nouns in our experiments, and a single relation between them, in our experiments the relation between a transitive main verb and the head noun of its direct object. Our raw knowledge about the relation consists of the frequencies"]},{"title":"f~n","paragraphs":["of occurrence of particular pairs (v,n) in the required configuration in a training corpus. Some form of text analysis is required to collect such a collection of pairs. The corpus used in our first experiment was derived from newswire text automatically parsed by"]},{"title":"183","paragraphs":["Hindle's parser Fidditch (Hindle, 1993). More recently, we have constructed similar tables with the help of a statistical part-of-speech tagger (Church, 1988) and of tools for regular expression pattern matching on tagged corpora (Yarowsky, 1992). We have not yet compared the accuracy and coverage of the two methods, or what systematic biases they might introduce, although we took care to fil-ter out certain systematic errors, for instance the misparsing of the subject of a complement clause as the direct object of a main verb for report verbs like"]},{"title":"\"say\".","paragraphs":["We will consider here only the problem of classifying nouns according to their distribution as direct objects of verbs; the converse problem is for-mally similar. More generally, the theoretical basis for our method supports the use of clustering to build models for any n-ary relation in terms of associations between elements in each coordinate and appropriate hidden units (cluster centroids) and associations between thosehidden units.","For the noun classification problem, the empirical distribution of a noun n is then given by the conditional distribution"]},{"title":"p,~(v) = f~./ ~v f\"~\"","paragraphs":["The problem we study is how to use the Pn to classify the n EAf. Our classification method will construct a set C of clusters and cluster membership probabilities"]},{"title":"p(c]n).","paragraphs":["Each cluster c is associated to a cluster"]},{"title":"centroid Pc,","paragraphs":["which is a distribution over l; obtained by averaging appropriately the pn."]},{"title":"Distributional Similarity","paragraphs":["To cluster nouns n according to their conditional verb distributions"]},{"title":"Pn,","paragraphs":["we need a measure of similarity between distributions. We use for this purpose the"]},{"title":"relative entropy","paragraphs":["or"]},{"title":"Kullback-Leibler (KL) distance","paragraphs":["between two distributions"]},{"title":"O(p","paragraphs":["I[ q) ="]},{"title":"ZP(x)","paragraphs":["log"]},{"title":"p(x) : q(x)","paragraphs":["This is a natural choice for a variety of reasons, which we will just sketch here)","First of all,"]},{"title":"D(p I[ q)","paragraphs":["is zero just when p = q, and it increases as the probability decreases that p is the relative frequency distribution of a random sample drawn according to q. More formally, the probability mass given by q to the set of all samples of length n with relative frequency distribution p is bounded by exp-nn(p I] q) (Cover and Thomas, 1991). Therefore, if we are try-ing to distinguish among hypotheses"]},{"title":"qi","paragraphs":["when p is the relative frequency distribution of observations,"]},{"title":"D(p II ql)","paragraphs":["gives the relative weight of evidence in favor of"]},{"title":"qi.","paragraphs":["Furthermore, a similar relation holds between"]},{"title":"D(p IIP')","paragraphs":["for two empirical distributions p and p' and the probability that p and p~ are drawn from the same distribution q. We can thus use the relative entropy between the context distributions for two words to measure how likely they are to be instances of the same cluster centroid. aA more formal discussion will appear in our paper"]},{"title":"Distributional Clustering,","paragraphs":["in preparation. From an information theoretic perspective"]},{"title":"D(p ]1 q)","paragraphs":["measures how inefficient on average it would be to use a code based on q to encode a variable distributed according to p. With respect to our problem,"]},{"title":"D(pn H Pc)","paragraphs":["thus gives us the information loss in using cluster centroid Pc instead of the actual distribution pn for word n when modeling the distributional properties of n.","Finally, relative entropy is a natural measure of similarity between distributions for clustering because its minimization leads to cluster centroids that are a simple weighted average of member distributions.","One technical difficulty is that"]},{"title":"D(p","paragraphs":["[1 p') is not defined when"]},{"title":"p'(x)","paragraphs":["= 0 but"]},{"title":"p(x)","paragraphs":["> 0. We could sidestep this problem (as we did initially) by smoothing zero frequencies appropriately (Church and Gale, 1991). However, this is not very satisfactory because one of the goals of our work is precisely to avoid the problems of data sparseness by grouping words into classes. It turns out that the problem is avoided by our clustering technique, since it does not need to compute the KL distance between individual word distributions, but only between a word distribution and average distributions, the current cluster centroids, which are guaranteed to be nonzero whenever the word distributions are. This is a useful advantage of our method compared with agglomerative clustering techniques that need to compare individual objects being considered for grouping. THEORETICAL BASIS In general, we are interested in how to organize a set of linguistic objects such as words according to the contexts in which they occur, for instance grammatical constructions or n-grams. We will show elsewhere that the theoretical analysis out-lined here applies to that more general problem, but for now we will only address the more specific problem in which the objects are nouns and the contexts are verbs that take the nouns as direct objects.","Our problem can be seen as that of learning a joint distribution of pairs from a large sample of pairs. The pair coordinates come from two large sets ./kf and 12, with no preexisting internal structure, and the training data is a sequence S of N independently drawn pairs"]},{"title":"Si = (ni, vi) 1 < i < N .","paragraphs":["From a learning perspective, this problem falls somewhere in between unsupervised and supervised learning. As in unsupervised learning, the goal is to learn the underlying distribution of the data. But in contrast to most unsupervised learning settings, the objects involved have no internal structure or attributes allowing them to be compared with each other. Instead, the only information about the objects is the statistics of their joint appearance. These statistics can thus be seen as a weak form of object labelling analogous to supervision. 184"]},{"title":"Distributional","paragraphs":["Clustering While clusters based on distributional similarity are interesting on their own, they can also be profitably seen as a means of summarizing a joint distribution. In particular, we would like to find a set of clusters C such that each conditional distribution"]},{"title":"pn(v)","paragraphs":["can be approximately decomposed as"]},{"title":"p,(v) = ~p(cln)pc(v) ,","paragraphs":["cEC where"]},{"title":"p(c[n)","paragraphs":["is the membership probability of n in c and"]},{"title":"pc(v) = p(vlc )","paragraphs":["is v's conditional probability","given by the centroid distribution for cluster c.","The above decomposition can be written in a","more symmetric form as"]},{"title":"~(n,v) = ~_,p(c,n)p(vlc )","paragraphs":["cEC"]},{"title":"= ~-~p(c)P(nlc)P(Vlc)","paragraphs":["(1) cEC assuming that"]},{"title":"p(n)","paragraphs":["and /5(n) coincide. We will take (1) as our basic clustering model.","To determine this decomposition we need to solve the two connected problems of finding suitable forms for the cluster membership"]},{"title":"p(c[n)","paragraphs":["and the centroid distributions"]},{"title":"p(vlc),","paragraphs":["and of maximizing the goodness of fit between the model distribution 15(n, v) and the observed data.","Goodness of fit is determined by the model's likelihood of the observations. The maximum likelihood (ML) estimation principle is thus the natural tool to determine the centroid distributions"]},{"title":"pc(v).","paragraphs":["As for the membership probabilities, they must be determined solely by the relevant measure of object-to-cluster similarity, which in the present work is the relative entropy between object and cluster centroid distributions. Since no other information is available, the membership is determined by maximizing the configuration entropy for a fixed average distortion. With the maximum entropy (ME) membership distribution, ML estimation is equivalent to the minimization of the average distortion of the data. The combined entropy maximization entropy and distortion minimization is carried out by a two-stage iterative process similar to the EM method (Dempster et al., 1977). The first stage of an iteration is a maximum likelihood, or minimum distortion, estimation of the cluster centroids given fixed membership probabilities. In the second stage of each iteration, the entropy of the membership distribution is maximized for a fixed average distortion. This joint optimization searches for a"]},{"title":"saddle point","paragraphs":["in the distortion-entropy parameters, which is equivalent to minimizing a linear combination of the two known as"]},{"title":"free energy","paragraphs":["in statistical mechanics. This analogy with statistical mechanics is not coincidental, and provides a better understanding of the clustering procedure. Maximum Likelihood Cluster Centroids For the maximum likelihood argument, we start by estimating the likelihood of the sequence S of N independent observations of pairs (ni,vi). Using (1), the sequence's model log likelihood is N"]},{"title":"l(S) = log p(c)p(n, le)p(vilc).","paragraphs":["i=l cEC Fixing the number of clusters (model size)"]},{"title":"Icl,","paragraphs":["we want to maximize"]},{"title":"l(S)","paragraphs":["with respect to the distributions"]},{"title":"P(nlc )","paragraphs":["and"]},{"title":"p(vlc).","paragraphs":["The variation of"]},{"title":"l(S)","paragraphs":["with respect to these distributions is"]},{"title":"N /v(v, Ic)@(n","paragraphs":["~fl(S) =~ 1 ~..~p(c)| + / (2) i=1"]},{"title":"P(ni, vi) c~c \\P(nilc)6p(vi Ic)]","paragraphs":["with"]},{"title":"p(nlc )","paragraphs":["and"]},{"title":"p(vlc )","paragraphs":["kept normalized. Using Bayes's formula, we have"]},{"title":"1 v( lni, ~(ni, vi) -- p(c)p(ni[c)p(vi[c)","paragraphs":["(3) for any c. 2 Substituting (3) into (2), we obtain"]},{"title":"N (,logp(n, lc)) ~l(S) = ZZp(clni,vi) +","paragraphs":["(4)"]},{"title":"logp(vi","paragraphs":["Ic) i=1 cEC since ~flogp --"]},{"title":"@/p.","paragraphs":["This expression is particularly useful when the cluster distributions"]},{"title":"p(n[c)","paragraphs":["and"]},{"title":"p(vlc )","paragraphs":["have an exponential form, precisely what will be provided by the ME step described below.","At this point we need to specify the clustering model in more detail. In the derivation so far we have treated, p(n c) and"]},{"title":"p(v c)","paragraphs":["symmetrically, corresponding to clusters not of verbs or nouns but of verb-noun associations. In principle such a symmetric model may be more accurate, but in this paper we will concentrate on"]},{"title":"asymmetric mod- els","paragraphs":["in which cluster memberships are associated to just one of the components of the joint distribution and the cluster centroids are specified only by the other component. In particular, the model we use in our experiments has noun clusters with cluster memberships determined by"]},{"title":"p(nlc)","paragraphs":["and centroid distributions determined by"]},{"title":"p(vlc ).","paragraphs":["The asymmetric model simplifies the estimation significantly by dealing with a single component, but it has the disadvantage that the joint distribution,"]},{"title":"p(n,","paragraphs":["v) has two different and not necessarily consistent expressions in terms of asymmetric models for the two coordinates.","2As usual in clustering models (Duda and Hart, 1973), we assume that the model distribution and the empirical distribution are interchangeable at the solution of the parameter estimation equations, since the model is assumed to be able to represent correctly the data at that solution point. In practice, the data may not come exactly from the chosen model class, but the model obtained by solving the estimation equations may still be the closest one to the data."]},{"title":"185","paragraphs":["Maximum Entropy Cluster Membership While variations of"]},{"title":"p(nlc )","paragraphs":["and"]},{"title":"p(vlc )","paragraphs":["iri equation (4) are not independent, we can treat them separately. First, for fixed average distortion between the cluster centroid distributions"]},{"title":"p(vlc )","paragraphs":["and the data"]},{"title":"p(vln),","paragraphs":["we find the cluster membership probabilities, which are the Bayes inverses of the"]},{"title":"p(nlc),","paragraphs":["that maximize the entropy of the cluster distributions. With the membership distributions thus obtained, we then look for the"]},{"title":"p(vlc )","paragraphs":["that maximize the log likelihood"]},{"title":"l(S).","paragraphs":["It turns out that this will also be the values"]},{"title":"ofp(vlc)","paragraphs":["that mini-","mize the average distortion between the asymmet-","ric cluster model and the data.","Given any similarity measure"]},{"title":"din , c)","paragraphs":["between nouns and cluster centroids, the average cluster distortion is"]},{"title":"(0) = ~_, ~,p(cln)d(n,c )","paragraphs":["(5) nEAr tEd If we maximize the cluster membership entropy"]},{"title":"H = - ~ Zp(cln)logp(nlc)","paragraphs":["(6) nEX cEd subject to normalization"]},{"title":"ofp(nlc)","paragraphs":["and fixed (5), we obtain the following standard exponential forms (Jaynes, 1983) for the class and membership distributions","1"]},{"title":"p(nlc)","paragraphs":["= Z-¢ exp"]},{"title":"-rid(n, c)","paragraphs":["(7) 1"]},{"title":"p(cJn)","paragraphs":["= ~ exp"]},{"title":"-rid(n, c)","paragraphs":["(8) where the normalization sums (partition func-tions) are Z~ = ~,~"]},{"title":"exp-fld(n,c)","paragraphs":["and Zn = ~exp-rid(n,c). Notice that"]},{"title":"d(n,c)","paragraphs":["does not need to be symmetric for this derivation, as the two distributions are simply related by Bayes's rule.","Returning to the log-likelihood variation (4), we can now use (7) for"]},{"title":"p(n[c)","paragraphs":["and the assumption for the asymmetric model that the cluster membership stays fixed as we adjust the centroids, to obtain","N"]},{"title":"61(S) = - ~ ~ p(elni)6rid(n,, c) + ~","paragraphs":["log Z~ (9) i=1 eEC where the variation of"]},{"title":"p(v[c)","paragraphs":["is now included in the variation of"]},{"title":"d(n, e).","paragraphs":["For a large enough sample, we may replace the sum over observations in (9) by the average over"]},{"title":"N","paragraphs":["61(s) = -"]},{"title":"p(n) -\"p(¢ln)6rid(n, ¢) + 6 logZ¢","paragraphs":["nEN cEC which, applying Bayes's rule, becomes 1"]},{"title":"61(S) = - ~ ~(~ ~ p(nlc)6rid(n, c) + 6","paragraphs":["log Z¢. eEC hEN At the log-likelihood maximum, this variation must vanish. We will see below that the use of relative entropy for similarity measure makes 6 log Zc vanish at the maximum as well, so the log likelihood can be maximized by minimizing the average distortion with respect to the class centroids while class membership is kept fixed","1","p(njc)6d(n,e)= o ,","cEC nEX or, sufficiently, if each of the inner sums vanish"]},{"title":"~ p(nlcl6d(n,c)=","paragraphs":["0 (10) tee nEAr Minimizing the Average KL Distortion We first show that the minimization of the relative entropy yields the natural expression for cluster centroids P(vle ) = ~ p(nlc)p(vln ) (11) nEW To minimize the average distortion (10), we observe that the variation of the KL distance between noun and centroid distributions with respect to the centroid distribution"]},{"title":"p(v[c),","paragraphs":["with each centroid distribution normalized by the Lagrange multiplier Ac, is given by"]},{"title":"( - ~evP(V[n)l°gp(v[c) ) ~d(n,c) = ~ + A¢(E,~ev p(vlc) -","paragraphs":["1)"]},{"title":"= ~-~( p(vln)+AO,p(vlc ) v(vl )","paragraphs":["Substituting this expression into (10), we obtain"]},{"title":", ,~ v p(vlc)","paragraphs":["Since the"]},{"title":"~p(vlc )","paragraphs":["are now independent, we obtain immediately the desired centroid expression (11), which is the desired weighted average of noun distributions.","We can now see that the variation (5 log Z~ vanishes for centroid distributions given by (11), since it follows from (10) that 6 log = exp-rid(, ,"]},{"title":"c)6d(n, e)","paragraphs":["Ze"]},{"title":"-ri","paragraphs":["--"]},{"title":"0","paragraphs":["n The Free Energy Function The combined minimum distortion and maximum entropy optimization is equivalent to the minimization of a single function, the"]},{"title":"free energy","paragraphs":["1 log Zn F = -~ = <D>-\"Hlri , where (D) is the average distortion (5) and H is the cluster membership entropy (6). 186","The free energy determines both the distortion and the membership entropy through"]},{"title":"OZF","paragraphs":["(D) -"]},{"title":"O~ OF","paragraphs":["H - OT ' where T =/~-1 is the temperature.","The most important property of the free energy is that its minimum determines the balance between the \"disordering\" maximum entropy and \"ordering\" distortion minimization in which the system is most likely to be found. In fact the probability to find the system at a given configuration is exponential in F Pocexp-flF , so a system is most likely to be found in its minimal free energy configuration."]},{"title":"Hierarchical Clustering","paragraphs":["The analogy with statistical mechanics suggests a deterministic annealing procedure for clustering Rose et al., 1990), in which the number of clusters s determined through a sequence of phase transitions by continuously increasing the parameter/? following an annealing schedule.","The higher is fl, the more local is the influence of each noun on the definition of centroids. Distributional similarity plays here the role of distortion. When the scale parameter fl is close to zero, the similarity is almost irrelevant. All words contribute about equally to each centroid, and so the lowest average distortion solution involves just one cluster whose centroid is the average of all word distributions. As fl is slowly increased, a critical point is eventually reached for which the lowest F solution involves two distinct centroids. We say then that the original cluster has split into the two new clusters.","In general, if we take any cluster c and a twin c' of c such that the centroid Pc' is a small random perturbation of Pc, below the critical fl at which c splits the membership and centroid reestimation procedure given by equations (8) and (11) will make pc and Pc, converge, that is, c and c' are really the same cluster. But with fl above the critical value for c, the two centroids will diverge, giving rise to two daughters of c.","Our clustering procedure is thus as follows. We start with very low /3 and a single cluster whose centroid is the average of all noun distributions. For any given fl, we have a current set of leaf clusters corresponding to the current free energy (local) minimum. To refine such a solution, we search for the lowest fl which is the critical value for some current leaf cluster splits. Ideally, there is just one split at that critical value, but for practical performance and numerical accuracy reasons we may have several splits at the new critical point. The splitting procedure can then be repeated to achieve the desired number of clusters or model cross-entropy. 3"]},{"title":"gun","paragraphs":["missile weapon rocket root"]},{"title":"1","paragraphs":["missile 0.835 officer rocket 0.850 aide bullet 0.917 chief 0.940 manager"]},{"title":"4","paragraphs":["0.758 shot 0.858 0.786 bullet 0.925 0.862 rocket 0.930 0.875 missile 1.037 2 0.484 0.612 0.649 0.651 Figure 1: Direct object clusters for fire CLUSTERING EXAMPLES All our experiments involve the asymmetric model described in the previous section. As explained there, our clustering procedure yields for each value of ~ a set CZ of clusters minimizing the free energy F, and the asymmetric model for fl estimates the conditional verb distribution for a noun n by cECB where p(cln ) also depends on ft.","As a first experiment, we used our method to classify the 64 nouns appearing most frequently as heads of direct objects of the verb \"fire\" in one year (1988) of Associated Press newswire. In this corpus, the chosen nouns appear as direct object heads of a total of 2147 distinct verbs, so each noun is represented by a density over the 2147 verbs.","Figure 1 shows the four words most similar to each cluster centroid, and the corresponding word-centroid KL distances, for the four clusters resulting from the first two cluster splits. It can be seen that first split separates the objects corresponding to the weaponry sense of \"fire\" (cluster 1) from the ones corresponding to the personnel action (cluster 2). The second split then further refines the weaponry sense into a projectile sense (cluster 3) and a gun sense (cluster 4). That split is some-what less sharp, possibly because not enough distinguishing contexts occur in the corpus.","Figure 2 shows the four closest nouns to the centroid of each of a set of hierarchical clusters derived from verb-object pairs involving the 1000 most frequent nouns in the June 1991 electronic version of Grolier's Encyclopedia (10 mil-"]},{"title":"187","paragraphs":["grant distinction form representation state 1.320 t residence ally 1.458 state residence 1.473 conductor /,..movement 1.534 teacher"]},{"title":"\"-number","paragraphs":["0.999 number material 1.361 material variety 1.401 mass mass 1.422'~ variety ~number diversity structure concentration"]},{"title":"J","paragraphs":["control 1.2011 recognition 1.317 nomination 1.363 ~i~i~im 1.366 1.392 ent 1.329 _ 1.554 voyage 1.338 -~- 1.571 ~migration 1.428 1.577 progress 1.441 ~ conductor 0.699 j Istate ]1.279 I vice-president 0.756~eople I 1.417] editor 0.814 Imodem 1.418 director 0.825 [farmer 1.425 1.082 j complex 1.161 ~aavy 1.096 I 1.102 network 1.175_._._~ommunity 1.099 I 1.213 community 1.276 ]aetwork 1.244 1.233 group 1.327~ Icomplex 1.259 \"~omplex [1.097 I"]},{"title":"Imaterial [ 0.976 ~network I 1\"2111","paragraphs":["1.026 ~alt ] 1.217[ lake 11.3601 1.093 ...------'-'-~mg 1.2441 ~region 11.4351 1.252 ~aumber 1.250[ ~ssay [0.695 I l'278~number 1.047 Icomedy 10.8001 comedy 1.060..------\"~oem [ 0\"8291 essay 1.142 f-reatise [ 0.850] piece 1.198\"~urnber","11.120 I ~¢ariety 1.217 I","~aterial 1.275 I Fluster"]},{"title":"1.3111","paragraphs":["~tructure [ 1.3711 ~elationship 1.460 I 1.429 change 1.561 j...~P ect 1.492[ 1.537 failure 1.562\"-\"'- ]system 1.497 I 1.577 variation 1.592~ iaollution 1.187] 1.582, structure 1.592 ~\"~ailure 1.290 I"]},{"title":"\\","paragraphs":["[re_crease","1.328 I Imtection 1.432] speed 1.177 ~number 11.4611 level 1.315 _.,__Jconcentration 1.478 I velocity 1.371 ~trength 1.488"]},{"title":"I","paragraphs":["size 1.440~ ~atio 1.488 I ~)lspeed 11.130 I ~enith 11.2141 epth"]},{"title":"1.2441","paragraphs":["ecognition 0.874] tcclaim 1.026"]},{"title":"I","paragraphs":["enown 1.079 nomination 1.104 form 1.110"]},{"title":"I","paragraphs":["~xplanation 1.255"]},{"title":"I","paragraphs":[":are 1.2911 :ontrol 1.295 I voyage"]},{"title":"0.8611","paragraphs":["Lrip 0.972] progress 1.016 I improvement 1.114 I )rogram 1.459 I ,peration 1.478 I :tudy 1.480 I nvestigation 1.4811 ;onductor 0.457] rice-president 0.474 I lirector 0.489 I :hairman 0.5001"]},{"title":"Figure 2: Noun Clusters for Grolier's Encyclopedia 188","paragraphs":["£ ~3 ~o","-~ ¢","train",",*-----,","test p k s- - - -D new","--tt- ........................ ~ ...................................... t t t 0 0 100 200 300 400 number of dusters Figure 3: Asymmetric Model Evaluation, AP88 Verb-Direct Object Pairs 0.8"]},{"title":"\"\\.","paragraphs":["m.......~ exceptional 3 0.6 -o 0.4 0.2 - s L , . , i 0 0 100 200 300 number of clusters 400 Figure 4: Pairwise Verb Comparisons, AP88 Verb-Direct Object Pairs lion words). MODEL EVALUATION The preceding qualitative discussion provides some indication of what aspects of distributional relationships may be discovered by clustering. However, we also need to evaluate clustering more rigorously as a basis for models of distributional relationships. So, far, we have looked at two kinds of measurements of model quality: (i) relative entropy between held-out data and the asymmetric model, and (ii) performance on the task of deciding which of two verbs is more likely to take a given noun as direct object when the data relating one of the verbs to the noun has been withheld from the training data.","The evaluation described below was performed on the largest data set we have worked with so far, extracted from 44 million words of 1988 Associated Press newswire with the pattern matching techniques mentioned earlier. This collection process yielded 1112041 verb-object pairs. We selected then the subset involving the 1000 most frequent nouns in the corpus for clustering, and randomly divided it into a training set of 756721 pairs and a test set of 81240 pairs. Relative Entropy Figure 3 plots the unweighted average relative entropy, in bits, of several test sets to asymmetric clustered models of different sizes, given by 1","~,,eAr, D(t,,ll/~-), where Aft is the set of direct objects in the test set and t,~ is the relative frequency distribution of verbs taking n as direct object in the test set. 3 For each critical value of f?, we show the relative entropy with respect to","awe use unweighted averages because we are interested her on how well the noun distributions are approximated by the cluster model. If we were interested on the total information loss of using the asymmetric model to encode a test corpus, we would instead use the asymmetric model based on gp of the training set (set"]},{"title":"train),","paragraphs":["of randomly selected held-out test set (set test), and of held-out data for a further 1000 nouns that were not clustered (set"]},{"title":"new).","paragraphs":["Unsurprisingly, the training set relative entropy decreases monotonically. The test set relative entropy decreases to a minimum at 206 clusters, and then starts increasing, suggesting that larger models are overtrained.","The new noun test set is intended to test whether clusters based on the 1000 most frequent nouns are useful classifiers for the selectional properties of nouns in general. Since the nouns in the test set pairs do not occur in the training set, we do not have their cluster membership probabilities that are needed in the asymmetric model. Instead, for each noun n in the test set, we classify it with respect to the clusters by setting"]},{"title":"p(cln)","paragraphs":["= exp"]},{"title":"-DD(p,~","paragraphs":["I lc)/Z, where p,~ is the empirical conditional verb distribution for n given by the test set. These cluster membership estimates were then used in the asymmetric model and the test set relative entropy calculated as before. As the figure shows, the cluster model provides over one bit of information about the selectional properties of the new nouns, but the overtraining effect is even sharper than for the held-out data involving the 1000 clustered nouns. Decision Task We also evaluated asymmetric cluster models on a verb decision task closer to possible applications to disambiguation in language analysis. The task consists judging which of two verbs v and v' is more likely to take a given noun n as object, when all occurrences of"]},{"title":"(v, n)","paragraphs":["in the training set were deliberately deleted. Thus this test evaluates how well the models reconstruct missing data in the the weighted average"]},{"title":"~,~e~t fnD(t,~ll~,,)","paragraphs":["where f,, is the relative frequency of n in the test set."]},{"title":"189","paragraphs":["verb distribution for n from the cluster centroids close to n.","The data for this test was built from the training data for the previous one in the following way, based on a suggestion by Dagan et al. (1993). 104 noun-verb pairs with a fairly frequent verb (between 500 and 5000 occurrences) were randomly picked, and all occurrences of each pair in the training set were deleted. The resulting training set was used to build a sequence of cluster models as before. Each model was used to decide which of two verbs v and v ~ are more likely to appear with a noun n where the (v, n) data was deleted from the training set, and the decisions were compared with the corresponding ones derived from the original event frequencies in the initial data set. The error rate for each model is simply the proportion of disagreements for the selected (v, n, v t) triples. Figure 4 shows the error rates for each model for all the selected (v, n, v ~) (al 0 and for just those exceptional triples in which the conditional ratio p(n, v)/p(n, v ~) is on the opposite side of 1 from the marginal ratio p(v)/p(v~). In other words, the exceptional cases are those in which predictions based just on the marginal frequencies, which the initial one-cluster model represents, would be consistently wrong.","Here too we see some overtraining for the largest models considered, although not for the exceptional verbs."]},{"title":"CONCLUSIONS","paragraphs":["We have demonstrated that a general divisive clustering procedure for probability distributions can be used to group words according to their participation in particular grammatical relations with other words. The resulting clusters are intuitively informative, and can be used to construct class-based word coocurrence models with substantial predictive power.","While the clusters derived by the proposed method seem in many cases semantically significant, this intuition needs to be grounded in a more rigorous assessment. In addition to predictive power evaluations of the kind we have already carried out, it might be worth comparing automatically-derived clusters with human judge: ments in a suitable experimental setting.","Moving further in the direction of class-based language models, we plan to consider additional distributional relations (for instance, adjective-noun) and apply the results of clustering to the grouping of lexical associations in lexicalized grammar frameworks such as stochastic lexicalized tree-adjoining grammars (Schabes, 1992)."]},{"title":"ACKNOWLEDGMENTS","paragraphs":["We would like to thank Don Hindle for making available the 1988 Associated Press verb-object data set, the Fidditch parser and a verb-object structure filter, Mats Rooth for selecting the objects of \"fire\" data set and many discussions, David Yarowsky for help with his stemming and concordancing tools, andIdo Dagan for suggesting ways of testing cluster models."]},{"title":"REFERENCES","paragraphs":["Peter F. Brown, Vincent J. Della Pietra, Peter V. deSouza, Jenifer C. Lal, and Robert L. Mercer. 1990. Class-based n-gram models of natural language. In Proceedings of the IBM Natural Language ITL, pages 283-298, Paris, France, March.","Kenneth W. Church and William A. Gale. 1991. A comparison of the enhanced Good-Turing and deleted estimation methods for estimating probabilities of English bigrams. Computer Speech and Language, 5:19-54.","Kenneth W. Church. 1988. A stochastic parts program and noun phrase parser for unrestricted text. In Proceedings of the Second Conference on Applied Natural Language Processing, pages 136-143, Austin, Texas. Association for Computational Linguistics, Morristown, New Jersey.","Thomas M. Cover and Joy A. Thomas. 1991. Elements of Information Theory. Wiley-Interscience, New York, New York.","Ido Dagan, Shaul Markus, and Shaul Markovitch. 1993. Contextual word similarity and estimation from sparse data. In these proceedings.","A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977. Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society, Series B, 39(1):1-38.","Richard O. Duda and Peter E. Hart. 1973. Pattern Classification and Scene Analysis. Wiley-Interseience, New York, New York.","Donald Hindle. 1990. Noun classification from predicate-argument structures. In 28th Annual Meeting of the Association for Computational Linguistics, pages 268-275, Pittsburgh, Pennsylvania. Association for Computational Linguistics, Morristown, New Jersey.","Donald Hindle. 1993. A parser for text corpora. In B.T.S. Atldns and A. Zampoli, editors, Computational Approaches to the Lexicon. Oxford University Press, Oxford, England. To appear.","Edwin T. Jaynes. 1983. Brandeis lectures. In Roger D. Rosenkrantz, editor, E. T. Jaynes: Papers on Probability, Statistics and Statistical Physics, number 158 in Synthese Library, chap-ter 4, pages 40-76. D. Reidel, Dordrecht, Holland.","Philip Resnik. 1992. WordNet and distributional analysis: A class-based approach to lexical discovery. In AAAI Workshop on Statistically-Based Natural-Language-Processing Techniques, San Jose, California, July.","Kenneth Rose, Eitan Gurewitz, and Geoffrey C. Fox. 1990. Statistical mechanics and phase transitions in clustering. Physical Review Letters, 65(8):945-948.","Yves Sehabes. 1992. Stochastic lexicalized tree-adjoining grammars. In Proceeedings of the 14th International Conference on Computational Linguistics, Nantes, France.","David Yarowsky. 1992. CONC: Tools for text corpora. Technical Memorandum 11222-921222-29, AT&T Bell Laboratories. 190"]}]}
