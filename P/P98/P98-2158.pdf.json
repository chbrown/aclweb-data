{"sections":[{"title":"A DP based Search Algorithm for Statistical Machine Translation S. Nieflen, S. Vogel, H. Ney,","paragraphs":["and"]},{"title":"C. Tillmann","paragraphs":["Lehrstuhl fiir Informatik VI","RWTH Aachen - University of Technology","D-52056 Aachen, Germany","Emaih niessen©informatik, rwth-aachen, de Abstract We introduce a novel search algorithm for statistical machine translation based on dynamic programming (DP). During the search process two statistical knowledge sources are combined: a translation model and a bigram language model. This search algorithm expands hypotheses along the positions of the target string while guaranteeing progressive coverage of the words in the source string. We present experimental results on the Verbmobil task. 1","Introduction In this paper, we address the problem of finding the most probable target language representation of a given source language string. In our approach, we use a DP based search algorithm which sequentially visits the target string positions while progressively considering the source string words.","The organization of the paper is as follows. After reviewing the statistical approach to machine translation, we first describe the statistical knowledge sources used during the search process. We then present our DP based search algorithm in detail. Finally, experimental results for a bilingual corpus are reported. 1.1 Statistical Machine Translation In statistical machine translation, the goal of the search strategy can be formulated as follows: We are given a source language ('French') string fl a = fl • .- f.t, which is to be translated into a target language ('English') string"]},{"title":"e~ = el...et","paragraphs":["with the unknown length I. Every English string is considered as a possible translation for the input string. If we assign a probability"]},{"title":"Pr(e~[f()","paragraphs":["to each pair of strings (e/, f~), then we have to choose the length"]},{"title":"Iopt","paragraphs":["and -/opt","the English string e 1 that maximize"]},{"title":"Pr(e f If J)","paragraphs":["for a given French string fJ. According to Bayes decision rule,"]},{"title":"Iopt","paragraphs":["and ~°\" can be found by"]},{"title":"(Iopt,J1 °\"') =argmax{Pr(e~lf~)}","paragraphs":["I,e{ ="]},{"title":"argmax{Pr(e().Pr(f~Jlel)}.","paragraphs":["(1) l,e I Pr(e~) is the English language model, whereas"]},{"title":"Pr(flJ[eZa)","paragraphs":["is the string translation model.","The overall architecture of the statistical translation approach is summarized in Fig. 1. In this figure, we already anticipate the fact that we will transform the source strings in a certain manner and that we will countermand these transformations on the produced output strings. This aspect is explained in more detail in Section 3. Source Language"]},{"title":"Text Transformation 1 [ Global Search: -1_ Pr(faIel) maximize Pr(el). Pr(f~lel) over e I I I Transformation I 1 Target Language Text Lexicon Model I I ..o,o.o.,o°., ]","paragraphs":["Figure 1: Architecture of the translation approach based on Bayes' decision rule.","The task of statistical machine translation can be subdivided into two fields:","1. the field of modelling, which introduces structures into the probabilistic dependencies and provides methods for estimating the parameters of the models from bilingual corpora;","2. the field of decoding, i.e. finding a search algorithm, which performs the argmax operation in Eq. (1) as efficient as possible. 960 1.2 Alignment with Mixture Distribution Several papers have discussed the first issue, especially the problem of word alignments for bilingual corpora (Brown et al., 1993), (Dagan et al., 1993), (Kay and RSscheisen, 1993), (Fung and Church, 1994), (Vogel et al., 1996).","In our search procedure, we use a mixture-based alignment model that slightly differs from the model introduced as"]},{"title":"Model 2","paragraphs":["in (Brown et al., 1993). It is based on a decomposition of the joint probability for f~ into a product of the probabilities for each word d"]},{"title":"Pr(flJle~) = p( J[I) \" H p(fJl eta),","paragraphs":["(2) 3=1 where the lengths of the strings are regarded as random variables and modelled by the distribution"]},{"title":"p(JlI).","paragraphs":["Now we assume a sort of pairwise interac-tion between the French word"]},{"title":"fj","paragraphs":["and each English","word ei in el i. These dependencies are captured in","the form of a mixture distribution:","I"]},{"title":"p(f31ezl) = ~'~p(ilj, J,I) \" p(fjlei) •","paragraphs":["(3) i=1","Inserting this into (2), we get J I"]},{"title":"Pr(Y(le~) = p(JII) YI ~~p(ilj, J, i) . p(f~le,)","paragraphs":["(4) j=l i=1 with the following components: the sentence length probability"]},{"title":"p(JlI),","paragraphs":["the mixture alignment probability"]},{"title":"p(ilj, J, I)","paragraphs":["and the translation probability"]},{"title":"p(fle).","paragraphs":["So far, the model allows"]},{"title":"all","paragraphs":["English words in the target string to contribute to the translation of a French word. This is expressed by the sum over i in Eq. (4). It is reasonable to assume that for each source string position j one position i in the target string dominates this sum. This conforms with the experience, that in most cases a clear word-to-word correspgndence between a string and its translation exists. As a consequence, we use the so-called maximum approximation: At each point, only the"]},{"title":"best","paragraphs":["choice of i is considered for the alignment path: d"]},{"title":"Pr(/( le~) =p( JII) ~I m~ p(ilj, J, I).p(fj]e~).","paragraphs":["(5) j=l \"el, ] We can now formulate the criterion to be maximized by a search algorithm:"]},{"title":"max [p( JlI) max { Pr(e[). I el H m.ax ~(ilj, J,I ) .p(f31e~)l .","paragraphs":["j=l ie[1,1]"]},{"title":"(6)","paragraphs":["Because of the problem of data sparseness, we use a parametric model for the alignment probabilities. It assumes that the distance of the positions relative to the diagonal of the (j, i) plane is the dominating factor:"]},{"title":"r(i _j I) p(ilj,","paragraphs":["J, I) = (7) ,"]},{"title":"Ei,=l r(i' - j )","paragraphs":["As described in (Brown et al., 1993), the EM algorithm can be used to estimate the parameters of the model. 1.3 Search in Statistical Machine Translation In the last few years, there has been a number of papers considering the problem of finding an efficient search procedure (Wu, 1996), (Tillmann et al., 1997a), (TiUmann et al., 1997b), (Wang and Waibel, 1997). All of these approaches use a bigram language model, because they are quite simple and easy-to-use and they have proven their prediction power in stochastic language processing, especially speech recognition. Assuming a bigram language model, we would like to re-formulate Eq. (6) in the following way: J"]},{"title":"max[p(J]I)m~axljX~l max~(eilei-1)'l","paragraphs":["e 1 .= iE[1,I]"]},{"title":"P(ilJ, J,I)'p(fjlei)] }] • (8)","paragraphs":["Any search algorithm tending to perform the maximum operations in Eq. (8) has to guarantee, that the predecessor word ei-1 can be determined at the time when a certain word"]},{"title":"ei","paragraphs":["at position i in the target string is under consideration. Different solutions to this problem have been studied.","(Tillmann et al., 1997b) and (Tillmann et al., 1997a) propose a search procedure based on dynamic programming, that examines the source string sequentially. Although it is very efficient in terms of translation speed, it suffers from the drawback of being dependent on the so-called monotonicity constraint: The alignment paths are assumed to be monotone. Hence, the word at position i - 1 in the target sentence can be determined when the algorithm produces"]},{"title":"ei.","paragraphs":["This approximation corresponds to the assumption of the fundamental similaxity of the sentence structures in both languages. In (Tillmann et al., 1997b) text transformations in the source language are used to adapt the word or-dering in the source strings to the target language grammar.","(Wang and Waibel, 1997) describe an algorithm based on A*-search. Here, hypotheses are extended 961 by adding a word to the end of the target string while considering the source string words in any order. The underlying translation model is"]},{"title":"Model 2","paragraphs":["from (Brown et al., 1993).","(Wu, 1996) formulates a DP search for stochastic bracketing transduction grammars. The bigram language model is integrated into the algorithm at the point, where two partial parse trees are combined."]},{"title":"2 DP Search 2.1 The Inverted Alignment Model","paragraphs":["For our search method, we chose an algorithm which is based on dynamic programming. Compared to an A'-based algorithm dynamic programming has the fundamental advantage, that solutions of subproblems are stored and can then be re-used in later stages of the search process. However, for the optimization criterion considered here dynamic programming is only suboptimal because the decomposition into independent subproblems is only approximately possible: to prevent the search time of a search algorithm from increasing exponentially with the string lengths and vocabulary sizes, local decisions have to be made at an earlier stage of the optimization process that might turn out to be suboptimal in a later stage but cannot be altered then. As a consequence, the global optimum might be missed in some cases.","The search algorithm we present here combines the advantages of dynamic programming with the search organization along the positions of the target string, which allows the integration of the bigram in a very natural way without restricting the alignment paths to the class of monotone alignments.","The alignment model as described above is defined as a function that assigns exactly one target word to each source word. We introduce a new interpretation of the alignment model: Each position i in e / is assigned a position"]},{"title":"bi = j","paragraphs":["in fl J. Fig. 2 illustrates the possible transitions in this inverted model.","At each position i of el, each word of the target language vocabulary can be inserted. In addition, the"]},{"title":"fertility l","paragraphs":["must be chosen: A position i and the word"]},{"title":"ei","paragraphs":["at this position are considered to correspond to a sequence of words"]},{"title":"f~:+1-t","paragraphs":["in f]. In most cases, the optimal fertility is 1. It is also possible, that a word"]},{"title":"ei","paragraphs":["has fertility 0, which means that there is no directly corresponding word in the source string. We call this a"]},{"title":"skip,","paragraphs":["because the position i is skipped in the alignment path.","Using a bigram language model, Eq. (9) specifies the modified search criterion for our algorithm. Here as above, we assume the maximum approximation to be valid. °~ e~ L,. .~ 0 .N O : : _. • : _\" : :"]},{"title":"c~.-... ~ ...-.~ position in source string","paragraphs":["Figure 2: Transitions in the inverted model. I"]},{"title":"max[p(dlI)ma:xH[p(eiJei-1)\"","paragraphs":["I el i=1"]},{"title":"' J max 1\"I {P(ilJ'J'I)'P(:31ei)}","paragraphs":["j,t_ l=j-l+l"]},{"title":"(9)","paragraphs":["For better legibility, we regard the second product in Eq. (9) to be equal to 1, ifl = 0. It should be stressed that the pair (I,e{) optimizing Eq. (9) is not guaranteed to be also optimal in terms of the original criterion (6). 2.2 Basic Problem: Position Coverage .4. closer look at Eq. (9) reveals the most important problem of the search organization along the target string positions: It is not guaranteed, that all the words in the source string are considered. In other words we have to force the algorithm to cover all input string positions. Different strategies to solve this problem are possible: For example, we can introduce a reward for covering a position, which has not yet been covered. Or a penalty can be imposed for each position without correspondence in the target string.","In preliminary experiments, we found that the most promising method to satisfy the position coverage constraint is the introduction of an additional parameter into the recursion formula for DP. In the following, we will explain this method in detail. 2.3 Recursion Formula for DP In the DP formalism, the search process is described recursively. Assuming a total length I of the target string,"]},{"title":"Q1(c, i, j, e)","paragraphs":["is the probability of the best partial path ending in the coordinates i in el / and j in f J, if the last word ei is e and if c positions in the source string have been covered. 962","This quantity is defined recursively. Leaving a word ei without any assignment"]},{"title":"(skip)","paragraphs":["is the easiest case:"]},{"title":"QS (c, i, j, e) = max {p(ele')Q1(c, i - 1, j,","paragraphs":["d)} . Note that it is not necessary to maximize over the predecessor positions jr: This maximization is subsumed by the maximization over the positions on the next level, as can easily be proved.","In the original criterion (6), each position j in the source string is aligned to exactly one target string position i. Hence, if i is assigned to I subsequent positions in fl s, we want to verify that none of these positions has already been covered: We define a control function v which returns 1 if the above constraint is satisfied and 0 otherwise. Then we can write:"]},{"title":"fH Qr](c,i.j,e) = max {p(il3, J,I )","paragraphs":["• p(f3iei)} • • l>0 \" ~=j--l+l"]},{"title":"max {p(eie')\"","paragraphs":["e j"]},{"title":"mjax [Q,(c- l,i - 1,f ,e') .v(c,l,f ,j,e')] )] ,","paragraphs":["We now have to find the maximum: Q,(c, i, j, e) = max {QS(c, i, j, e), Qn(c, i, j, e)} . The decisions made during the dynamic programming process (choices of l, j' and e ~) are stored for recovering the whole translation hypothesis.","The best translation hypothesis can be found by optimizing the target string length I and requiring the number of covered positions to be equal to the source string length J:"]},{"title":"max {P(JlI) \" maxQ1(J'I'j'e) } j,e","paragraphs":["(10) 2.4 Acceleration Techniques The time comple.,dty of the translation method as described above is"]},{"title":"O(i2ax \" j3 . iSl 2)","paragraphs":[", where I~] is the size of the target language vocabulary C. Some refinements of this algorithm have been implemented to increase the translation speed.","1. We can expect the progression of the source string coverage to be roughly proportional to the progression of the translation procedure along the target string. So it is legitimate to define a minimal and maximal coverage for each level i:"]},{"title":"Cmin(i)= [iJJ -r, Cmax(i)= [i~] + r ,","paragraphs":["where r is a constant integer number. In preliminary experiments we found that we could set r to 3 without any loss in translation accuracy. This reduces the time complexity by a factor J.","2. Optimizing the target string length as formulated in Eq. (10) requires the dynamic programming procedure to start all over again for each I. If we assume the dependence of the alignment probabilities"]},{"title":"p(ilj, J, I)","paragraphs":["on I to be negligible, we can renormalize them by using an estimated target string length/~ and use"]},{"title":"p(ilj , J, I).","paragraphs":["Now we can produce one translation e~ at each level i = I without restarting the whole process:"]},{"title":"max Vi(J, I, j, e) .","paragraphs":["(11) 3,e For/~ we choose: /~ = ](J) = J-/~- where p¢ and #j denote the average lengths of the target and source strings, respectively. This approximation is partly undone by what we call"]},{"title":"rescoring:","paragraphs":["For each translation hypothesis e / with length I, we compute the \"true\" score (~(I) by searching the best inverted alignment given e / and fs and evaluating the probabilities along this alignment. Hence, we finally find the best translation via Eq. (12):"]},{"title":"max{,(JII) (12)","paragraphs":["The time complexity for this additional step is negligible, since there is no optimization over the English words, which is the dominant factor in the overall time complexity"]},{"title":"O(Imax \" j2 .","paragraphs":["[E.[2) . 3. We introduced two thresholds:","SL\" If e' is the predecessor word of e and e is not aligned to the source string (\"skip\"), then"]},{"title":"p(eie')","paragraphs":["must be higher than SL.","ST\" A word e can only be associated with a source language word f, if"]},{"title":"p(f[e)","paragraphs":["is higher than ST. This restricts the optimization over the target language vocabulary to a relatively small set of candidate words. The resulting time complexity is","O(Im~x. J2-IEI).","4. When searching for the best partial path to a gridpoint"]},{"title":"G = (c,i,j,e),","paragraphs":["we can sort the arcs leading to G in a specific manner that allows us to stop the computation whenever it becomes clear that no better partial path to G exists. The effect of this measure depends on the qual-ity of the used models; in preliminary experiments we observed a speed-up factor of about 3.5."]},{"title":"963","paragraphs":["3 Experiments The search algorithm suggested in this paper was tested on the Verbmobil Corpus. The results of preliminary tests on a small automatically generated Corpus (Amengual et al., 1996) were quite promising and encouraged us to apply our search algorithm to a more realistic task.","The Verbmobil Corpus consists of spontaneously spoken dialogs in the domain of appointment scheduling (Wahlster, 1993). German source sentences are translated into English. In Table 1 the characteristics of the training and test sets are summarized. The vocabularies include category labels for dates, proper names, numbers, times, names of places and spellings. The model parameters were trained on 16 296 sentence pairs, where names etc. had been replaced by the appropriate labels. Table 1: Training and test conditions of the Verbmobil task. formed sample translations (i.e. after labelling) was 13.8.","In preliminary evaluations, optimal values for the thresholds"]},{"title":"OL","paragraphs":["and"]},{"title":"OT","paragraphs":["had been determined and kept fixed during the experiments.","As an automatic and easy-to-use measure of the translation performance, the Levenshtein distance between the produced translations and the sample translations was calculated. The translation results are summarized in Table 2. Table 2: Word error rates on the Verbmobil Corpus: insertions (INS), deletions (DEL) and total rate of word errors (WER) before (BL) and after (AL) rule-based translation of the labels. before / after Error Rates (%) INS DEL WER BL 7.3 18.4 45.0 AL 7.6 17.3 39.6","Words in Vocabulary German 4 498 English 2 958","Number of Sentences in Training Corpus 16 296 in Test Corpus 150","Given the vocabulary sizes, it becomes quite obvious that the lexicon probabilities"]},{"title":"p(f[e)","paragraphs":["can not be trained sufficiently on only 16 296 sentence pairs. The fact that about 40% of the words in the lexicon are seen only once in training illustrates this. To im-prove the lexicon probabilities, we interpolated them with lexicon probabilities"]},{"title":"pM(fle)","paragraphs":["manually created from a German-English dictionary:"]},{"title":"{'o","paragraphs":["~ if"]},{"title":"(e, f)","paragraphs":["is in the dictionary"]},{"title":"pM(fle)","paragraphs":["_ otherwise ' where Ne is the number of German words listed as translations of the English word e. The two lexica were combined by linear interpolation with the interpolation parameter A. For our first experiments, we set A to 0.5.","The test corpus consisted of 150 sentences, for which sample translations exist. The labels were translated separately: First, the test sentences were preprocessed in order to replace words or groups of words by the correct category label. Then, our search algorithm translated the transformed sentences. In the last step, a simple rule-based algorithm replaced the category labels by the translations of the original words.","We used a bigram language model for the English language. Its perplexity on the corpus of trans-","(Tillmann et al., 1997a) report a word error rate of 51.8% on similar data.","Although the Levenshtein distance has the great advantage to be automatically computable, we have to keep in mind, that it depends fundamentally on the choice of the sample translation. For example, each of the expressions \"thanks\", \"thank you\" and \"thank you very much\" is a legitimate translation of the German \"danke schSn\", but when calculating the Levenshtein distance to a sample translation, at least two of them will produce word errors. The more words the vocabulary contains, the more important will be the problem of synonyms.","This is why we also asked five experts to classify independently the produced translations into three categories, being the same as in (Wang and Waibel, 1997):"]},{"title":"Correct translations","paragraphs":["are grammatical and convey the same meaning as the input."]},{"title":"Acceptable translations","paragraphs":["convey the same meaning but with small grammatical mistakes or they convey most but not the entire meaning of the input."]},{"title":"Incorrect translations","paragraphs":["are ungrammatical or convey little meaningful information or the information is different from the input.","Examples for each category are given in Table 3. Table 4 shows the statistics of the translation performance. When different judgements existed for one sentence, the majority vote was accepted. For the calculation of the subjective sentence error rate (SSER), translations from the second category counted as \"half-correct\".","When evaluating the performance of a statistical machine translator, we would like to distinguish errors due to the weakness of the underlying models 964 Table 3: Examples of Correct (C), Acceptable (A), and Incorrect (I) translations on Verbmobil. The source language is German and the target language is English. C Input: Output: Input: Output: Ah neunter M/irz bin ich in KSln.","I am in Cologne on the ninth of March.","Habe ich mir notiert.","I have noted that. A Input: Output: Input: Output: Samstag und Februar sind gut, aber der siebzehnte w~ire besser. Saturday and February are quite but better the seventeenth. Ich kSnnte erst eigentlich jetzt wieder dann November vorschlagen. Ab zweiten November. I could actually coming back November then. Suggest beginning the second of November. Input: Output: Ja, also mit Dienstag und mittwochs und so h/itte ich Zeit, aber Montag kommen wir hier nicht weg aus Kiel. Yes, and including on Tuesday and Wednesday as well, I have time on Monday but we will come to be away from Kiel. Input:","Dann fahren wir da los. Output: We go out. Table 4: Subjective evaluation of the translation performance on Verbmobil: number of sentences evaluated as Correct (C), Acceptable (A) or Incorrect (I). For the total percentage of non-correct translations (SSER), the \"acceptable\" translations are counted as half-errors.","I Total Correct Acceptable Incorrect SSER I 150 61 45 44 44.3% from"]},{"title":"search errors,","paragraphs":["occuring whenever the search algorithm misses a translation hypothesis with a higher score. Unfortunately, we can never be sure that a search error"]},{"title":"does not occur,","paragraphs":["because we do not know whether or not there is another string with an even higher score than the produced output.","Nevertheless, it is quite interesting to compare the score of the algorithm's output and the score of the sample translation in such cases in which the output is not correct (it is classified as \"acceptable\" or \"incorrect\" ).","The original value to be maximized by the search algorithm (see Eq. (6)) is the score as defined by the underlying models and described by Eq. (13). J"]},{"title":"Pr(e~).p(JII) H max ~(ilj , J, I). p(fjlei)] •","paragraphs":["(13) j=l ie[1,1]","We calculated this score for the sample translations as well as for the automatically generated translations. Table 5 shows the result of the comparison. In most cases, the incorrect outputs have higher scores than the sample translations, which leads to the conclusion that the improvement of the models (stronger language model for the target language, better translation model and especially more training data) will have a strong impact on the qual-ity of the produced translations. The other cases, i. e. those in which the models prefer the sample translations to the produced output, might be due to the difference of the original search criterion (6) and the criterion (9), which is the basis of our search algorithm. The approximation made by the introduction of the parameters"]},{"title":"OT","paragraphs":["and"]},{"title":"OL","paragraphs":["is an additional reason for search errors. Table 5: Comparison: Score of Reference Translation e and Translator Output e ~ for \"acceptable\" translations (A) and \"incorrect\" translations (I). For the total number of non-correct translations (T), the \"acceptable\" translations are counted as half-errors. A I T % Total number 45 44 66.5 100.0"]},{"title":"Score(e) >_ Score(C)","paragraphs":["11 13 18.5 27.8"]},{"title":"Score(e) < Score(C)","paragraphs":["34 31 48.0 72.2","As far as we know, only two recent papers have dealt with decoding problem for machine translation systems that use translation models based on hid-den alignments without a monotonicity constraint: (Berger et al., !994) and (Wang and Waibel, 1997). The former uses data sets that differ significantly from the Verbmobil task and hence, the reported results cannot be compared to ours. The latter presents experiments carried out on a corpus corn-965 parable to our test data in terms of vocabulary sizes, domain and number of test sentences. The authors report a subjective sentence error rate which is in the same range as ours. An exact comparison is only possible if exactly the same training and test-ing data are used and if all the details of the search algorithms are considered. 4","Conclusion and Future Work In this paper, we have presented a new search algorithm for statistical machine translation. First experiments prove its applicability to realistic and complex tasks such as spontaneously spoken dialogs.","Several improvements to our algorithm are planned, the most important one being the implementa-tion of pruning methods (Ney et al., 1992). Pruning methods have already been used successfully in machine translation (Tillmann et al., 1997a). The first question to be answered in this context is how to make two different hypotheses H1 and/-/2 comparable: Even if they cover the same"]},{"title":"number","paragraphs":["of source string words, they might cover"]},{"title":"different","paragraphs":["words, especially words that are not equally difficult to translate, which corresponds to higher or lower translation probability estimates. To cope with this problem, we will introduce a heuristic for the estimation of the cost of translating the remaining source words. This is similar to the heuristics in A'-search.","(Vogel et al., 1996) report better perplexity results on the Verbmobil Corpus with their HMM-based alignment model in comparison to"]},{"title":"Model 2","paragraphs":["of (Brown et al., 1993). For such a model, however, the new interpretation of the alignments becomes essential: We cannot adopt the estimates for the alignment probabilities"]},{"title":"p(ili', I).","paragraphs":["Instead, we have to re-calculate them as inverted alignments. This will provide estimates for the probabilities"]},{"title":"P(JlJ', J).","paragraphs":["The most important advantage of the HMM-based alignment models for our approach is the fact, that they do not depend on the unknown target string length I.","Acknowledgement. This work was partly supported by the German Federal Ministry of Educa-tion, Science, Research and Technology under the Contract Number 01 IV 601 A (Verbmobil)."]},{"title":"References","paragraphs":["J. C. Amengual, J. M. Benedi, A. Castafio, A. Marzal, F. Prat, E. Vidal, J. M. Vilar, C. Delogu, A. di Carlo, H. Ney, and S. Vogel. 1996. Example-Based Understanding and Translation Systems (EuTrans): Final Report, Part I. Deliverable of ESPRIT project No. 20268, October.","A.L. Berger, P.F. Brown, J. Cocke, S.A. Della Pietra, V.J. Della Pietra, J.R. Gillett, J.D. Lafferty, R.L. Mercer, H. Printz, and L. Ures. 1994. The Candide System for Machine Translation. In"]},{"title":"Proc. ARPA Human Language Technology Work- shop, Plainsboro, N J,","paragraphs":["pages 152-157. Morgan Kanfmann Publ., March.","P.F. Brown, S.A. Della Pietra, V.J. Della Pietra, and R.L. Mercer. 1993. Mathematics of Statistical Machine Translation: Parameter Estimation."]},{"title":"Computational Linguistics,","paragraphs":["19(2):263-311.","I. Dagan, K. W. Church, and W. A. Gale. 1993. Robust Bilingual Word Alignment for Machine Aided Translation. In"]},{"title":"Proceedings of the Work- shop on Very Large Corpora, Columbus, Ohio,","paragraphs":["pages 1-8.","P. Fung and K.W. Church. 1994. K-vet: A new Approach for Aligning Parallel Texts. In"]},{"title":"Proceedings of the 15th International Conference on Compu- tational Linguistics, Kyoto, Japan,","paragraphs":["pages 1096-1102.","M. Kay and M. RSscheisen. 1993. Text-Translation Alignment."]},{"title":"Computational Linguistics,","paragraphs":["19(1):121-142.","H. Ney, D. Mergel, A. Noll, and A. Paeseler. 1992. Data Driven Search Organization for Continuous Speech Recognition."]},{"title":"IEEE Transactions on Sig- nal Processing,","paragraphs":["40(2):272-281, February.","C. Tillmann, S. Vogel, H. Ney, H. Sawaf, and A. Zubiaga. 1997a. Accelerated DP based Search for Statistical Translation. In"]},{"title":"Proceedings of the 5th European Conference on Speech Communication and Technology, Rhodes, Greece,","paragraphs":["pages 2667-2670, September.","C. Tillmann, S. Vogel, H. Ney, and A. Zubiaga. 1997b. A DP-Based Search using Monotone Alignments in Statistical Translation. In"]},{"title":"Proceed- ings of the ACL/EACL '97, Madrid, Spain,","paragraphs":["pages 289-296, July.","S. Vogel, H. Ney, and C. Tillmann. 1996. HMM-Based Word Alignment in Statistical Translation. In"]},{"title":"Proceedings of the 16th International Confer- ence on Computational Linguistics, Copenhagen, Denmark,","paragraphs":["pages 836-841, August.","W. Wahlster. 1993. Verbmobih Translation of Face-to-Face Dialogs. In"]},{"title":"Proceedings of the MT Sum- mit IV,","paragraphs":["pages 127-135, Kobe, Japan.","Ye-Yi Wang and A. Waibel. 1997. Decoding Algorithm in Statistical Translation. In"]},{"title":"Proceedings of the ACL/EACL '97, Madrid, Spain,","paragraphs":["pages 366-372, July.","D. Wu. 1996. A Polynomial-Time Algorithm for Statistical Machine Translation. In"]},{"title":"Proceedings of the 34th Annual Conference of the Association for Computational Linguistics, Santa Cruz, CA,","paragraphs":["pages 152 - 158, June. 966 Zusammenfassung Wir stellen einen neuartigen..Suchalgorithmus flit die statistische maschinelle Ubersetzung vor, der auf der dynamischen Programmierung (DP) beruht. W~ihrend des Suchprozesses werden.zwei statistische Wissensquellen kombiniert: Ein Ubersetzungsmodell und ein Bigramm-Sprachmodell. Dieser Suchalgorithmus erweitert Hypothesen entlang den Positionen des Zielsatzes, wobei garantiert wird, dab alle WSrter im Quellsatz berficksichtigt werden. Es wer-den experimentelle Ergebnisse auf der Verbmobil-Aufgabe angegeben."]},{"title":"Rdsumd","paragraphs":["Nous prdsentons un nouveau algorithme de recherche pour la traduction automatiquestatistique qui est basde sur la programmation dynamique (DP). Pendant la recherche deux sources d'information statistiques sont combindes: Un module de traduction et un bigram language model. Cet algorithme de recherche construit des hypotheses le long des positions de la phrase en langue de cible tout en garantissant la considdration progressive des mots dans la phrase en langue de source. Des rdsultats expdrimentaux sur la t~che Verbmobil sont prdsentds."]},{"title":"967","paragraphs":[]}]}
