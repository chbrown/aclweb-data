{"sections":[{"title":"Automatic Acquisition of Language Model based on Head-Dependent Relation between Words","paragraphs":["Seungmi Lee and Key-Sun Choi","Department of Computer Science Center for Artificial Intelligence Research","Korea Advanced Institute of Science and Technology"]},{"title":"e-mail: {leesm, kschoi}@world, kaist, ac. kr Abstract","paragraphs":["Language modeling is to associate a sequence of words with a priori probability, which is a key part of many natural language applications such as speech recognition and statistical machine translation. In this paper, we present a language modeling based on a kind of simple dependency grammar. The grammar consists of head-dependent relations between words and can be learned automatically from a raw corpus using the reestimation algorithm which is also introduced in this paper. Our experiments show that the proposed model performs better than n-gram models at 11% to 11.5~ reductions in test corpus entropy. 1","Introduction Language modeling is to associate a priori probability to a sentence. It is a key part of many natural language applications such as speech recognition and statistical machine translation. Previous works for language modeling can be broadly divided into two approaches; one is n-gram-based and the other is grammar-based.","N-gram model estimates the probability of a sentence as the product of the probability of each word in the sentence. It assumes that probability of the nth word is dependent on the previous n- 1 words. The n-gram probabilities are estimated by simply counting the n-gram frequencies in a training corpus. In some cases, class (or part of speech) n-grams are used instead of word n-grams(Brown et al., 1992; Chang and Chen, 1996). N-gram model has been widely used so far, but it has always been clear that n-gram can not represent long distance dependencies.","In contrast with n-gram model, grammar-based approach assigns syntactic structures to a sentence and computes the probability of the sentence using the probabilities of the structures. Long distance dependencies can be represented well by means of the structures. The approach usually makes use of phrase structure grammars such as probabilistic context-free grammar and recursive transition network(Lari and Young, 1991; Sneff, 1992; Chen, 1996). In the approach, however, a sentence which is not accepted by the grammar is assigned zero probability. Thus, the grammar must have broadcoverage so that any sentence will get non-zero probability. But acquisition of such a robust grammar has been known to be very difficult. Due to the difficulty, some works try to use an integrated model of grammar and n-gram compensating each other(McCandless, 1994; Meteer and Rohlicek, 1993). Given a robust grammar, grammar-based language modeling is expected to be more powerful and compact in model size than n-gram-based one.","In this paper we present a language modeling based on a kind of simple dependency grammar. The grammar consists of head-dependent relations between words and can be learned automatically from a raw corpus using the reestimation algorithm which is also introduced in this paper. Based on the dependencies, a sentence is analyzed and assigned syntactic structures by which long distance dependences are represented. Because the model can be thought of as a linguistic bi-gram model, the smoothing functions of n-gram models can be applied to it. Thus, the model can be robust, adapt easily to new domains, and be effective.","The paper is organized as follows. We introduce some definitions and notations for the dependency grammar and the reestimation algorithm in section 2, and explain the algorithm in section 3. In section 4, we show the experimental results for the suggested model compared to n-gram models. Finally, section 5 concludes this paper. 2 A Simple Dependency Grammar In this paper, we assume a kind of simple dependency grammar which describes a language 723 by a set of head-dependent relations between words. A sentence is analyzed by establishing dependency links between individual words in the sentence. A dependency analysis, :D, of a sentence can be represented with arrows pointing from head to dependent as depicted in Figure 1. For structural generality, we assume that there is always a marking tag, \"EOS\"(End of Sentence), at the end of a sentence and it has the head word of the sentence as its own dependent(\"gave\" in Figure 1). I gave him a book EOS Figure 1: An example dependency analysis","A/) is a set of inter-word dependencies which satisfy the following conditions: (1) every word in the sentence has its head in the sentence except the head word of the sentence. (2) every word can have only one head. (3) there is neither crossing nor cycle of dependencies.","The probabilistic model of the simple dependency grammar is given by"]},{"title":"p(sentence)","paragraphs":["= ~-'~ p(D) 2)"]},{"title":"= }2 II","paragraphs":["2) x.-.+y6D where"]},{"title":"p(x--+ y) = p(yl x) freq(x --+ y) E, z)\" Complete-Link and Complete-Sequence","paragraphs":["Here, we define complete-link and complete-sequence which represent partial :Ds for substrings. They are used to construct overall 79s and used as the basic structures for the reestimation algorithm in section 3.","A set of dependency relations on a word sequence, wij l, is a complete-link when the following conditions are satisfied: • there is (wi -+ wi) or (wi e--"]},{"title":"wj)","paragraphs":["exclusively.","• Every inner word has a head in the word sequence.","• Neither crossing nor cycle of dependency relations is allowed.","tWe use wi for ith word in a sentence and wi,j for the word sequence from wl to wj(i < j). k her second child the bus Figure 2: Example complete-links A complete-link has direction. A complete-link on"]},{"title":"wij","paragraphs":["is said to be \"rightward\" if the outermost relation is"]},{"title":"(wi --+ wj),","paragraphs":["and \"leftward\" if the relation is"]},{"title":"(wi e-- wj).","paragraphs":["Unit complete-link is defined on a string of two adjacent words,"]},{"title":"wi,;+l.","paragraphs":["In Figure 2, (a) is a rightward complete-link, and both of (b) and (c) are leftward ones. bird in the cage the bus book Figure 3: Example complete-sequences","A complete-sequence is a sequence of 0 or more adjacent complete-links that have the same direction. A unit complete-sequence is defined on a string of one word. It is 0 sequence of complete-links. The direction of a complete-sequence is determined by the direction of the component complete-links. In Figure 3, (a) is a rightward complete-sequence composed of two complete-links, and (b) is a leftward one. (c) is a complete-sequence composed of zero complete-links, and it can be both leftward and rightward.","The word of \"complete\" means that the dependency relations on the inner words are completed and that consequently there is no need to process further on them. From now on, we use"]},{"title":"Lr(i,j)/Lt(i,j)","paragraphs":["for rightward/leftward complete-links and"]},{"title":"Sr(i,j)/St(i,j)","paragraphs":["for rightward/leftward complete-sequences on"]},{"title":"wi, j.","paragraphs":["Any complete-link on"]},{"title":"wi, j","paragraphs":["can be viewed as the following combination."]},{"title":"• L~(i,j): {(wi --+ wj), S~(i,m), St(m+l,j)} • Ll(i,j): {(wi e-- wj), St(i, m), St(m+l,j)}","paragraphs":["foram(i<m<j). Otherwise, the set of dependencies does not satisfy the conditions of no crossing, no cycle and no multiple heads and is not a complete-link any more.","Similarly, any complete-sequence on"]},{"title":"wi,j","paragraphs":["can be viewed as the following combination."]},{"title":"• S~(i,j): {Sr(i,m), L~(m,j)} • St(i,j): {Lt(i,m), St(m,j)}","paragraphs":["foram(i<m<j). In the case of complete-sequence, we can prevent multiple constructions of the same 724 complete-sequence by the above combinational restriction. Figure 4: Abstract representation of/)","Figure 4 shows an abstract representation of a/) of an n-word sentence. When"]},{"title":"wk(1 < k <_","paragraphs":["n) is the head of the sentence, any D of the sentence can be represented by a St(l,"]},{"title":"EOS)","paragraphs":["uniquely by the assumption that there is always the dependency relation,"]},{"title":"(wk +-- wEos). 3","paragraphs":["Reestimation Algorithm The reestimation algorithm is a variation of Inside-Outside algorithm(Jelinek et al., 1990) adapted to dependency grammar. In this section we first define the inside-outside probabilities of complete-links and complete-sequences, and then describe the reestimation algorithm based on them 2.","In the followings, ~ indicates inside probability and a, is for outside probability. The superscripts, l and s, are used for \"complete-link\" and \"complete-sequence\" respectively. The subscripts indicate direction: r for \"rightward\" and I for \"leftward\".","The inside probabilities of complete-links"]},{"title":"(n~(i,j), Lt(i,j))","paragraphs":["and complete-sequences"]},{"title":"(Sr(i,j), Sl(i,j))","paragraphs":["are as follows. j-1"]},{"title":"/3t~(i,j) = ~ p(wi --+ wj)/3~(i, m)t3~(m +","paragraphs":["1,j). rn=i j--I"]},{"title":"/3[(i,j) = E p(wi 6.- wj)t3~(i,m)13?(m +","paragraphs":["1,j). rn=i j--1"]},{"title":"fl~(i,j) = ~ /3~(i,m)~t~(m,j).","paragraphs":["mini"]},{"title":"J /3?(i,j) = ~ /3[(i,m)t3?(m,j).","paragraphs":["m=i+l The basis probabilities are:"]},{"title":"/31r(i,i +","paragraphs":["1) ="]},{"title":"p(wi \"~ wi+l) /3[(i,i +","paragraphs":["1) ="]},{"title":"p(wi (-\" wi+l) /3~(i, i) = fl?(i, i) = 1","paragraphs":["/37(1,"]},{"title":"EO S) = p( wL, )","paragraphs":["~A little more detailed explanation of the expressions can be found in (Lee and Choi, 1997). /3~(i,i+ 1) ="]},{"title":"p(L~(i,i+","paragraphs":["1)) ="]},{"title":"p(wi ~ wi+t)","paragraphs":["/37 (i, i + 1) ="]},{"title":"p(Lt(i, i +","paragraphs":["1)) ="]},{"title":"p(wi +-- wi+t).","paragraphs":["/37(1,"]},{"title":"EOS)","paragraphs":["is the sentence probability because every dependency analysis, D, is represented by a"]},{"title":"St(l, EOS)","paragraphs":["and/37(1 ,"]},{"title":"EOS)","paragraphs":["is sum of the probability of every St(l,"]},{"title":"EOS).","paragraphs":["probabilities for complete- (i, j)) and complete-sequences are as follows. The outside","links"]},{"title":"(L,.(i,j), Lt (S~(i,j), St(i,j))","paragraphs":["i"]},{"title":"at~(i,j) =","paragraphs":["n c~ (v, j)/3i~(v, i)."]},{"title":"a~ (i, h)/3?(j, h).","paragraphs":["h=j"]},{"title":"a~(i,j) = ~ a~(i,h)/3tr(j,h)","paragraphs":["h=j+l"]},{"title":"+atr(i , h)/3i~(j + 1, h)p(wi -+ Wh)","paragraphs":["+al(i,"]},{"title":"h)/3?(j + 1, h)p(wi ~ wh).","paragraphs":["i-I"]},{"title":"a~(i,j) = ~ a~(v,j)fl~(v,i)","paragraphs":["v----I +dr(v,j)Z;(v, i - t)p(wv wA"]},{"title":"+al(v,j)t3;(v , i-","paragraphs":["1)p(wv e-"]},{"title":"wj).","paragraphs":["The basis probability is"]},{"title":"~(1, EOS) = 1.","paragraphs":["Given a training corpus, the initial grammar is just a list of all pairs of unique words in the corpus. The initial pairs represent the tentative head-dependent relations of the words. And the initial probabilities of the pairs can be given randomly. The training starts with the initial grammar. The train corpus is analyzed with the grammar and the occurrence frequency of each dependency relation is calculated. Based on the frequencies, probabilities of dependency relations are recalculated by"]},{"title":"C(wp --+ w~)","paragraphs":["The process"]},{"title":"w,) = C(w","paragraphs":["continues until the entropy of the training corpus becomes the minimum. The frequency of occurrence,"]},{"title":"C(wi --+ wj),","paragraphs":["is calculated by w) = -+ 1 t • • t"]},{"title":"= p(wt,.)a.(,,3)/3~(i,j)","paragraphs":["where"]},{"title":"O~(wi ~","paragraphs":["wj, D, wl,n) is 1 if the dependency relation, (wi --+ wj), is used in the D, 725 and 0 otherwise. Similarly, the occurrence frequency of the dependency relation,"]},{"title":"(wi +- wj),","paragraphs":["is computed by"]},{"title":"~----L---o~l(i,j)~[(i,j ).","paragraphs":["4","Preliminary experiments We have experimented with three language models, tri-gram model (TRI), bi-gram model (BI), and the proposed model (DEP) on a raw corpus extracted from KAIST corpus 3. The raw corpus consists of 1,589 sentences with 13,139 words, describing animal life in nature. We randomly divided the corpus into two parts: a training set of 1,445 sentences and a test set of 144 sentences. And we made 15 partial training sets which include the first s sentences in the whole training set, for s ranging from 100 to 1,445 sentences. We trained the three language models for each partial training set, and tested the training and the test corpus entropies.","TRI and BI was trained by counting the occurrence of tri-grams and bi-grams respectively. DEP was trained by running the reestimation algorithm iteratively until it converges to an optimal dependency grammar. On the average, 26 iterations were done for the training sets.","Smoothing is needed for language modeling due to the sparse data problem. It is to compensate for the overestimated and the under-estimated probabilities. Smoothing method it-self is an important factor. But our goal is not to find out a better smoothing method. So we fixed on an interpolation method and applied it for the three models. It can be represented as (McCandless, 1994)","..., w,-x) = ,\\P,(wilw,-,+l, ..., wi_l)","+(1 - ..., where =","C(wl, ..., w,-1) C(w,, ..., + K,\" The Ks is the global smoothing factor. The bigger the Ks, the larger the degree of smoothing. For the experiments we used 2 for Ks.","We take the performance of a language model to be its cross-entropy on test corpus, 1 s IVl"]},{"title":"E-l°g2Pm(Si)","paragraphs":["i=1","3KAIST (Korean Advanced Institute of Science and Technology) corpus has been under construction since 1994. It consists of raw text collection(45,000,000 words), POS-tagged collection(6,750,000 words), and tree-tagged collection(30,000 sentences) at present. where the test corpus contains a total of IV] words and is composed of S sentences. 3.4 i | | i | ! I 3.23 2.8 >\" 2.6 O. 2.4 u~ 2.2 ~ (DEP model) o 2 a (TRI model) i 1.8 1.6 1.4","0","200 400 600 800 1000 1200 1400 600 No. of training sentences Figure 5: Training corpus entropies","Figure 5 shows the training corpus entropies of the three models. It is not surprising that DEP performs better than BI. DEP can be thought of as a kind of linguistic bi-gram model in which long distance dependencies can be represented through the head-dependent relations between words. TRI shows better performance than both BI and DEP. We think it is because TRI overfits the training corpus, judging from the experimental results for the test corpus. 9.5 i I I I I I I 8.5 uJ 7.5 .=( (TRI model) 7 / (DEP model) o 6.5 a i I I I I I 0","200 400 600 800 1000 1200 1400 1600 No. of training sentences Figure 6: Test corpus entropies","For the test corpus, BI shows slightly better performance than TRI as depicted in Figure 6. Increase in the order of n-gram from two to three shows no gains in entropy reduction. DEP, however, Shows still better performance than the n-gram models. It shows about 11.5% entropy reduction to BI and about 11% entropy reduction to TRI. Figure 7 shows the entropies for the mixed corpus of training and test sets. From the results, we can see that head-dependent relations between words are more useful information than the naive n-gram sequences, for language modeling. We can see also that the reestimation algorithm can find out properly the hidden head-dependent relations between words, from a raw corpus. 726 ,r, fuJ (n o Z 10 9 8 7 6 i i | i ! i i (BI model) (TRI model) (DEP model) 5 3 0","200 400 600 800 1000 1200 1400 No. of training sentences Figure 7: Mixed corpus entropies 60000 50000 40000 30000 20000 10000 0 600 i ! | i i i ! (DEP model) o (TRI model) \"*'-- rT I I I I I I","200 400 600 800 1000 1200 1400 1600 No. of training sentences Figure 8: Model size","Related to the size of model, however, DEP has much more parameters than TRI and BI as depicted in Figure 8. This can be a serious problem when we create a language model from a large body of text. In the experiments, however, DEP used the grammar acquired automatically as it is. In the grammar, many inter-word dependencies have probabilities near 0. If we exclude such dependencies as was experimented for n-grams by Seymore and Rosenfeld (1996), we may get much more compact DEP model with very slight increase in entropy. 5 Conclusions In this paper, we presented a language model based on a kind of simple dependency grammar. The grammar consists of head-dependent relations between words and can be learned automatically from a raw corpus by the reestimation algorithm which is also introduced in this paper. By the preliminary experiments, it was shown that the proposed language model performs better than n-gram models in test corpus entropy. This means that the reestimation algorithm can find out the hidden information of head-dependent relation between words in a raw corpus, and the information is more useful than the naive word sequences of n-gram, for language modeling.","We are planning to experiment the performance of the proposed language model for large corpus, for various domains, and with various smoothing methods. For the size of the model, we are planning to test the effects of excluding the dependency relations with near zero probabilities."]},{"title":"References","paragraphs":["P. F. Brown, V. J. Della Pietra, P. V. deSouza, J. C. Lai, and R. L. Mercer. 1992. \"Class-Based n-gram Models of Natural Language\"."]},{"title":"Computational Linguistics,","paragraphs":["18(4):467-480.","C. Chang and C. Chen. 1996. \"Application Issues of SA-class Bigram Language Models\"."]},{"title":"Computer Processing of Oriental Languages, io(1):i-i5.","paragraphs":["S. F. Chen. 1996."]},{"title":"\"Building Probabilistic Models for Natural Language\".","paragraphs":["Ph.D. the-sis, Havard University, Cambridge, Massachusetts.","F. Jelinek, J. D. Lafferty, and R. L. Mercer. 1990. \"Basic Methods of Probabilistic Context Free Grammars\". Technical report, IBM - T.J. Watson Research Center.","K. Lari and S. J. Young. 1991. \"Applications of stochastic context-free grammars using the inside-outside algorithm\"."]},{"title":"Computer Speech and Language,","paragraphs":["5:237-257.","S. Lee and K. Choi. 1997. \"Reestimation and Best-First Parsing Algorithm for Probabilistic Dependency Grammar\". In"]},{"title":"WVLC-5,","paragraphs":["pages 11-21.","M. K. McCandless. 1994. \"Automatic Acquisi-tion of Language Models for Speech Recognition\". Master's thesis, Massachusetts Institute of Technology.","M. Meteer and J.R. Rohlicek. 1993. \"Statistical Language Modeling Combining N-gram and Context-free Grammars\". In"]},{"title":"ICASSP- 93,","paragraphs":["volume II, pages 37-40, January.","K. Seymore and R. Rosenfeld. 1996. \"Scalable Trigram Backoff Language Models\". Technical Report CMU-CS-96-139, Carnegie Mellon University.","S. Sneff. 1992. \"TINA: A natural language system for spoken language applications\"."]},{"title":"Com- putational Linguistics,","paragraphs":["18(1):61-86. 727"]}]}
