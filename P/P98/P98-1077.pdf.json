{"sections":[{"title":"Efficient Linear Logic Meaning Assembly","paragraphs":["Vineet Gupta Caelum Research Corporation NASA Ames Research Center","Moffett Field CA 94035 vgupt a@pt olemy, arc. nasa. gov","John Lamping","Xerox PARC 3333 Coyote Hill Road","Palo Alto CA 94304 USA i amping©parc, xerox, tom 1 Introduction The \"glue\" approach to semantic composition in Lexical-Functional Grammar uses linear logic to assemble meanings from syntactic analyses (Dalrymple et al., 1993). It has been compurationally feasible in practice (Dalrymple et al., 1997b). Yet deduction in linear logic is known to be intractable. Even the propositional tensor fragment is NP complete(Kanovich, 1992). In this paper, we investigate what has made the glue approach computationally feasible and show how to exploit that to efficiently deduce underspecified representations.","In the next section, we identify a restricted pattern of use of linear logic in the glue analyses we are aware of, including those in (Crouch and Genabith, 1997; Dalrymple et al., 1996; Dalrymple et al., 1995). And we show why that fragment is computationally feasible. In other words, while the glue approach could be used to express computationally intractable analyses, actual analyses have adhered to a pattern of use of linear logic that is tractable.","The rest of the paper shows how this pattern of use can be exploited to efficiently capture all possible deductions. We present a conservative extension of linear logic that allows a reformulation of the semantic contributions to better exploit this pattern, almost turning them into Horn clauses. We present a deduction algorithm for this formulation that yields a compact description of the possible deductions. And finally, we show how that description of deductions can be turned into a compact underspecifled description of the possible meanings.","Throughout the paper we will use the illustrative sentence \"every gray cat left\". It has flHlctional structure"]},{"title":"(1) [PRED 'LEAVE' 1 PRED 'CAT' f: SUBJ g: [SPEC 'EVERY' [MODS {[ PRED 'GRAY']}","paragraphs":["and semantic contributions leave :Vx. ga',-*x --.o fo',-*leave(x)"]},{"title":"cat :w. (ga VAR)--** ~ (ga RESTR)-,~ Cat(*)","paragraphs":["gray NP. [Vx. (ga VAtt)---* x --o (g~ RESTR)-,-* P(x)]"]},{"title":"-o [w. (g~ VAR)~,","paragraphs":["every :VH, R, S."]},{"title":"[w. (g~ VAR)--~, --o (g~ RESTR)--~R(z)]","paragraphs":["®[Vx. g,,'...*x ~ g-,-*S(x)] --~ H',-* every(R, S)","For our purposes, it is more convenient to follow (Dalrymple et al., 1997a) and separate the two parts of the semantic contributions: use a lambda term to capture the meaning formulas, and a type to capture the connections to the f-structure. In this form, the contributions are leave : cat : gray : every : Ax.leave(x) : g,, --o fa .,~x.cat(x) : (ga"]},{"title":"VAR) --o","paragraphs":["(ga"]},{"title":"RESTR)","paragraphs":["AP.Ax.gray(P)(x) :"]},{"title":"((g~ vAR) --o (ga RESTR)) --o (g~ VAn) ~ (ga RESTR)","paragraphs":["AR.AS.every(R, S) : VH. (((g~ 'CAR) --o (ga RESTR))"]},{"title":"®(g. ~ H))","paragraphs":["--oH With this separation, the possible derivations are determined solely by the \"types\", the connections to the f-structure. The meaning is as-sembled by applying the lambda terms in accordance with a proof of a type for the sentence. We give the formal system behind this approach, C in Figure 1 -- this is a different presentation of the system given in (Dalrymple 464 et al., 1997a), adding the two standard rules for tensor, using pairing for meanings. For the types, the system merely consists of the Inear logic rules for the glue fragment.","We give the proof for our example in Figure 2, where we have written the types only, and have omitted the trivial proofs at the top of the tree. The meaning every(gray(cat),left) may be as-sembled by putting the meanings back in according to the rules of C and r/-reduction. M : A ~-c M / : A where M --a,n M ~ F,P,Q,A~-c R F,Q,P, AF-c R r,",": A[B/X] -o R F,M :VX.AF-c R r M: A[Y/X]"]},{"title":"(r new)","paragraphs":["F t-c M : VX.A F ~-c N : A A,M[N/x] : B F-c R F,A, Ax.M : A --o B ~-c R r,y : A be M[y/x] : B r F-c Ax.M : A .-.o B (y new) F,M :A,N : B I-- R r, (M, N) :A® B ~- R FF-M:A","A~-N:B F,A~-(M,N):A®B Figure 1: The system C. M,N are meanings, and x, y are meaning variables. A, B are types, and X, Y are type variables. P, Q, R are formulas of the kind M : A. F,A are multisets of formulas. 2 Skeleton references and modifier references The terms that describe atomic types, terms Ike ga and (g~ vA1Q, are semantic structure references, the type atoms that connect the semantic assembly to the syntax. There is a pattern to how they occur in glue analyses, which reflects their function in the semantics.","Consider a particular type atom in the example, such as g~. It occurs once positively in the contribution of \"every\" and once negatively in the contribution of \"leave\". A sightly more compIcated example, the type (ga l~nSTR) occurs once positively in the contribution of \"cat\", once negatively in the contribution of \"every\", and once each positively and negatively in the contribution of \"gray\".","The pattern is that every type atom occurs once positively in one contribution, once negatively in one contribution, and once each positively and negatively in zero or more other contributions. (To make this generaIzation hold, we add a negative occurrence or \"consumer\" of fa, the final meaning of the sentence.) This pattern holds in all the glue analyses we know of, with one exception that we will treat shortly. We call the independent occurrences the skeleton occurrences, and the occurrences that occur paired in a contribution modifier occurrences.","The pattern reflects the functions of the lexical entries in LFG. For the type that corresponds to a particular f-structure, the idea is that, the entry corresponding to the head makes a positive skeleton contribution, the entry that subcategorizes for the f-structure makes a negative skeleton contribution, and modifiers on the f-structure make both positive and negative modifier contributions.","Here are the contributions for the example sentence again, with the occurrences classified. Each occurrence is marked positive or negative, and the skeleton occurrences are underlined.","leave : g_Ka- --o fa+ cat : (ga VAtt)- --o (ga ttESWtt) + gray : ((ga VAn) + --o (ga aESTR)-)","---o (ga VAn)- --o (ga RESTR) +","every : VH. (((ga VAR) + --o (ga RESTR.)-)","®(g_z.~ ~ --~ g-))","---o H +","This pattern explains the empirical tractability of glue inference. In the general case of multiplicative Inear logic, there can be complex combinatorics in matching up positive and negative occurrences of literals, which leads to NPcompleteness (Kanovich, 1992). But in the glue fragment, on the other hand, the only combinatorial question is the relative ordering of moditiers. In the common case, each of those order-ings is legal and gives rise to a different meaning. So the combinatorics of inference tends to be proportional to the degree of semantic ambiguity. The complexity per possible reading is thus roughly tnear in the size of the utterance.","But, this simple combinatoric structure suggests a better way to exploit the pattern. Rather than have inference explore all the combinatorics of different modifier orders, we can get a single underspecitied representation that captures all possible orders, without having to 465 cat F- (ga VAR) --o (go RESTR) (go VAR) --O (go RESTR) ~ (go\" VAR) ---O (ga RESTR) cat, ((ga VAR) ---o (ga RESTR)) --o (ga VAR) ---o (ga RESTR) ~ (ga VAR) ---o (ga RESTR) gray, cat ~- (ga VAR) --~ (ga"]},{"title":"RESTR)","paragraphs":["leave F- ga --o fo gray, cat, leave F ((go VAR) --~ (go RESTR)) ~(ga .--o fa) fo ~- fa gray, cat,leave, (((ga VAR) --o (ga RESTR)) ® (go. ---o fo)) --o fo ~- fo every, gray, cat, leave F- fa Figure 2: Proof of \"Every gray cat left\", omitting the lambda terms explore them.","The idea is to do a preliminary deduction involving just the skeleton, ignoring the modifier occurrences. This will be completely deterministic and linear in the total length of the formulas. Once we have this skeletal deduction, we know that the sentence is well-formed and has a meaning, since modifier occurrences essentially occur as instances of the identity axiom and do not contribute to the type of the sentence. Then the system can determine the meaning terms, and describe how the modifiers can be attached to get the final meaning term. That is the goal of the rest of the paper. 3 Conversion toward horn clauses The first hurdle is that the distinction between skeleton and modifier applies to atomic types, not to entire contributions. The contribution of \"every\", for example, has skeleton contributions for go, (go VAR), and (ga RESTR), but modifier contributions for H. Furthermore, the nested implication structure allows no nice way to disentangle the two kinds of occurrences. When a deduction interacts with the skeletal go in the hypothetical it also brings in the modifier H.","If the problematic hypothetical could be converted to Horn clauses, then we could get a better separation of the two types of occurrences. We can approximate this by going to an indexed linear logic, a conservative extension of the system of Figure 1, similar to Hepple's system(Hepple, 1996).","To handle nested implications, we introduce the type constructor A{B}, which indicates an A whose derivation made use of B. This is similar to Hepple's use of indices, except that we indicate dependence on types, rather than on indices. This is sufficient in our application, since each such type has a unique positive skeletal occurrence.","We can eliminate problematic nested implications by translating them into this construct, in accordance with the following rule:","For a nested hypothetical at top level that has a mix of skeleton and modifier types: M : ( A -o B ) -o C replace it with x:A, M:(B{A}---oC) where x is a new variable, and reduce complex dependency formulas as follows: 1. Replace A{B ---o C} with A{C{B}}. 2. Replace (A --o B){C} with A --o B{C}.","The semantics of the new type constructors is captured by the additional proof rule:","F,x:AF-M:B F,x : A ~- Ax.M : B{A}","The translation is sound with respect to this rule: Theorem 1 If F is a set of sentences in the unextended system of Figure 1, A is a sentence in that system, and F ~ results from F by applying the above conversion rules, then F F- A in the system of Figure 1 iff F' F- A in the extended system.","The analysis of pronouns present a different problem, which we discuss in section 5. For all other glue analyses we know of, these conversions are sufficient to separate items that mix interaction and modification into statements of 466 the form S, Jr4, or S -o .h4, where S is pure skeleton and M is pure modifier. Furthermore, .h4 will be of the form A -o A, where A may be a formula, not just an atom. In other words, the type of the modifier will be an identity axiom. The modifier will consume some meaning and produce a modified meaning of the same type.","In our example, the contribution of \"every\", can be transformed by two applications of the nested hypothetical rule to"]},{"title":"every","paragraphs":[":AR.AS.every(R, S) :"]},{"title":"VH. (ga RESTR){(ga VAR)}","paragraphs":["--o H{gq} -o H x :(go VAR) Y :ga Here, the last two sentences are pure skeleton, producing (g~ VAR) and ga, respectively. The first is of the form S -o M, consuming"]},{"title":"(ga RESTR), to","paragraphs":["produce a pure modifier.","While the rule for nested hypotheticals could be generalized to eliminate all nested implications, as Hepple does, that is not our goal, because that does remove the combinatorial combination of different modifier orders. We use the rule only to segregate skeleton atoms from modifier atoms. Since we want modifiers to end up looking like the identity axiom, we leave them in the A -o A form, even if A contains further implications. For example, we would not apply the nested hypothetical rule to simplify the entry for gray any further, since it is already in the form A ---o A.","Handling intensional verbs requires a more precise definition of skeleton and modifier. The type part of an intensional verb contribution looks like (VF.(ha -o F) --o F) -o ga -o fa (Dalrymple et al., 1996).","First, we have to deal with the small technical problem that the VF gets in the way of the nested hypothetical translation rule. This is easily resolved by introducing a skolem constant, 5', turning the type into ((h~ -o 5') --o 5') --o g~ --o f~. Now, the nested hypothetical rule can be applied to yield (ho -o S) and S{5\"{h~}} ---o ga --o fa.","But now we have the interesting question of whether the occurrences of the skolem constant, S, are skeleton or modifier. If we observe how 5' resources get produced and consumed in a deduction involving the intensional verb, we find that (ha --o 5') produces an 5', which may be modified by quantifiers, and then gets consumed by S { S { ha } } ---o ga -o f~. So unlike a modifier, which takes an existing resource from the environment and puts it back, the intentional verb places the initial resource into the environment, allows modifiers to act on it, and then takes it out. In other words, the intensional verb is act-ing like a combination of a skeleton producer and a skeleton consumer.","So just because an atom occurs twice in a contribution doesn't make the contribution a modifier. It is a modifier if its atoms must in-teract with the outside, rather than with each other. Roughly, paired modifier atoms function as f -o f, rather than as f ® f±, as do the S atoms of intensional verbs.","Stated precisely: Definition 2 Assume two occurrences of the same type atom occur in a single contribution. Convert the formula to a normal form consisting of just ®, ~ , and J_ on atoms by converting subformulas A -o B to the equivalent A ± :~ B, and then using DeMorgan's laws to push all J_ 's down to atoms. Now, if the occurrences of the same type atom occur with opposite polarity and the connective between the two subexpressions in which they occur is ~ , then the occurrences are modifiers. All other occurrences are skeleton. For the glue analyses we are aware of, this definition identifies exactly one positive and one negative skeleton occurrence of each type among all the contributions for a sentence. 4 Efficient deduction of underspecified representation In the converted form, the skeleton deductions can be done independently of the modifier deductions. Furthermore, the skeleton deductions are completely trivial, they require just a linear time algorithm: since each type occurs once positively and once negatively, the algorithm just resolves the matching positive and negative skeleton occurrences. The result is several deductions starting from the contributions, that collectively use all of the contributions. One of the deductions produces a meaning for fa, for the whole f-structure. The others produce pure modifiers -- these are of the form A --o A. For 467 Lexical contributions in indexed logic: leave :","cat : gray : everyx : every2 : everya : Ax.leave(x) : ga --o fc,"]},{"title":"ax.eat(x): VAR) R .STR) : VAR) --o R STR)) VAR) --o RESTR)","paragraphs":["AR.AS.every(R, S) : vg. (g~ RnSTR){(g~ 'CAR)} --o g{ga} ---o H"]},{"title":"z VAR)","paragraphs":["Y :g~ The following can now be proved using the extended system: gray ~- AP.Ax.gray(P)(x) : ((ga VAR) --o (g~"]},{"title":"RESTR)) ----O","paragraphs":["(g~ VAR) --o (ga RESTR)","every2, cat, every1 ~- AS.every(Ax.eat(x), S): VH. H{ga} --o H everya, leave F- leave(y) : fa Figure 3: Skeleton deductions for \"Every gray cat left\". the example sentence, the results are shown in Figure 3.","These skeleton deductions provide a compact representation of all possible complete proofs. Complete proofs can be read off from the skeleton proofs by interpolating the deduced moditiers into the skeleton deduction. One way to think about interpolating the modifiers is in terms of proof nets. A modifier is interpolated by disconnecting the arcs of the proof net that connect the type or types it modifies, and reconnecting them through the modifier. Quantifiers, which turn into modifiers of type VF.F ---o F, can choose which type they modify.","Not all interpolations of modifiers are legal. however. For example, a quantifier must outscope its noun phrase. The indices of the modifier record these limitations. In the case of the modifier resulting from \"every cat\", VH.H{ga} ---o H, it records that it must outscope \"every cat\" in the {ga}. The indices determine a partial order of what modifiers must outscope other modifiers or skeleton terms.","In this particular example, there is no choice about where modifiers will act or what their relative order is. In general, however, there will be choices, as in the sentence \"someone likes every cat\", analyzed in Figure 4.","To summarize so far, the skeleton proofs provide a compact representation of all possible deductions. Particular deductions are read off by interpolating modifiers into the proofs, subject to the constraints. But we are usually more in-terested in all possible meanings than in all possible deductions. Fortunately, we can extract a compact representation of all possible meanings from the skeleton proofs.","We do this by treating the meanings of the skeleton deductions as trees, with their arcs annotated with the types that correspond to the types of values that flow along the arcs. Just as modifiers were interpolated into the proof net links, now modifiers are interpolated into the links of the meaning trees. Constraints on what modifiers must outscope become constraints on what tree nodes a modifier must dominate.","Returning to our original example, the skeleton deductions yield the following three trees:","!g"]},{"title":"RESTR) / ~/-/Iga~ tga VAR) ---o • ,~Z. ] ga RESTR)","paragraphs":["leave (go RESTR)I gray cat lga VAR) --o"]},{"title":"I g~ (go VAR) ~ I tgo' RESTR)","paragraphs":["y leave(y) aS.every(;~x.cat(x),S) aP.ax.gray(P)(x) Notice that higher order arguments are reflected as structured types, like"]},{"title":"(g~ VAR) ----o","paragraphs":["(g~"]},{"title":"RESTR).","paragraphs":["These trees are a compact description of the possible meanings, in this case the one possible meaning. We believe it will be possible to translate this representation into a UDRS representation(Reyle, 1993), or other similar representations for ambiguous sentences.","We can also use the trees directly as an underspecified representation. To read out a particular meaning, we just interpolate modifiers into the arcs they modify. Dependencies on a 468","The functional structure of \"Someone likes every cat\". PRED SUBJ"]},{"title":"/:","paragraphs":["OBJ The lexical entries after 'LIKE'"]},{"title":"h:[ pRro 'soMroNE']","paragraphs":["PRED 'eAT' ]","g: SPEC ~EVERY'","conversion to indexed form:","like :","cat :","someonel :","someone2 : everyl : every2 : everya :"]},{"title":"Ax.Ay.tike(x, y): (ho ® go) -o/o Ax.cat(x): (go","paragraphs":["VAR) -o (ga RESTR)"]},{"title":"z:hv AS.some(person, S) : VH. H{ho) --o H AR.AS.every(R, S) : vg. (go","paragraphs":["RESTR){(go VA1Q) --o"]},{"title":"H{go) --o H","paragraphs":["x : (go VAR)"]},{"title":"Y:go","paragraphs":["From these we can prove: someone1, everya, like ~-"]},{"title":"like(z, y) : fo","paragraphs":["someone2 F-"]},{"title":"AS.some(person, S) : VH. H{ho} --o H","paragraphs":["every2, cat, every1 b"]},{"title":"AS.every(cat, S) : VH. H{go} -o H","paragraphs":["Figure 4: Skeleton deductions for \"Someone likes every cat\" modifier's type indicate that a lambda abstraction is also needed. So, when \"every cat\" modifies the sentence meaning, its antecedent, in-stantiated to"]},{"title":"fo{go)","paragraphs":["indicates that it lambda abstracts over the variable annotated with"]},{"title":"go","paragraphs":["and replaces the term annotated fo. So the result is:"]},{"title":"Ifo , every","paragraphs":["RESTR.) A"]},{"title":"Ax. Y.","paragraphs":["(go RESTR)] ]fo"]},{"title":"cat leave","paragraphs":["(go VAR)!:"]},{"title":"/o","paragraphs":["Similarly \"gray\" can modify this by splicing it into the line labeled (go VAR) --o (go"]},{"title":"RESTR)","paragraphs":["to yield (after y-reduction, and removing labels on the arcs)."]},{"title":"Ifo /ver gray leave I","paragraphs":["cat This gets us the expected meaning"]},{"title":"every(gray(cat), leave).","paragraphs":["In some cases, the link called for by a higher order modifier is not directly present in the tree, and we need to do A-abstraction to support it. Consider the sentence \"John read Hamlet quickly\". We get the following two trees from the skeleton deductions: re!fd"]},{"title":"g/ \\ho John Hamlet read(John, Hamlet) I go --o fo quickly Igo-o fo AP.Ax.quickly( P )( x )","paragraphs":["There is no link labeled ga --o fa to be modified. The left tree however may be converted by A-abstraction to the following tree, which has a required link. The @ symbol represents A application of the right subtree to the left."]},{"title":"I/o Ax. John I/o read gj \\ho x Hamlet","paragraphs":["Now"]},{"title":"quickly","paragraphs":["can be interpolated into the link labeled go --o fo to get the desired meaning"]},{"title":"quickly(read(Hamlet), John),","paragraphs":["after r/- reduction. The cases where A-abstraction is required can be detected by scanning the modifiers and noting whether the links to be modified are present in the skeleton trees. If not, A-abstraction can introduce them into the un-469 derspecified representation. Furthermore, the introduction is unavoidable, as the link will be present in any final meaning. 5 Anaphora As mentioned earlier, anaphoric pronouns present a different challenge to separating skeleton and modifier. Their analysis yields types like f~ --o (f~ ® g~) where g~ is skeleton and f~ is modifier. We sketch how to separate them.","We introduce another type constructor"]},{"title":"(B)A,","paragraphs":["informally indicating that A has not been fully used, but is also used to get B.","This lets us break apart an implication whose right hand side is a product in accordance with the following rule:","For an implication that occurs at top level, and has a product on the right hand side that mixes skeleton and modifier types:"]},{"title":"Ax.(M, N) : A ---o (B ® C)","paragraphs":["replace it with"]},{"title":"Ax.M : (C)A -o B, N : C","paragraphs":["The semantics of this constructor is captured by the two rules: M1 : AI~...,M,~ : An ~- M : A"]},{"title":"M1 : (B)A1,...,Mn: (B)A,~ t- M: (B)A","paragraphs":["F, M1 : (B)A,"]},{"title":"M2 :B~-N :C F t, M~:A, M~:B~-N':C","paragraphs":["where the primed terms are obtained by replacing free x's with what was applied to","the Ax. in the deduction of"]},{"title":"(B)A","paragraphs":["With these rules, we get the analogue of Theorem 1 for the conversion rule. In doing the skeleton deduction we don't worry about the"]},{"title":"(B)A","paragraphs":["constructor, but we introduce constraints on modifier positioning that require that a hypothetical dependency can't be satisfied by a deduction that uses only part of the resource it requires. 6 Acknowledgements We would like to thank Mary Dalrymple, John Fry, Stephan Kauffmann, and Hadar Shemtov for discussions of these ideas and for comments on this paper. References","Richard Crouch and Josef van Genabith. 1997. How to glue a donkey to an f-structure, or porting a dynamic meaning representation into LFG's linear logic based glue-language semantics. Paper to be presented at the Second International Workshop on Computational Semantics, Tilburg, The Netherlands, January 1997.","Mary Dalrymple, John Lamping, and Vijay Saraswat. 1993."]},{"title":"LFG","paragraphs":["semantics via constraints. In"]},{"title":"Proceedings of the Sixth Meeting of the European ACL,","paragraphs":["pages 97-105, University of Utrecht. European Chapter of the Association for Computational Linguistics.","Mary Dalrymple, John Lamping, Fernando C. N. Pereira, and Vijay Saraswat. 1995. Linear logic for meaning assembly. In"]},{"title":"Proceed- ings of CLNLP,","paragraphs":["Edinburgh.","Mary Dalrymple, John Lamping, Fernando C. N. Pereira, and Vijay Saraswat. 1996. In-tensional verbs without type-raising or lexical ambiguity. In Jerry Seligman and Dag Westerst£hl, editors,"]},{"title":"Logic, Language and Com- putation,","paragraphs":["pages 167-182. CSLI Publications, Stanford University.","Mary Dalrymple, Vineet Gupta, John Lamping, and Vijay Saraswat. 1997a. Relating resource-based semantics to categorial semantics. In"]},{"title":"Proceedings of the Fifth Meeting on Mathematics of Language (MOL5),","paragraphs":["Schloss Dagstuhl, Saarbriicken, Germany.","Mary Dalrymple, John Lamping, Fernando C. N. Pereira, and Vijay Saraswat. 1997b. Quantifiers, anaphora, and intensionality."]},{"title":"Journal of Logic, Language, and Information,","paragraphs":["6(3):219-273.","Mark Hepple. 1996. A compilation-chart method for linear categorical deduction. In"]},{"title":"Proceedings of COLING-96,","paragraphs":["Copenhagen.","Max I. Kanovich. 1992. Horn programming in linear logic is NP-complete. In"]},{"title":"Seventh An- nual IEEE Symposium on Logic in Computer Science,","paragraphs":["pages 200-210, Los Alamitos, California. IEEE Computer Society Press.","Uwe Reyle. 1993. Dealing with ambiguities by underspecification: Construction, representation, and deduction."]},{"title":"Journal of Semantics,","paragraphs":["10:123-179. 470"]}]}
