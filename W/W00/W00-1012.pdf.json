{"sections":[{"title":"Dialogue Management in the Agreement Negotiation Process: A Model that Involves Natural Reasoning Mare KOIT","paragraphs":["Institute of Computer Science, Tartu University","Liivi 2","50409 Tartu, Estonia","koit@ut.ee","Haldur ~)IM","Dept. of General Linguistics, Tartu University","Tiigi 78 51014 Tartu, Estonia hoim@psych.ut.ee Abstract In the paper we describe an approach to dialogue management in the agreement negotiation where one of the central roles is attributed to the model of natural human reasoning. The reasoning model consists of the model of human motivational sphere, and of reasoning algorithms. The reasoning model is interacting with the model of communication process. \"/'he latter is considered as rational activity where central role play the concepts of communicative strategies and tactics. Introduction Several researches have modelled the process of argument negotiation in cooperative dialogue where one participant makes a proposal to another participant and as the result of negotiation this is accepted or rejected. Chu-Carroll and Carberry (1998) present a cooperative response-generation model as a recursive cycle Propose-Evaluate-Modify. They concentrate on dialogues of information sharing and negotiation. An information sharing dialogue is started, when the agent recognised a turn of his/her partner as a proposal, but does not have enough information to decide whether to accept it or not. A negotiation dialogue is started, when the agent concludes that the proposal is in conflict with his/her beliefs and preferences, i.e. tends to reject it. Heeman and Hirst (1995) model cooperation by the cycle Present-Judge-Refashion. They use two levels of modelling - planning and cooperation. On the first level utterances are generated and interpreted, on the second level the cooperation of agents is modelled, relating it to agent's mental states and planning processes. The Shared Plans cooperation model deals with planning processes in which participate multiple agents, see Lochbaurn (1998). The model concentrates on group tasks that can be divided into separate, but interacting subtasks, and the central problem is coordination of intentions and goals of partners. Di Eugenio et al. (2000) present a model BalanceProposeDispose: first, the relevant information concerning the task is considered and discussed, then a proposal is made and, lastly, the decision concerning the proposal is made - it is accepted or rejected. In our model we depart from the same type of situation. One agent, A, addresses another agent, B, with the intention that B will carry out an action D. After some negotiation, B agrees or rejects the proposal. In this paper we concentrate on the problems connected with modelling participants as conversation agents who are able to participate in negotiation in the form of natural dialogue - dialogue that is carried out in natural language and according to the rules of human communication. Such a dialogue can be considered as rational behaviour which is based on beliefs, wants and intentions of agents, at the same time being restricted by their resources, see Jokinen (1995), Webber (2000). Conversation agent is a kind of intelligent agent - a computer program that is able to communicate with humans as another human being. As it is generally accepted, in a model of conversation agent it is necessary to represent its cognitive states as well as cognitive processes. 102 One of the most well-known models of this type is the BDI model, see Allen (1994). Our main point in this paper is that the general concepts of cognitive states and processes used in BDI-type models should be extended in order to include certain factors from human motivational sphere and certain social principles in order to guarantee naturalness of dialogues of the type we are concerned with. This is especially important in connection with the fact that interest in modelling cooperative dialogues where partners are pursuing a common goal has considerably increased in recent years. On the one hand, this is connected with rapid spreading of Internet-based services. On the other hand, the interest in models of full natural dialogue derives from the possibility of building speech interfaces with different knowledge and databases, see Dybkjaer (2000). Both of these developments broaden the concept of naturalness of dialogue considerably and present to it much stronger requirements concerning its empirical adequacy as it has been generally accepted thus far."]},{"title":"1 Model of Conversation Agent","paragraphs":["In our model a conversation agent, A, is a program that consists of 6 (interacting) modules: A = (PL, PS, DM, INT, GEN, LP), where PL - planner, PS - problem solver, DM - dialogue manager, INT - interpreter, GEN - generator, LP - linguistic processor. PL directs the work of both DM and PS, where DM controls communication process and PS solves domain-related tasks. The task of INT is to make semantic analysis of partner's utterances and that of GEN is to generate semantic representations of agent's own contributions. LP carries out linguistic analysis and generation. Conversation agent uses in its work goal base GB and knowledge base KB. In our model, KB consists of 4 components: KB = (KBw, KBL, KBD, KBs), where KBw contains world knowledge, KBL - linguistic knowledge, KBD - knowledge about dialogue and KBs - knowledge about interacting subjects. For instance, KBD contains definitions of communicative acts, turns and transactions (declarative knowledge), and algorithms that are applied to reach communicative goals - communicative strategies and tactics (procedural knowledge); KBs contains knowledge about evaluative dispositions of participants towards the world (e.g. what do they consider as pleasant or unpleasant, useful or harmful), and, on the other hand, algorithms that are used to generate plans for acting on the world. A necessary precondition of a communicative interaction is existence of shared (mutual) knowledge of interacting agents. This concerns goal bases as well as all types of knowledge bases; the intersections of the corresponding bases of interacting agents A and B cannot be empty: GB g n GB a ~:~, KBAw n KBBw ~, KBAL n KBBL ~O, KI3AD n KBBD ~, KB ABS"]},{"title":"KBBs :.7~:~, KBBAs (\"h KBAs -~:~.","paragraphs":["In this paper we will consider a specific type of dialogue where the communicative goal of agent A is to get agent B to agree to carry out an action D - so-called agreement negotiation dialogue. We will concentrate here on dialogue management in such kind of interaction, i.e. on the functioning of the module DM."]},{"title":"2 Dialogue Management 2.1 Reasoning Model","paragraphs":["A dialogue participant chooses his/her responses to the parter's communicative acts as a result of certain reasoning process. After A has made B a proposal to do D, B can respond with agreement or rejection, depending on the result of his/her reasoning. Because we consider the model of natural human reasoning as one of the important components in attaining naturalness of dialogue as a whole, we will discuss our model of reasoning in some detail. From the point of view of practical NLP the approach we will present below may seem too abstract. But without solid theoretical basis it will appear impossible to guarantee naturalness of dialogues carried out by computers with human users. We think that the model we describe here can be taken as a basis for the corresponding discussion. Our model is not based on any scientific theory of how human reasoning proceeds; our aim is to model a \"naive theory of reasoning\" which humans follow in everyday life when trying to understand, predict and influence other persons' decisions and behavior, see Koit and C)im (2000). The reasoning model consists of two functionally linked parts: 1) a model of human motivational sphere; 2) reasoning schemes. 103 In the motivational sphere three basic factors that regulate reasoning of a subje, ct concerning D are differentiated. First, subject may wish to do D, if pleasant aspects of D for him/her overweight unpleasant ones; .second, subject may fmd reasonable to do D, if D is needed to reach some higher goal, and useful aspects of D overweight harmful ones; and tlfird, subject can be in a situation where he/she must (is obliged) to do D - if not doing D will lead to some kind of punishment. We call these; factors wish-, needed- and must-factors, respectively. For instance, in reasoning about some action D (e.g. proposed by another agent), an agent as an individual subject typically starts with checking his/her wish-factor, i.e. whether D's pleasant aspects overweight unpleasant ones. If this holds, then the subject checks his/her resources, and if these exist, proceeds to other positive and negative aspects of D: its usefulness and harmfulness, and if D is prohibited, then also possible punishment(s). If the positive aspects in sum overweight negative ones, the resulting decision will be to do D, otherwise - not to do D. There can exist other typical situations. If the agent is an \"official\" person, or a group of subjects formed to fulfil certain tasks and/or to pursue certain pre-established goal(s), then typically the starting point of reasoning is needed- and/or mast-factor. This means that there exist certain general principles that determine how the reasoning process proceeds. These principles depend, in part, on the type of the reasoning agent. Before starting to construct a concrete reasoning model the types of agents involved should be established. In our implementation the agent is supposed to be a \"simple\" human being and the actions under consideration are from everyday life. In this case as examples of such principles used in our model we can present the following ones. For more details, see Oim (1996). P1. People prefer pleasant (more pleasant) states to unpleasant (less pleasant) ones. P2. People don \"t take an action of which they don't assume that its consequence will be a pleasant (useful) situation, or avoidance of an unpleasant (harmful) situation. The following principles illustrate more concrete (operational) rules. P3. In assessing an action D the values of (internal - wish- and needed-)factors are checked before the external (must-) factors. P4. If D is found pleasant enough (i.e. D's pleasant aspects overweight unpleasant ones), then the needed- and must-factors will first be checked from the point of view of their negative aspects (\"to what harmful consequences or punishments D would lead? \"~). The rule P4 explains, for example, why in Figure 1 step 1 is immediately followed by step 2. The weights of different aspects of D (pleasantness, unpleasantness, usefulness, harmfulness, punishment for doing a prohibited action or not-doing an obligatory action) must be summed up in some way. Thus, in a computational model weights must have numerial values. In reality people do not operate with numbers but, rather, with some fuzzy sets. On the other hand, existence of certain scales also in human everyday reasoning is apparent. For instance, for the characterisation of pleasant and unpleasant aspects of some action there are specific words: enticing, delighOCul, enjoyable, attractive, acceptable, unattractive, displeasing, repulsive etc. Each of these adjectives can be expressed quantitatively. This presupposes empirical studies, though. We have represented the model of motivational sphere by the following vector of weights: w A = (w(resourcesAol), w(pleasAm), w(unpleasAm), w(useAm), w(harmAm), w(obligatoryAm), w(prohibitedAm),"]},{"title":"w(punishgm), w(punishAnot.Di),..., w(resourcesADn), w(pleasAon),","paragraphs":["w(unpleasADn), W(useAD~), w(harmAo,), w(obligatoryAD,), w(prohibitedADn), w(punishAm),"]},{"title":"W(punishAnot.Dn)). Here D~, ..., Dn represent human actions; W(resourcesADi)=I, if A has resources necessary to do Di (otherwise 0); w(obligatoryAsi)=l, if Di is obligatory for A (otherwise 0); w(prohibitedADi)=l, if Di is prohibited for A","paragraphs":["(otherwise 0). The values of other weights are non-negative natural numbers. The second part of the reasoning model consists of reasoning schemas, that supposedly regulate human action-oriented reasoning. A reasoning scheme represents steps that the agent goes through in his reasoning process; these consist in computing and comparing the weights of 104 different aspects of D; and the result is the decision to do or not to do D. Figure 1 presents the reasoning scheme that departs from the wish of a subject to do D. The scheme also illustrates one of the general principles referred to above. It explains the order the steps are taken by the reasoning agent: if a subject is in a state where he/she wishes to do D, then he/she checks first the harmful/useful aspects of D, and after this proceeds to aspects connected with possible punishments. Presupposition: w(pleas) > w(unpleas). i) for doing D?","If not then not to do D. 2) Is w(pleas) > w(unpleas) + w(harm)?","If not then go to step 6. 3) Is D prohibited?","If not then to do D. 4) Is w(pleas) > w(unpleas) + w(harm) + w(punishD)?","If yes then to do D. 5) Is w(pleas) + w(use) > w(unpleas) + w(harm) + w(punish~)?","If yes then to do D else not to do D. 6) Is w(pleas) + w(use) > w(unpleas) + w(harm)?","If yes then go to step 9. 7) Is D obligatory?","If not then not to do D. 8) Is w (pleas) + w (use) + w (punishnot. ~) > w (unpleas) + w (harm) ?","If yes then to do D else not to do D. 9) Is D prohibited?","If not then to do D. i0) Is w (pleas) + w(use) > w (unpleas) + w (harm) + w (punish~) ? Are there enough resources","If yes then to do D else not to do D. Figure 1. The reasoning procedure that departs from the wish of a subject to do D. The prerequisite for triggering this reasoning procedure is w(pleas) > w(unpleas), which is based on the following assumption: if a person wishes to do something, then he/she assumes that the pleasant aspects of D (including its consequences) overweigh its unpleasant aspects. The same kinds of reasoning schemes are constructed for the needed- and must-factors. The reasoning model is connected with the general model of conversation agent in the following way. First, the planner PL makes use of reasoning schemes and second, the KBs contains the vector w A (A's subjective evaluations of all possible actions) as well as vectors w AB (A's beliefs concerning B's evaluations, where B denotes agents A may communicate with). The vector w As do not represent truthful knowledge, it is used as a partner model. When comparing our model with BDI model, then belier are represented by knowledge of the conversation agent with reliability less than 1; desires are generated by the vector of weights WA; and intentions correspond to goals in GB. In addition to desires, from the weights vector we also can derive some parameters of the motivational sphere that are not explicitly covered by the basic BDI model: needs, obligations and prohibitions. Some wishes or needs can be stronger than others: if w(pleasADi) - W(unpleasAoi) > w(pleasAoj) - w(unpleasAt~), then the wish to do Di is stronger than the wish to do Dj. In the same way, some obligations (prohibitions) can be stronger than others, depending on the weight of the corresponding punishment. It should be mentioned that adding obligations to the standard BDI model is not new. Traum and Allen (1994) show how discourse obligations can be used to account in a natural manner for the connection between a question and its answer in dialogue and how obligations can be used along with other parts of the discourse context to extend the coverage of a dialogue system."]},{"title":"2.2 Communicative Strategies and Tactics","paragraphs":["Knowledge about dialogue KBD, which is used by the Dialogue Manager, consists of two functional parts: knowledge of the regularities of dialogue, and rules of constructing and combining speech acts. The top level concept of dialogue rules in our model is communicative strategy. This concept is reserved for such basic communication types as information exchange, directive dialogue, 105 phatic communication, etc. On the more concrete level, the conversation agent can realise a communicative strategy by means of several communicative tactics; this concept more closely corresponds to the: concept of communicative strategy as us~l in some other approaches, see e.g. Jokinen (1996). In the case of directive communication (which is the strategy we are interested in) the agent A can use tactics of enticing, persuading, threatening. In the case of enticing, A stresses pleasant aspects, in the case of persuading - usel~ aspects of D for B; in the case of ordering A addresses obligations of B, in the case of threatening A explicitly refers to possible punishment for not"]},{"title":"doing D.","paragraphs":["Which one of these tactics A chooses depends on several factors. There is one: relevant aspect of human-human communication which is relatively well studied in pragmatics of human communication and which we have included in our model as the concept of communicative space. Communicative space is defined by a number of coordinates that characterise the relationships of participants in a communicative encounter. Communication can be collaborative or confrontational, personal or impersonal; it can be characterised by the social distance between participants; by the modality (friendly, ironic, hostile, etc.) and by intensity (peaceful, vehement, etc.). Just as in case of motivations of human behaviour, people have an intuitive, \"naive theory\" of these coordinates. This constitutes a part of the social"]},{"title":"conceptualisation","paragraphs":["of communication, and it also should not be ignored in serious attempts to model natural communication in NLP systems. In our model the choice of a communicative tactics depends on the \"point\" of the communicative space in which the participants place themselves. The values of the coordinates are again given in the form of numerical values. The communicative strategy can be presented as an algofithra (Figure 2). Figure 3 presents a tactic of enticement. In our model there are three different communicative tactics that A can use within the frames of the directive communicative strategy: those of enticement, persuasion and threatening. Each communicative tactic constitutes a procedure for compiling a turn in the ongoing dialogue. i) Choose the communicative tactic. 2) Implement the tactic to generate an expression (inform the partner of the communicative goal). 3) Did the partner agree to do D? If yes then finish (the communicative goal has been reached). 4) Give up? If yes then finish (the communicative goal has not been reached). 5) Change the communicative tactic? If yes then choose the new tactic. 6) Implement the tactic to generate an expression. Go to step 3. Figure 2. Communicative strategy used by the initiator of communication. i) If wB(resources)=0 then present a counterargument in order to point at the presence of possible resources or at the possibility to gain them. 2) If w s(harm) > w as(harm) then present a counterargument in order to downgrade the value of harm. 3) If wB(obligatory)=l & w B (punish .... o) < w~ (punishno~-~) then present a counterargument in order to decrease the weight of the punishment. 4) If wB (prohibited) =l & w ~(punis~) > w ~(punis~) then present a counterargument in order to downgrade the weight of the punishment. 5) If wB(unpleas) > w~(unpleas) then present a counterargument in order to downgrade the value of the unpleasant aspects of D. 6) Present a counterargument in order to stress the pleasant aspects of D. Figure 3. A's tactics of enticement 106 The tactic of enticement consists in increasing B's wish to do D; the tactic of persuasion consists in increasing B's belief of the usefulness of D for him/her, and the tactic of threatening consists in increasing B's understanding that he/she must do D. Communicative tactics are directly related to the reasoning process of the partner. IrA is applying the tactics of enticement he/she should be able to imagine the reasoning process in B that is triggered by the input parameter wish. If B refuses to do D, then A should be able to guess at which point the reasoning of B went into the \"negative branch\", in order to adequately construct his/her reactive turn. Analogously, the tactic of persuasion is related to the reasoning process triggered by the needed-parameter, and the threatening tactic is related to the reasoning process triggered by the must-parameter. For more details see, for example, Koit (1996), Koit and 0im (1998), Koit and Oim (1999). Thus, in order to model various communicative tactics, one must know how to model the process of reasoning. 2.3 Speech Acts The minimal communicative unit in our model is speech act (SA). In the implementation we make use of a limited number of SAs the representational formalism of which is flames. Figure 4 presents the frame of SA Proposal in the context of co-operative interaction. Other SAs are represented in the same form. Each SA contains a static (declarative) and a dynamic (procedural) part. The static part consists of preconditions, goal, content (immediate act) and consequences. The dynamic part is made up from two kinds of procedures: 1) those that the author of the SA applies in the generation of a communicative turn that contains the given SA; 2) those that the addressee applies in the process of response generation. As one can see, such a two-part representation contains also rules for combining SAs in a turn, and on the other hand, guarantees coherence of turn-takings: when we have tagged in KBD initiating SAs (such as Question or Proposal), then the following chain of SAs follows from the interpretation-generation procedures as applied by participants. PROPOSAL (author A, recipient B, A proposes B to do an action D) I. Static part SETTING (i) A has a goal G (2) A believes that B in the","same way has the goal G (3) A believes that in order to","reach G an instrumental goal","G i should be reached (4) A believes that B in the","same way believes that in","order to reach G an","instrumental goal G i should be","reached (5) A believes that to attain","the goal Gi B has to do D (6) A believes that B has","resources for doing D (7) A believes that B will","decide to do D GOAL: B decides to do D CONTENT: A informs B that he/she wishes B to do D CONSEQUENCES (i) B knows the SETTING, GOAL","and CONTENT (2) A knows that B knows the","SETTING, GOAL and CONTENT II. Dynamic part"]},{"title":"Generating procedures","paragraphs":["(A's possibilities to build his/her turn that contains Proposal as the dominant SA). A has Goal G; A believes that B also has Goal G; A believes that in order to reach G, Gi should be reached; A has decided to formulate this as Proposal to B to do D. Procedures (before formulating the turn) consist in checking whether the preconditions of proposal hold and in making decisions about information to be added in the turn: -","in case of (2) : is G","actualised in B? If not, then","actualise it by adding SA","Inform; -","in case of (4) : does B","believe that in order to","reach B, G~ should be reached","first. If not, then add SA","Explanation (Argument); -","in case of (6) : if A is not","sure that B has resources for","D, then add Question; -","in case of (7) : if A is not","sure that B will agree to do 107 D (for this A should model B's reasoning) , then add Argument.","Procedures of interpretation-","generation","(B's possibilities to react to","proposal) are started after B","has recognised SA Proposal:","- in case of (2), (4), (5) : if B does not have Goal G and/or he/she does not haw~ the corresponding beliefs and A has not provided the needed additional information, then add Question (ask for additional information) ;","- in case of (6) : if B does not have Resources for D, then Reject + Argument;","- in case of (7) : if the decision of B to do D (as the result of the application of reasoning scheme (s)) is negative, then Reject + Argument. Figure 4. Speech act Proposal in the context of co-operative interaction. Such a representation does not guarantee coherence of dialogic encounters (transactions) on a more general level. For instance, it does not cover such phenomena as topic change, inadequate responses caused by misunderstandings; but, more importantly, also various kinds of initiative overtakings. For instance, after rejecting the Proposal made by A, B can, in addition to explaining the rejection by Argument, initiate various \"compensatory\" communicative activities. Such things are normal in human co-operative interaction and they are regulated by general pragmatic principles that require from participants, in addition to being co-operative and informative, also being considerate and helpful. In our case this means that KBo should also include general level dialogue scenarios (in the form of a graph) and formalisations of the mentioned"]},{"title":"pragmatic","paragraphs":["principles; for an example of the latter, see Jokinen (1996)."]},{"title":"3 Process of Dialogue","paragraphs":["Let us describe the case where both A and B are","intelligent agents; i.e. computer programs.","1. A constructs","a) the frame exemplar of D, putting in it all relevant information A has about D;","b) the model of partner B, putting in it all relevant information it has about B's evaluations concerning the contents of the slots in D's frame.","2. A chooses the point in communicative space from which it intents to start the interaction.","3. A starts to apply communicative strategy. A models B's reasoning process, using B's model. First A applies the reasoning scheme based on the wish of B. If it results in 'to do D', then A actualises the tactic of enticing and generates its first turn which contains a frame exemplar of Proposal. If the result of modelled reasoning results in 'not to do D', then A tries reasoning which starts from needed-factor and then the one triggered by must-factor, and according to the result actualises tactics of persuading or threatening, and generates the first utterance. If the application of all reasoning schemes results in 'not to do D', then A abandons its goal.","4. B interprets A's turn and recognises Proposal in it. B constructs it's the exemplar representation of D (this may not coincide with that of A). B starts reasoning, in the course of which it may need additional information from A. On the basis of the frame of Proposal B formulates the result of reasoning as its response turn: yes/no + (maybe) Argument.","5. A interprets B's answer and determines which point in the dialogue scenario this corresponds to. If B's answer was positive (decision to do D), then according to communicative strategy the encounter has come to its successful end. If B's answer is negative, then according to the dialogue scenario A must formulate a (counter)Argument. The communicative strategy also allows to choose a new point in communicative space and/or a new merle. To formulate the counter-argument, A uses information from the exemplar of D (it may be updated on the basis of B's negative answer) and its model of B (which it had to 108 change because of B's negative answer). A models anew B's reasoning, i.e. the process is repeated cyclically."]},{"title":"4 Dialogue examples 4.1 Example 1","paragraphs":["The example represents a dialogue where the computer plays A's role and is implementing the tactic of enticement. The user implements the simple tactic: point out little pleasantness and unpleasantness of the action. Let us suppose that the action D is \"travel to Venice and conclude a contract there\". The computer has chosen the tactics of enticement and has generated the following user model: wAa(resources) = 1, wAB(pleas) =10, wAB(unpleas)=4, w~a3(use)=0, wAB(harm)=l 0, wAB(obligatory)= 1, wAB(prohibited)=0,"]},{"title":"wAB(puniShD)=0 , WAB(puniShnot.D)=6.","paragraphs":["The reasoning procedure WISH on this model yields a positive decision. A following dialogue was generated (translated from Estonian): C(omputer): Do you want to travel to Venice? Our firm needs to conclude a contract there. The computer informed the user about its communicative goal. U(ser) : It.is not interesting for me. The user pointed out little pleasantness of the action. The computer must correct win(pleas) in the user model. The new value of wAS(pleas) will be 8. C: You can meet interesting people. The computer chose its reply from the class of expressions for increasing pleasantness. After this turn the value ofwaB(pleas) will be 9. U: What's so pleasant about this trip? The user pointed out little pleasantness once more. The corrected value of wAS(pleas) will be"]},{"title":"8.","paragraphs":["C: There are many nice men in Italy. After this turn wAB(pleas)=9. U: This trip is too intense for me. The user indicated unpleasantness of D. The corrected value for wm(unpleas) will be 5. C: The company will pay all your expenses. The computer chose its reply from the class of expressions for decreasing unpleasantness and supposes that after its reply the value of wAa(unpleas) will be 4. U: I don \"t like to travel. The user indicated unpleasantness of the action once more. Thus, the new value of wAB(unpleas) will be 5. C: You can take plane. The computer decreased the unpleasantness of D once more. The new value of wAB(unpleas) is 4. U: You are right - I shall travel. The user agreed to do D, the communicative goal of the computer is achieved. C: I am glad."]},{"title":"4.2 Example 2","paragraphs":["The example represents a dialogue where the computer is implementing the tactic of threatening. The user implements the tactic as before: point out little pleasantness and unpleasantness of the action. The action D is \"travel to Venice and conclude a contract there\" as before. The computer has chosen the tactic of threatening and has generated the following user model: wAB(resources) = 1, wAB(pleas)= 10, wAB(unpleas)=4, wAB(use)=4, wAB(harm)=6, WAB(obligatory)=l, wAa(prohibited)=0, wAB(puniShD)=0, w~(punishnot.D)=8. The reasoning procedure MUST on this model yields a positive decision thus threatening is possible. A following dialogue was generated: C: You must travel to Venice. Our firm needs to conclude a contract there. U: It is not interesting for me. The user pointed out little pleasantness of the action. The computer must correct wAB(pleas) in the user model. The new value of wAB(pleas) will be 0. C: There are many people interested on getting your job. Using the tactic of threatening the computer chose its reply from the class of expressions for increasing punishment for not doing D. After this turn the value of wAB(punish~ot.D) will be 9. U: What ~ so pleasant about this trip?. The user pointed out little pleasantness once more. C: Refusing will be harmful for your reputation. After this"]},{"title":"turn WAB(punishnot.D)=10.","paragraphs":["U: This trip is too intense for me. The user indicated unpleasantness of D. The corrected value for waB(unpleas) will be 14. C: lf you have money you can travel by plane. 109 The computer chose its reply fi:om the class of expressions for decreasing unpleasantness. The value ofwAS(unpleas) will be 13. U: 1don \"t like to travel The user indicated unpleasantness of the action once more. The reasoning procedure MUST on the user model will give the negatiw~ decision thus threatening is impossible. The computer can't choose new tactics because reasoning procedures WISH and NEEDED will give on the user model the negative decision too. The computer must give up. C: 1am sorry."]},{"title":"Conclusion","paragraphs":["At present there exists implemented program which can play the role of both A and B in a simple communication situation where the goal of A is that B would decide to do D. At the moment the computer operates with semantic representations of linguistic input/output only, the surface linguistic part of interaction is provided in the form of a list of possible utterances. The work on linguistic processor is in progress. We have deliberately concentrated on modelling the processes of reasoning of conversation agents, as these processes form the heart of the \"cognitive\" part of human communication, and on modelling the use of communicative strategies and tactics which constitute the \"social\" part of communication. Although the concepts and models we have reported in the paper may seem too abstract from the point of view of practical NLP, we are convinced that without serious study and modelling of cognitive and social aspects of human communication it will appear impossible to guarantee naturalness of dialogues carried out by a computer system with a human user. As we have so far mostly dealt with agre ment negotiation dialogues, we have planned as one of the practical applications of the system as a participant in communication training sessions. Here the system can, for instance, establish certain restrictions on argument types, on the order in the use of arguments and counterarguments, etc. Second, we have started to work, using our experience in modelling cognitive and social aspects of dialogue, on modelling information seeking dialogues in the same lines. This type of dialogue clearly will be the area where in the next few years already systems will be required that would be practically reliable, but at the same time could follow the rules of natural human communication."]},{"title":"Acknowledgements","paragraphs":["This research was supported by Science Foundation (grant No 4467). Estonian"]},{"title":"References","paragraphs":["James Allen (1994) Natural Language Understanding. 2nd ed. The Benjamin/Cummings Publ. Comp., Inc.","Jennifer Chu-Carroll and Sandra Carberry (1998) Collaborative Response Generation in Planning Dialogues. Computational Linguistics, 24/3, pp. 355-400.","Barbara Di Eugenio, Pamela W. Jordan, Richmond H. Tlaomason, Johanna D. Moore (2000) The Acceptance Cycle: An empirical investigation of human-human collaborative dialogues, to appear in International Journal of Human Computer Studies.","Laila Dybkj~er (2000) Preface. - From Spoken Dialogue to Full Natural Interactive Dialogue-Theory, Empirical Analysis and Evaluation. LREC 2000 Workshop Proceedings. L. Dybkjaer, ed. Athen, pp. 1-2.","Peter Heeman and Graeme Hirst (1995) Collaborating on referring expressions. Computational Linguistics, 21/3, pp. 351-382.","Kristiina Jokinen (1995) Rational Agency. In \"Rational Agency: Concepts, Theories, Models, and Applications\", M. Fehling, ed.. Proe. of the AAAI Fall Symposium. MIT, Boston, pp. 89-93.","Kristiina Jokinen (1996) Cooperative Response Planning in CDM.\" Reasoning about Communicative Strategies. In \"TWLT11. Dialogue Management in Natural Language Systems\", S. LuperFoy, A. Nijholt & G. Veldhuijzen van Zauten, ed. Enschede: Universiteit Twente, pp. 159-168.","Mare Koit (1996) lmplementing a dialogue model on the computer. In \"Estonian in the Changing World. Papers in Theoretical and Computational Linguieties\", H. Oim, ed. Tartu, pp.. 99-114.","Mare Koit and Haldur Oim (2000) Developing a model of natural dialogue. In \"From spoken dialogue to full natural interactive dialogue-theory, 110 Empirical analysis and evaluation. LREC2000 Workshop proceedings\", L. Dybkj~r, ed."]},{"title":"Athen,","paragraphs":["pp. 18-21.","Mare Koit and Haldur 0im (1999) Communicative strategies in human-computer interaction: a model that involves natural reasoning. In \"23. Deutsche Jahrestag fiir Kfmstliche Intelligenz\". Bonn, http://www.ikp.uni-bonn.de/NDS 99/Finals/1 2.ps","Mare Koit and Haldur Oim (1998) Developing a model of dialog strategy. In \"Text, Speech, Dialogue - TSD'98 Proceedings\". Brno, pp. 387-390.","Karen Lochbaum (1998) A Collaborative Planning Model of Intentional Structure. Computational Linguistics, 24/4, pp. 525-572.","Haldur Oim (1996) Na~'ve theories and communicative competence: reasoning in communication. In \"Estonian in the Changing World. Papers in Theoretical and Computational Linguictics\", H. C)im, ed. Tartu, pp. 211-231.","David R. Traum and James F. Allen (1994) Discourse Obligations in Dialogue Processing. In \"Proceedings of the 32rid Annual Meeting of the Association for Computational Linguistics (ACL-94)\", pp 1-8.","Bonnie Webber (2000) Computational Perspectives on Discourse and Dialogue. In \"The Handbook of Discourse Analysis\". D. Schiffrin, D. Tannen, H. Hamilton, ed. Blackwell Publishers Ltd. 111"]}]}