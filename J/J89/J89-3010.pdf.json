{"sections":[{"title":"Book Reviews Information-based Syntax and Semantics. Vol 1: Fundamentals","paragraphs":["ity over the technical or implementational point of view. In this regard, it cannot be criticized as designed in a too rigid way from the implementational viewpoint and not adaptable to new situations and unforeseen phenomena. The separation of data structures from the procedures and the modularity of the system are features that are essential to the extendability of the system to other domains.","In general, the work is a good example of:","1. the necessity of creating extensive lexicons,","where \"extensive\" must be intended both in","breadth (i.e., in quantitative terms) and in depth","(i.e., from aqualitative viewpoint, as to the types","of information associated with the entries);","2. the necessity of working with large textual cor-","pora, both for obtaining linguistic data and for","testing systems. This is encouraging for a trend that is in recent years showing up, and having, for example, in Europe, great success also in projects sponsored by national and international organizations. REFERENCES Harris, Z.S. 1963 Discourse analysis reprints. The Hague: Mouton. Harris, Z.S. 1964 String analysis in sentence structure. The Hague:","Mouton. Harris, Z.S. 1982 A grammar of English on mathematical principles.","New York: Wiley-Interscience. Nicoletta Calzolari is a researcher at the Department of Linguistics of the University of Pisa and at the Institute of Computational Linguistics of CNR, Pisa. Her main research areas are in the field of computational lexicography and lexicology. Calzolari's address is: Dipartimento di Linguistica, Universit~i di Pisa, Via S. Mafia 36, I 56100 Pisa, Italy. E-mail: glottolo@icnucevm.bitnet INFORMATION-BASED SYNTAX AND SEMANTICS. VOL 1: FUNDAMENTALS Carl Pollard and Ivan A. Sag (Carnegie Mellon University and Stanford University,","resp.)","Stanford: Center for the Study of Language and Information, Stanford University, 1987, x + 227 pp.","(CSLI lecture notes no. 13) Distributed by the University of Chicago Press","ISBN 0-937073-23-7, $39.95 (hb); ISBN 0-937073-24-5, $17.95 (sb) Reviewed by Edward P. Stabler, Jr. University of Western Ontario This book is an introductory text in linguistics, a very pleasant and readable introduction to head-driven phrase structure grammar (HPSG). HPSG will be of particular interest to computational linguists who have wanted to see situation semantics integrated with a unification-based phrase structure syntax. However, computational linguists should be warned, in the first place, that the book is truly introductory, focusing on the preliminaries to a sophisticated account of the language.. Many of the serious problems to be faced are not discussed at all. In other places good, hard problems are posed, only to reward the reader's anticipation of a resolution with a promissory note about the forthcoming Volume 2. There are so many promissory notes at crucial places that it becomes clear that Volume 2 will be the real test of the framework. The two volumes are apparently organized not by topic, but by difficulty. All the difficult material on a whole range of topics-- syntactic and semantic--is left for the second volume. The second warning for readers of this journal is that this book does not consider the computational properties of HPSG at all. No standard characterization of the HPSG-definable languages, no algorithm for unification or for parsing, and no complexity results are presented. As one exPects in all but the most superficial or artificial approaches to human language, the grammar is incomplete in both its universal and its language-specific components. The grammatical principles, rules, and lexical entries are feature-based, where feature values can be complex (i.e., lists or sets), and computationally oriented readers might wonder how many features are needed and whether the set of possible values of syntactic features is finite, but we are not told.","Pollard and Sag describe HPSG as a \"synthetic and eclectic\" theory that draws on the insights of GPSG, LFG, GB, FUG, categorial grammar, situation semantics, and other approaches to language, which makes the title rather puzzling. How is HPSG \"information-based\"? Even when acquainted with the contents of this volume, I was still puzzled: I am not attuned to the Californian sense of \"information.\" HPSG is \"an information-based (or unification-based) theory of language,\"' and it fundamentally regards \"the objects that make up a human language as bearers of information within the community of people who know how to use them.\" To call HPSG information-based for both reasons, because it unifies the partial information structures called features and because utterances bear information, strikes me as a pun. But if smoke meaning fire is very much the same as fire meaning fire, then it is no doubt natural to think thatfire meaning fire is quite a lot like a feature's being a partial specification, an element of a meet semilattice under subsumption where unification corresponds to the greatest lower bound.","HPSG includes a rather complex array of different kinds of propositions. We are given, in the first place, some basic facts about what types of linguistic objects there are. For example, there are two mutually exclusive types of signs, lexical and phrasal. We are told that \"in general, such facts about relationships among types of linguistic objects are obvious, and we will not explicitly state them,\" but then in later pages we are infor-198 Computational Linguistics, Volume 15, Number 3, September 1989 Book Reviews Information.Based Syntax and Semantics. Vol 1: Fundamentals mally given an extensive inventory of non-obvious facts of this kind. Signs have attributes of PHONOLOGY, SYNTAX, and SEMANTICS, and phrasal signs have in addition a DAUGHTERS attribute. The values of DAUGHTERS are HEAD-DTR (head), COMP-DTR (complement), FILLER-DTR (filler), CONJ-DTR (conjunct), ADJ-DTR (adjunct), etc. The list of possibilities and restrictions on features and values continues through the book, and summaries of some of them are graphically presented. These presumably universal facts are not obvious, and they should be given the same attention as any other aspect of the theory. The more fundamental distinctive aspects of the approach to features and unification, such as the use of intuitionistic negation, are not emphasized or defended here either, though references are provided. The syntax and semantics of the very rich language for defining the signs and restrictions on feature values is presented informally.","An HPSG for a particular language comprises, in addition tothese basic facts, a lexicon, principles, and grammar rules. The lexicon contains basic lexical signs together with lexical rules. The relation between active and passive constructions, for example, is handled by an LFG-like rule stating the relation between active and passive verb forms, rather than by a metarule as in GPSG, or by a movement as in GB. (Seven lexical rules are mentioned altogether.)The principles are either universal or language-specific and are all constraints on the signs. For example, the universal \"head feature principle\" says that the \"head daughters\" of a phrase must have the same \"head features\" as the head. This volume mentions six universal principles but only one language-specific principle, by my count. As in GPSG, constituent structures are specified by a combination of immediate domination constraints and linear precedence constraints. The only language-specific principle presented here is the \"constituent ordering principle,\" which defines the linear precedence constraints. A very tentative formulation of the English constraints is be-gun: a lexical head precedes its sisters; phrasal complements other than VP and S occur in order of increasing obliqueness; and fillers precede gapped clauses. Pollard and Sag note that a focused complement can follow a more oblique -N sister, as in: Sandy gave [to Kim] [the book she bought in Vienna] but they do not work out an account of focus and a modification of the linear precedence restrictions that works here. 1 HPSG grammar rules constrain the immediate dominance relations. The presence of other general principles allows these rules to be very general, and consequently it is rather hard to get a good intuitive grasp of the range of Structures they license. One of the four rules presented here simply says that a phrasal sign with an empty subcategory list can have as constituents a single complement and a phrasal head. Of computational interest is the fact that some of the rules formulated in the text allow purely structural ambiguities that do not correspond to any semantic ambiguity.","So in spite of the fact that this volume presents only the fundamentals or preliminaries for a sophisticated theory, the syntactic theory is already getting fairly complex. Pollard and Sag provide honest discussions of many of the issues that have not been worked out yet, and provide useful references to the more technical literature at these points. As noted, a good deal is left to Volume 2. We are promised there an account of many important things that are just left out of this volume. I kept a list of them: agreement relations, the distribution of expletive pronouns, long-distance dependencies and head-filler constructions, a semantically motivated account of indices, quantifier-scoping ambiguities, syntax and semantics of various raising constructions, control theory, binding theory, alternative approaches to subcategorization, parasitic gaps, relative clauses, toughmovement, extraposition with expletive it, and subject extractions from sentential complements.","For the phenomena that are treated by the theory, the presentation of motivating linguistic data is nice and the main lines of argument are easy to follow. My main complaint is that although the grammar is eclectic and synthetic, its presentation is by and large not comparative. That is, the volume introduces HPSG without explicitly presenting an argument that HPSG is a better linguistic theory than the competing approaches. For example, the question of whether complement order really is defined primarily by independent linear precedence constraints together with immediate dominance constraints is never raised: alternative accounts from other traditions are not mentioned. At only a few points is the HPSG account of a phenomenon explicitly compared with alternatives. I like these sections and wish they came more frequently and went into more detail. It is at these points where HPSG needs to make its case.","Pollard and Sag say that \"in one important respect, HPSG differs from all the syntactic theories which have influenced its development,\" namely, that \"syntactic and semantic aspects of grammatical theory are built up in an integrated way from the start\" rather than simply being added as an afterthought. Why should they be developed in an integrated way? We are not told. One imagines that it must be because semantic and syntactic constraints are interdependent and cannot be isolated (without introducing excessive and unnecessary complication), but no evidence of this kind of interaction is provided here. The HPSG semantics presented in this volume is pretty well limited to one chapter that presents a unification-based naive semantics for a tiny fragment of English. A notation inspired by situation semantics is used and rather superficially contrasted with first-order logic. 2 The linking of argument positions to items in the subcategorization list of a constituent is done in the most straightforward way. No treatment is given to quantifier scope ambiguities, to intensional contexts, or to the foundational problems with utter-Computadonal Linguistics, Volume 15, Number 3, September 1989 199 Book Reviews Machine Translation Systems ances that carry \"false information.\" But more importantly for the claim about the distinctiveness of HPSG, we do not see substantial interactions between semantic and syntactic phenomena. This contrasts with GB theory, for example, in which the subtleties of quantifier interaction are supposed to depend in a very direct way on details of syntactic structure, and at least some of the semantic rules operate under the same substantial constraints as syntactic rules. In HPSG, on the other hand, the \"flow\" of semantic information is defined by a special semantic principle tailored to fit the needs of the fragment considered here. The work done so far could just as well have been done as an afterthought. However, if the promissory notes are redeemed in Volume 2, I expect that the semantics will play a larger role. NOTES 2. They mention the \"speculative\" solution that simply disjoins the increasing obliqueness constraint with a constraint saying that -N constituents precede focused constituents, but this idea obviously needs further development to work even on the range of cases considered in the text. Pollard and Sag refer to technical reports by Uszkoreit on this problem. The contrast is not clearly formulated. For example, Pollard and Sag note that whereas the first-order formulas laugh(rebecca) and run(rebecca) may both denote the same truth value (in the actual world at a time), the formulas ((laugh, laugher:rebecca; 1)) and ((run, runner:rebecca; 1)) (in the actual world at a time) will always be \"more contentful.\" In an introduction, though, it is worth considering the clear sense in which the first-order formulas, like the sentences Rebecca laughs and Rebecca runs, have more content: they assert (under the intended interpretation) something about the world, whereas the others (under the intended interpretation) simply denote abstract objects without telling us anything true or false. What is the motivation for going to the lengths of saying that a situation in which the circumstance holds is a fact? Furthermore, the latter expressions denote different circumstances only if the run relation is different from the laugh relation, and it would be useful, even in an introduction, to alert a student to the reasons that defining appropriate identity conditions on these relations is a very tricky business. The situation is not clarified by Pollard and Sag's further suggestion that while ((believe, believer:claire, believed:((laugh, laugher: rebecca; 1)); 1)) is well formed, the first-order formula believe- (claire,laugh (rebecca)) is syntactically ill-formed. This is not even correct, since laugh can be both a predicate and a function in a first-order language. In fact, we can define the function laugh in such a way that laugh(rebecca) denotes the very fact of a situation in which the circumstance denoted by ((laugh, laugher: rebecca; 1)) holds. The real issues are missed without a slightly more careful development. Edward P. Stabler, Jr. is an assistant professor of computer science at the University of Western Ontario and an Institute Scholar in the Artificial Intelligence and Robotics Program of the Canadian Institute for Advanced Research. He received his Ph.D. in philosophy from MIT, and has recently been working on parsing as deduction and transparent implementations of GB theories. Stabler's address is: Department of Computer Science, University of Western Ontario, London, Ontario N6A 5B7, Canada. E-mail: stabler@uwo.uunet MACHINE TRANSLATION SYSTEMS Jonathan Sloeum (ed.)","Cambridge, England: Cambridge University Press, 1988, :ix + 341 pp. (Studies :in natural language processing) ISBN 0..521-35166-9, $49.50 (hb); ISBN 0-521-35963-5,","$16.95 (sb) [20% discount to ACL members] Reviewed by John S. White Planning Research Corporation Machine translation systems is a successful attempt at presenting the breadth of issues on machine translation from the most relevant of perspectives, namely the systems that exist. The orientation toward presenting the research platform systems, the prototypical systems, and the systems in use enhances the current efforts toward finding the common ground between researchers and implementers. Such convergence leads to fresh insight for the implementers, and, for the researchers, solutions to the practical but vexing problems that the production systems have already solved.","The papers in this volume are a new presentation of articles in the special two-issue Computational Linguistics coverage of machine translation. There have been some updates to the content of these articles, though more updates would have painted a more accurate picture about changes, for better and worse, in the fortunes of these systems.","Though there is no explicit explanation of the format of the articles, it is apparent that they were written in accordance with some suggested outline or questionnaire. Thus the heading numbering and organization of the articles are roughly parallel. The advantage of this organization is, of course, that the different systems can be readily compared on the basis of design, theory, and performance. The disadvantage is that there is a tendency to respond to the guidelines without giving a clear indication of what the guidelines were.","The papers in the volume are the following: Jonathan Slocum, \"A survey of machine translation: its history, current status, and future prospects\" This paper is a version of the invited paper Slocum presented at the 1984 COLING conference at Stanford. It is a valuable statement about machine translation, and one which could bear up well with periodic updated republication. The theme is that an understanding of the issues of machine translation presupposes an understanding of translation itself. The need for translation, the way in which professional human translation is done today, and therefore the way that machine translation approaches fit in, should be critical components in any machine translation design. Yet it frequently is not, 200 Computational Linguistics, Volume 15, Number 3, September 1989"]}]}