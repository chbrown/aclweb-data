{"sections":[{"title":"Book Reviews Text Coherence in Translation TEXT","paragraphs":["COHERENCE IN TRANSLATION Bart Papegaaij and Klaus Schubert (BSO/Research, Utrecht) Dordrecht: Foils Publications, 1988, 211 pp. (Distributed language translation 3) ISBN 90-6765-360-8, Dfl 110.- (hb); ISBN 90-6765-","361-6, Dfl 52.- (sb) Reviewed by Chrysanne DiMarco University of Toronto Machine translation (MT) has focused on the problems of syntax and semantics at the sentence level, but the real goal of MT is to translate texts, a fact that has been generally overlooked. There is a crucial difference between a text and a set of unrelated sentences, and in MT, one must avoid destroying the former by translating it into the latter. It is the coherence of text in particular that Papegaaij and Schubert address. They aim to examine the role of text phenomena in machine translation, to assess the feasibility of a number of suggested models of text coherence for MT, and to propose solutions.","The book consists of three chapters: introductory material, a chapter on the clues and devices of text coherence, and a chapter on text coherence in translation.","The introductory chapter (11 pages) summarizes the main approaches to text linguistics, reviews the Distributed Language Translation project (with which the authors are associated), and introduces the relevant terminology. Chapter 2 (139 pages) is the background chapter for understanding the techniques that can be used to render text coherent. It begins with a study of the kinds of decisions involved in a sample translation, and then surveys the maintenance of text coherence through deictic reference, word disambiguation (by means of a shared context), thematic progression, the structure-building properties of verbs, and, on a higher level, rhetorical patterns. This review of diverse devices for text coherence is illustrated by showing how they contribute to the coherence of a sample text. This chapter covers essentially the same ground as Halliday and Hasan (1976), but in less detail and with a slant to translation.","In Chapter 3 (41 pages), the authors follow up on this review to consider text coherence from the standpoint of translation. A systematic summary of the coherence devices discussed earlier is given, grouped under the headings: coherence of entities, coherence of focus, and coherence of events. This chapter also focuses more explicitly on the question of how to maintain text coherence in a MT system.","The strength (and the bulk) of the book is the quite thorough discussion of the text coherence devices that are relevant in MT. A reader unfamiliar with the problem of maintaining coherence in translation will receive a useful tutorial in the possible approaches. However, it should be noted that, as the authors themselves point out, \"the reader [may be] somewhat unsatisfied and [have] a feeling of having been offered just another series of sample analyses and just another, sketchy, model of text structure. This may appear to be so, at least to a casual reader... [However], it has.., been our aim to make some steps towards (preliminary) implementation possible right now, although we are aware of the fact that much more research of both a fundamental and an application-oriented nature remains to be done\" (p. 198). The book will therefore be of interest to readers who want an introduction to a difficult problem in MT but who recognize that, as of now, solutions remain preliminary and still theoretical. REFERENCE","Halliday, M.A.K. and Hasan, Ruqaiya 1976 Cohesion in English. London: Longman. Chrysanne DiMarco is a doctoral student in the natural language understanding group, University of Toronto. Her thesis research is in stylistics in machine translation. DiMarco's address is: Department of Computer Science, University of Toronto, Toronto, Ontario, Canada M5S IA4. E-mail: cdi@ai.toronto.edu WORD EXPERT SEMANTICS\" AN INTERLINGUAL KNOWLEDGE-BASED APPROACH Bart C. Papegaaij, Victor Sadler, and A.P.M. Toon Witkam (eds.) (BSO/Research, Utrecht) Dordrecht: Foris Publications, 1986, ix + 254 pp. (Distributed language translation 1) ISBN 90-6765-261-X, $29 / Dfl 57.- (sb); ISBN 90-","6765-262-3, $59 / Dfl 120.- (hb) Reviewed by Jonathan Slocum Symantec Corporation This book describes the Dutch firm BSO's machine translation system DLT at a very early stage in its development. The book is organized into four parts, each with its separate set of chapters. Part I is an introduction to the problems of NLP in general, and MT in particular, for the neophyte. Part II describes the \"Semantic Word Expert System\" in some (not exhaustive) detail, describing its workings primarily in the future tense. Part III describes the \"Semantic Work Bench\" development tool that is being developed for development of the DLT lexicons. Part IV discusses \"Future Developments\" in DLT, all the way up to \"The Ultimate Aim,\" which turns out (p. 207) to be a system for multidirectional machine translation, multilingual information retrieval and document indexing, Computational Linguistics, Volume 15, Number 3, September 1989 207 Book Reviews Word Expert Semantics: An Interlingual Knowledge-Based Approach automatic abstracting and summarizing, and the provision of an NL interface in the language of the user's choice. Who in the field does not share this ultimate goal? My chief complaint is that the great bulk of the description of DLT is in the future tense. This is not, of course, a complaint that can be lodged against this book/system alone.","Content Summary. Part I constitutes a review of MT and the problem of ambiguity in NLP. Chapter 1 discusses NLP (needed for \"user-friendly front-ends\") and MT (needed to overcome the growing language barrier), offering Bar-Hillel's famous pair of \"pen\" sentences as an illustration of how computers are rigid, while natural language is supple and complex. Chapter 2 explores lexical ambiguity--polysemy---contrasting the \"determinism\" of mathematics/logic with the \"nondeterminism\" of natural language in order to demonstrate the need for knowledge of the world to cope with the creativity of language. Just how this works is never quite explained.","Chapter 3 offers a brief historical survey of NLP, touching upon Warren Weaver's seminal memo, word-for-word MT (citing Oettinger), strictly-syntactic approaches (citing Chomsky, somewhat out of place), closed worlds (Woods's LUNAR and Winograd's SHRDLU), selection restrictions (Kelly and Stone, CuUingford and Onyshkevych), case grammar (Fill-more), scripts and plans (Schank and Abelson), preference semantics (Wilks), and word experts (Small). Chapter 4 talks about MT and NL understanding today, in terms of semantic primitives, a knowledge bank (as opposed to a program), sublanguage and limited domains, semantic networks, frames, and inferencing/ reasoning through \"non-monotonous [sic] logic.\"","Chapter 5 emphasizes the importance of dictionaries, implying (falsely) that large lexicons entail a move to word grammars stored as a lexical knowledge bank (the natural follow-up to global syntax rules, which in turn succeeded \"word lists\"), It draws a parallel between this development and innovations in theoretical lexicography leading to increased \"precision and clarity.\" Chapter 6 laments the problem of fragmented research brought about by concentration on mono-theoretical approaches, and calls for a combination of strategies to effect the integration required to solve the NLP puzzle.","Part II is entitled \"The Semantic Word Expert System,\" and sets about describing the DLT system (more accurately, proposal). Briefly characterized, DLT is intended to provide multidirectional translation by capturing and parsing text at creation--as the user types it in--and engaging him/her in a multiple-choice \"disambiguation dialogue\" at sentence boundaries in order to resolve any ambiguities in the analysis. Once the meaning of the sentence has been identified and expressed in the interlingua, which happens to be (a customized version of) Esperanto, translation into any supported target language can be done fully automatically, with no recourse to the writer or a translator. Indeed,, it is anticipated that a producer's text will be disseminated in Esperanto interlingual form only, and translated upon demand by the consumer's workstation.","Chapter 1 explains that parsing is performed by first computing all possible syntactic analyses without reference to semantics, then filtering the analyses semantically to produce a graded list of possible interpretations of each content word and attachment. In the disambiguation dialogue, the typist is queried about each ambiguity (with possible readings listed in decreasing order of likelihood, according to DLT's rule base), and must respond by identifying (by number or mouse click) the intended reading. It explains that analysis (apparently, including semantic filtering) is performed as one types, so that the dialog can commence immediately upon completing the sentence. Something (unimplemented) called \"macrocontext semantics\" relates the sentence being typed with any previous input, for anaphoric reference and polysemy resolution: this is another name for discourse analysis.","Chapter 2 presents \"The Structure of the Lexical Knowledge Bank\". SWESIL by name (Semantic Word Expert System for the Intermediate Language), this is a system with \"three major parts: two bi-lingual word lists--SL-to-IL [Source Language to InterLingua] and IL-to-TL [InterLingua to Target Language]--and the central IL knowledge bank [in Esperanto].\" According to the figure caption on the same page (85), the three parts are \"an TL-to-IL dictionary, an IL-to-SL dictionary, and the central Knowledge Bank, entirely in IL\" [sic]. Presumably the text governs, and the figure caption is wrong---especially since the diagram agrees with the textual description, which also makes more sense. But to compound the confusion, the next section, describing the \"SL to IL\" module, talks about its \"listing TL words with their IL counterparts\" (p. 86). These and many other pieces of evidence (e.g., the initial identification of the acronym SWESIL, back on page 82, as \"the Semantic Word Expert Module...\" [sic], or the definition of \"bibliographical references\" (p. 103) as \"names of famous . . . persons\" [sic]) lead one to guess that---as with texts to be composed in the DLT system--the first draft of this book may have been the final one. At the very least, it is unclear what role the editors played in its composition and publication.","La'ter, this chapter takes pride in showing how the Interlingua replaces multiple world models, one per human language, with a single model based on Esperanto. Lost on the writers, seemingly, is the concept that languages might embodY culturally distinct world models, with obvious advantages to be obtained by somewhat more direct translation between \"similar\" languages---especially those somewhat removed from Esperanto's Western European outlook. (Japanese (p. 90) is one of the \"world model\" host languages replaced by Esperanto.) More charitably, and probably more 208 Computational Linguistics, Volume 15, Number 3, September 1989 Book Reviews Word Expert Semantics: An lnterlingual Knowledge-Based Approach accurately, they are aware of this critical philosophical issue, but fail to acknowledge it as they ought.","The next section delves into the advantages oflexical taxonomies, which provide structure to the world model and, through property inheritance, concision as well. Here and elsewhere, the work of Amsler and Calzolari is summarized and cited; some of their ideas are adopted into SWESIL. Since most of the detail presented here is derived from these and other works, I will not summarize it.","Chapter 3 deals with \"Disambiguation with SWESIL\" in depth. The fact that this is the only segment of the book that goes into really substantial detail argues for the conclusion that this is the only functioning component of DLT. Certainly the level of detail far exceeds anything else in the book; so much so, indeed, that it seems out of place. I infer that much of this material was written very late--immediately before publication?--since the textual errors are, if anything, even more obvious (e.g., a number of what seem to be references to figures appear as \"(&&&)\").","Chapter 4 covers \"The Disambiguation Dialogue.\" It justifies this approach to MT on the grounds that:","1. \"[MT with pre/post-editing] would distract the","user who wants to communicate something away","[sic] from the immediate task\" (p. 129); and","2. \"The editor [translator?] has to consult the writer","about the correct interpretation of the text\" (p.","130).","That interruptions for dialogue disambiguation after each sentence, in order for DLT to consult the writer about his intent, might also lead to considerable delays, seems to elude the authors entirely. This, despite the fact that they decry \"annoying the user with frequent interruptions that interfere with the task of writing\" (p. 131). The remainder of the chapter, which relates the results of some laboratory simulation experiments, argues to this reviewer that these delays are likely to be substantial and frustrating.","In Part III, Chapter 1 opens by dismissing claims based on small prototypes:","1. \"There is no guarantee that the worked-out exam-","ples are representative\";","2. \"A small vocabulary... [often allows] . . . con-","trol over all possible contexts\"; and","3. \"Very detailed dictionary entries . . . [in a large","system].., can become prohibitively expensive\"","(pp. 144-5).","This reviewer is in full sympathy with such objections. The chapter continues by encouraging reality checks (bravo!) using the Melby Test--which, upon reading its description, I recognize to be an old, wellknown (if too seldom practiced) test regimen in MT circles. But never mind. DLT's benchmark against such a test is described in Chapter 3. Or, rather, the tiny existing subset of SWESIL and the variation on the Melby Test described in Chapter 2 are used to conduct an experiment. The writers find the test results to be (need I say it?) encouraging. As for the real test, Chapter 4 (\"Melby Test Results\") informs us that \"the databases and software [are] still in preparation. [Thus] â€¢ . . no actual results of the test can be presented here\" (p. 191). One may be forgiven for imagining a much-delayed delivery date due to problems not reported.","Part IV, \"Future Developments,\" discusses aspects of \"[future] Computerized Lexicography\" (Chapter I), \"[future] Macrocontext and Discourse Analysis\" (Chapter 2), and \"The [future] Self-Improving System\" (Chapter 3). It is in Chapter 1 where \"The Ultimate Aim--The General-Purpose IL Knowledge Bank,\" which is the heart of DLT, has its possibilities laid out: translation, information retrieval, indexing, abstracting, summarizing, etc. In Chapter 2, discourse analysis is revisited (cf. Part II, Chapter 1). In Chapter 3, \"The Self-Improving System\" is presented. This is a fullblown artificial intelligence that, among other things, learns from its experience. It \"will probably be preceded by several years of DLT operation on a large corpus of suitable informative texts\" (p. 219). To be sure.","Critique. In a number of ways this book would be useful to members of the general public interested in learning something about NLP; about MT, which is implied to be a focus of attention, much less is actually said. The level of detail--except in Part II, Chapter 3, and probably Part III, Chapter 3--is just about right. Yet even to this audience, a number of glaring problems make their presence known. First of all there is the typography. This book was photo-reproduced from an original printed on a dot-matrix printer, with fixed pitch. Then there is the matter of filling: it was poorly done (probably manually), with mistakes. There is no justification. It is not an attractive package: one has to want to read it.","Second, there are the textual problems, some of which were commented on earlier (e.g., missing figure [?] references). Certain errors are attributable to the fact that the writers are apparently not native English speakers; I forgive all these, as I hope they would my Dutch, were I to attempt writing it. But typographical errors, misspellings, and grammatical errors (frequently number agreement) abound. This is especially ironic when one notes that the writers envisage a DLT system that incorporates a number of text processing aids, of which they mention spelling and grammar checkers/correctors; these tools have been available on the open market for years, and in research labs for longer. There are references to figures that seem not to exist. These are the kinds of problems that editors are supposed to catch. One gathers the impression that this book was assembled (in part from existing texts?) in a great hurry----or at least at a time when the writers were very much concerned with other matters. Since it would appear that their intended audience includes NLP researchers (else why all the details about disambiguation Computational Linguistics, Volume 15, Number 3, September 1989 209 Book Reviews Knowledge Systems and Prolog: A Logical Approach to Expert Systems and Natural Language Processing with SWESIL?), it might have been better to wait until more care could be taken.","Although the title leads one to assume the authors will concern themselves with \"word experts,\" which usually implies lexical procedures in Small's sense (they cite him several times), it turns out that nothing in DLT can be construed as such. The dictionary entries are intended to be numerous, but the authors themselves proscribe large entries (p. 145), or even ones wherein procedures appear (\"There is no level of abstraction within the LKB in which meaning can be viewed as separate from words\" (p. 118), and other pro-modularity arguments). DLT's semantic rules seem much more akin to Wilks's preference semantics than to Small's word expert procedures; and syntactic parsing in DLT, it turns out, is based on dependency grammar (p. 105) and is quite independent of any semantic processing.","More serious are certain scientific and technical claims that are un(der)supported. Here, of course, one tends to leave the realm of reviewing the book and move on to critiquing the work behind it. As this may not be in the purview of a book review proper, I will constrain myself to two points. First, the \"lexical taxonomy\" issue, for all the attention paid to it, is glossed over. In particular, while the authors promote the \"good\" results, identification of noun and verb hierarchies, achieved by Calzolari and Amsler, they fail to mention the problems that Amsler, at least, brings up: the noun hierarchies tend to attach themselves to the verb hierarchies eventually (which seems to violate a structural assumption built into DLT) and, worse, hierarchies seem quite inappropriate for any other part of speech (e.g., adjectives and adverbs).","Second, it is perhaps most unfortunate that the writers, who discount results reported on the basis of small prototypes (Part III, Chapter 1), are reduced to just such claims themselvesmtesting but seven phrases (p. 172) and one single sentence (p. 176) using a system with a \"Semantic Dictionary [that] contains some 800 headwords\" (p. 205). True, such a vocabulary is an order of magnitude larger than that found in most academic AUNLP systems; but it is still an order of magnitude below the I0,000 that the authors themselves think reasonable (p. 145)---which is in turn one or two orders of magnitude below that probably required for such uses as they anticipate for DLT, if claims like those of Walker and Amsler (based on several months of New York Times text) are to be believed. Regardless, the trivial number of test cases put to SWESIL prove absolutely nothing, notwithstanding statements like \"some interesting successes have already been achieved\" (p. 205). (I grade SWESIL at 79% on the 7-phrase test (p. 172). At least they are honest: many AUNLP reports neglect to mention errors at all, and fewer yet quantify anything.)","In short, the authors are inconsistent about the evaluation of other NLP systems against their own, and too much of thisbook talks about DLT in the future tense (again, not problems unique to this book). It was written too early, and apparently in too great a hurry. For the most part, it is interesting reading for the uninformed; there is no news for the professional. But for this I could have liked it, whether or not I agree with their approach. Jonathan Slocum is staff scientist at Symantec, a software firm that includes AI applications--notably Q&A, a file management system with a natural language interface--among its offerings. Previously, Dr. Slocum directed the successful METAL machine translation project at the University of Texas. Slocum's address is: Symantec, 1302 Darter Lane, Austin, TX 78746. E-mail: slocum@cs.utexas.edu KNOWLEDGE SYSTEMS AND PROLOG: A LOGICAL APPROACH TO EXPERT SYSTEMS AND NATURAL LANGUAGE PROCESSING Adrian Walker (ed.); Michael McCord, John F. Sowa, and Walter G. Wilson (IBM T.J. Watson Research Center and IBM Systems","Research Institute) Reading, MA: Addison-Wesley, corrected reprinting,","March 1987, xii + 475 pp. ISBN 0-201-09044-9, $35.50 (hb) Reviewed by Stan C. Kwasny Washington University Books written by committeesmwhere everyone writes a chaptermusually suffer from problems of readability, continuity, and stylistic variations that are difficult for a reader to overcome. Such were my expectations in reviewing this book. To the credit of the committee that produced it, none of these expectations were met. On the contrary, I found this book to be among the very best Prolog-based descriptions of expert systems, natural language processing, and knowledge systems.","The book takes a formal, logical approach to its"]},{"title":"subject.","paragraphs":["Every opportunity is taken to demonstrate in concise terms the relationship of Prolog with logic. Too many Prolog textbooks fail to point out connections with classical logic where possible. They approach Prolog as a conventional programming language. An important and obvious aspect of Prolog is thus missed: that Prolog, since it is based on logic, permits relatively easy translation from logical form to program. More-over, given the close relationship between logical form and program, various mixtures of declarative and procedural styles can be incorporated into problem solutions. To emphasize one to the exclusion of the other does not tell the whole story.","Thi,~ book does tell the whole story, and gives the reader a good sense of Prolog's flexibility in addressing difficult issues across a spectrum of problems. Care-210 Computational Linguistics, Volume 15, Number 3, September 1989"]}]}