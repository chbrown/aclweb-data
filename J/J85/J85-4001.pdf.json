{"sections":[{"title":"ON THE COMPLEXITY OF ID/LP PARSING 1 G. Edward Barton, Jr. MIT Artificial Intelligence Laboratory","paragraphs":["545 Technology Square Cambridge, MA 02139","Modern linguistic theory attributes surface complexity to interacting subsystems of constraints. For instance, the ID/LP grammar formalism separates constraints on immediate dominance from those on linear order. An ID/LP parsing algorithm by Shieber shows how to use ID and LP constraints directly in language processing, without expanding them into an intermediate context-free \"object grammar\". However, Shieber's purported runtime bound underestimates the difficulty of ID/LP parsing. ID/LP parsing is actually NP-complete, and the worst-case runtime of Shieber's algorithm is actually exponential in grammar size. The growth of parser data structures causes the difficulty. Some computational and linguistic implications follow; in particular, it is important to note that, desplte its potential for combinatorial explosion, Shieber's algorithm remains better than the alternative of parsing an expanded object grammar. 1 INTRODUCTION It is common in recent linguistic theories for various surface characteristics of a language to be described in terms of several different kinds of underlying constraints. ID/LP grammars involve immediate-dominance rules and linear-order constraints; more broadly, GPSG systems can also involve feature relationships and metarules (Gazdar et al. 1985). The tree adjunetion grammars of Kroch and Joshi (1985) separate the statement of local constraints from the projection of those constraints to larger structures. The GB-framework of Chomsky (1981:5) identifies the subtheories of bounding, government, 0-marking, binding, Case, and control. When several independent constraints are involved, a system that explicitly multiplies out their effects is large, cumbersome, and uninformative. 2 If done properly, the disentanglement of different kinds of constraints can result in shorter and more illuminating language descriptions.","With any such modular framework, two questions immediately arise: how can the various constraints be put back together in parsing, and what are the computational characteristics of the process? One approach is to compile a large object grammar that expresses the combined effects of the constraints in a more familiar format such as an ordinary context-free grammar (CFG). The context-free object grammar can then be parsed with Earley's (1970) algorithm or any of several other well-known procedures with known computational characteristics.","However, in order to apply this method, it is necessary to expand out the effects of"]},{"title":"everything","paragraphs":["that falls outside the strict context-free format: rule schemas, metarules, ID rules, LP constraints, feature instantiations, case-marking constraints, etc. The standard algorithms operate on CFGs, not on extended variants of them. Unfortunately, the object grammar may be huge after the effects of all nonstandard devices have been expanded out. Estimates of the object-grammar size for typical systems vary from hundreds or thousands 3 up to trillions of rules (Shieber 1983:4). With some formalisms, the context-free object-grammar approach is not even possible because the object grammar would be infinite (Shieber 1985:145). Grammar size matters beyond questions of elegance and clumsiness, for it typically affects processing complexity. Berwick and Weinberg (1982) argue that the effects of grammar size can actually"]},{"title":"dominate","paragraphs":["complexity for a relevant range of input lengths. Given the disadvantages of multiplying out the effects of separate systems of constraints, Shieber's (1983) work on direct parsing leads in a welcome direction. Shieber considers how one might do parsing with ID/LP grammars, which involve two orthogonal kinds of rules. ID rules constrain immediate dominance irrespective of constituent order (\"a sentence can be composed of V with NP and SBAR complements\"), while LP rules Copyright1985 by the Association for Computational Linguistics. Permission to copy without fee all or part of this material is granted provided that the copies are not made for direct commercial advantage and the CL reference and this copyright notice are included on the first page. To copy otherwise, or to republish, requires a fee and/or specific permission. 0362-613X/85/040205-218503.00 Computational Linguistics, Volume 11, Number 4, October-December 1985 205 G. Edward Barton, Jr. On the Complexity of ID/LP Parsing constrain linear precedence among the daughters of any node (\"if V and SBACR are sisters, then V must precede SBAR\"). Shieber shows how Earley's (1970) algorithm for parsing context-free grammars (CFGs) can be adapted to use the constraints of ID/LP grammars directly, without the combinatorially explosive step of convert-ing the ID/LP grammar into standard context-free form. Instead of multiplying out all of the possible surface interactions among the ID and LP rules, Shieber's algorithm applies them one step at a time as needed. Surely this should work better in a parsing application than applying Earley's algorithm to an expanded grammar with trillions of rules, since the worst-case time complexity of Earley's algorithm is proportional to the square of the grammar size!","Shieber's general approach is on the right track. On pain of having a large and cumbersome rule system, the parser designer should first look to linguistics to find the correct set of constraints on syntactic structure, then discover how to apply some form of those constraints in parsing without multiplying out all possible surface manifestations of their effects.","Nonetheless, nagging doubts about computational complexity remain. Although Shieber (1983:15) claims that his algorithm is identical to Earley's in time complexity, it seems almost too much to hope for that the size of an ID/LP grammar should enter into the time complexity of ID/LP parsing in exactly the same way that the size of a CFG enters into the time complexity of CFG parsing. An ID/LP grammar G can enjoy a huge size advantage over a context-free grammar G p for the same language; for example, if G contains only the rule S-~ ~D abcde, the corresponding G r contains 5! = 120 rules. In effect, the claim that Shieber's algorithm has the same time complexity as Earley's algorithm means that this tremendously increased brevity of expression comes free (up to a constant). The paucity of supporting argument in Shieber's article does little to allay these doubts: We will not present a rigorous demonstration of time complexity, but it should be clear from the close relation between the presented algorithm and Earley's that the complexity is that of Earley's algorithm. In the worst case, where the LP rules always specify a unique ordering for the right-hand side of every ID rule, the presented algorithm reduces to Earley's algorithm. Since, given the grammar, checking the LP rules takes constant time, the time complexity of the presented algorithm is identical to Earley's .... That is, it is O( I G 12 n3), where I G I is the size of the grammar (number of ID rules) and n is the length of the input. (: 14f) Many questions remain; for example, why should a situation of maximal constraint represent the worst case, as Shieber claims? 4","The following sections will investigate the complexity of ID/LP parsing in more detail. In brief, the outcome is that Shieber's direct-parsing algorithm usually does have a time advantage over the use of Earley's algorithm on the expanded CFG, but that it blows up in the worst case. The claim of O( I G 12 n 3) time complexity is mistaken; in fact, the worst-case time complexity of ID/LP parsing cannot be bounded by any polynomial in the size of the grammar and input, unless ~' = ,A'@. ID/LP parsing is NP-complete.","As it turns out, the complexity of ID/LP parsing has its source in the immediate-domination rules rather than the linear precedence constraints. Consequently, the precedence constraints will be neglected. Attention will be focused on unordered context-free grammars (UCFGs), which are exactly like standard context-free grammars except that when a rule is used in a derivation, the symbols on its right-hand side are considered to be unordered and hence may be written in any order. UCFGs represent the special case of ID/LP grammars in which there are no LP constraints. Shieber's ID/LP algorithm can be used to parse UCFGs simply by ignoring all references to LP constraints. 2 GENERALIZING EARLEY'S ALGORITHM Shieber generalizes Earley's algorithm by modifying the progress datum that tracks progress through a rule. The Earley algorithm uses the position of a dot to track.linear advancement through an ordered sequence of constituents. The major predicates and operations on such dotted rules are these: • A dotted rule is initialized with the dot at the left edge, as in X --, .ABC . • A dotted rule is advanced across a terminal or nonterminal that was predicted and has been located in the input by simply moving the dot to the right. For example, X -~ A.BC is advanced across a B by moving the dot to obtain X -~ AB.C . • A dotted rule is complete iff the dot is at the right edge. For example, X -~ ABC. is complete. • A dotted rule predicts a terminal or nonterminal iff the dot is immediately before the terminal or nonterminal. For example, X -~ A.BC predicts B. UCFG rules differ from CFG rules only in that the right-hand sides represent unordered multisets (that is, sets with repeated elements allowed). It is thus appropriate to use successive accumulation of set elements in place of linear advancement through a sequence. In essence, Shieber's algorithm replaces the standard operations on dotted rules with corresponding operations on what will be called dotted UCFG rules. 5 • A dotted UCFG rule is initialized with the empty multiset before the dot and the entire multiset of right-hand elements after the dot, as in X --- { } • {A, B, C}. • A dotted UCFG rule is advanced across a terminal or nonterminal that was predicted and has been located in the input by simply moving one element from the multiset after the dot to the multiset before the dot. For example, X -- {d} • {B, C} is advanced across a B by moving the B to obtain X -* {A, B} • {C}. Similar-206 Computational Linguistics, Volume 11, Number 4, October-December 1985 G. Edward Barton, Jr. On the Complexity of ID/LP Parsing ly, X -~ {A } • {B, C, C} may be advanced across a C to obtain X-~ {A, C}\" {B, C}. • A dotted UCFG rule is complete iff the multiset after the dot is empty. For example, X -~ {A, B, C} • {} is complete. • A dotted UCFG rule predicts a terminal or nonterminal iff the terminal or nonterminal is a member of the multiset after the dot. For example, X -~ {A} • {B, C} predicts B and C. Given these replacements for operations on dotted rules, Shieber's algorithm operates in the same way as Earley's algorithm. As usual, each state in the parser's state sets consists of a dotted rule tracking progress through a constituent plus the if~terword position defining the constituent's left edge (Earley 1970:95, omitting lookahead). The left-edge position is also referred to as the return pointer because of its role in the complete operation of the parser. 3 THE ADVANTAGES OF SHIEBER'S ALGORITHM The first question to ask is whether Shieber's algorithm saves anything. Is it faster to use Shieber's algorithm on a UCFG than to use Earley's algorithm on the corresponding expanded CFG? Consider the UCFG G1 that has only the single rule S -* abcde. The corresponding CFG Grl has 120 rules spelling out all the permutations of abcde: S -~ abcde, S -- abced, and so forth. If the string abcde is parsed using Shieber's algorithm directly on G1, the state sets of the parser remain small. 6 S O : [S \"-\" { } \" {a,b,c,d,e}, O] S 1 : [S ~ {a} • {b,c,d,e}, O] S z : [S -*. {a,b} • {c,d,e}, O] S 3 \" [S ~ {a,b,c} • {d,e}, O] 3 4 : [S -~"]},{"title":"{a,b,c,d}","paragraphs":["• {e},"]},{"title":"0]","paragraphs":["S 5 : [S ~ {a,b,c,d,e} • { }, O] In contrast, consider what happens if the same string is parsed using Earley's algorithm on the expanded CFG with its 120 rules. As Figure 1 illustrates, the state sets of the Earley parser are much larger. In state set S~, the Earley parser uses 4! = 24 states to spell out all the possible orders in which the remaining symbols {b,c,d,e} could appear. Shieber's modified parser does not spell them out, but uses the single state [S ~ {a} • {b,c,d,e}, O] to summarize them all. Shieber's algorithm should thus be faster, since both parsers work by successively processing all of the states in the state sets.","Similar examples show that the Shieber parser can enjoy an arbitrarily large advantage over the use of the Earley parser on the expanded CFG. Instead of multiplying out all surface appearances ahead of time to produce an expanded CFG, Shieber's algorithm works out the possibilities one step at a time, as needed. This can be an advantage because not all of the possibilities may arise with a particular input."]},{"title":"(a) (b)","paragraphs":["[S ~ {a} • {b,c,d,e}, O] [S --~ a.edcb, O] [S ~ a.ecbd, O] [S -* a.decb, O] [S ~ a.cebd, O] [S --~ a.ecdb, O] [S -~ a.ebcd, O] [S ~ a.cedb, O] [S --~ a.becd, O] [S ~ a.dceb, O] [S --~ a.cbed, O] [S --~ a.cdeb, O] [S --,. a.bced, O] [S --,. a.edbc, O] [S --~ a.dcbe, O] [S -~ a.debc, O] [S --,. a.cdbe, O] [S --~ a.ebdc, O] [S -~ a.dbce, O] [S -,. a.bedc, O] [S --~ a.bdce, O] [S --~ a.dbec, O] [S ~ a.cbde, O] [S ~ a.bdec, O] [S ~ a.bcde, O] Figure 1. The use of the Shieber parser on a UCFG can enjoy a large advantage over the use of the Earley parser on the corresponding expanded CFG. After having processed the terminal a while parsing the string abcde as discussed in the text, the Shieber parser uses the single state shown in (a) to keep track of the same information for which the Earley parser uses the 24 states in (b).","4 COMBINATORIAL EXPLOSION WITH SHIEBER'S ALGORITHM The answer to the first question is yes, then: it can be more efficient to use Shieber's parser than to use the Earley parser on an expanded object grammar. The second question to ask is whether Shieber's parser always enjoys a large advantage. Does the algorithm blow up in difficult cases?","In the presence of lexical ambiguity, Shieber's algorithm can suffer from combinatorial explosion. Consider the following UCFG, Gz, in which x is five-ways ambiguous: S -~ ABCDE"]},{"title":"A -~alx","paragraphs":["B"]},{"title":"-~blx C -~clx","paragraphs":["D ~dlx E -~elx What happens if Shieber's algorithm is used to parse the string xxxxa according to this grammar? After the first three occurrences of x have been processed, the state set of Shieber's parser will reflect the possibility that any three of the phrases A, B, C, D, and E might have been encountered in the input and any two of them might","5 remain to be parsed. There will be (q) = 10 states reflecting progress through the rule expaffding S, in addition to 5 states reflecting phrase completion and 10 states reflecting phrase prediction (not shown): Computational Linguistics, Volume 11, Number 4, October-December 1985 207 G. Edward Barton, Jr. On the Complexity of ID/LP Parsing","S 3\" IS--~ [s-. [s-~ [ s -. [s-~ [s-~ [s -, [s-. [ s -~ [s-~ {A,B,C} {D,E}, O] {A,B,D} {C,E}, O] {A,C,D} {B,E}, O] {B,C,D} {A,E}, O] {A,B,E} {C,D}, O] {A,C,E} {B,D}, O] {B,C,E} {A,D}, O] {A,D,E} {B,C}, O] {B,D,E} {A,C}, O] {C,D,E} {A,B}, O] In cases like this, Shieber's algorithm enumerates all of the combinations of k elements taken i at a time, where k is the rule length and i is the number of elements already processed. Thus it can be combinatorially explosive.","It is important to note that even in this case Shieber's algorithm wins out over parsing the expanded CFG with Earley's algorithm. After the same input symbols have been processed, the state set of the Earley parser will reflect the same possibilities as the state set of the Shieber parser: any three of the required phrases might have been located, while any two of them might remain to be parsed. However, the Earley parser has a less concise representation to work with. In place of the state involving S -~ {A,B,C} • {D,E}, for instance, there will be 3! • 2! = 12 states involving S -, ABC.DE, S -~ BCA.ED, and so forth. Instead of a total of 25 states, the Earley state set will contain 135 = 12 • 10 + 15 states. 7","In the above case, although the parser could not be sure of the"]},{"title":"eategorial identities","paragraphs":["of the phrases parsed, at least there was no uncertainty about the number of phrases and their extent. We can make matters even worse for the parser by introducing uncertainty in those areas as well. Let G3 be the result of replacing every x in Gz with the empty string e: S -~ ABCDE A \"~ ale B --, ble C -cle D --, die E -'-ele Then an A, for instance, can be either an a or nothing. Before any input has been read, the first state set So in Shieber's parser must reflect the possibility that the correct parse may include any of the 25 = 32 possible subsets of A, B, C, D, and E as empty initial constituents. For example, S O must include [S --- {A,B,C,D,E} • {}, 0] because the input might turn out to be the null string. Similarly, it must include [S -~ {A,C,E} • {B,D}, O] because the input might turn out to be bd or db. Counting all possible subsets in addition to other states having to do with predictions, completions, and the parser's start symbol, there are 44 states in S O . (There are 338 states in the corresponding state when the expanded CFG Gt3 is used.) 5 THE SOURCE OF THE DIFFICULTY Why is Shieber's algorithm potentially exponential in grammar size despite its \"close relation\" to Earley's algorithm, which has time complexity polynomial in grammar size? The answer lies in the size of the state space that each parser uses. Relative to grammar size, Shieber's algorithm~ involves a much larger bound than Earley's algorithm on the number of states in a state set. Since the main task of the Earley parser is to perform scan, predict, and complete operations on the states in each state set (Eariey 1970:97), an explosion in the size of the state sets will be fatal to any small runtime bound.","Given a CFG Go, how many possible dotted rules are there? Resulting from each rule X -- A t ... Ak, there are k+ 1 possible dotted rules. Then the number of possible dotted rules is bounded by I G I, if this notation is taken to mean the number of symbols that it takes to write G down. An Earley state is a pair [r,i], where r is a dotted rule and i is an interword position ranging from 0 to the length n of the input string. Because of these limits, no state set in the Earley parser can contain more than O( I Q l'n) (distinct) states.","The limited size of a state set allows an O( I Go 12 • n3) bound to be placed on the runtime of the Earley parser. Informally, the argument (due to Earley) runs as follows. The scan operation on a state can be done in constant time; the scan operations in a state set thus contribute no more than O( [ Ga [ \" n) computational steps. All of the predict operations in a state set taken together can add no more states than the number of rules in the grammar, bounded by I G al, since a nonterminal needs to be expanded only once in a state set regardless of how many times it is predicted; hence the predict operations need not take more than O( I G I \" n+ [ G"]},{"title":"I ) = o( I G I","paragraphs":["• n) steps. Finally, there are the complete operations to be considered. A given completion can do no worse than advancing every state in the state set indicated by the return pointer. Therefore, any bound k on state set size leads to a bound of k z on the number of steps it takes to do all the completions in a state set. Here k = O( [ G I'n), so the complete operations in a state set can take at most O( I G I z. n 2) steps. Overall, then, it takes no more than O( I G 12 • n 2) steps to process one state set and no more than O(IG a [ z • n 3) steps for the Earley parser to process them all.","In Shieber's parser, though, the state sets can grow much larger relative to grammar size. Given a UCFG Gb, how many possible dotted UCFG rules are there? Resulting from a rule X -~ A 1 ... A k, there are not k+l possible dotted rules tracking linear advancement, but 2 k possible dotted UCFG rules tracking accumulation of set elements. In the worst case, the grammar contains only one rule and k is on the order of I Gel; hence the number of possible dotted UCFG rules for the whole grammar is not bounded by I Gbl, but by 216b I. (The bound can be reached; recall that exponentially many dotted rules are created in the processing of G 3 from section 4.) 208 Computational Linguistics, Volume 11, Number 4, October-December ! 985 G. Edward Barton, Jr. On the Complexity of ID/LP Parsing","Informally speaking, the reason why Shieber's parser sometimes suffers from combinatorial explosion is that there are exponentially more possible ways to progress through an unordered rule expansion than an ordered one. When disambiguating information is scarce, the parser must keep track of all of them. In the more general task of parsing ID/LP grammars, the most tractable case occurs when constraint from the LP relation is strong enough to force a unique ordering for every rule expansion. Under such conditions, Shieber's parser reduces to Earley's. However, the case of strong constraint represents the best case computationally, rather than the worst case as Shieber ( 1983:14) claims. 6"]},{"title":"ID/LP","paragraphs":["PARSING IS INHERENTLY DIFFICULT The worst-case time complexity of Shieber's algorithm is exponential in grammar size rather than quadratic as Shieber (1983:15) believed. Did Shieber simply choose a poor algorithm, or is ID/LP parsing inherently difficult in the general case? In fact, the simpler problem of recognizing sentences according to a UCFG is NP-complete. 8 Consequently, unless ~ = ,./F~, no algorithm for ID/LP parsing can have a runtime bound that is polynomial in the size of the grammar and input.","The proof of NP-completeness involves reducing the vertex cover problem (Garey and Johnson 1979:46) to the UCFG recognition problem. Through careful construction of the grammar and input string, it is possible to \"trick\" the parser into solving a known hard problem. The vertex cover problem involves finding a small set of vertices in a graph with the property that every edge of the graph has at least one endpoint in the set. Figure 2 shows a trivial example. e$ b e4 Figure 2. This graph illustrates a trivial instance of the vertex cover problem. The set {c,d} is a vertex cover of size 2.","To construct a grammar that encodes the question of whether the graph in Figure 2 has a vertex cover of size 2, first take the vertex names a, b, c, and d as the alphabet. Take START as the start symbol. Take H 1 through H 4 as special symbols, one per edge; also take U and D as special dummy symbols.","Next, write the rules corresponding to the edges of the graph. Edge e a runs from a to c, so include the rules H~ -, a and H t -, c. Encode the other edges similarly. Rules expanding the dummy symbols are also needed. Dummy symbol D will be used to soak up excess input symbols, so D -, a through D -, d should be rules. Dummy symbol U will also be used to soak up excess input symbols, but U will be allowed to match only when there are four occurrences in a row of the same symbol (one occurrence for each edge). Take U ~ aaaa, U -, bbbb, U -, cccc, and U -~ dddd as the rules expanding U.","Now, what does it take for the graph to have a vertex cover of size k = 2? One way to get a vertex cover is to go through the list of edges and underline one endpoint of each edge. If the vertex cover is to be of size 2, the underlining must be done in such a way that only two distinct vertices are ever touched in the process. Alternatively, since there are 4 vertices in all, the vertex cover will be of size 2 if there are 4-2=2 vertices left untouched in the underlining process. This method of finding a vertex cover can be translated into a UCFG rule as follows: START--,. HIH2H3H4UUDDDD That is, each H-symbol is supposed to match the name of one of the endpoints of the corresponding edge, in accordance with the rules expanding the H-symbols. Each U-symbol is supposed to correspond to a vertex that was left untouched by the H-matching, and the D-symbols are just there for bookkeeping. Figure 3 lists the complete grammar that encodes the vertex-cover problem of Figure 2. START -, HtHzH3H 4UUDDDD H I -~ a l c H 2 -, blc H 3 -,cld H 4 --bid U -, aaaa I bbbb I cccc I dddd D -, a I b I c I d Figure 3. For k = 2, the construction described in the text transforms the vertex-cover problem of Figure 2 into this UCFG. A parse exists for the string aaaabbbbccccdddd iff the graph in the previous figure has a vertex cover of size < 2. To make all of this work properly, take o = aaaabbbbccccdddd as the input string to be parsed. (In general, for every vertex name x, include in o a contiguous run of occurrences of x, one occurrence for each edge in the graph.) The grammar encodes the underlining procedure by requiring each H-symbol to match one of its endpoints in o. Since the right-hand side of the START rule is unordered, the grammar allows an H-symbol to match anywhere in the input, hence to match any vertex name (subject to interference from other rules that have already matched). Furthermore, since there is one occur-Computational Linguistics, Volume 11, Number 4, October-December 1985 209 G. Edward Barton, Jr. On the Complexity of ID/LP Parsing rence of each vertex name for every edge, all of the edges could conceivably be matched up with the same vertex; that is, it's impossible to run out of vertex-name occurrences. Consequently, the grammar will allow either endpoint of an edge to be \"underlined\". The parser will have to figure out which endpoints to choose - in other words, which vertex cover to select. However, the grammar also requires two occurrences of U to match somewhere. U can only match four contiguous identical input symbols that have not been matched in any other way, and thus if the parser chooses a vertex cover that is too large, the U-symbols will not match and the parse will fail. The proper number of D-symbols is given by the length of the input string, minus the number of edges in the graph (to account for the /-/~-matches), minus k times the number of edges (to account for the U-matches): in this case, 16 - 4 - (2.4) = 4, as illustrated in the START rule.","The net result of this construction is that in order to decide whether o is in the language generated by the UCFG, the parser must in effect search for a vertex cover of size 2 or less. 9 If a parse exists, an appropriate vertex cover can be read off from beneath the H-symbols in the parse tree; conversely, if an appropriate vertex cover exists, it indicates how to construct a parse. Figure 4 shows the parse tree that encodes a solution to the vertex-cover problem of Figure 2.","The construction shows that vertex-cover problem is reducible to UCFG recognition. Furthermore, the construction of the grammar and input string can be carried out in polynomial time. Consequently, UCFG recognition and the more general task of 1D/LP parsing must be computationally difficult. For a more careful and detailed treatment of the reduction and its correctness, see the appendix. 7 COMPUTATIONAL IMPLICATIONS The reduction of Vertex Cover shows that 'the ID/LP parsing problem is NP-complete. Unless ~ = ,./V~P, the time complexity of ID/LP parsing cannot be bounded by any polynomial in the size of the grammar and input, l° An immediate conclusion is that complexity analysis must be done carefully: despite its similarity to Earley's algorithm, Shieber's algorithm does not have complexity O( I G 12 . n3). For some choices of grammar and input, its internal structures undergo exponential growth. Other consequences also follow. 7.1 PARSING THE OBJECT GRAMMAR Even in the face of its combinatorially explosive worst-case behavior, Shieber's algorithm should not be immediately cast aside. Despite the fact that it sometimes blows up, it still has an advantage over the alternative of parsing the expanded object grammar. One interpreta-tion of the NP-completeness result is that the general case of ID/LP parsing is inherently difficult; hence it should not be surprising that Shieber's algorithm for solving that problem can sometimes suffer from combinatorial explosion. More significant is the fact that parsing with the expanded CFG blows up in cases that should not be difficult. There is nothing inherently difficult about parsing the language that consists of all permutations of the string abcde, but while parsing that language the Earley parser can use 24 states or more to encode what the Shieber parser encodes in only one (section 3). To put the point another way, the significant fact is not that the Shieber parser can blow up; it is that the use of an expanded CFG blows up unnecessarily. START U U HI H2 Hs D a aa a b b bb e c e c 114 D D D d d d d Figure 4. The grammar of Figure 3, which encodes the the vertex-cover problem of Figure 2, generates the string o = aaaabbbbccccdddd according to this parse tree. The vertex cover {c,d} can be read off from the parse tree as the set of elements dominated by H-symbols. 210 Computational Linguistics, Volume 11, Number 4, October-December 1985 G. Edward Barton, Jr. On the Complexity of ID/LP Parsing 7.2 IS PRECOMPILATION POSSIBLE? The present reduction of Vertex Cover to ID/LP Parsing involves constructing a grammar and input string that"]},{"title":"both","paragraphs":["depend on the problem to be solved. Consequently, the reduction does not rule out tlae possibility that through clever programming one might concentrate most of the computational difficulty of ID/LP parsing into a separate"]},{"title":"precompilation","paragraphs":["stage, dependent on the grammar but independent of the input. According to this optimistic scenario, the entire procedure of preprocessing the grammar and parsing the input string would be as difficult as any NP-complete problem, but after precompilation, the time required for parsing a particular input would be bounded by a polynomial in grammar size and sentence length.","Regarding the case immediately at hand, Shieber's modified Earley algorithm has no precompilation step. 11 The complexity result implied by the reduction thus applies with full force; any possible precompilation phase has yet to be proposed. Moreover, it is by no means clear that a clever precompilation step is even"]},{"title":"possible;","paragraphs":["it depends on exactly how I GI and n enter into the complexity function for ID/LP parsing. If n enters as a"]},{"title":"factor","paragraphs":["multiplying an exponential, precompilation cannot help enough to ensure that the parsing phase will run in polynomial time.","For example, suppose some parsing problem is known to require 2161 • n 3 steps for solution. 12 If one is willing to spend, say, 10 • 2161 steps in the precompilation phase, is it possible to reduce parsing-phase complexity to some-thing like I GI 8 • n3? The answer is no. Since by hypothesis it takes at least 2161 • n 3 steps to solve the problem, there must be at least 2161 • n 3 -- 10 • 2161 steps left to perform after the precompilation phase. The parameter n is necessarily absent from the precompilation complexity, hence the term 2161 • n 3 will eventually dominate.","In a related vein, suppose the precompilation step is conversion from ID/LP to CFG form and the runtime step is the use of the Earley parser on the expanded CFG. Although the precompilation step does a potentially exponential amount of work in producing G p from G, another exponential factor still shows up at runtime because I G p I in the complexity bound I G r 1 2 \" n3 is exponentially larger than the original I G I •","7.3 POLYNOMINAL-TIME PARSING OF A FIXED GRAMMAR As noted above, both grammar and input in the current vertex-cover reduction depend on the vertex-cover problem to be solved. The NP-completeness result would be strengthened if there were a reduction that used the same fixed grammar for all vertex-cover problems, for it would then be possible to prove that a precompilation phase would be of little avail. However, unless ~ = ,./t'~, it is impossible to design such a reduction. Since grammar size is not considered to be a parameter of a fixed-grammar parsing problem, the use of the Earley parser on the object grammar constitutes a polynomial-time algorithm for solving the fixed-grammar ID/LP parsing problem.","Although ID/LP parsing for a fixed grammar can therefore be done in cubic time, that fact represents little more than an accounting trick. The object grammar G p corresponding to a practical ID/LP grammar would be huge, and if I G'I 2 • n 3 complexity is too slow, then it remains too slow when I G r 1 2 is regarded as a constant.","The practical irrelevance of polynomial-time parsing for a fixed grammar sheds some light on another question that is sometimes asked. Can't we have our cake and eat it too by using the ID/LP grammar G directly when we want to see linguistic generalizations, but parsing the object grammar G ~ when we want efficient parsing? After all, the Earley algorithm runs in cubic time based on the length of the input string, and its dependence on grammar size is only I G r 1 2","Essentially, the answer is that using the object grammar doesn't help. The reduction shows that it's not always easy to process the ID/LP form of the grammar, but it is no easier to use the Earley algorithm on the expanded form. As the examples that have been presented clearly illustrate, both the Shieber parser and the Earley parser for a given language can end up with state sets that contain large numbers of elements. The object grammar does not promote efficient processing; the Shieber parser operating on the ID/LP grammar can often do"]},{"title":"better","paragraphs":["than the Earley parser operating on the object grammar, be.cause of its more concise representation (section 4).","The Earley-algorithm grammar-size factor I G r 1 2 looks smaller than the Shieber factor 2161 until one recalls that G ~ can be exponentially larger than G. In other words, we can hide the factor 2161 inside I Grl, but that doesn't make it any smaller. Thus parsing is likely to take a great many steps even if we parse the object grammar, which might mistakenly be thought to be more efficient than direct parsing. If an algorithm runs too slowly, it doesn't make it faster if we cover up the exponential factor and make it a constant K, and it's unlikely that ID/LP parsing can be done quickly in the general case. 7.4 THE POWER OF THE UCFG FORMALISM The Vertex Cover reduction also helps pin down the computational power of the UCFG formalism. As G 1 and Grl in section 3 illustrated, a UCFG (or an ID/LP grammar) can enjoy considerable brevity of expression compared to the equivalent CFG. The NP-completeness result illuminates this property in two ways. First, the result shows that this brevity of expression is sufficient to allow an instance of any problem in ,Tg~ to be stated in a UCFG that is only polynomiaUy larger than the original problem instance. In contrast, if an attempt is made to replicate the current reduction with a CFG rather than UCFG, the necessity of spelling out all the orders in which the H-, U-, and D-symbols might appear makes Computational Linguistics, Volume 11, Number 4, October-December 1985 211 G. Edward Barton, Jr. On the Complexity of ID/LP Parsing the CFG more than polynomially larger than the problem instance. Consequently, the reduction fails to establish NP-completeness, which indeed does not hold. Second, the result shows that the increased expressive power does not come free; while the CFG recognition problem can be solved in time O( [ G lZ'n 3) unless ~ = ,.,F~, the general UCFG recognition problem cannot be solved in polynomial time.","The details of the reduction show how powerful a single UCFG rule can be. If the UCFG formalism is extended to permit ordinary CFG rules in addition to rules with unordered expansions, the grammar that expresses a vertex-cover problem needs only one UCFG rule, although that rule may need to be arbitrarily long. t3 7.5 THE ROLE OF CONSTRAINT Finally, the discussion of section 5 illustrates the way in which the weakening of constraints can often make a problem computationally more difficult. It might erroneously be thought that weak constraints represent the best case in computational terms, for \"weak\" constraints sound easy to verify. However, oftentimes the weakening of constraint multiplies the number of possibilities that must be considered in the course of solving a problem. In the case at hand, the removal of constraints on the order in which constituents can appear causes the dependence of parsing complexity on grammar size to grow from [ G 12 to 21 o l 8 LINGUISTIC IMPLICATIONS The key factors that cause difficulty in ID/LP parsing are familiar to linguistic theory. GB-theory and GPSG both permit the existence of constituents that are empty on the surface, and thus in principle they both allow the kind of pathology illustrated by G 3 in section 4, subject to amelioration by additional constraints. Similarly, every current theory acknowledges lexical ambiguity, a key ingredient of the vertex-cover reduction. Though the reduction illuminates the power of certain mechanisms and formal devices, the direct implications of the NP-completeness result for grammatical theory are few.","The reduction does expose the weakness of attempts to link context-free generative power directly to efficient parsability. Consider, for instance, Gazdar's (1981 : 155) claim that the use of a formalism with only context-free power can help explain the rapidity of human sentence processing: Suppose ...","that the permitted class of generative grammars constituted a subset of those phrase structure grammars capable only of generating context-free languages. Such a move would have two important metatheoretical consequences, one having to do with learnability, the other with processability .... We would have the beginnings of an explanation for the obvious, but largely ignored, fact that humans process the utterances they hear very rapidly. Sentences of a context-free language are provably parsable in a time which is proportional to the cube of the length of the sentence or less. As the arguments and examples in this paper have illustrated, context-free generative power does not guarantee efficient parsability. Every ID/LP grammar technically generates a context-free language, but the potentially large size of the corresponding CFG means that we can't count on that fact to give us efficient parsing. Thus it is impossible to sustain this particular argument for the advantages of such formalisms as (early) GPSG over other linguistic theories; instead, GPSG and other modern theories seem to be (very roughly) in the same boat with respect to complexity. In such a situation, the linguistic merits of various theories are more important than complexity results. (See Berwick (1982), Berwick and Weinberg (1982, 1984), and Ristad (1985) for further discussion.)","The reduction does not rule out the use of formalisms that decouple ID and LP constraints; note that Shieber's direct parsing algorithm wins out over the use of the object grammar. However, if we assume that natural languages are efficiently parsable (EP), then computational difficulties in parsing a formalism do indicate that the formalism itself does not tell the whole story. That is, they point out that the range of possible languages has been incorrectly characterized: the additional constraints that guarantee efficient parsability remain unstated. Since the general case of parsing ID/LP grammars is computationally difficult, if the linguistically relevant ID/LP grammars are to be efficiently parsable, there must be additional factors that guarantee a certain amount of constraint from some source, t4 (Constraints beyond the bare ID/LP formalism are required on linguistic grounds as well.) Note that the subset principle of language acquisition (cf. Berwiek and Weinberg 1984:233) would lead the language learner to initially hypothesize strong order constraints, to be weakened only in response to positive evidence.","However, there are other potential ways to guarantee efficient parsability. It might turn out that the principles and parameters of the best grammatical theory permit languages that are not efficiently parsable in the worst case - just as grammatical theory permits sentences that are deeply center-embedded (Miller and Chomsky 1963). 15 In such a situation, difficult languages or sentences would not be expected to turn up in general use, precisely because they would be difficult to process. 16 The factors that guarantee efficient parsability would not be part of grammatical theory because they would result from extragrammatical factors, i.e. the resource limitations of the language-processing mechanisms. This \"easy way out\" is not automatically available, depending as it does on a detailed account of processing mechanisms. For example, in the Earley parser, the difficulty of parsing a construction can vary widely with the amount of lookahead used (if any). Like any other theory, an explanation based on resource limitations must make the right predictions about which constructions will be difficult to parse. 212 Computational Linguistics, Volume 11, Number 4, October-December 1985 G. Edward Barton, Jr. On the Complexity of ID/LP Parsing","In the same way, the language-acquisition procedure could potentially be the source of some constraints relevant to efficient parsability. Perhaps not all of the languages permitted by the principles and parameters of syntactic theory are accessible in the sense that they can potentially be constructed by the language-acquisition component. It is to be expected that language-acquisition mechanisms will be subject to various kinds of limitations just as all other mental mechanisms are. Again, however, concrete conclusions must await a detailed proposal."]},{"title":"9 APPENDIX","paragraphs":["This appendix contains the details of a more careful reduction of the vertex-cover problem to the UCFG recognition problem. This version of the reduction establishes that the difficulty of UCFG recognition is not due either to the possibility of empty constituents (e-rules) or to the possibility of repeated symbols in rules (i.e., to the use of multisets rather than sets). Consequently, it is somewhat different from and more complex than the one sketched in the text. 9.1 DEFINING UNORDERED CONTEXT-FREE GRAMMARS"]},{"title":"Definition: An unordered","paragraphs":["CFG (UCFG) is a quadruple (N, E, R, S), where (a) N is a finite set of"]},{"title":"nonterminals.","paragraphs":["(b) Y disjoint from N is a finite, nonempty set of"]},{"title":"terminal symbols.","paragraphs":["(c) R is a nonempty set of"]},{"title":"rules","paragraphs":["(,4, a), where A E N and a E (N U E)*. The rule (,4, a) may be written as A 0g.","(d) S e N is the"]},{"title":"start symbol. Convention:","paragraphs":["The grammar G and its components N, E, R, S need not be explicitly mentioned when clear from context."]},{"title":"Convention:","paragraphs":["Unless otherwise noted, (a) A, A', A~, ... denote elements of N; (b) a, a', ai ... denote elements of E; (c) X, Y, X ~, Y, X, Y, .... denote elements of N U E; (d) o, u, u', ui, ... denote elements of E*; (e) a,/3, ~,, 4, ~, denote elements of (NU E)*."]},{"title":"Definition","paragraphs":["G = (N, E, R, S) is e-free iff for every (,4, a)"]},{"title":"~R, I~1 ~0. Definition:","paragraphs":["G = (N, E, R, S) is branching iff for some (,4,"]},{"title":"a>cR, lal","paragraphs":[">1."]},{"title":"Definition:","paragraphs":["G = (N, E, R, S) is duplicate-free iff for every (,4, a) E R, a = Y1 --- Y and for all id\" ~ [1,n], Y = Yj iff i=j."]},{"title":"Definition:","paragraphs":["G = (N, E, R, S) is simple iff it is e-free, duplicate-free, and branching. Note. The notion of a simple UCFG is introduced in order to help pin down the source of any computational difficulties associated with UCFGs. For example, since simple UCFGs are restricted to be duplicate-free, a difficulty that arises with simple UCFGs cannot result from the possibility that a symbol may occur more than once on the right-hand side of a rule."]},{"title":"Definition:","paragraphs":["4A~-->4a~P (by r) just in case (for some) r =","G","(A', Y1 \"\" Y) ~ R and for some permutation p of [1,n], A","= A' and a = Yp<l)... Yp<n). If 4 c E*, also write","4A4,~lm 4aq~.","G"]},{"title":"Definition:","paragraphs":["L(G) = {o E E*: S ~* o}"]},{"title":"Definition:","paragraphs":["An n-step derivation of q~ from 4 is a sequence (4o ..... 4,) such that 40, = 4, 4, = ~P, and for all i ~ [0, n-l], 4i ~ 4i+r If it is also true for all i that 4i => lm 4i+1, say that the derivation is"]},{"title":"leftmost.","paragraphs":["9.2 DEFINING THE COMPUTATIONAL PROBLEMS"]},{"title":"Definition:","paragraphs":["A possible instance of the problem VERTEX COVER is a triple (V,E,k), where (V,E) is a finite graph with at least one edge and at least two vertices, k c N, and k < I V[ .17 VERTEX COVER itself consists of all possible instances (V,E,k) such that for some 1f ___ V, I U I < k and for all edges e c E, at least one endpoint of e is in l/. (Figure 5 gives an example of a VERTEX COVER instance.) 11) W $ X"]},{"title":"6y e2","paragraphs":["V = {v,w,x,y,z} E = {e 1, e 2, eye 4, e 5, e 6, eT}","with the e i as indicated k = 3"]},{"title":"Figure","paragraphs":["5. The triple (V,E,k) is an instance of VERTEX COVER. The set I / = {v,x,z,'pf;} is a vertex cover of size k=3."]},{"title":"Fact:","paragraphs":["VERTEX COVER is NP-complete. (Garey and Johnson 1979:46)"]},{"title":"Definition:","paragraphs":["A possible instance of the problem SIMPLE UCFG RECOGNITION is a pair (G, a), where G is a simple UCFG and o c Z*. SIMPLE UCFG Computational Linguistics, Volume 11, Number 4, October-December 1985 213 G. Edward Barton, Jr. On the Complexity of ID/LP Parsing RECOGNITION itself consists of all possible instances (G, o) such that o •"]},{"title":"L(G). Notation:","paragraphs":["Take [[. [[ to be any reasonable measure of the encoded input length for a computational problem; continue to use [. [ for set cardinality and strihg length. It is reasonable to require that if S is a set, k • H, and"]},{"title":"I s I >","paragraphs":["k, then"]},{"title":"II s [I > II/~ II,","paragraphs":["that is, the encoding of numbers is better than unary. It is also reasonable to require that l[ ( .... x .... ) II"]},{"title":"_","paragraphs":["II x II. 9.3 THE UCFG RECOGNITION PROBLEM IS IN NP Lemma 9.1: Let (if0 ..... q~) be a shortest leftmost derivation of ~ from ~0 in a branching e-free UCFG. If k >"]},{"title":"INI+I then Iq,~l > I%1. Proof.","paragraphs":["There exists some sequence of rules (,4 o, %) ... (Ak_ 1, a~_ 1) such that for all i • [0, k-l], ~ => lm q~i+l by (,4 i, %). Since G is e-free, I cPi+l [ > I <Pi[ always."]},{"title":"Case 1.","paragraphs":["For some i, I%1 > 1. Then I~+11 > Iq~il. Hence [ ~kl < [ ~0"]},{"title":"I. Case 2.","paragraphs":["For every i, I%1 = 1. Then there exist u, 3' such that for every i • [0, k-2], there is Ari • N such that ~+1 = u A'~ 3'. Suppose the A'~ are all distinct. Then IN[ > k-l, hence INI+I > k, hence INI+I > I NI+I, which is impossible. Hence for some ij • [0, k-2], i < j, Ar~ = A~j. Hence ~+~ = q,j+~, since [1,1] has only one permutation. Then ~0 ..... ff~, q~j+l ..... q~k) is a leftmost derivation of ~ from ~0 and has length less than k, which is also impossible. Then I~1 > I~01. []"]},{"title":"Corollary","paragraphs":["9.2: If G is a branching e-free UCFG and o •"]},{"title":"L(G),","paragraphs":["then o has a leftmost derivation of length at most 1o1 \"mwherem= [N[+2."]},{"title":"Proof.","paragraphs":["Let (q'0 .... , q'k) be a shortest leftmost derivation of ofromS. Supposek > [a[ \"m. Consider the subderivations (~0,..., ~m)(~ ...... ~2m) (~'( I o i -,).m .... ' '~lo I'm)\" ('~lo I\""]},{"title":"..... '/'~)\"","paragraphs":["Each one except the last has m steps and m > [N[ + 1. Then by lemma, I~lol.ml > I~(lol-1).m[ > \"\"> Iq'ml >"]},{"title":"I,~01","paragraphs":["= 1. Then 1ol _> l+[ol,whichisimpossible. Hencek _<"]},{"title":"I~'1 \"m. []","paragraphs":["Lemma 9.3: H = SIMPLE UCFG RECOGNITION is in the computational class ,/F~P."]},{"title":"Proof.","paragraphs":["Let G = (N, X, R, S) be a simple UCFG and o • Y.*. Consider the following nondeterministic algorithm with input (G, o):"]},{"title":"Step 1.","paragraphs":["Write down ~0 = S."]},{"title":"Step 2.","paragraphs":["Perform the following steps for i from 0 to [ a [ \" m-l, where m = ] NI +2. (a) Express ~i as"]},{"title":"ur4i7 ~","paragraphs":["by finding the leftmost nontermihal, or loop if impossible.","(b) Guess a rule (A~, Yt,l -.- Y,,k i) • R and a permutation p~ of [ 1,k~], or loop if there is no such rule. (c) Write down ~i+1 = u~ Y~, pi(1) \"'\" Y/, Pi(/q) 3ì\"","(d) If t~i+l ---~ O then halt."]},{"title":"Step 3.","paragraphs":["Loop. It should be apparent that the algorithm runs in time at worst polynomial in l[ (G, o)II; note that the length of ~ increases by at most a constant amount on each iteration. Assume (G, o) • H. Then o has a leftmost derivation of length at most 1o1 \" m by Corollary 9.2; hence the nondeterministic algorithm will be able to guess it and will halt. Conversely, suppose the algorithm halts on input (G, o). On the iteration when the algorithm halts, the sequence (~0, ..., ff~+l) will constitute a leftmost derivation of o from S; hence o •"]},{"title":"L(G)","paragraphs":["and ( G, o) • H. Then there is a nondeterministic algorithm that runs in polynomial time and accepts exactly H. Hence H •"]},{"title":"JF@.D","paragraphs":["9.4 THE UCFG RECOGNITION PROBLEM IS NP-COMPLETE Lemma 9.4: Let"]},{"title":"(V,E,k)","paragraphs":["= (V,{ei} , k) be a possible instance of VERTEX COVER. Then it is possible to construct, in time polynomial in"]},{"title":"II VII, UEII,","paragraphs":["and k, a simple UCFG"]},{"title":"G(V,E,k)","paragraphs":["and a string"]},{"title":"o(V,E,k)","paragraphs":["such that"]},{"title":"(G(V,E,k), o(V,E,k)) ~","paragraphs":["SIMPLE UCFG RECOGNITION iff"]},{"title":"(V,E,k) •","paragraphs":["VERTEX COVER."]},{"title":"Proof.","paragraphs":["Construct"]},{"title":"G(V,E,k)","paragraphs":["as follows. Let the set N of nonterminals consist of the following symbols not in V:"]},{"title":"START, U, D,","paragraphs":["for i• [1, IEI 1, U~ for i e [1, I V I-k], D, for i• [1, [El \"(k-l)]."]},{"title":"II Nil","paragraphs":["will be at worst polynomial in"]},{"title":"IIE II, II vii,","paragraphs":["and k for a reasonable length measure. Define the terminal vocabulary Y to consist of subscripted symbols as follows:"]},{"title":"Z ={ai'a• V,i•","paragraphs":["[1, [E[]}. Designate"]},{"title":"START","paragraphs":["as the start symbol. Include the following as members of the rule set R: 214 Computational Linguistics, Volume 11, Number 4, October-December 1985 G. Edward Barton, Jr. On the Complexity of ID/LP Parsing (a) Include the rule START -~ 111 ... Hlel Ut ... Ui vl-kDt • \" DIEI \"(k-l)\" (b) For each ei • E, include the rules {/-/~ -~ a~ : a an endpoint of e~}.","(c) For each i• [1, I VI -k], include the rule U~ -~ U. Also include the rules {U~ a a...ale I","(d) For each i E [1, D. Also include :aE V}."]},{"title":"I E I •","paragraphs":["(k-I)], include the rule D~ -~ the rules {D-~a:aE Y~}. Take G(V,E,k) to be (N, E, R, START). (Figure 6 shows the results of applying this construction to the VERTEX COVER instance of Figure 5.) Let h : [1, I V I ] -~ V be some standard enumeration of the elements of V. Construct o(V,E,k) as h(1) t ... h(1)lE I ... h(I VI)1 ... h(J VI)IEI thus o(V,E,k) will have length I EI \" I V I. It is easy to see that"]},{"title":"II","paragraphs":["(G(V,E,k), o(V,E,k))II will'be at worst polynomial in"]},{"title":"II Ell, II vii,","paragraphs":["and k for reasonable"]},{"title":"II\" II.","paragraphs":["It will also be possible to construct the grammar and string in polynomial time. Finally, note that given the definition of a possible instance of VERTEX COVER, the grammar will be branching, e-free, and duplicate-free, hence simple. Now suppose (V,E,k) • VERTEX COVER. Then there exist 1/ ~ V and f : E -~ 1/such that I I/I < k and for every e • E, f(e) is an endpoint of e. E is nonempty by hypothesis and I/ must hit every edge, hence [l/[ cannot be zero. Construct a parse tree for o(V,E,k) according to G(V,E,k) as follows. Step 1. Number the elements of V- 1/ as {x~ : i • [1, I V-U I]}. For each x i where i < I Vl-k, construct a node dominating the substring (xi) t ... (x i) I EI of o(V,E,k) and label it U. Then construct a node dominating only the U-node and label it U r Note that the available symbols U/ are numbered from 1 to"]},{"title":"IV I-k,","paragraphs":["so it is impossible to run out of U-symbols. Also, J U [ < k and U---V, hence"]},{"title":"Iv-l/I = Ivl- I~l > Ivl-k, so","paragraphs":["all of the U-symbols will be used. Finally, note that U -~ a t ... ale I is a rule for any a • S and that U~ -- Uis a rule for any U r Step 2. For each ei E E, construct a node dominating the (unique) occurrence of f(e)~ ~ o(V,E,k) and label it H. Step 2 cannot conflict with step 1 because f(e) • V t, hence f(ea) ¢ V - V'. Different parts of step 2 cannot conflict with each other because each one affects a symbol with a different subscript. Also note that f(e) is an endpoint of ei and that//~ -- a~ is a rule for any e~ e E and a an endpoint of e r Step 3. Number all occurrences of terminals in o(V,E,k) that were not attached in step 1 or step 2. For the ith such occurrence, construct a node dominating the occurrence and label it D. Then construct another node domi-START -~ 111 I L -~"]},{"title":"H 7","paragraphs":["-~"]},{"title":"U1 -\" U4 -~","paragraphs":["U -~"]},{"title":"I Da","paragraphs":["D 4 --~ D 7 D10 O13 --~ D"]},{"title":"I","paragraphs":["H1 HE H 3 H4 H 5 1t6 H7 U1 Uz D t D 2 D 3 Dn D 5 D 6 D 7 D 8 D 9 Dto Du D12 D13 DIn Vl I wl I-I2 -\" v~ l y2 1t3 -\" w3 I x3"]},{"title":"wn I z4 115 -\" Xs I Y5 H6 -\" Y6 I z6","paragraphs":["X 7 I Z 7 u v~ -~ u v~ ~. v U V 1 V 2 V 3 V 4 V 5\"v 6 117 J W 1 W 2 W 3 W 4 W 5 W 6 W 7 I X lx 2X 3 X 4x 5X 6x 7 Yl 22 Y3 Y4 -)/5 26 27 I Z 1 Z 2 Z 3 Z 4 Z 5 Z 6 Z 7 D D 2 ~ D D 3 ~ D D D 5 --*. D D 6 ~ D D D 8 ~ D D 9 ~ D D Dlt ~ D D12 --~ D D D14 ~ D"]},{"title":"vl lv21v31v41vslv61vvl wt lw21w31wnlwslw61wTI xl Ix2 ix3 Ix4 Ix5 Ix6 Ix7 I Y, lY2 lY3 lY4 lY5 lY6 lYTI I z, I zz[z3 [zglz 5 [z 6[z 7 Figure","paragraphs":["6. The construction of Lemma 9.4 produces this grammar when applied to the VERTEX COVER problem of Figure 5. The H-symbols ensure that the solution that is found must hit each of the edges, while the U-symbols ensure that enough elements of V remain untouched to satisfy the requirement I l/I _< k. The D-symbols are dummies that absorb excess input symbols. A shorter grammar than this will suffice if the grammar is not required to be duplicate-free. Computational Linguistics, Volume 11, Number 4, October-December 1985"]},{"title":"215","paragraphs":["G. Edward Barton, Jr. On the Complexity of 1D/LP Parsing nating the D-node and label it D r Note that the stock of D-symbols runs from 1 to (k-l) I EI. Exactly ( I VI -k) • I EI symbols of tr(V,E,k) were accounted for in step 1. Also, exactly I E I symbols were accounted for in step 2. The length of a(V,E,k) is I vI • IEI, hence exactly IVI \" IEI -(IVI-k). IEI -IEI =IVI\" IEI- IVI\" IEI +k. IEI- IEI =(k-l)\" IEI symbols remain at the beginning of step 3. D --- a is a rule for any a in Z; Di -~D is a rule for any D r Step 4. Finally, construct a node labeled START that dominates all of the/at, ~, and D~ nodes constructed in steps 1, 2, and 3. The rule START -~ Ha ... Hie I Ua ... U I vl-kDl ... Dlel.<k_t ) is in the grammar. Note also that nodes labeled H t ..... Hlel were constructed in step 2, nodes labeled U a .... UI v l-k were constructed in step 1, and nodes labeled D1, • .. DI e l.~k-1) were constructed in step 3. Hence the application of the rule is in accord with the grammar. Then o(V,E,k) E L(G). (Figure 7 illustrates the application of this parse-tree construction procedure to the grammar and input string derived-from the VERTEX COVER example in Figure 5.) Conversely, suppose o(V,E,k) e L(G). Then the derivation of o(V,E,k) from START must begin with the application of the rule START -~ H a ... HIe I U a ... U I vl-kD1 ... DIEI'<k-I) and each H must later be expanded as some subscripted terminal g(H). Define"]},{"title":"f(ei)","paragraphs":["tO be g(H) without the START"]},{"title":"HtH2DID2D3D4D5 Ut DoDTHaDaHsDgD,o U2 DttDt2DxsHiD,4HeH;","paragraphs":["DDDDD DD U D D D D D"]},{"title":"U /A","paragraphs":["'01 '02 '03 104 '05 '°6 07 Wl~'2'W3W4'WSff)6%07 Xl X2 :Z3 :C4 Z5 Z6 .T7 YlY2Y$~/4YSYeY7 Zl ,Z'2"]},{"title":"I I ! i","paragraphs":["Z 3 Z- 4 Z 5 Z6 Z7"]},{"title":"Figure","paragraphs":["7. This parse tree shows how the grammar shown in Figure 6 can generate the string o(V,E,k) constructed in Lemma 9.4 for the VERTEX COVER problem of Figure 5. The corresponding VERTEX COVER solution l/. = {v,x,z} and its intersection with the edges can be read off by noticing which terminals the H-symbols dominate. 216 Computational Linguistics, Volume 11, Number 4, October-December 1985 G. Edward Barton, Jr. On the Complexity of ID/LP Parsing subscript; then by construction of the grammar, f(e) is an endpoint of e i for all e i E E. Define 1/ = {f(e) : e i E}; then it is apparent that V' __q Vand that V' contains at least one endpoint of e~ for all e i E E. Also, each U~ for i E [1, I VI-k] must be expanded as U, then as some substring (a), ... (ai)le I of o(V,E,k), t8 Since the substrings dominated by the /7 and U~ must all be disjoint, and since there are only I E I subscripted occurrences of any single symbol from V in o(V,E,k), there must be"]},{"title":"I Vl","paragraphs":["- k distinct elements of V that are not dominated in any of their subscripted versions by any H r Then IV- V'I -> [VI - k. Since in addition V_c IJ, ] ~ I _< k. Then (V,E,k) in VERTEX COVER. [] Theorem 1: SIMPLE UCFG RECOGNITION is NP-complete. Proof. SIMPLE UCFG RECOGNITION is in the class ,.,4\"~ by Lemma 9.3, hence a polynomial-time reduction of VERTEX COVER to SIMPLE UCFG RECOGNITION is sufficient. Let (V,E,k) be a possible instance of VERTEX COVER. Let G be G(V,E,k) and o be (V,E,k) as constructed in Lemma 9.4. Note that G is simple. The construction of G and o can, by lemma, be carried out at time at worst polynomial in"]},{"title":"IIEII, II VII,","paragraphs":["and k. Also by lemma (G,o) c SIMPLE UCFG RECOGNITION iff (V,E,k) c VERTEX COVER. k is not polynomial in [I k"]},{"title":"II","paragraphs":["under a reasonable encoding scheme. However, IEI > k, hence IIEll > Ilkll; also II(V,E,k}II > IIEII, hence"]},{"title":"II (V,E,k)II _> k,","paragraphs":["all by properties assumed to hold of"]},{"title":"I1\" II.","paragraphs":["Then G and o can in fact be constructed in time at worst polynomial in"]},{"title":"II (V,E,k} II.","paragraphs":["Hence the VERTEX COVER problem is polynomial-time reduced to SIMPLE UCFG RECOGNITION. [] REFERENCES","Barton, G. Edward, Jr. 1984 Toward a Principle-Based Parser. A.I. Memo No. 788, M.I.T. Artificial Intelligence Laboratory, Cambridge, Massachusetts.","Berwick, Robert C. 1982 Computational Complexity and Lexical-Functional Grammar. American Journal of Computational Linguistics 8(3-4): 97-109.","Berwick, Robert C. and Weinberg, Amy S. 1982 Parsing Efficiency, Computational Complexity, and the Evaluation of Grammatical Theories. Linguistic Inquiry 13(2): 165-191.","Berwick, Robert C. and Weinberg, Amy S. 1984 The Grammatical Basis of Linguistic Performance. M.I.T. Press, Cambridge, Massachusetts.","Chomsky, Noam A. 1980. Rules and Representations. Columbia University Press, New York, New York.","Chomsky, Noam A. 1981 Lectures on Government and Binding. Foris Publications, Dordrecht, Holland.","Earley, Jay 1970 An Efficient Context-Free Parsing Algorithm. Communications of the ACM 13(2): 94-102.","Garey, Michael R. and Johnson, David S. 1979 Computers and Intractability. W. H. Freeman and Co., San Francisco, California.","Gazdar, Gerald 1981 Unbounded Dependencies and Coordinate Structure. Linguistic Inquiry 12(2): 155-184.","Gazdar, Gerald; Klein, Ewan; Pullum, Geoffrey K.; and Sag, Ivan 1985 Generalized Phrase Structure Grammar. Basil Blackwell, Oxford, U.K.","Guerssel, Mohamed; Hale, Kenneth; Laughren, Mary; Levin. Beth; and White Eagle, Josie 1985 A Cross-Linguistic Study of Transitivity Alternations. Paper presented at the Parasession on Causatives and Agentivity at the Twenty-First Regional Meeting of the Chicago Linguistic Society, April 1985.","Hopcroft, John E. and Ullman, Jeffrey D. 1979 Introduction to Automata Theory, Languages, and Computation. Addison-Wesley, Read-ing, Massachusetts.","Kroch, Anthony S. and Joshi, Aravind K. 1985 The Linguistic Relevance of Tree Adjoining Grammars. Technical Report No. MS-CIS-85-16, Department of Computer and Information Science, Moore School, University of Pennsylvania, Philadelphia, Pennsylvania.","Levin, Beth; and Rappaport, Malka 1985 The Formation of Adjectival Passives. Lexicon Project Working Papers #2, M.I.T. Center for Cognitive Science, Cambridge, Massachusetts.","Miller, George A. and Chomsky, Noam A. 1963 Finitary Models of Language Users. In: Luce, R. D.; Bush, R. R.; and Galanter, E., Eds., Handbook of Mathematical Psychology, vol. II. John Wiley and Sons, New York, New York: 419-492.","Ristad, Eric S. 1985 GPSG-Recognition is NP-Hard. A.I. Memo No. 837, M.I.T. Artificial Intelligence Laboratory, Cambridge, Massachusetts.","Shieber, Stuart M. 1983 Direct Parsing of ID/LP Grammars. Technical Report 291R, SRI International, Menlo Park, California. Also appears in Linguistics and Philosophy 7(2).","Shieber, Stuart M. 1985 Using Restriction to Extend Parsing Algorithms for Complex Feature-Based Formalisms. ACL-85 conference proceedings, pp. 145-152. NOTES","1. This report describes research done at the Artificial Intelligence Laboratory of the Massachusetts Institute of Technology. Support for the Laboratory's artificial intelligence research has been provided in part by the Advanced Research Projects Agency of the Department of Defense under Office of Naval Research contract N00014-80-C-0505. Partial support for the author's graduate studies was provided by the Fannie and John Hertz Foundation.","Useful guidance and commentary during the writing of this paper have been provided by Bob Berwick, Michael Sipser, and Joyee Friedman. The paper was also improved in response to interesting remarks from the anonymous referees for Computational Linguistics. The author gratefully acknowledges the assistance of Blythe Heepe in preparing the fitures.","2. See Barton (1984) for discussion.","3. This estimate is from an anonymous referee.","4. See section 5; it is in fact the best case.","5. Shieber's representation differs in some ways from the representation used here, which was developed independently by the author. The differences are generally inessential, but see note 7.","6. The states related to the auxiliary start symbol and endmarker that are added by some versions of the Earley parser have been omitted for simplicity.","7. In contrast to the representation illustrated here, Shieber's representation actually suffers to some extent from the same problem. Shieber (1983:10) uses an ordered sequence instead of a multiset before the dot; consequently, in place of the state involving S ~ {A,B,C} • {D,E}, Shieber would have the 3! = 6 states involving S -*- a • {D,E}, where a ranges over the six permutations of ABC.","8. Recognition is simpler than parsing because a recognizer is not required to recover the structure of an input string, but only to decide whether the string is in the language Computational Linguistics, Volume 1 I, Number 4, October-December 1985 217 G. Edward Barton, Jr. On the Complexity of ID/LP Parsing generated by the grammar: that is, whether or not there exists a parse.","9. If the vertex cover is smaller than expected, the D-symbols will soak up the extra contiguous runs that could have been matched by more U-symbols.","10. Even assuming ~ # ,/F~, it does not follow that the time complexity must be exponential, though it seems likely to be. There are functions such as n ~°g\" that fall between polynomials and exponentials. See Hopcroft and Ullman (1979:341).","11. Shieber (1983:15 n. 6) mentions a possible precompilation step, but it is concerned with the LP relation rather than the ID rules.","12. It is not known whether the worst-case complexity of ID/LP parsing is exponential, since more generally it is not known for sure that ,~ # ,.,¢'~.","13. The complexity of ID/LP parsing drops as maximum rule length drops, but so does the succinctness advantage of an ID/LP grammar over a standard CFG.","14. In the GB-framework of Chomsky (1981), for instance, the syntactic expression of unordered 0-grids at the .~ level is constrained by the principles of Case theory. In a related framework, the limited possibilities for projection from \"lexical-conceptual structure\" to syntactic argument structure combine with Case-assignment rules to severely restrict the possible configurations (Guerssel et al. 1985, Levin 1985). See also Berwick's (1982) discussion of constraints that could be placed on another grammatical formalism - lexicai-functional grammar - to avoid a similar intractability result.","15. Indeed, one may not conclude a priori that all the sentences of every language permitted by linguistic theory are algorithmically parsable at all (Chomsky 1980). This is true for a variety of reasons. Imagine, for instance, that linguistic theory allowed the strings ruled out by filters to be specified by complex enumerators. Then the strings of a language would be defined in part by subtracting off an r.e. set, which could lead to nonrecursiveness because the complement of an r.e. set is not always r.e. But even if nonreeursive, the set of strings would be perfectly well-defined.","16. It is often anecdotally remarked that languages that allow relatively free word order tend to make heavy use of inflections. A rich inflectional system can supply parsing constraints that make up for the lack of ordering constraints; thus the situation we do not find is the computationally difficult case of weak constraint.","17. This formulation differs trivially from the one cited by Garey and Johnson.","18. The grammar would allow the substring (a}i ... (a~)lE I to appear in any permutation, but in o(V,E,k) it appears only in the indicated order. 218 Computational Linguistics, Volume l l, Number 4, October-December 1985"]}]}