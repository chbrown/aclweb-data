{"sections":[{"title":"ABSTRACTS OF CURRENT LITERATURE","paragraphs":["The following Technical Reports cover work done under the DARPA research described on page 136. available from Department of Computer and Information Service University of Pennsylvania Philadelphia, PA 19104 Interactive Classification: A Technique for the Acquisition and Maintenance of Knowledge Bases Tim Finin, David Silverman MS-CIS-84-17 Reports are The practical application of frame-based knowledge-based systems, such as in expert systems, requires the maintenance of potentially very large amounts of declarative knowledge stored in their knowledge bases (KBs). As a KB grows in size and complexity, it becomes more difficult to maintain and extend. Even someone who is familiar with the representation and the contents of the existing KB may introduce inconsistencies and errors whenever an addition or modification is made.","This paper describes an approach to this problem based on a tool called an interactive classifier. An interactive classifier uses the contents of the existing KB and knowledge about its representation to assist the person who is maintaining the KB in describing new KB objects. The interactive classifier will identify the appropriate taxonomic location for the newly described object and add it to the KB. The new object is allowed to be a generalization of existing KB objects, enabling the system to learn more about existing objects. The ideas have been tested in a system call KuBIC, for Knowledge Base Interactive Classifier, and are being extended to a more complete knowledge representation language. Correcting Object-Related Misconceptions: How Should The System Respond? Kathleen F. McCoy MS-CIS-84-18 This paper describes a computational method for correcting users' misconceptions concerning the objects modeled by a computer system. The method involves classifying object-related misconceptions according to the knowledge-base feature involved in the incorrect information. For each resulting class, sub-types are identified, according to the structure of the knowledge base, which indicate what information may be supporting the misconception and, therefore, what information to include in the response. Such a characterization, along with a model of what the user knows, enables the system to reason in a domain-independent way about how best to correct the user. Default Reasoning in Interaction Aravind Joshi, Bonnie Webber, Ralph Weischedel MS-CIS-84-58 Nonmonotonic reasoning is usually studied in the context of a logical system in its own right or as reasoning done by an agent, in which the agent reasons about the world from partial information and hence may draw conclusions unsupported by traditional logic. The main point of departure here is looking at nonmonotonic reasoning in the context of interacting with another agent. This information is partial, in that the other agent neither will nor can make everything explicit. Knowing this, the agent may attempt to derive more from the interaction than what has been made explicit, by reasoning by default about what has been made explicit (often by contrast with what he assumes would have been made explicit, were something else the case). Thus there can be rules for default reasoning that are operative in the interactive situation (\"interactional defaults\") that are not operative with only a single agent. Preventing False Inferences Aravind Joshi, Bonnie Webber, Ralph M. Weischedel MS-CIS-84-59 In cooperative man-machine interaction, it is taken as necessary that a system truthfully and informatively respond to a user's question. It is not, however, sufficient. In particular, if the system has reason to believe that its planned response might lead the user to draw an inference that it knows to be false, then it must block it by modifying or adding to its response. The problem is that a system neither can nor should explore all conclusions 148 Computational Linguistics, Volume 12, Number 2, April-June 1986 The FINITE STRING Newsletter Abstracts of Current Literature"]},{"title":"Living Up To Expectations: Computing Expert Responses","paragraphs":["Aravind Joshi, Bonnie Webber, Ralph Weischedel MS-CIS-84-60"]},{"title":"A Modal Temporal Logic for Reasoning about Changing Databases with Appfications to Natural Language Question Answering Eric Mays, Aravind Joshi, Bonnie Webber","paragraphs":["MS-CIS-85-01 Explaining Concepts in"]},{"title":"Expert Systems: The CLEAR System","paragraphs":["Robert Rubinoff MS-CIS-85-06, LINC LAB 02"]},{"title":"The Linguistic Relevance of Tree","paragraphs":["Adjoining"]},{"title":"Grammars Anthony S. Kroch","paragraphs":["Department of Linguistics Aravind K. Joshi Department of Computer and Information Science MS-C[S-85-16, LINC LAB 03 A"]},{"title":"Computational Logic Approach to Syntax and Semantics Dale A. Miller, Gopalan Nadathur","paragraphs":["MS-CIS-85-17 a user might possibly draw: its reasoning must be constrained in some systematic and well motivated way. In cooperative man-machine interaction, it is necessary but not sufficient for a system to respond truthfully and informatively to a user's question. In particular, if the system has reason to believe that its planned response might mislead the user, then it must block that conclusion by modifying its response. This paper focuses on identifying and avoiding potentially misleading responses by acknowledging types of \"informing behavior\" usually expected of an expert. We attempt to give a formal account of several types of assertions that should be included in response to questions concerning the achievement of some goal (in addition to the simple answer), lest the questioner otherwise be misled. A database that models a changing world must evolve in correspondence to the world. Previous work on natural language question answering systems for databases has largely ignored the issues which arise when the database is viewed as a dynamic (rather than a static) object. We investigate the question answering behaviors that become possible with the ability to represent and reason about the possible evolution of a database. These behaviors include offering to monitor for a possible future state of the database as an indirect response to a query, and directly answering questions about prior and future possibility. We apply a propositional modal temporal logic that captures possibility and temporality to represent and reason about dynamic databases, and present a sound axiomatization and proof, and proof procedure. Existing expert systems provide limited explanatory ability. They can explain the specific reasoning the system uses, but if the user is confused about the concepts and terms the system is using, no help is available. The CLEAR system allows users to ask for explanations of specific concepts. The system generates the explanations by examining the rule base, select-ing rules that are relevant to the concept asked about. These rules are then turned into English by various simple translation schemes and presented to the user, providing an explanation of how the concept is used by the system. In this paper the linguistic significance of the Tree Adjoining Grammar (TAG) has been investigated. An important property of TAG is that it defines a constrained theory of syntactic embedding, one requiring that embedded structures be composed out of elementary structures in a fixed way, and one that forces co-occurrence relations between elements that are separated in surface constituent structures to be stated broadly as constraints on elementary trees in which those elements are co-present. The extra generative power of TAG beyond context-free grammar emerges as a corollary of factoring recursion and co-occurrence relations. The linguistic details specifically discussed are raising constructions, passive, and WH-movements. It is well known that higher-order logics are very expensive, and for this reason have been used to represent many problems in mathematics and theoretical computer science. In the latter domain, higher-order logics are often used to describe the semantics of first-order logics, natural languages, or programs, since the formalization of such semantics needs a recourse to quantification over the domain of functions and sets. In these settings, higher-order logic has generally been limited to a descriptive role. Once the formalization is made, little has been made of it computationally, largely because there is abundant evidence that theorem proving in high-Computational Linguistics, Volume 12, Number 2, April-June 1986 149 The FINITE STRING Newsletter Abstracts of Current Literature The Role of Perspective in Responding to Property Misconceptions Kathleen F. McCoy MS-CIS-85-31, May 1985 Some Computational Properties of Tree Adjoining Grammars K. Yijay-Shaukar, A. Joshi MS-CIS-85-O 7 Grammar, Phrase Structure Aravind K. Joshi MS-CIS-85-45 Question, Answer and Responses: Interacting with Knowledge Base Systems Bonnie Lynn Webber MS-CIS-85-50, LINC LAB 04 A Theory of Scalar lmplicature Julia Bell Hirschberg MS-CIS-85-56 The Relationship Between Tree Adjoining er-order logics is very difficult. In this paper we look at a sublogic of a particular higher-order logic that is derived from Church's Theory of Types, and examine its representational power and its computational tract-ability. This sublogic can also be described as Horn clauses logic extended with quantifications over function variables and A-contraction. We shall present a sound and complete theorem prover for this logic, which uses higher-order unification and may be described as an extension of a unification procedure for the typed A-calculus. There are at least three ways in which this logic is different from the first-order logic it generalizes. First it possesses function variables which can be instantiated with A-terms and evaluated through A-contractions. This provides the logic with a new source of computation. Second, since A-terms do not have most general unifiers, the process of finding appropriate unifiers must branch, and hence involves real search. This facet provides a new source of nondeterminism in specifying computations. Finally, this logic can directly encode first-order logic in its term structure and can manipulate such terms in logically meaningful ways. We illustrate this with examples taken from knowledge representation and natural language parsing. In order to adequately respond to misconceptions involving an object's properties, we must have a context-sensitive method for determining object similarity. Such a method is introduced here. Some of the necessary contextual information is captured by a new notion of object perspective. It is shown how object perspective can be used to account for different responses to a given misconception in different contexts. Tree Adjoining Grammar (TAG) is a formalism for natural language grammars. Some of the basic notions of TAGs were introduced in Joshi, Levy, and Takahashi (1975) and by Joshi (1983). A detailed investigation of the linguistic relevance of TAGs has been carded out in Kroch and Joshi (1985). In this paper, we describe some new results for TAGs, especially in the following areas: (1) parsing complexity of TAGs, (2) some closure results for TAGs, and (3) the relationship to Head grammars. Phrase-structure trees (phrase-markers) provide structural descriptions for sentences. Phrase-structure trees can be generated by phrase-structure grammars. Phrase-structure trees can be shown to be appropriate to characterize structural descriptions for sentences, including those aspects which are usually characterized by transformational grammars, by making certain amendations to CFGs, without increasing their power, or by generating them from elementary trees (phrase-markers) by a suitable rule of composition, increasing the power only mildly beyond that of CFGs. Structural descriptions provided by phrase-structure trees are used explicitly or implicitly in natural language processing systems. The purpose of this chapter is to examine the character of information-seeking interactions between a user and a knowledge base system (KBS). In doing so, I advocate that a clear distinction be made between an answer to a question and a response. The chapter characterizes questions, answers, and responses, the role they play in effective information interchanges, and what is involved in facilitating such interactions between user and KBS. (No abstract.) Tree Adjoining Grammars (TAG) and Head Grammars (HG) were intro-150 Computational Linguistics, Volume 12, Number 2, April-June 1986 The FINITE STRING Newsletter Abstracts of Current Literature Grammars and Head Grammars K. Vijay-Shanker, David J. Weir, Aravind K. Joshi MS-CIS-86-O1, LINC LAB 06 Natural Language Interactions with Artificial Experts Tim Finin, Aravind K. Joshi, Bonnie Lynn Webber MS-CIS-86-16, LINC LAB 08 Higher-Order Logic Programming Dale A. Miller, Gopalan Nadathur MS-CIS-86-17 Some Uses of Higher-Order Logic in Computational Linguistics Dale A. Miller, Gopalan Nadathur MS-CIS-86-31, LINC LAB 08 Some Aspects Of Default Reasoning in Interactive Discourse Aravind K. Joshi, Bonnie L. Webber, Ralph M. Weischedel duced to capture certain structural properties of natural languages. These formalisms, which were developed independently, appear to be quite different notationally. In this paper we discuss the formal relationship between the class of languages generated by TAGs (TAL) and the class of languages generated by HGs (HL). In particular, we show that HLs are included in TALs and that TAGs are equivalent to a modification of HGs called Modified Head Grammars (MHG). The inclusion of MHL in HL, and thus the equivalence of HGs and TAGs, in the most general case remains to be established. We show that this relationship is very close both linguistically and formally, the difference hinging on the status of heads of empty strings and whether one deals with heads directly or with the left and right wrapping positions around the head. The aim of this paper is to justify why Natural Language NLP interaction, of a very rich functionality, is critical to the effective use of Expert Systems and to describe what is needed and what has been done to support such interaction. Interactive functions discussed here include defining terms, paraphrasing, correcting misconceptions, avoiding misconceptions and modifying questions. In this paper we consider the problem of extending Prolog to include predicate and function variables and typed ?~-terms. For this purpose, we use a higher-order logic to describe a generalization to first-order Horn clauses. We show that this extension possesses certain desirable computational properties. Specifically, we show that the familiar operational and least fixpoint semantics can be given to these clauses. A language, hProlog, that is based on this generalization is then presented, and several examples of its use are provided. We also discuss an interpreter for this language in which new sources of branching and backtracking must be accommodated. An experimental interpreter has been constructed for the language, and all the examples in this paper have been tested using it. Consideration of the question of meaning in the framework of linguistics often requires an allusion to sets and other higher-order notions. The traditional approach to representing and reasoning about meaning in a computational setting has been to use knowledge representation systems that are either based on first-order logic or that use mechanisms whose formal justifications are to be provided after the fact. In this paper we shall consider the use of a higher-order logic for this task. We first present a version of definite clauses (positive Horn clauses) that is based on this logic. Predicate and function variables may occur in such clauses; the terms in the language are the typed ~,-terms. Such term structures have a richness that may be exploited in representing meanings. We also describe a higher-order logic programming language, called hProlog, which represents programs as higher-order definite clauses and interprets them using a depth-first interpreter. A virtue of this language is that it is possible to write programs in it that integrate syntactic and semantic analyses into one computational paradigm. This is to be contrasted with the more common practice of using two entirely different computation paradigms, such as DCGs or ATNs for parsing and frames or semantic nets for semantic processing. We illustrate such an integration in this language by considering a simple example, and we claim that its use makes the task of providing formal justifications for the computations specified much more direct. In cooperative interaction, it is taken as necessary that a system truthfully and informatively respond to a user's question. It is not, however, sufficient. In particular, if the system has reason to believe that its planned response might lead the user to draw an inference that it knows to be false, Computational Linguistics, Volume 12, Number 2, April-June 1986 151 The FINITE STRING Newsletter Abstracts of Current Literature MS- CIS-86-2 7 (revised version of MS-CIS-84-58) then it must block it by modifying or adding to its response. In this paper we investigate several aspects of such reasoning in interactive discourse. Adapting MUMBLE: Experience with Natural Language Generation Robert Rubinoff MS-CIS-86-32, LINC LAB 09 This paper describes the construction of a MUMBLE-based (McDonald 1983) tactical component for the TEXT text generation system (McKeown 1985). This new component, which produces fluent English sentences from the sequence of structured message units output from TEXT's strategic component, has produced a 60-fold speed-up in sentence production. Adapting MUMBLE required work on each of the three parts of the MUMBLE framework: the interpreter, the grammar, and the dictionary. It also provided some insight into the generation process and the consequences of MUMBLE's commitment to a deterministic model. GUMS 1\" A General User Modeling System Tim Finin, David Drager MS-CIS-86-35 This paper describes a general architecture of a domain independent system for building and maintaining long term models of individual users. The user modeling system is intended to provide a well-defined set of services for an application system which is interacting with various users and has a need to build and maintain models of them. As the application system interacts with a user, it can acquire knowledge of him and pass that knowledge on to the user model maintenance system for incorporation. We describe a prototype general user modeling system we have implemented in Prolog. This system satisfies some of the desirable characteristics we discuss. Breaking the Primitive Concept Barrier Robert Kms, Ron Katriel, Tim Finln MS-CIS-86-36 Building and maintaining a large knowledge base of general information requires a knowledge representation system with precise semalatics and an easy knowledge acquisition procedure. Systems such as KL-ONE meet these criteria by using a classifier to install new concepts into a taxonomic structure. These systems use a formal notion of a definition for concepts. Unfortunately, many concepts do not seem to have such precise definitions, and end up represented as primitive concepts. Primitive concepts form a barrier to classification, forcing the user to manually classify a new concept with respect to all primitive concepts in the knowledge base.","We propose an extension to KL-ONE that retains its soundness and greatly reduces the burden on the user during knowledge acquisition. This extension consists of adding an explicit definitional component to concepts and relaxing the strictness of concept definitions themselves. The relaxed definition reduces the number primitive concepts in a knowledge base, enables the classifier to handle concepts that do not have complete definitions, and enhances the usefulness of an interactive classifier.","The following Technical Reports are available from Computer Science Department Boston University Boston, MA 02215 Please enclose a check, made payable to Boston University, for the total amount due ($3.00 per report to cover cost of copying and postage). Constrained Semantic Transference: A Formal Theory of Metaphors Bipin Indurkhya B UCS Tech Report 85-008 In this paper we propose a formal theory of metaphors called Constrained Semantic Transference (CST). We start from the assumptions that metaphors are characterized by the description of one domain, called the target domain, in terms of another domain, called the source domain; and that a metaphor works by transferring a set of structural relationships from the source domain to the target domain coherently.","Starting from these assumptions, we formally define the concept of T-MAPs which are partial coherent mappings from the source domain to the target domain. We also define two operators, called Augmentation and 152 Computational Linguistics, Volume 12, Number 2, April-June 1986 The FINITE STRING Newsletter Abstracts of Current Literature Positing Structure, that define a given T-MAP by adding new structure to the target domain.","We show how T-MAPs can be used to characterize metaphorical interpretations of a given set of sentences. This characterization allows several cognitive features of metaphors to be described in CST. In particular, our characterization used the same criterion for deciding metaphorical truth as for literal truths. Approximate Semantic Transference: A Computational Theory of Metaphors and Analogies Bipin Indurkhya BUCS Tech Report 85-012 In this paper we start from the assumption that in a metaphor, or an analogy, some terms belonging to one domain (source domain) are used to refer to objects other than their conventional referents belonging to a possibly different domain (target domain). We describe a formalism, which is based on the First Order Predicate Calculus, for representing knowledge structure associated with a domain and then develop a theory of Constrained Semantic Transference (CST), which allows the terms and the structural relationships of the source domain to be transferred coherently across to the target domain. We show how metaphors and analogies can be characterized in CST in such a way that many of their cognitive properties can be explained.","We then propose a theory of approximate Semantic Transference (AST), which is a computational version of CST and is derived from it by replacing the coherence requirement with approximate coherency. We show how AST can be used as a basis for designing models of cognitive processes involved in comprehending metaphors and analogies. Computational Linguistics Technical Notes Weiguo Wang B UCS Tech Report 85-013, November 1985 This technical report contains two technical notes on the analysis of the Kilbury algorithm for parsing ID/LP Grammars which is part of the GPSG framework. In the first one, two implementations of this algorithm are given, one in C-prolog, one in Interlisp-D. In the second one, the algorithm is compared against the Earley-Shieber algorithm on efficiency.","The following reports are available from Computing Research Laboratory Box 3CRL New Mexico State University Las Cruces, NM 88003 Syntax, Preference and Right Attachment Yorick Wilks, Xiuming Huang, Dan Fass MCCS-85-5 The paper claims that the right attachment rules for phrases originally suggested by Frazier and Fodor are wrong, and that none of the subsequent patchings of the rules by syntactic methods have improved the situation. For each rule there are perfectly straightforward and indefinitely large classes of simple counter-examples. We then examine suggestions by Ford et al., Schubert and Hirst which are quasi-semantic in nature and which we consider ingenious but unsatisfactory. We offer a straightforward solution within the framework of preference semantics, and argue that the principal issue is not the type and nature of information required to get appropriate phrase attachments, but the issue of where to store the information and with what process to apply it. We present a Prolog implementation of a best first algorithm covering the data and contrast it with closely related ones, all of which are based on the preferences of nouns and prepositions, as well as verbs. Machine Translation in the Semantic Definite Clause Grammars Formalism Xiuming Huang MCCS-85- 7 The paper describes the SDCG (Semantic Definite Clause Grammars), a formalism for Natural Language Processing (NLP), and the XTRA (English Chinese Sentence TRAnslator) machine translation (MT) system based on it. The system translates general domain English sentences into grammatical Chinese sentences in a fully automatic manner. It is written in Prolog and implemented on the DEC-10, the GEC, and the SUN workstation, Computational Linguistics, Volume 12, Number 2, April-June 1986 153 The FINITE STRING Newsletter Abstracts of Current Literature Bad Metaphors: Chomsky and Artificial Intelligence Yorick Wilks MCCS-85-8 To appear in S. Mogdil (ed.): Noam Chomsky: Consensus and Controversy (with replies by Chomsky) Collative Semantics Dan Fass MCCS-85-23 Relevance, Points of View and Speech Acts: An Artificial Intelligence View Yorick Wilks MCCS-85-25 Impatient Users and Foreign-Speak Brian M. Slator MCCS-85-26on the respectively. SDCG is an augmentation of the DCG (Definite Clause Grammar) of Pereira et al. (1980), which in turn is based on CFG (Context Free Grammar). Implemented in Prolog, the SDCG is highly suitable for NLP in general, and MT in particular. A wide range of linguistic phenomena is covered by the XTRA system, including multiple word senses, coordinate constructions, and prepositional phrase attachment, among others. The paper argues that the historical divisions between, on the one hand, the cluster of approaches to language understanding by computer known as Artificial Intelligence and, on the other, the Transformational Grammar system of Chomsky were caused not so much by matters of principle (as between scientific linguistics and computational applications) or by methodology, as by Chomsky's attachment over the years to a succession of unfortunate metaphors to explain his position. As recent developments in linguistics have shown, once these are removed, there are no issues of principle (though there may continue to be differences of fact and evidence) between linguistics, on the one hand, and Artificial Intelligence and Computational Linguistics on the other. Collative Semantics (CS) is a new domain independent semantics for natural language understanding. CS can resolve instances of word-sense and case ambiguity and offers an approach to discriminating between a subset of semantic relations that can exist between word-senses: conventional meaningful semantic relations, semantic redundancy, contrariness and contradiction (it cannot differentiate between the two), metonymy, metaphor, and meaninglessness. Collative Semantics is described. The word-sense is the basic unit of representation. Sense-frames, the sole form of knowledge structure, perform a double function: they are dictionary entries for individual word-senses and, in certain situations, behave like semantic primitives. Semantic vectors are scores that represent the degree of match between word-senses (and their associated sense-frames). Brief examples are given of how metonymic relations are computed, and how metaphor is recognised. The paper compares two approaches to modelling human discourse and, more particularly, dialogue: one is the relevance logic of Sperber and Wilson, the other the environments and points-of-view approach of Wilks and Bien. Although both descend from general insights of Grice that understanding is a matter of inference from what is said and what is assumed, the paper criticizes the Sperber and Wilson work on the ground that it is weak exactly where Grice is weak and that, contrary to its authors' claims, it is not an information processing model, and does not model the beliefs of individual speakers and hearers. The latter system, like others in AI, attempts to do that, and it is argued that any system that models discourse must do so. There is a class of Natural Language interface that translates English input into mnemonic formal language commands. These interfaces appear \"front-end\" of data bases and other applications software. Translation time, system response time, and user keyboard time, can all be improved if users acquire and use the mnemonics of the applications software. This paper argues that users, when shown the mnemonic output of their English input, will learn to communicate on a mnemonic level much the same way native speakers learn to speak on the level of intelligent but inarticulate foreigners. Foreign-Speak (so called) will act as a CAI teaching tool to help users more effectively use systems and might usefully be included as part of the design of these Natural Language interfaces. 154 Computational Linguistics, Volume 12, Number 2, April-June 1986 The FINITE STRING Newsletter Abstracts of Current Literature The Effects of Restricted Syntax on Human-Computer Interaction Gregg 1). Bailey MCCS-85-36 The study compared the performance of 48 computer novices on two different problem solving tasks that involve computer interactions. One third of the subjects interacted with very limited natural language restrictions. Another group (16 subjects) interacted with command-oriented restriction. The final group (16 subjects) had object-oriented restrictions. The results of the study indicate that command restriction may facilitate performance over both unrestricted and object-oriented conditions. Parsing in Parallel Xiuming Huang, Louise Guthrie MCCS-85-40 This paper is a description of a parallel model for natural language parsing, and a design for its implementation on the Hypercube multiprocessor. The parallel model is based on the Semantic Definite Clause Grammar formalism and integrates syntax and semantics through the communication of processes. The main processes, of which there are six, contain either purely syntactic or purely semantic information, giving the advantage of simple, transparent algorithms dedicated to only one aspect of parsing. Communication between processes is used to impose semantic constraints on the syntactic processes. Natural Language Processing and Expert Systems Jerry T. Ball MCCS-85-41 Natural Language Processing (NLP) systems fall within the domain of problems suitable for solution using techniques developed for use in Expert Systems. Given an NLP system design involving the integration of syntax, semantics and pragmatics, a production system formalism augmented with techniques for managing a large solution space with interacting subproblems and reliable data is needed. The combined techniques of constraint propagation - to constrain the solution space, predictive selection of alternatives - to avoid least-commitment deadlock, and dependency-directed backtracking - to recover from selection of failed alternatives, are recommended. The following reports are available from the Department of Information Science, University of Constance: Universitaet Konstanz Informationswissenschaft Projekt TOPIC II Postfach 5560 D-7750 Konstanz 1, F.R.G.","Orders from non-profit organizations (universities, governments, etc.) are handled free of charge. Requests from other","organizations or from individuals should be accompanied by payment for postage and handling costs ($5.00 per report). TOPIC II / TOPOGRAPHIC II. Automatische Textkondensierung und text-orientiertes Informationsmanagement (Projektziele - State of the Art) U. Hahn, R. Hammwoehner, R. Kahlen, U. Reimer, U. Thiel Bericht TOPIC-12~844 & TOPOGRAPHIC-3/84, in German, Dec. 1984, 72 pp. A concise overview is given of the aims of the second phase of the text condensation (automatic abstracting) and information retrieval projects of the Information Science Department at the University of Constance (FRG) After providing a state-of-the-art report of current string-processing, statistical, linguistical, knowledge-based, and cognitive approaches to full text analysis and retrieval in information systems, the conception of a knowledge-based full text information system is outlined. Combining both, sophisticated text analysis as well as information retrieval procedures, text analysis is dealt with applying a word expert system to text cohesion and text coherence phenomena for text parsing, a frame representation model for text representation, and appropriate text condensation, mechanisms for text summarization (project TOPIC II). Text information management on the other hand is based on various forms of graphical man-machine interaction and alternative ways of graphics-based text information retrieval (project TOPOGRAPHIC). On Formal Semantic Properties of a Frame Data Model Ulrich Reimer, Udo Hahn Standard knowledge representation languages (e.g., KRL, FRL) are almost exclusively characterized by syntactic specifications but are seriously lack-ing of an explicit formal semantics. The formal description of the frame Computational Linguistics, Volume 12, Number 2, April-June 1986 155 The FINITE STRING Newsletter Abstracts of Current Literature Bericht TOPIC-13~84, in English, Dec. 1984, 24 pp. The TOPIC Project: Text-Oriented Procedures for Information Management and Condensation of Expository Texts (Final Report) Udo Hahn, Ulrich Reimer Bericht TOPIC-17/85, in English, May 1985, l l 4 pp. Enhancing Knowledge Base Reliability: A Claim for Specifying the Empirical Semantics of Knowledge Representation Models Ulrich Reimer Bericht TOPIC-18/86, in English, May 1986 TOPIC Essentials Udo Hahn, Ulrich Reimer Berticht TOPIC-19~86, in English, April 1986, 35 pp. On Lexically Distributed Text Parsing: Computational Model for the Analysis of Textuality on the Level of Text Cohesion and Text Coherence data model outlined in this paper is given with emphasis on explicit semantic specifications applying a combination of a denotational and an axiomatic approach. After introducing the formal syntax and the basic conceptual relations (e.g., is-a, instance-of, part-of) of the frame data model, the set of semantic integrity constraints is outlined, finally leading to the specification of a set of basic operations in the frame data model. Based on an abstract data type view of knowledge representation languages, these operations completely specify the frame data model in terms of its behavioral properties. As these operations are the only means to access data in a frame data base, semantic integrity is always guaranteed. After considering requirements for advanced full text information systems, the major methodological concepts underlying the automatic text condensation (automatic abstracting) system TOPIC is outlined: partial parsing as a text-specific parsing strategy, validity and reliability of the knowledge base under the scope of a formally specified frame data model, recognition of text cohesion and coherence phenomena of expository texts by an appropriately tuned text parser, and variable degrees of thematic aggregation of an input text, basically on a level of text summarization comparable to that of structured abstracts (indicative abstracts plus facts acquired from the original text). Subsequently, a coherent formal description of the whole TOPIC system is given that distinguishes three major processes: text parsing based on the close interaction between a frame knowledge base and a word expert parser, text condensation that utilizes various activation, property assignment, and connectivity patterns of the text representation structures, and finally, the construction of the text graph that represents the knowledge structures of text condensates on different levels of thematic abstraction accessible to appropriate information retrieval procedures. Starting from a request for higher functionality of knowledge-based systems, this paper argues for a more elaborate semantic specification of knowledge representation models. Semantics in the understanding of this paper requires the representation constructs to reflect the empirical regularities holding in some domain of discourse. This way the possibility of representing illegal knowledge is reduced and semantic integrity of a knowledge base is increased with an accordingly positive effect on a corresponding application system. The paper further calls for high-level representation constructs to be made available to a knowledge representation model and argues that they are needed to cope with increasing complexity and size of knowledge bases. Finally, integrity preserving operations are demanded as the only means to access a knowledge base so that an abstract data type view on knowledge bases is achieved which guarantees knowledge based always to be in valid states An overview of TOPIC, a knowledge-based text information system for the analysis of German-language texts, is provided. TOPIC supplies text condensates (summaries) on variable degrees of generality and makes available facts acquired from the texts. The presentation focuses on the major methodological principles underlying the design of TOPIC: a frame representation model that incorporates text cohesion and text coherence properties of expository texts, a lexically distributed semantic text grammar in the format of word experts, a model of partial text parsing, and text graphs as appropriate representation structures for text condensates. Adequate linguistic models for text analysis must account for textuality on the level of text cohesion and text coherence, i.e., for local texture and global well-formedness of text propositions by text grammars on a semantic basis. Considering the requirements of corresponding text parsing 156 Computational Linguistics, Volume 12, Number 2, April-June 1986 The FINITE STRING Newsletter Abstracts of Current Literature Udo Hahn Bericht TOPIC-20/85, in English, Dec. 1985, 46pp. devices, a distributed lexical text grammar is provided in the format of word experts. The current specification level aims at the recognition of semantic and thematic relations holding among nominal expressions of a text. An empirical fragment of the semantic text grammar supplied thus is confined to the description of text cohesion phenomena due to nominal anaphora and lexical cohesion. Repeated application of corresponding word experts yields text representation structures in a frame knowledge base which reveal highly standardized topic/comment patterns, namely thematic progression based on the regular modes of constant theme, linear thematization of themes, and derives themes. A Generalized Word Expert Model of LexicaHy Distributed Text Parsing Udo Hahn Bericht TOPIC-21~86, in English, April 1986, 16pp. Adequate linguistic models for text analysis must account for textuality on the level of text cohesion and text coherence, i.e., for local texture and global well-formedness of text propositions by text grammars on a semantic basis. Considering the requirements of corresponding text parsing devices, a distributed lexical text grammar is provided in the format of word experts. The generalizations proposed concern linguistic generality and methodological requirements of the formal specification of word expert parsers. The approach is illustrated by an informal account of word experts applying to nominal anaphora and lexical cohesion phenomena. Repeated application of corresponding cohesion devices yields regular text coherence structures in a frame knowledge base in terms of basic patterns of thematic progression. Single copies of the following reports and memos can be ordered free of charge from Mrs. D. Borchers Universitaet des Saarlandes FR 10.2 Informatik IV IM Stadtwald 15 D - 6600 Saarbrucken 11 Federal Republic of Germany Dialog-Based User Models W. Wahlster, A. Kobsa Report No. 3 to appear in Ferrari, G., Ed., Proceedings of the IEEE, Special Issue on Natural Language Processing, July 1986 The paper investigates several approaches to user modeling in natural-language dialog systems. First, reasons are pointed out why user modeling has become so important in the last few years, and definitions are proposed for the notions of user model and user modeling. Then techniques for constructing user models in the course of a dialog are presented and recent proposals for representing a wide range of assumptions about a user's beliefs and goals in a system's knowledge base are surveyed. Examples for the application of user models in systems developed to date are presented, and some social implications are discussed. Finally, unsolved problems, like coping with collective beliefs or resource-limited processes, are investigated, and prospects for application-oriented research are outlined. SYCON - Ein Rahmensystem zur Constraint-Propagierung auf Netzwerken von beliebigen symbolischen Constraints M. Fendler, R. Wichlacz Report No. 4 In Stoyan, H., Ed.GWAI-85. 9th German Workshop on Artificial Intelligence Dassel/Solling, September 1985 Springer, Heidelberg, 1986 Constraint-propagation on networks has been successfully tested as a processing mechanism in several fields of Artificial Intelligence. This paper presents SYCON, with which symbolic constraints of any kind can be defined and processed. After a short introduction, the propagation of restricted variables on constraint networks is explained. Algorithms are presented for the handling of cycles, for checking the consistency of the network, and for backtracking during the propagation process. To demonstrate the essential differences between SYCON and other systems, it is contrasted to the systems of Steele, Gosling, and Freuder. SYCON is implemented in Franz-Lisp on a VAX 11/780. Entwurf eines aktiven, wissenbasierten Hilfesystems fiir SINIX The SC system is a natural language help facility for the SINIX operating system. SC should answer natural language questions about concepts and Computational Linguistics, Volume 12, Number 2, April-June 1986 157 The FINITE STRING Newsletter Abstracts of Current Literature Ch. Kemke Report No. 5 Shortened version in LDV-Forum Nr. 2, December 1985:43-60 The Role of Natural language in Advanced Knowledge-Based Systems W. Wahlster Report No. 6 to appear in Winter, H., Ed., Artificial Intelligence and Man-Machine Systems. Springer, Heidelberg, 1986 Combining Deictic Gestures and Natural Language for Referent Identification A. Kobsa, J. AIIgayer, C. Reddig-Sieknmnn, N. Reithinger, D. Sdunauks, K. Harbusch, W. Wahlster Report No. 7 to appear in Proceedings of COLING-86 Wissenbasierte Fertigungsplanung in Stanzereien mit FERPLAN: Ein Systemiiberlick R. Grasmiick, A. Guldner Memo No. 2 commands of the SINIX system. Furthermore, as an active help system it should give unsolicited advice to a user during his work with SINIX","In this paper, first a short introduction to intelligent help systems is given, followed by an overview of the help systems UC, UCC, and WIZARD, whose domains are the operating systems UNIX and VMS, respectively. In the next sections the basic structure and function of the SINIX Consultant is explained, with special regard to the aspects of planning/problem solving and knowledge representation. The functions and tasks of the system's components (natural language interface, problem solving/planning module, plan recognition module) and knowledge bases (knowledge about language, planning, the SINIX domain, the general/ individual user) are described according to the present state of development. Finally, important characteristics of the implementation and the actual status of the project are given. Natural language processing is a prerequisite for advanced knowledge-based systems since the ability to acquire, retrieve, exploit, and present knowledge critically depends on natural language comprehension and production. Natural language concepts guide the interpretation of what we see, hear, read, or experience with other senses. In the first part of the paper, we illustrate the needed capabilities of cooperative dialog systems with a detailed example: the interaction between a customer and a clerk at an information desk in a train station. It is shown that natural language systems cannot just rely on knowledge about syntactic and semantic aspects of language but also have to exploit conceptual and inferential knowledge, and a user model. In the remainder, we analyze and evaluate three natural language systems introduced to the commercial market in 1985: Language Craft (TM of Carnegie Group, Inc.), NLMenu (TM of Texas Instruments, Inc.), and Q&A (TM of Symantec, Inc.). The detailed examination of these systems shows their capabilities and limitations.","We conclude that the technology for limited natural-language access systems is available now, but in the foreseeable future the capabilities of such systems will in no way match human performance in face-to-face communication. In virtually all current NL dialog systems, users can refer to objects by iinguistic descriptions only. In human face-to-face conversation, however, participants also frequently use various sorts of deictic gestures. In this paper, we present the referent identification component of XTRA, a natural-language access system for expert systems. XTRA allows the user to combine NL input with pointing gestures on the terminal screen for refer-ring to objects on the display. Information about the location and the type of this deictic gesture, as well as about the linguistic description of the referred object, the case frame, and the dialog memory, are utilized for identifying the object. The system is tolerant in respect to impreciseness of both the deictic and the natural language input. The user can thereby refer to objects more easily, avoid referential failures, and employ vague everyday terms instead of precise technical notions. Production planning is the process of designing plans for manufacturing parts. FERPLAN is a knowledge-based system designed for production planning in punching plants. Based on a qualitative part description, a raw-material specification, and the planned number of pieces, the system works out a handling plan with the corresponding advance times.","After a survey of the system's architecture, the plan generation is exemplified, starting with a qualitative part description. Finally, FERPLAN is contrasted in tabular form to other systems designed for similar purposes. 158 Computational Linguistics, Volume 12, Number 2, April-June 1986 The FINITE STRING Newsletter Abstracts of Current Literature The system is implemented on a VAX 11/780 in Franz-Lisp, using the production-system interpreter OPS5. GABI - Ein wissensbasiertes Geldanlageheratungsprogramm 1t. Baltes Memo No. 3 This paper presents the interactive expert system GABI, a consulting and information system for investment that deduces investment proposals from customer-specific statements. After a short description of the system's capabilities, its design criteria, and range of applications, the system architecture is explained. In addition to a dialog component, a simple explanation component, and an inference component, it comprises a highly partitioned knowledge base. The representation of knowledge about the various forms of investment, the situation of the financial market, and the structure of the inference rules are exemplified. The system is implemented in Franz-Lisp, version 38.79, and the embedded PLANNER-like AI programming language PEARL on a VAX 11/780 under UNIX 4.2 BSD. A distinguishing feature of the system is that the propositional knowledge as well as the inference rules are represented as frame-like structures. Formulardeixis und ihre Simulation auf dem Bildschirm. Ein (Jberblick aux linguistischer Sicht 1). Schmauks Memo No. 4 Up to the end of the seventies, local deixis was dealt with mostly from a rather general point of view. As far as pointing gestures are concerned, a detailed treatment is missing in most linguistic investigations even up to the present. The present paper, unlike the previous ones, only deals with a very special kind of deixis, namely pointing gestures onto a form, which puts up a very restricted deictic space. In this universe of discourse, objects are often (and efficiently) referred to by pointing gestures. Deixis with regard to forms only offers a definite advantage from a linguistic point of view: some problems of local deixis are reduced in complexity without the setting having to become unnatural. The first chapter of the paper defines some technical terms and presents the various linguistic and non-linguistic deictic means of reference. Chapters 2-3 list the essential characteristics of deixis with regard to forms. Emphasis is put on investigating in how far means of local deixis become more salient and thus easier to describe in such a setting. Chapter 5 deals with the problem from an artificial intelligence point of view, since also some non-linguistic aspects of local deixis can now be simulated on a terminal screen. For example, the NL dialog system XTRA currently under development at the University of Saarbrticken, is designed to allow the user to refer to subareas of a form not only by linguistic means but also by pointing gestures. Characterizing Trajectories of Moving Objects using Natural Language Path Descriptions E. Andre, G. Bosch, G. Herzog, 1\". Rist Memo No. 5 The topic of this paper is the analysis of the semantics of the particular spatial relations along and past, which are used to characterize the path of moving objects. The German dialog system CITYTOUR, which answers questions about the spatial relations of objects in a scene, is presented by means of a simple example dialog. The representational prerequisites of the static and dynamic objects, required for computational analysis are presented. Two concrete predicate functions testing whether a dynamic object moves along or moves past a static object are described in detail. Selected Dissertation Abstracts Compiled by: Susanne M. Humphrey, National Library of Medicine, Bethesda, MD 20894 Bob Krovetz, University of Massachusetts, Amherst, MA 01002 The following are citations selected by title and abstract as being related to computational linguistics or knowledge representation, resulting from a computer search, using the BRS Information Technologies retrieval service, of the Dissertation Abstracts International (DAD data base produced by University Microfilms International.","Included are the title; author; university, degree, and, if available, number of pages; DAI subject category chosen by the author of the dissertation; UM order number and year-month of entry into the data base; and abstract. References Computational Linguistics, Volume 12, Number 2, April-June 1986 159 The FINITE STRING Newsletter Abstracts of Current Literature","are sorted first by DAI subject category and second by author. Citations denoted by an MAI reference do not yet have","abstracts in the data base and refer to abstracts in the published Masters Abstracts International. Unless otherwise specified, paper or microform copies of dissertations may be ordered from","University Microfilms International","Dissertation Copies","Post Office Box 1764","Ann Arbor, MI 48106","telephone for U.S. (except Michigan, Hawaii, Alaska): 1-800-521-3042","for Canada: 1-800-268-6090.","Price lists and other ordering and shipping information are in the introduction to the published DAI. An alternate","source for copies is sometimes provided at the end of the abstract. The dissertation titles and abstracts contained here are published with permission of University Microfilms Interna-","tional, publishers of Dissertation Abstracts International (copyright 1985) by University Microfilms International), and","may not be reproduced without their prior permission. Plans and Situated Actions: an Inquiry into the Idea of Human-Machine Communication Lucille Alice Suchman University of California, Berkeley Ph.D. 1984, 183 pages Anthropology, Cultural SO DAI V46(04), SecA, pp1018 University Microfilms Order Number ADG85-13007. 8510 This dissertation examines two alternative views of purposeful action and shared understanding. The first, adopted by researchers in Cognitive Science, views the organization and significance of action as derived from plans, which are prerequisite to and prescribe action at whatever level of detail one might imagine. Mutual intelligibility on this view is a matter of the recognizability of plans, due to common conventions for the expression of intent, and common knowledge about typical situations and appropriate actions. The second view, drawn from recent work in social science, treats plans as derivative from situated action. Situated action as such comprises necessarily ad hoc responses to the actions of others and to the contingencies of particular situations. Rather than depend upon the reliable recognition of intent, successful interaction consists in the collaborative production of intelligibility through mutual access to situation resources, and through the detection, repair, or exploitation of differences in understanding.","Researchers interested in machine intelligence attempt to remedy the inherent vagueness of plans, to make them the basis for computational arti-facts intended to embody intelligent behavior, including the ability to interact with their human users. I examine that project through a case study of people using a machine designed on the planning model, and intended to be intelligent and interactive. A conversation analysis of \"interactions\" between users and the machine reveals that the machine's insensitivity to particular circumstances is a central design resource, and a fundamental limitation. I conclude that problems in Cognitive Science's theorizing about purposeful action as a basis for machine intelligence are due to the project of substituting plans for actions, and representations of the situation of action for action's actual circumstances. An Investigation of Information Requirements Determination and Analogical Problem Solving Lance Brian Eliot University of Southern California Ph.D. 1985 Business Administration, General DAI V46(04), SeeA, ppl021 This item is not available from University Microfilms International. ADG05-56025. 8510 Recent studies on the design of information systems have indicated that one important factor in the development of a computer-based information system is the problem solving behavior of systems analysts. This study was an investigation of the problem solving behavior of expert and novice systems analysts, and examined the implications that these skills have for their selection, training, and use of system methodologies.","Two major aspects of analyst behavior were examined: (1) information cues of analyst knowledge used during the requirements determination task, and (2) influence of past experiences as used during an analogical problem solving process. Focus of the study was on the cognitive behavior of analysts and differences in performance between analyst levels (expert and novice). Empirical work consisted of an experiment measuring specific forms of similarity judgments by analysts in order to examine the two major aspects of analyst behavior. 160 Computational Linguistics, Volume 12, Number 2, April-June 1986 The FINITE STRING Newsletter Abstracts of Current Literature A Framework for Expert Control of Interactive Software Systems Chidanand 11. Apte Rutgers University The State U. of New Jersey (New Brunswick) Ph.D. 1984, 146 pages Computer Science DAI V46(02), SecB, pp574 University Microfilms Order Number ADG85-07087. 8508 Understanding the Bugs of Novice Programmers Jeffrey Guy Bonar University of Massachusetts Ph.D. 1985, 288 pages Computer Science SO DAI V46(03), SecB, pp896 University Microfilms Order Number ADG85-09525. 8509","Results of the investigation indicated that: (1) analysts appeared to divide requirements into two dimensions, organizational requirements and technical requirements; (2) expert analysts divided requirements into two dimensions with greater consistency and clarity than did novice analysts; (3) analogical problem solving can have an influence during requirements determination; and (4) influence of analogies was more pronounced with novice analysts than expert analysts.","Interpretation of the results suggested that attention to analyst behavior may improve the development of information systems by matching of certain underlying skills with project requirements and systems methodologies. Future research recommendations included the need for further assumption surfacing of analyst behavior, and inspection of reward structures for assumption establishment. Implications for research in human problem solving, including analogical thinking, were also considered.","(Copies available exclusively from Micrographics Department, Doheny Library, USC, Los Angeles, CA 90089.). Expert problem-solving strategies in many domains require the use of detailed mathematical techniques coupled with experiential knowledge about how and when to use the appropriate techniques. In many of these domains, such techniques are made available to experts in large software packages. In attempting to build expert systems for these domains, we wish to make use of these existing packages, and are therefore faced with an important problem: how to integrate the existing software, and knowledge about its use, into a practical expert system. The expert knowledge is used, in dynamic selection of appropriate programs and parameters, to reach a successful goal in the problem-solving. This kind of expert problem-solving is achieved through two interacting bodies of knowledge; problem domain knowledge, and knowledge about the programs that comprise the software package.","This thesis describes the framework of a hybrid expert system for representing problem-solving knowledge in these domains. This hybrid system may be characterized as consisting of a surface model and a deep model. The surface model is a production-rule based expert subsystem that c6nsists of heuristics used by an expert. The deep model is a collection of methods, each parameterized by a set of controlling and observed parameters. The methods and their results are reasoned about using their parameter sets. The existing software is reorganized as necessary to map it into the deep model structure of a hybrid system. This framework has evolved out of an effort to build an expert system for performing well-log analysis (ELAS - Expert Log Analysis System). A generalized expert-system building methodology based upon principles drawn from ELAS is introduced. The use of method-abstractions in assembling a hybrid system is discussed. The notion of worksheet-reasoning is defined, and discussed. Why do people have trouble learning to program? This dissertation presents a theory of novice programming bugs motivated by interviews where novice programmers solved simple programming problems. Novice programming knowledge is represented in two components. Fragments of pragmatic programming knowledge (PKP) describe the expert programming knowledge that a novice has acquired. Step-by-step Natural Language procedural knowledge (SSK) describes the experience a novice brings to programming from Natural Language. These two knowledge bases are tied together by Computational Linguistics, Volume 12, Number 2, April-June 1986 161 The FINITE STRING Newsletter Abstracts of Current Literature Computationally Efficient and Linguistically Adequate Parsing of Some Natural Language Structures Pradip Dey University of Pennsylvania Ph.D. 1984, 204 pages Computer Science; Language, Linguistics SO DAI V46(01), SecB, pp231 University Microfilms Order Number ADG85-05054. 8507 functional and surface parallels. PK and SSK are represented by Plans: frame-like bundles of knowledge about the pragmatic features of programming.","When a novice is programming and encounters a gap or inconsistency in the PK, he or she has reached an impasse. The theory proposes bug generators as strategies for patching the impasse and continue developing a solution. Usually this patch introduces a bug. Three types of bug generators are discussed. SSK/PK bug generators use SSK knowledge to patch an impasse, exploiting functional and surface parallels to convert a PK impasse into a problem in SSK, the better known domain. With Intra-PK bug generators, novices rely on interconnections between fragments of PK to patch an impasse. Other/PK bug generators use other knowledge, from algebra, for example, to patch an impasse.","The theory is evaluated based on interviews of novice programmers working on Pascal programming problems. A key set of predictions from the theory are formulated and a two part method to analyze the protocol data is described. The plan analysis describes the plans being used by the novice. The bug analysis interprets each bug in terms of all plausible bug generators. An entire analyzed protocol is shown. Based on that and three other protocols, the predictions of the theory are evaluated. Also, that analysis is used to comment on general patterns of plan usage and bug generator coverage.","The dissertation concludes by summarizing what has been accomplished, the implications of the work, and future directions for the work. A plan-based novice programming environment and tutor is described as a natural extension of this work. Computationally efficient and linguistically adequate strategies for parsing sentences with word order variations and gapped structures are presented. At first the general aspects of the strategies are presented at an abstract level. Then, the strategies are applied to Hindi. The strategies are implemented in the Augmented Transition Network (ATN) formalism. However, they are general enough to be implemented in other parsers. The standard ATN is inefficient due to its usual backtracking, and so we add look-ahead facilities to reduce backtracking. We also use temporary registers to hold well-formed substrings temporarily until a decision can be taken about them.","A problem in parsing word order variation is that subject, object, etc., cannot be identified from their position in the sentence. Postpositions and inflection often help to identify them. But, there is a large body of data in Hindi for which semantic information is required in addition to syntactic information to make these identifications. Our analysis of Hindi shows that in order to check grammatical agreement the parser must identify the subject and object. In order to identify the subject and object the parser must have access to semantic information. That means, in order to check grammatical agreement the parser must have access to semantic information. This has serious consequences for the relation between syntax and semantics.","As a general strategy we suggest that, for parsing free constituent orders, the right hand side of a phrase structure rule can be treated as a set. The parser can proceed by checking set membership. By imposing restrictions on sets, other word order variations can also be parsed. The basic strategy is compatible with ID/LP analysis of Gazdar and Pullum (see Pullum 1983), but we do not use metagrammar. We show that 'flat structures' are suitable for languages with word order variations. The flat structure hypothesis helps to parse gapping, because structural matching required for gapping is limited to higher constituents. 162 Computational Linguistics, Volume 12, Number 2, April-June 1986 The FINITE STRING Newsletter Abstracts of Current Literature A Computational Theory of Metaphor Comprehension and Analogical Reasoning Bipin lndurkhya University of Massachusetts Ph.D. 1985, 207 pages Computer Science SO DAI V46(03), SecB, pp898 University Microfilms Order Number ADG85-09557. 8509 Knowledge Representation and Intei-Ugent Systems: from Semantic Networks to Cognitive Maps James Richard Levenick The University of Michigan Ph.D. 1985, 253 pages Computer Science DAI V46(04), SecB, pp1242 University Microfilms Order Number ADG85-12454. 8510","Grammatical phenomena often interact with each other in intricate ways. We investigate several interacting phenomena, such as word order variation, grammatical agreement, relative clause constructions, dative constructions and gapping, in a coordinated way. In this thesis we propose a formal theory of metaphors and analogies. We start from the assumption that in a metaphor, or an analogy, some terms of one domain (source domain) are applied to terms of another domain (target domain). We address the problem of representing the information contained in a metaphor, or an analogy, and the means of computing it from the domains' knowledge.","We describe a formalism, called Schema-Language SL, for representing domain knowledge which is based on the First Order Predicate Calculus. We then develop a theory of Constrained Semantic Transference CST which shows how the terms and structural relationships of the source domain can be coherently transferred to the target domain. The concept of a T-MAP, which is a partial coherent mapping from the terms of the source domain to the target domain, lies at the heart of CST.","We introduce two operators, called Augmentation and Positing Structure, that make it possible to create a new structure in the target domain constrained by the structure of the source domain. We show how to characterize metaphors and analogies by using T-MAPs which can explain many cognitive properties associated with them. We also present a characterization of metaphorical truth and metaphorical inference in CST. A major limitation of CST is that the notion of coherency is not computational.","We propose a theory of Approximate Semantic Transference AST, which is derived from CST by replacing the coherency requirement on T-MAPs by approximate coherency. The partial approximate-coherent mappings of AST, called AT-MAPs, are computational and can be used as a basis for developing models of cognitive processes involved in comprehending metaphors and analogies. We propose two alternative formulations of approximate coherency. Based on one of these versions, we present several algorithms, and principles that can be used in designing algorithms, for computing AT-MAPs from the knowledge of the source and target domains. AI systems have long relied on propositional semantic network knowledge representation. Although many AI projects produce impressive results, they tend to be difficult to generalize and have yielded only meagre progress towards a theory of intelligence. The field lacks coherence in definitions, assumptions, and methods. A systematic treatment of the underlying knowledge representation issues appears essential for the development of a more unified theory.","This dissertation considers knowledge networks in the context of an attempt to model and produce intelligent, adaptive behavior. Natural intelligent systems build perceptual and predictive capacity on the basis of ordinary experience, and function routinely in ill-defined, context sensitive situations. Propositional AI systems have not demonstrated these capabilities; this may be inevitable given their underlying knowledge representations. To facilitate an analysis of these issues a set of parameters characterizing the space of such networks is developed. The networks so defined range from Quillian's semantic network to the more theoretically based cognitive map","A network activity passing simulation (NAPS) was developed to investigate the effects of these various parameters on performance. NAPS searches for subgoals between two given locations in a familiar environment. This wayfinding task is a simplification of the more general problem Computational Linguistics, Volume 12, Number 2, April-June 1986 163 The FINITE STRING Newsletter Abstracts of Current Literature A Data Base System for Small Interactive Computers John Robert Levine Yale University Ph.D. 1984, 193 pages Computer Science DAI V46(05), SecB, pp1618 University Microfilms Order Number ADG85-14672. 8511 The Organization of Knowledge in a Multi-lingual, Integrated Parser Steven Leo L ytinen Yale University Ph.D. 1984, 314 pages Computer Science DAI V46(05), SecB, pp1618 University Microfilms Order Number ADG85-14879. 8511 Functional Entity Relationship Model Masanobu Matsuo University of California, Santa Barbara solving method of searching for subgoals between a perceived state and a desired state. NAPS inputs a set of knowledge representation parameters and constructs the sort of network specified for use in testing. Then given a start and a goal location, NAPS propagates activity in the net to select a subgoal.","Experiments were performed in several environments; networks were compared in terms of speed, reliability, flexibility and robustness. The results were somewhat surprising; marker passing semantic networks are more reliable than activity passing cognitive maps in simple environments but prove to be less reliable in complex situations. The semantic networks are shown to be rigid and inflexible, and so unsuitable for use in unpredictable, or difficult environments without the inclusion of additional mechanisms. The cognitive maps by contrast, are able to handle unexpected environmental vagaries without the intervention of an intelligent executive. A data base system oriented toward flexible single-user systems is described, in three parts. First is a network data structure with extensive provision for dynamic restructuring and omitted data. Second is a graphical inquiry language that allows the user literally to navigate among his data in an imaginary space. The inquiry language allows manipulation of complex network structured data at the high level usually associated with relational data bases. Last is a report language that allows the user to draw his report on the screen in an intuitively appealing way, without having to specify many of the details of the reports to be generated. The three parts share tree-structured selections from the underlying network data. An implementation mostly in Lisp is described in detail, with sketches of how it could be implemented in other languages. A controversy has existed over the interaction of syntax and semantics in natural language understanding systems. On the one hand, theories of integrated parsing have argued that syntactic and semantic processing must take place at the same time. In addition, these theories have also argued that syntactic and semantic knowledge should be mixed together, and that the role of syntax should be completely subservient to semantic processing. On the other hand, opponents of this theory argue that parsing should be more modular, with syntactic and semantic processing taking place separately. Along with this processing modularity, these opponents also argue that syntactic and semantic knowledge should be more modular, and that syntax, since it is largely autonomous from semantics, plays a more important role in natural language understanding.","This thesis presents a theory of natural language understanding which is a compromise between these two views. I argue that natural language understanding should be integrated, in the sense that syntactic and semantic processing should take place at the same time. However, instead of mixing syntactic and semantic knowledge together in the knowledge base of a parser, I argue that power can be gained by organizing syntax and semantics as two largely separate bodies of knowledge, which are combined only at the time of processing. The result is a parser which retains the predictive power gained by using semantic information during syntactic processing, but which is more robust in parsing complex syntactic constructions, and which is more amenable to the organization of knowledge about more than one language. In this thesis a new data model called FERM (Functional Entity Relationship Model) is introduced in order to improve the semantic modeling capability of database systems. FERM is an integration 164 Computational Linguistics, Volume 12, Number 2, ApriI-J-qe 1986 The FINITE STRING Newsletter Abstracts of Current Literature Ph.D. 1984, 213 pages Computer Science DAI V46(03), SecB, pp899 University Microfilms Order Number ADG85-09444. 8509 A Modal Temporal Logic for Reasoning about Changing Databases with Applications to Natural Language Question Answering Eric Keener Mays University of Pennsylvania Ph.D. 1984, 136 pages Computer Science DAI V46(01), SecB, pp235 University Microfilms Order Number ADG85-05104. 8507 Managing Permanent Objects Nathaniel William Mishkin Yale University Ph.D. 1984, 160 pages Computer Science DAI V46(05), SecB, pp1619 University Microfilms Order Number ADG85-14880. 8511 and extension of the Functional Data Model and the Entity Relationship Model. FERM consists of a small number of primitive elements such as entity sets, attribute types, and database storage functions together with a set of integrity constraints called domain-range constraints. Functional relationships between entities and attributes of an entity are represented in FERM by database storage functions. High-level semantics such as ISA relationships, existence dependencies, mandatory relationships, case and disjointedness relationships are all expressed in FERM in terms of domain-range constraints on database storage functions. Thus, consistent database storage states with respect to such high-level semantics can be studied by analyzing the effect of database update operations on domain-range constraints.","This thesis gives a formal definition of FERM including update (insertion, deletion, replacement) operations. This remedies two major shortcomings of previous work in the Functional and Entity Relationship Model area: (1) the lack of a formal definition; (2) the absence of update operations in these models.","A major emphasis in this thesis is placed upon the study of correct update operations. A major contribution is the introduction of a finite set of update operations called unit update operations which have three important properties: (1) they are all consistent (i.e., correct) transactions with respect to a given set of domain-range constraints; (2) they are nfinimal in the sense that no unit update operation can be simplified any further without becoming incorrect, and (3) all other consistent transactions can be constructed from unit update operations using a given set of correctness-preserving constructors (completeness property). An algorithm for generating a complete and minimal set of unit operations is given and correctness of this algorithm is proven.","These results may be used as a basis for the development of a FERM database design methodology. The Database Administrator can construct user-level correct transactions by composing unit operations generated automatically. This approach may ultimately result in improved productivity and reliability of database application development. A database which models a changing world must evolve in correspondence to the world. Previous work on natural language question answering systems for databases has largely ignored the issues which arise when the database is viewed as a dynamic (rather than a static) object. We investigate the question answering behaviors that become possible with the ability to represent and reason about the possible evolution of a database. These behaviors include offering to monitor for a possible future state of the database as an indirect response to a query, and directly answering questions about prior and future possibility. We apply a propositional modal temporal logic that captures possibility and temporality to represent and reason about dynamic databases, and present a sound axiomatization and proof procedure. This work describes a programming system that facilitates the management of data objects that live across multiple invocations of programs that read and modify those objects; we call such data objects \"permanent objects\". Typically, programmers needing to save data objects permanently do so either (1) by writing an ad hoc set of procedures that convert data from some internal representation to some external representation (and back), or (2) by interfacing their programs with an existing database system. We discuss the problems encountered by a programmer adopting either of these strategies, and we describe our system whose design is an attempt to strike a balance between Computational Linguistics, Volume 12, Number 2, Ap~-June 1986 165 The FINITE STRING Newsletter Abstracts of Current Literature Schema-based Problem Solving Daniel Martin Russell The University of Rochester Ph.D. 1985, 186 pages Computer Science DAI V46(04), SecB, pp1245 University Microfilms Order Number ADG85-11910. 8510 An Analysis of the Problem-Solving Technique of a Talmudieal Expert Steven Ira Levenson New York University Ph.D. 1984, 420 pages Education, Curriculum and Instruction DAI V46(01), SecA, pp62 University Microfilms Order Number ADG85-05431. 8507 the flexibility of the ad hoc approach and the rigidity of the approach that employs a database.","A key goal of our work is the design and implementation of a system that makes the manipulation of permanent objects nearly as easy and flexible as the manipulation of \"transient\" objects - i.e. the memory resident data structures that programmers are accustomed to dealing with. We wish to hide the details associated with the fact that permanent objects must have their permanent home in a disk file system.","Our system is written in T, a dialect of Scheme, which is in turn a dialect of Lisp and runs on the Apollo workstation. The system provides tools to make it relatively easy to write T programs that manipulate these permanent objects.","A secondary goal of our work is to support distributed computing by allowing multiple processors to have access to permanent objects. While the system does not address all the issues associated with distributed computing, we believe that the mechanisms provided can be effectively used in the course of solving certain problems in a distributed way. Much evidence supports the use of schemata as a basic element of human problem solving. Yet, little work has been done to show how schematic information can be represented, manipulated or used for problem solving.","A schema maps goals to abstract action sequences. Schema-based problem solving is the process of identifying appropriate schemata, and then adapting that schematic knowledge to the particular circumstances of a problem. Schemata adapt to the problem environment in three ways: (1) Multiple schema expansions are pursued simultaneously and only the best expansion is selected for execution; (2) As a schema expands, constraints defining and limiting future expansion are established. Constraint satisfaction ensures that the plan will be internally consistent and still attain the goal; (3) Schema components may be selectively deleted or integrated in response to goal requirements or constraints set up during problem solving. The three methods translate a selected schema into a fully-developed plan.","However, problem solving in complex domains requires an ability to switch rapidly and reliably between problem solving and error recovery. Plan generation and execution are tightly interwoven to allow dynamic error recovery through plan alteration and replanning. A fully-developed problem solution never exists at any one time. Instead, the plan constantly evolves in response to execution and problem solving requirements. Because deviation from a plan due to inaccurate world models or incorrect planning is so common, error recovery at many levels is seen to be a normal, rather than exceptional, part of problem solving. Expanding alternate plans in parallel and distributing control over many sites allows the system to be responsive to error and quick in execution.","Methods for deriving plans from schemata and a problem solving philosophy are demonstrated in a program, SHEM, which generates and executes plans to assemble small block figures in a simulated world. A theory of expert skill in the Brisker style of Talmudic study is presented. It identifies particular problem-solving skills an expert uses in analyzing a Talmudical text. Within the framework of this theory, a more explicit model of the analysis process was developed. The model is based on a production system and describes the solution path of the expert as recorded in verbal protocols. Content analysis of the protocols was used as the research methodology. The implications of the model are discussed for the training of Talmudical students in general problem-solving skills by teaching them computer 166 Computational Linguistics, Volume 12, Number 2, April-June 1986 The FINITE STRING Newsletter Abstracts of Current Literature The Script Schema in Children's Comprehension and Memory Joel A. Seltzer City University of New York Ph.D. 1985, 120 pages Education, Psychology DAI V46(05), SeeA, pp1232 University Microfilms Order Number ADG85-15658. 8511 The Semantic Features of Text: Their Interaction and Influence on Comprehending Karen M. Feathers Indiana University ED.D. 1985, 786 pages Education, Reading DAI V46(04), SecA, pp937 University Microfilms Order Number ADG85-07850. 8510 programming, for the differential model of Intelligent Computer Assisted Instruction and for Talmudical pedagogy. This study examined how script strength, prior scriptal knowledge, and age differences affect children's comprehension and memory, and relate to reading skill. A script schema represents stereotypical action sequences of familiar events that are goal-oriented, for example, going-to-a-restaurant. Script strength refers to a script's ability to evoke a familiar temporal-causal sequence of events. Strong - canonical - schemata facilitate story recall. Prior knowledge and experience affect performance as measured by inference-making ability, recall, and errors in recall with words, sentences, and text. Skilled readers spontaneously use strategies that facilitate comprehension and recall.","A strong script was predicted to facilitate comprehension and recall of a picture series. Prior scriptal knowledge was expected to facilitate performance, and older children were predicted to have higher comprehension and recall than their younger counterparts.","The subjects were 139 second- and fourth-grade children. They were shown a picture series evoking the script getting-ready-for-school-in-the-morning. Within each grade, children were randomly assigned to one of six conditions in which a script-header - or title - was presented before or after exposure to the stimuli and varied by three levels of canonical strength. The children were asked to generate a story as a measure of their comprehension and were given traditional memory tasks to measure verbal recall, visual recognition, and serial reconstruction of the pictures.","As predicted, the strong script-header was found to facilitate comprehension and serial reconstruction ability. Children in the weak script-header condition produced more intrusions in their stories. Prior knowledge facilitated only the ability to produce more detailed information. Recall and recognition were not affected by the treatments. Fourth graders performed better than second graders on most tasks. A modest relationship between second grade task performance and reading ability was noted.","It was concluded the children's level of information processing must be consistent throughout a task to facilitate comprehension and recall. Scripts that organize new information to fit the learner's knowledge base may be a useful pedagogical tool. Research in the last ten years has begun to focus on text as a complete semantic unity. A variety of techniques for viewing text from this new semantic perspective have been proposed. Research using these techniques suggests the viability of approaching the reading process from this perspective. However, because of materials and procedures used and a focus on comprehension as measured by recall and recognition, these studies leave unanswered the question of the feasibility of applying the procedures to longer texts as well as the more important question of how the semantic features interact and influence the actual process of reading. This study investigated the impact of semantic features of discourse on in-process reading behavior as measured by the Reader Miscue Inventory. A 1300 word story was analyzed using six different techniques (propositional analysis, macrostructure, story schema, cohesion analysis, propositional mapping, and conceptual chaining). Twenty fourth grade subjects read the story orally and retold it. Cross tabulations, multiple regression analysis, and factor analysis were used to consider the semantic features in relation to subjects' reading behavior. Results suggest that these features do influence reader behavior, but that this influence is highly interactive in nature. That is, Computational Linguistics, Volume 12, Number 2, April-June 1986 167 The FINITE STRING Newsletter Abstracts of Current Literature Reading, Writing, and Mutual Knowledge Gordon Philip Thomas University of Minnesota Ph.D. 1985, 169 pages Language, General DAI V46(04), SecA, pp961 University Microfilms Order Number ADG85-12082. 8510 An Operator-Argument Grammar of Quantity Expressions Michiko Kasaka New York University Ph.D. 1984, 257 pages Language, Linguistics DAI V46(01), SeeA, ppl40 University Microfilms Order Number ADG85-05504. 8507 reading behavior is related not to one semantic feature but to multiple and complex patterns of features. These findings are significant since they suggest a need to reconceptualize current views of language processing and text evaluation. \"Mutual Knowledge\", a topic in the philosophy of language, is knowledge that is known by at least two persons in any meaningful situation and known to be known by both those persons. Mutual knowledge encompasses such issues in the philosophy of language as how intention and convention contribute to meaning and where meaning itself resides. These issues are important to composition theory as well, for the field deals with the practical application of how writers make meaning for their readers through texts. This dissertation explains certain philosophical approaches to these issues, while developing and modifying these concepts so that they can be applied to the reading-writing situation. In addition it uses them to examine two other theories in composition: Linda Flower and John Hayes's cognitive process model of composing and E.D. Hirsch's views on \"relative readability\" and their application to composition instruction.","The first chapter traces the history of the notion of mutual knowledge, explaining its significance to H.P. Grice's general theory of meaning and arguing against the infinite regress that some philosophers have claimed occurs with mutual knowledge. The chapter also defines and distinguishes three types of mutual knowledge necessary to the reading-writing situation: \"world knowledge\", \"knowledge of conventions\", and \"knowledge of language\". Finally, it amplifies the concept of \"world knowledge\", giving examples of how it can work for or against a writer.","The second chapter explores the nature of conventions in language, building on the work of David K. Lewis. It argues against confusing the forms that conventions assume with the conventions themselves, which are formal regularities used to convey writers' intentions to particular audiences. The third chapter develops a theory of meaning for the reading-writing situation, by applying the speech-act theories of J.L. Austin and of Kent Bach and Robert M. Harnish.","Chapter 4 incorporates the notion of mutual knowledge into Flower and Hayes's model, elaborating on their concepts of meaning and the long-term memory. Chapter 5 uses Hirsch's 1977 work as an example to argue against emphasizing the formal properties of texts in the teaching of composition. It argues instead that successful writing is a communicative intention, conveyed through mutually known, linguistic conventions. Within the language of quantitative description, exemplified by the use of quantity expressions in portions of the scientific literature, this thesis provides a grammatical analysis of quantity expressions and fits them into an overall grammatical analysis of English sentences. It provides a set of word classes, and the grammatical relations which relate these classes to each other and to the overall grammatical structures of English sentences.","The set of quantity expressions dealt with in this study includes numbers, numerical expressions, universal quantifiers, quantity adjectives, quantity verbs, comparatives, superlatives, and others.","This thesis contains and validates two major hypotheses with respect to the treatment of quantity expressions. One hypothesis is the existence of a subgrammar of quantity expressions. The second hypothesis deals with the possibility of the regularization of the surface grammar by positing implicit elements in the underlying representation. The third major contribution of this study comes from a finding that in some cases the choice of analysis within the available alternatives could be 168 Computational Linguistics, Volume 12, Number 2, April-June 1986 The FINITE STRING Newsletter Abstracts of Current Literature A Speech Act Theory Based Interpretation Model for Written Texts Takashi Manabe The University of Texas at Arlington Ph.D. 1984, 405 pages Language, Linguistics DAI V46(05), SecA, pp1265 University Microfilms Order Number ADG85-15329. 8511 The Modal Verbs: Univoeal Lexical Items Heather McCallum-Bayliss Georgetown University Ph.D. 1984, 267 pages Language, Linguistics DAI V46(05), SecA, pp1265 University Microfilms Order Number ADG85-15691. 8511 identified with the occurrence of different classes of verbs operating on a particular class of quantity expressions. In recent years, so called Speech Act Theory (a branch of the philosophy of language) has focused on such concepts as 'intention' (or 'intentionality'), 'interaction' and 'comprehension'. The author believes that this theory gives the theoretical basis for an interpretation model which is developed in this study.","After human speech activities are placed in the larger context of human actions, a model which explains the performing process of speech acts is presented. In this model, four levels of intentionality are posited: utterance intent, propositional intent, illocutionary intent, and perlocutionary intent. These levels of intentionality are inseparable from each other and operate simultaneously in performing speech acts. Speech acts are categorized according to their illocutionary intents. Six major categories are suggested: Representatives, Directives, Commissives, Expressives, Declarations, and Responsives.","The model of the speaker presented above is completed when the model of the hearer (or interpreter) is added to it. The combined model, which can be called a cyclic or interactive model, is developed. Corresponding to the four levels of intentions on the part of the speaker, four levels of effects are posited in order to explain the 'comprehension' process of the hearer. These four levels of effects are: utterance effect, propositional effect, illocutionary effect, and perlocutionary effect. It is contended that only after this stage of comprehension can the hearer make a legitimate response to the speaker by his 'decision' and 'response action'. The process of comprehension, then, is explained extensively, paying particular attention to the Concepts of 'context' and 'inference'.","The main contributions of the study are: (i) the resolutions of some of the lingering theoretical problems in Speech Act Theory; (ii) the development of an interactive model of speech acts, which is applied to text analysis; and (iii) the development of such concepts as 'author's intent structure' and 'referential intent structure' for the application of Speech Act Theory to text analysis. This study is about the modal verbs of American English: can, may, could, might, must, will, shall, would and should. Many traditional analyses consider the modals ambiguous between the so-called epistemic and root interpretations (e.g., may of 'possibility' and 'permission', respectively). This study demonstrates, however, that the modal verbs are not ambiguous but systematically polysemous, a type of non-ambiguity. Each modal has a singulary semantic representation from which the various modal interpretations are contextually derived. These interpretations depend on two classes of world, not linguistic, knowledge; as such, they need not be specified in the linguistic description of the language but will be accounted for in the pragmatics of the language.","Each modal is a univocal marker of probability. Together, they constitute a class of items that exhibits semantic gradience and manifest both the semantic and conversational characteristics typical of quantitative scales. Individually, the modal verbs are distinguished in part by different degrees of probability. However, several items can represent the same notion of probability (e.g., can, may, could and might all correspond to the concept of 'possibility'), so other features must distinguish among items that are apparent semantic equivalents. This analysis demonstrates that the concept of grounds and the semantics of the past marker are what differentiate parallel items. Computational Linguistics, Volume 12, Number 2, April-June 1986 169 The FINITE STRING Newsletter Abstracts of Current Literature The Pronoun and the Topic of Discourse: a Functional Perspective on Text Mary Ng En Tzu The University of Wisconsin - Madison Ph.D. 1984, 108 pages Language, Linguistics DAI V46(02), SecA, pp413 University Microfilms Order Number ADG85-O0839. 8508 Temporal Inferences in Computational Linguistic Information Processing Klaus Karl Obermeier The Ohio State University Ph.D. 1984, 190 pages Language, Linguistics DAI V46(01), SecA, pp142 University Microfilms Order Number ADG85-04060. 8507","The notion of grounds is a semantic concept that signals that the speaker has significant, relevant information available to him that serves as testimony to the validity of his statement. Such knowledge accords the speaker the status of \"expert\". This analysis also shows that the past marker has retained its function of signalling various types of remoteness in the case of the modal verbs. The past-marked modals (e.g., could) are therefore not independent semantic units (as often claimed) but are composite semantic entities.","Probability, grounds, and the semantics of the past marker explain the great variety of modal interpretation, the difficulty in isolating a specific range of applicability of the modals and the contrastive behavior of seemingly parallel items.","It is from these univocal semantic representations that the interpretations that have been attributed to the modal verbs are contextually derived. This study demonstrates that these interpretations are reasonable and predictable from the univocal semantic representations proposed. This thesis proposes to show through the presentation of 'logical proofs' that the formal, surface concatenation of pronouns in a discourse/text is patterned to reveal the underlying continuity in a particular semantic category - the topic of discourse. In so doing, a new approach for finding the topic in discourse/text is presented - specifically through the analysis of the pattern of pronominalization in discourse/text.","In the course of this thesis, we will show hoW a whole theory on discourse/text grammar must take into account both its formal and structural level of representation, as well as its functional and semantic level of representation. This is explicated in terms of its organizational structure - its structural relationships (cohesive and staging relationships) and its semantic content (the topic).","Then, through investigating the nature and the 'capabilities' of the pronoun, one of many phoric referential forms in discourse/text, it will be demonstrated that it is both a referential element that explicates the 'wholeness' in text organization as well as a language form that represents an underlying functional and semantic category - the topic.","Finally, through a look at the actual use of the pronoun in discourse/text, it will be argued that the surface concatenation of pronominals in a discourse/text is patterned to reveal the underlying continuity in its topic. This dissertation aims for an integration of insights gained from linguistic, psychological, and Artificial Intelligence based research to provide a pragmatic theory and mental model of how natural language processing and temporal inferences can be explained within the framework of computational information processing. A pragmatic theory focuses on the information from the context (e.g., co=text, text-type, and audience) to explain linguistic behavior. A mental model provides an internal representation of the state of affairs that are are described in a given sentence.","The objectives of this research are twofold, whereby the computer program is meant to be a particular implementation of a general natural language processing system which could be used for a variety of domains. The first objective is to show how an integration of linguistic and extra-linguistic knowledge achieves a form of comprehension, where comprehension is characterized as a conversion of information based on knowledge from one representation into another. The second objective is to show that such a procedural approach is a basis for an event-based theory for temporal information processing. 170 Computational Linguistics, Volume 12, Number 2, April-June 1986 The FINITE STRING Newsletter Abstracts of Current Literature Computer-Assisted Instruction for Learners of English as a Second Language: Design and Field-Test of a Program to Review Word Order in a Sentence Andrew Martin Sofia Georgetown University Ph.D. 1984, 250 pages Language, Linguistics DAI V46(05), SecA, pp1266 University Microfilms Order Number ADG85-15690. 8511 The Nature of Subjects, Topics and Agents: a Cognitive Explanation Jeanne Hillechiena Van Oosten University of California, Berkeley Ph.D. 1984, 343 pages Language, Linguistics DAI V46(04), SecA, pp969 University Microfilms Order Number ADG85-13027. 8510","The computer program (implemented in ELISP on EFRL on a DEC 20/60) performs the following tasks: (1) parse a text from a medical journal while using linguistic and extra-linguistic knowledge; (2) map the parsed linguistic structure into an event-representation; (3) draw temporal and factual inferences within the domain of liver diseases; (4) create and update a data base containing the pertinent information about a patient. The best existing computer-assisted language learning programs take advantage of the computer's motivational sound, graphics, and game-like atmosphere. They are highly interactive and individualized, and keep track of scores and grades. However, these programs use little or no linguistic knowledge in responding to a student.","In this dissertation I have written a linguistically sensitive microcomputer program in BASIC. The intelligence in the program simulates an English instructor's knowledge of sentence structure over a precisely defined range of words. The program presents a list of words to the student, who arranges them to create a sentence. Words may be used more than once. The program's expert knowledge is stored in the form of lists. The program parses the sentence by matching it against the lists, indicating correct noun phrases, prepositional phrases, and relative clauses to the student. It also shows errors of omission and adjective order. It then gives an overall judgement on the grammatical correctness of the sentence, based on the word order.","The program offers collaborative guidance by judging the sentence, and advising the student. This linguistic component of the program adds a pedagogically significant dimension to answer processing. The student is free to experiment and review word order within a range of sentence structures. Field-testing indicates that students enjoy the freedom of composing their own sentences. Although the program accurately recognizes an almost infinite set of correct sentences, students requested a broader vocabulary to work with, and more available sentence structures. These results confirm the educational need for this type of program.","This dissertation describes the program and the results of field-testing. The first three chapters discuss the context, contributions, and evaluation of the program. The fourth chapter suggests future directions. The program is divided into separate modules which may be changed independently. They may also be extracted and applied to other problems involving specific sequences.","The last chapter is useful in adapting the program to meet new needs. It is a technical discussion of how the program operates, discussing and cross-referencing the variables for each module. The program listing for the Apple II microcomputer is included in the appendix. English subjects form a category that is both meaning-based and grammaticized. This position, demonstrated in this dissertation by an examination of the uses of seven constructions of English - existentials, it-clefts, two types of property-factoring, the Tough-Construction, the Patient-Subject Construction and passive - is a departure from the transformational tradition, Relational Grammar and Role-and-Reference Grammar. The transformational tradition holds that grammatical relations are to be defined purely structurally and syntactically; Relational Grammar holds that subjects are totally grammaticized (primitive), and Role-and-Reference Grammar maintains that subjects are totally meaning-based (derived from pragmatic and semantic notions) and not grammaticized at all.","The English subject is a category with prototype structure; the central members of the prototype are both prototypical agents and prototypical topics. Prototypical subjects are found in prototypical basic sentences Computational Linguistics, Volume 12, Number 2, April--June 1986 171 The FINITE STRING Newsletter Abstracts of Current Literature Representational Semantics Barry E. Brown The University of Rochester Ph.D. 1985, 239 pages Philosophy DAI V46(04), SecA, pp997 University Microfilms Order NUmber ADG85-11889. 8510 Learning to Understand Speech Sounds: a Theory and Model Gary L. Bradshaw Carnegie-Mellon University Ph.D. 1984, 123 pages DAI V46(05), SecB, pp1720 Psychology, Experimental University Microfilms Order Number ADG85-13677. 8511 (defined in this dissertation); each special construction, as tested by the seven special constructions mentioned above, has an individual meaning for its subject which deviates from but is motivated by the meaning of the subjects of basic sentences. (Both basic sentences and special constructions can themselves have deviations from the prototype.)","I also redefine the notion of \"topic\" in terms of a prototype containing such pragmatic notions as focus of attention, old information, focus of interest, perspective, salience, and aboutness. Prototypical topics have all these features; nonprototypical topics do not have all of them. These pragmatic features have semantic reflexes like referentiality and definiteness (and a strong tendency towards agency). I distinguish between discourse and sentence topics; the former are in the normal case layered within a single discourse. I also distinguish between superordinate, basic-level, and subordinate topics. Superordinate topics are semantic structures like schemata or scenes; basic-level topics are individual participants in a superordinate topic (typically human); and subordinate topics are aspects or parts of basic-level topics. Since basic-level topics are prototypical sentence topics, it is this level that has typically been identified as \"topic\" in the past.","I corroborate and expand the notion that the category agent also has prototype structure, and propose, on the basis of the special constructions investigated, that the notion of \"primary responsibility\" rather than intentionality or volition is the central characteristic of the category of agents. I present and defend an original semantic theory which assigns representatives to expressions, in addition to referents. The theory is nominalistic - i.e., it avoids reference to possible worlds and other abstract entities - and yet is strong enough, I claim, to serve as a theory of meaning. More precisely, it provides a means for interpret-ing, in a nominalistically acceptable manner, the non-extensional linguistic contexts in which the \"meanings\" of expressions are supposed to play a semantic role.","The intuitive ancestry of the theory can be traced back to an analysis of meaning first proposed by Nelson Goodman and later expanded by Roll Eberle. I construct a precise formal semantics which embodies the basic ideas of these earlier proposals, and apply this theory to the interpretation of a language which contains one primitive non-extensional predicate - about. In addition, I furnish a rigorous axiomatic treatment of this language, and demonstrate formally the soundness and completeness of this axiomatic theory relative to the semantics. Theories of human speech perception have emphasized the role of innate feature detectors in speech comprehension. Empirical evidence suggests that theories based on specialized feature detectors are wrong, and that human listeners improve in their ability to identify the basic sounds of their language. A learning theory of speech perception is proposed to account for the evidence. To test the theory, a computer simulation, NEXUS, was created. When provided with a simple vocabulary of the names of the letters of the alphabet, NEXUS was able to create descriptions of all words, identify the similarities between words, and simplify the network by eliminating redundant information. The resulting word network was used to classify new instances of speech. Performance of NEXUS was superior to that of a state-of-the-art speech recognition system, Cicada, on both speakers tested. NEXUS serves as a sufficiency proof of the learning theory, although the lack of detailed learning data precludes stronger comparisons with human performance. NEXUS also demonstrates that learning heuristics can be very useful in building computer systems to perform perceptual tasks, such as speech recognition or vision. These heuristics do not require 172 Computational Linguistics, Volume 12, Number 2, April-June 1986 The FINITE STRING Newsletter Abstracts of Current Literature Referential Choice and Focus of Attention in Narratives Ellen Palmer Francik Stanford University Ph.D. 1985, 110 pages Psychology, Experimental DAI V46(04), SecB, pp1363 University Microfilms Order Number ADG85-11308. 8510 Interlingual Semantic Facilitation: Evidence for a Common Representational System in the Bilingual Lexicon Mario Rey Florida Atlantic University M.A. 1985, 104 pages Psychology, Experimental MAI V23(03), pp405 University Microfilms Order Number ADG13-25149. 8510 statistical assumptions about the form of the distribution underlying the data. English forms for definite reference range from descriptions (the artist) to names (Columbine) to reduced forms carrying little information (such as the pronoun she). Speakers can choose more or less explicit forms depending upon what other information in the discourse will help the addressee to determine the referent. One important source of information is focus, the importance of a referent at a particular moment in the discourse.","This experiment used simple narratives to compare speakers' use of three sources of information: discourse organization, lexical cues (e.g. gender), and recency of mention. Forty university students were each given a story (a 10-page picture booklet) with two equally active characters, and were asked to tell it from one character's viewpoint. That is, speakers organized their narratives around a protagonist. Since protagonists are important for story planning, comprehension, and recall, they should tend to become focal and be referred to with reduced forms (pronouns). The availability of gender information was manipulated by creating two versions of each story, one with same-sex characters and one with opposite-sex Characters. Recency of mention was determined from each speaker's narrative.","Speakers mark the protagonist's importance in several ways: they mention the protagonist more often, foreground the protagonist syntactically, and pronominalize the protagonist more readily in a variety of circumstances. Recency of mention also leads speakers to pronominalize the character. But when recency is taken into account, speakers still pronominalize protagonists more frequently. The cue of gender (when the two characters are of opposite sexes) is most useful when speakers switch reference from one character to another. Yet though gender cues make pronouns unambiguous for both characters, protagonists are still pronominalized more often. The impact of protagonist status on speakers' refer-ences and the subtle interaction of these three sources of information lead us to reject several simple models of referential choice. (No abstract.) Computational Linguistics, Volume 12, Number 2, April-June 1986 173"]}]}