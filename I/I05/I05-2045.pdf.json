{"sections":[{"title":"Unsupervised Feature Selection for Relation Extraction Jinxiu Chen","paragraphs":["1"]},{"title":"Donghong Ji","paragraphs":["1"]},{"title":"Chew Lim Tan","paragraphs":["2"]},{"title":"Zhengyu Niu","paragraphs":["1 1"]},{"title":"Institute for Infocomm Research","paragraphs":["2"]},{"title":"Department of Computer Science 21 Heng Mui Keng Terrace National University of Singapore 119613 Singapore 117543 Singapore {jinxiu,dhji,zniu}@i2r.a-star.edu.sg tancl@comp.nus.edu.sg Abstract","paragraphs":["This paper presents an unsupervised relation extraction algorithm, which in-duces relations between entity pairs by grouping them into a “natural” number of clusters based on the similarity of their contexts. Stability-based criterion is used to automatically estimate the number of clusters. For removing noisy feature words in clustering procedure, feature selection is conducted by optimizing a trace based criterion subject to some constraint in an unsupervised manner. After relation clustering procedure, we employ a discriminative category matching (DCM) to find typical and discriminative words to represent different relations. Experimental results show the effectiveness of our algorithm."]},{"title":"1 Introduction","paragraphs":["Relation extraction is the task of finding relationships between two entities from text contents. There has been considerable work on supervised learning of relation patterns, using corpora which have been annotated to indicate the information to be extracted (e.g. (Califf and Mooney, 1999; Zelenko et al., 2002)). A range of extraction models have been used, including both symbolic rules and statistical rules such as HMMs or Kernels. These methods have been particularly successful in some specific domains. However, manually tagging of large amounts of training data is very time-consuming; furthermore, it is difficult for one extraction system to be ported across different domains.","Due to the limitation of supervised methods, some weakly supervised (or semi-supervised) approaches have been suggested (Brin, 1998; Eugene and Luis, 2000; Sudo et al., 2003). One common characteristic of these algorithms is that they need to pre-define some initial seeds for any particular relation, then bootstrap from the seeds to acquire the relation. However, it is not easy to select representative seeds for obtaining good results.","Hasegawa, et al. put forward an unsupervised approach for relation extraction from large text corpora (Hasegawa et al., 2004). First, they adopted a hierarchical clustering method to cluster the contexts of entity pairs. Second, after context clustering, they selected the most frequent words in the contexts to represent the relation that holds between the entities. However, the approach exists its limitation. Firstly, the similarity threshold for the clusters, like the appropriate number of clusters, is somewhat difficult to pre-defined. Secondly, the representative words selected by frequency tends to obscure the clusters.","For solving the above problems, we present a novel unsupervised method based on model order selection and discriminative label identification. For achieving model order identification, stability-based criterion is used to automatically estimate the number of clusters. For removing noisy feature words in clustering procedure, feature selection is conducted by optimizing a trace based criterion subject to some constraint in an"]},{"title":"262","paragraphs":["unsupervised manner. Furthermore, after relation clustering, we employ a discriminative category matching (DCM) to find typical and discriminative words to represent different relations types."]},{"title":"2 Proposed Method","paragraphs":["Feature selection for relation extraction is the task of finding important contextual words which will help to discriminate relation types. Unlike supervised learning, where class labels can guide feature search, in unsupervised learning, it is expected to define a criterion to assess the importance of the feature subsets. Due to the interplay between feature selection and clustering solution, we should define an objective function to evaluate both feature subset and model order.","In this paper, the model selection capability is achieved by resampling based stability analysis, which has been successfully applied to several unsupervised learning problems (e.g. (Levine and Domany, 2001), (Lange et al., 2002), (Roth and Lange et al., 2003), (Niu et al., 2004)). We extend the cluster validation strategy further to address both feature selection and model order identification.","Table 1 presents our model selection algorithm. The objective function MFk,k is relevant with both feature subset and model order. Clustering solution that is stable against resampling will give rise to a local optimum of MFk,k, which indicates both important feature subset and the true cluster number. 2.1 Entropy-based Feature Ranking Let P = {p1, p2, ...pN } be a set of local context vectors of co-occurrences of entity pair E1 and E2. Here, the context includes the words occurring between, before and after the entity pair. Let W = {w1, w2, ..., wM } represent all the words occurred in P . To select a subset of important features from W , words are first ranked according to their importance on clustering. The importance can be assessed by the entropy criterion. Entropy-based feature ranking is based on the assumption that a feature is irrelevant if the presence of it obscures the separability of data set(Dash et al., 2000).","We assume pn, 1 ≤ n ≤ N , lies in feature space W , and the dimension of feature space is Table 1: Model Selection Algorithm for Relation Extraction Input: Corpus D tagged with Entities(E1, E2); Output: Feature subset and Model Order (number of relation types);","1. Collect the contexts of all entity pairs in the document corpus D, namely P ;","2. Rank features using entropy-based method described in section 2.1;","3. Set the range (Kl, Kh) for the possible number of relation clusters;","4. Set estimated model order k = Kl;","5. Conduct feature selection using the algorithm presented in section 2.2;","6. Record F̂k,k and the score of the merit of both of them, namely MF,k;","7. If k < Kh, k = k + 1, go to step 5; otherwise, go to Step 7;","8. Select k and feature subset F̂k which maximizes the score of the merit MF,k; M . Then the similarity between i-th data point pi and j-th data point pj is given by the equa-tion: Si,j = exp(−α ∗ Di,j), where Di,j is the Euclidean distance between pi and pj, and α is a positive constant, its value is − ln 0.5","D",", where D is the average distance among the data points. Then the entropy of data set P with N data points is defined as: E = − N ∑ i=1 N ∑ j=1 (Si,j log Si,j + (1 − Si,j) log(1 − Si,j))","(1) For ranking of features, the importance of each word I(wk) is defined as entropy of the data after discarding feature wk. It is calculated in this way: remove each word in turn from the feature space and calculate E of the data in the new feature space using the Equation 1. Based on the observation that a feature is the least important if the removal of it results in minimum E, we can obtain the rankings of the features.","2.2 Feature Subset Selection and Model Order Identification In this paper, for each specified cluster number, firstly we perform K-means clustering analysis on each feature subset and adopts a scattering criterion ”Invariant Criterion” to select an optimal feature subset F from the feature subset space. Here, trace(P −1","W PB) is used to compare the cluster quality for different feature subsets 1",", which","1","trace(P −1","W PB) is trace of a matrix which is the sum of its diagonal elements. PW is the within-cluster scatter"]},{"title":"263","paragraphs":["Table 2: Unsupervised Algorithm for Evaluation of Fea-","ture Subset and Model Order Function: criterion(F, k, P, q) Input: feature subset F , cluster number k, entity pairs set P , and sampling frequency q; Output: the score of the merit of F and k;","1. With the cluster number k as input, perform k-means clustering analysis on pairs set P F",";","2. Construct connectivity matrix CF,k based on above clustering solution on full pairs set P F",";","3. Use a random predictor ρk to assign uniformly drawn labels to each entity pair in P F",";","4. Construct connectivity matrix CF,ρ","k based on above clustering solution on full pairs set P F",";","5. Construct q sub sets of the full pairs set, by randomly selecting αN of the N original pairs, 0 ≤ α ≤ 1;","6. For each sub set, perform the clustering analysis in Step 2, 3, 4, and result Cμ","F,k, Cμ","F,ρ","k ;","7. Compute MF,k to evaluate the merit of k using Equation 3;","8. Return MF,k; measures the ratio of between-cluster to within-cluster scatter. The higher the trace(P −1","W PB), the higher the cluster quality.","To improve searching efficiency, features are first ranked according to their importance. Assume Wr = {f1, ..., fM } is the sorted feature list. The task of searching can be seen in the feature subset space: {(f1, ..., fk),1 ≤ k ≤ M }.","Then the selected feature subset F is evaluated with the cluster number using the objective function, which can be formulated as: F̂k = arg maxF ⊆Wr {criterion(F, k)}, subject to coverage(P, F ) ≥ τ 2",". Here, F̂k is the optimal feature subset, F and k are the feature subset and the value of cluster number under evaluation, and the criterion is set up based on resampling-based stability, as Table 2 shows.","Let P μ","be a subset sampled from full entity pairs set P with size α|P | (α set as 0.9 in this paper.), C(Cμ",") be |P | × |P |(|P μ","| × |P μ","|) connectivity matrix based on the clustering results on P (P μ","). Each entry cij(cμ","ij) of C(Cμ",") is calculated in the following: if the entity pair pi ∈ P (P μ","), pj ∈ P (P μ",") belong to the same cluster, then cij(cμ","ij) equals 1, else 0. Then the stability is de-matrix as: PW =","∑c j=1","∑","X","i∈χ","j (Xi − mj)(Xj − mj)t","and PB is the between-cluster scatter matrix as: PB =∑c j=1(mj − m)(mj − m)t",", where m is the total mean vec-","tor and mj is the mean vector for jth","cluster and (Xj −mj)t","is the matrix transpose of the column vector (Xj − mj). 2 let coverage(P, F ) be the coverage rate of the feature","set F with respect to P . In practice, we set τ = 0.9. fined in Equation 2:","M(Cμ , C) =","∑","i,j 1{Cμ i,j = Ci,j = 1, pi ∈ P μ",", pj ∈ P μ","}","∑","i,j 1{Ci,j = 1, pi ∈ P μ",", pj ∈ P μ","}","(2)","Intuitively, M (Cμ",", C) denotes the consistency between the clustering results on Cμ","and C. The assumption is that if the cluster number k is actually the “natural” number of relation types, then clustering results on subsets P μ","generated by sampling should be similar to the clustering result on full entity pair set P . Obviously, the above function satisfies 0 ≤ M ≤ 1.","It is noticed that M (Cμ",", C) tends to decrease when increasing the value of k. Therefore for avoiding the bias that small value of k is to be selected as cluster number, we use the cluster validity of a random predictor ρk to normalize M (Cμ",", C). The random predictor ρk achieved the stability value by assigning uniformly drawn labels to objects, that is, splitting the data into k clusters randomly. Furthermore, for each k, we tried q times. So, in the step 7 of the algorithm of Table 2, the objective function M (C μ F,k, CF,k) can be normalized as equations 3:","Mnorm F,k = 1 q q ∑ i=1","M(Cμ i F,k, CF,k) − 1 q q ∑ i=1","M(Cμ i F,ρ k , CF,ρ","k )","(3) Normalizing M (Cμ",", C) by the stability of the random predictor can yield values independent of k.","After the number of optimal clusters and the feature subset has been chosen, we adopted the K-means algorithm for the clustering phase. The output of context clustering is a set of context clusters, each of them is supposed to denote one relation type. 2.3 Discriminative Feature identification For labelling each relation type, we use DCM (discriminative category matching) scheme to identify discriminative label, which is also used in document classification (Gabriel et al., 2002) and weights the importance of a feature based on their distribution. In this scheme, a feature is not important if the feature appears in many clusters and is evenly distributed in these clusters, otherwise it will be assigned higher importance.","To weight a feature fi within a category, we take into account the following information:"]},{"title":"264","paragraphs":["Table 3: Three domains of entity pairs: frequency distribution for different relation types PER-ORG # of pairs:786 ORG-GPE # of pairs:262 ORG-ORG # of pairs:580 Relation types Percentage Relation types Percentage Relation types Percentage Management 36.39% Based-In 46.56% Member 27.76% General-staff 29.90% Located 35.11% Subsidiary 19.83% Member 19.34% Member 11.07% Part-Of 18.79% Owner 4.45% Affiliate-Partner 3.44% Affiliate-Partner 17.93% Located 3.28% Part-Of 2.29% Owner 8.79% Client 1.91% Owner 1.53% Client 2.59% Other 1.91% Management 2.59% Affiliate-Partner 1.53% Other 1.21% Founder 0.76% Other 0.52%","• The relative importance of fi within a cluster is defined as: W Ci,k = log 2(pf","i,k+1) log","2(N","k+1) , where pfi,k is the","number of those entity pairs which contain feature fi","in cluster k. Nk is the total number of term pairs in","cluster k.","• The relative importance of fi across clusters is given by: CCi = log","N·max k∈C","i{WC","i,k}","∑N k=1 WC","i,k · 1 log N , where Ci is the set of clusters which contain feature fi. N is the total number of clusters.","Here, W Ci,k and CCi are designed to capture both local information within a cluster and global information about the feature distribution across clusters respectively. Combining both W Ci,k and CCi we define the weight Wi,k of fi in cluster k as: Wi,k =","W C2 i,k·CC2","i","√","W C2 i,k+CC2","i ·","√ 2, 0 ≤ Wi,k ≤ 1."]},{"title":"3 Experiments and Results 3.1 Data","paragraphs":["We constructed three subsets for domains PER-ORG, ORG-GPE and ORG-ORG respectively from ACE corpus3","The details of these subsets are given in Table 3, which are broken down by different relation types. To verify our proposed method, we only extracted those pairs of entity mentions which have been tagged relation types. And the relation type tags were used as ground truth classes to evaluate. 3.2 Evaluation method for clustering result Since there was no relation type tags for each cluster in our clustering results, we adopted a permutation procedure to assign different relation type tags to only min(|EC|,|T C|) clusters, where |EC| is the estimated number of clusters, and |T C| is the number of ground truth classes","3","http://www.ldc.upenn.edu/Projects/ACE/ (relation types). This procedure aims to find an one-to-one mapping function Ω from the T C to EC. To perform the mapping, we construct a contingency table T , where each entry ti,j gives the number of the instances that belong to both the i-th cluster and j-th ground truth class. Then the mapping procedure can be formulated as: Ω̂ = arg maxΩ","∑|T C|","j=1 tΩ(j),j, where Ω(j) is the index of the estimated cluster associated with the j-th class. Given the result of one-to-one mapping, we can define the evaluation measure as follows: Accuracy(P ) = ∑ j tˆ","Ω(j),j ∑ i,j ti,j . Intuitively, it reflects","the accuracy of the clustering result. 3.3 Evaluation method for relation labelling For evaluation of the relation labeling, we need to explore the relatedness between the identified labels and the pre-defined relation names. To do this, we use one information-content based measure (Lin, 1997), which is provided in Wordnet-Similarity package (Pedersen et al., 2004) to evaluate the similarity between two concepts in Wordnet. Intuitively, the relatedness between two concepts in Wordnet is captured by the information content of their lowest common subsumer (lcs) and the information content of the two concepts themselves , which can be formalized as follows: Relatednesslin(c1, c2) =","2×IC(lcs(c1,c2))","IC(c1)+IC(c2) . This measure depends upon the corpus to estimate information content. We carried out the experiments using the British National Corpus (BNC) as the source of information content. 3.4 Experiments and Results For comparison of the effect of the outer and within contexts of entity pairs, we used five dif-"]},{"title":"265","paragraphs":["Table 4: Automatically determined the number of relation types using different feature ranking methods.","Domain Context","Window","Size # of real relation types Model Order Baseline Model Order with χ2 Model Order with Freq","Model Or-","der with","Entropy","PER-ORG 0-5-0 9 7 7 7 7 2-5-2 9 8 6 7 8 0-10-0 9 8 6 8 8 2-10-2 9 6 7 6 8 5-10-5 9 5 5 6 7","ORG-GPE 0-5-0 6 3 3 3 4 2-5-2 6 2 3 4 4 0-10-0 6 6 4 5 6 2-10-2 6 4 3 4 5 5-10-5 6 2 3 3 3","ORG-ORG 0-5-0 9 7 7 7 7 2-5-2 9 7 5 6 7 0-10-0 9 9 8 9 9 2-10-2 9 6 6 6 7 5-10-5 9 8 5 7 9 ferent settings of context window size (WINpre-WINmid-WINpost) for each domain.","Table 4 shows the results of model order identification without feature selection (Baseline) and with feature selection based on different feature ranking criterion( χ2",", Frequency and Entropy). The results show that the model order identification algorithm with feature selection based on entropy achieve best results: estimate cluster numbers which are very close to the true values. In addition, we can find that with the context setting, 0-10-0, the estimated number of the clusters is equal or close to the ground truth value. It demonstrates that the intervening words less than 10 are appropriate features to reflect the structure behind the contexts, while the intervening words less than 5 are not enough to infer the structure. For the contextual words beyond (before or after) the entities, they tend to be noisy features for the relation estimation, as can be seen that the performance deteriorates when taking them into consideration, especially for the case without feature selection.","Table 5 gives a comparison of the average accuracy over five different context window size settings for different clustering settings. For each domain, we conducted five clustering procedures: Hasegawa’s method, RLBaseline, RLF Sχ2, RLF SF req and RLF SEntropy. For Hasegawa’s method (Hasegawa et al., 2004), we set the cluster number to be identical with the number of ground truth classes. For RLBaseline, we use the estimated cluster number to cluster contexts without feature selection. For RLF Sχ2,RLF SF req and RLF SEntropy, we use the selected feature subset and the estimated cluster number to cluster the contexts, where the feature subset comes from χ2",", frequency and entropy criterion respectively. Comparing the average accuracy of these clustering methods, we can find that the performance of feature selection methods is better than or comparable with the baseline system without feature selection. Furthermore, it is noted that RLF SEntropy achieves the highest average accuracy in three domains, which indicates that entropy based feature pre-ranking provides useful heuristic information for the selection of important feature subset.","Table 6 gives the automatically estimated labels for relation types for the domain PER-ORG. We select two features as labels of each relation type according to their DCM scores and calculate the average (and maximum) relatedness between our selected labels (E) and the predefined labels (H). Following the same strategy, we also extracted relation labels (T) from the ground truth classes and provided the relatedness between T and H. From the column of relatedness (E-H), we can see that it is not easy to find the hand-tagged relation labels exactly, furthermore, the identified labels from the ground-truth classes are either not always comparable to the pre-defined labels in most cases (T-H). The reason may be that the pre-defined relation names tend to be some abstract labels over the features, e.g., ‘management’ vs. ‘president’,"]},{"title":"266","paragraphs":["Table 5: Performance of the clustering algorithms over three domains: the average accuracy over 5 different context window size. Domain Hasegawa’s","method RLBaseline RLFSχ2 RLFSFreq RLFSEntropy PER-ORG 32.4% 34.3% 33.9% 36.6% 41.3% ORG-GPE 43.7% 47.4% 47.1% 48.4% 50.6% ORG-ORG 26.5% 36.2% 36.0% 38.7% 42.4% Table 6: Relation Labelling using DCM strategy for the domain PER-ORG. Here, (T) denotes the identified relation labels from ground truth classes. (E) is the identified relation labels from our estimated clusters. ‘Ave (T-H)’ denotes the average relatedness between (T) and (H). ‘Max (T-H)’ denotes the maximum relatedness between (T) and (H). Hand-tagged Label (H) Identified Label (T) Identified Label (E) Ave (T-H) Max (T-H) Ave (E-H) Max (E-H) Ave (E-T)","Max","(E-T) management head,president president,control 0.3703 0.4515 0.3148 0.3406 0.7443 1.0000 general-staff work,fire work,charge 0.6254 0.7823 0.6411 0.7823 0.6900 1.0000 member join,communist become,join 0.394 0.4519 0.1681 0.3360 0.3366 1.0000 owner bond,bought belong,house 0.1351 0.2702 0.0804 0.1608 0.2489 0.4978 located appear,include lobby,appear 0.0000 0.0000 0.1606 0.3213 0.2500 1.0000 client hire,reader bought,consult 0.4378 0.8755 0.0000 0.0000 0.1417 0.5666 affiliate-partner affiliate,associate assist,affiliate 0.9118 1.0000 0.5000 1.0000 0.5000 1.0000 founder form,found invest,set 0.1516 0.3048 0.3437 0.6875 0.4376 0.6932 ‘head’ or ‘control’; ‘member’ vs. ‘join’, ‘be-come’, etc., while the abstract words and the features are located far away in Wordnet. Table 6 also lists the relatedness between (E) and (T). We can see that the labels are comparable by their maximum relatedness(E-T)."]},{"title":"4 Conclusion and Future work","paragraphs":["In this paper, we presented an unsupervised approach for relation extraction from corpus. The advantages of the proposed approach includes that it doesn’t need any manual labelling of the relation instances, it can identify an important feature subset and the number of the context clusters automatically, and it can avoid extracting those common words as characterization of relations."]},{"title":"References","paragraphs":["Mary Elaine Califf and Raymond J.Mooney. 1999. Relational Learning of Pattern-Match Rules for Information Extraction, AAAI99.","Sergey Brin. 1998. Extracting patterns and relations from world wide web. In Proc. of WebDB’98. pages 172-183.","Kiyoshi Sudo, Satoshi Sekine and Ralph Grishman. 2003. An Improved Extraction Pattern Representation Model for Automatic IE Pattern Acquisition. Proceedings of ACL 2003; Sapporo, Japan.","Eugene Agichtein and Luis Gravano. 2000. Snowball: Extracting Relations from large Plain-Text Collections, In Proc. of the 5th","ACM International Conference on Digital Libraries (ACMDL’00).","Takaaki Hasegawa, Satoshi Sekine and Ralph Grishman. 2004. Discovering Relations among Named Entities from Large Corpora, ACL2004. Barcelona, Spain.","Dmitry Zelenko, Chinatsu Aone and Anthony Richardella. 2002. Kernel Methods for Relation Extraction, EMNLP2002. Philadelphia.","Lange,T., Braun,M.,Roth, V., and Buhmann,J.M.. 2002. Stability-Based Model Selection, Advances in Neural Information Processing Systems 15.","Levine,E. and Domany,E.. 2001. Resampling Method for Unsupervised Estimation of Cluster Calidity, Neural Computation, Vol.13, 2573-2593.","Zhengyu Niu, Donghong Ji and Chew Lim Tan. 2004. Document Clustering Based on Cluster Validation, CIKM’04. November 8-13, 2004, Washington, DC, USA.","Volker Roth and Tilman Lange. 2003. Feature Selection in Clustering Problems, NIPS2003 workshop.","Manoranjan Dash and Huan Liu. 2000. Feature Selection for Clustering, Proceedings of Pacific-Asia Conference on Knowledge Discovery and Data Mining.","Gabriel Pui Cheong Fung, Jeffrey Xu Yu and Hongjun Lu. 2002. Discriminative Category Matching: Efficient Text Classification for Huge Document Collections, ICDM2002. December 09-12, 2002, Japan.","D.Lin. 1997. Using syntactic dependency as a local context to resolve word sense ambiguity. In Proceedings of the 35th Annual Meeting of ACL,. Madrid, July 1997.","Ted Pedersen, Siddharth Patwardhan and Jason Michelizzi. 2004. WordNet::Similarity-Measuring the Relatedness of Concepts, AAAI2004."]},{"title":"267","paragraphs":[]}]}