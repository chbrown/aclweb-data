{"sections":[{"title":"PLSI Utilization for Automatic Thesaurus Construction","paragraphs":["Masato Hagiwara, Yasuhiro Ogawa, and Katsuhiko Toyama Graduate School of Information Science, Nagoya University,","Furo-cho, Chikusa-ku, Nagoya, JAPAN 464-8603","{hagiwara, yasuhiro, toyama}@kl.i.is.nagoya-u.ac.jp Abstract. When acquiring synonyms from large corpora, it is important to deal not only with such surface information as the context of the words but also their latent semantics. This paper describes how to utilize a latent semantic model PLSI to acquire synonyms automatically from large corpora. PLSI has been shown to achieve a better performance than conventional methods such as tf·idf and LSI, making it applicable to automatic thesaurus construction. Also, various PLSI techniques have been shown to be effective including: (1) use of Skew Divergence as a distance/similarity measure; (2) removal of words with low frequencies, and (3) multiple executions of PLSI and integration of the results."]},{"title":"1 Introduction","paragraphs":["Thesauri, dictionaries in which words are arranged according to meaning, are one of the most useful linguistic sources, having a broad range of applications, such as information retrieval and natural language understanding. Various thesauri have been constructed so far, including Wo rd Net [6] and Bunruigoihyo [14]. Conventional thesauri, however, have largely been compiled by groups of language experts, making the construction and maintenance cost very high. It is also difficult to build a domain-specific thesaurus flexibly. Thus it is necessary to construct thesauri automatically using computers.","Many studies have been done for automatic thesaurus construction. In doing so, synonym acquisition is one of the most important techniques, although a thesaurus generally includes other relationships than synonyms (e.g., hypernyms and hyponyms). To acquire synonyms automatically, contextual features of words, such as co-occurrence and modification are extracted from large corpora and often used. Hindle [7], for example, extracted verb-noun relationships of subjects/objects and their predicates from a corpus and proposed a method to calculate similarity of two words based on their mutual information. Although methods based on such raw co-occurrences are simple yet effective, in a naive implementation some problems arise: namely, noises and sparseness. Being a collection of raw linguistic data, a corpus generally contains meaningless information, i.e., noises. Also, co-occurrence data extracted from corpora are often very sparse, making them inappropriate for similarity calculation, which is also known as the “zero frequency problem.” Therefore, not only surface information but also latent semantics should be considered when acquiring synonyms from large corpora.","Several latent semantic models have been proposed so far, mainly for information retrieval and document indexing. The most commonly used and prominent ones are Latent Semantic Indexing (LSI) [5] and Probabilistic LSI (PLSI) [8]. LSI is a geometric R. Dale et al. (Eds.): IJCNLP 2005, LNAI 3651, pp. 334–345, 2005. c⃝ Springer-Verlag Berlin Heidelberg 2005 PLSI Utilization for Automatic Thesaurus Construction 335 model based on the vector space model. It utilizes singular value decomposition of the co-occurrence matrix, an operation similar to principal component analysis, to automatically extract major components that contribute to the indexing of documents. It can alleviate the noise and sparseness problems by a dimensionality reduction operation, that is, by removing components with low contributions to the indexing. However, the model lacks firm, theoretical basis [9] and the optimality of inverse document frequency (idf) metric, which is commonly used to weight elements, has yet to be shown [13].","On the contrary, PLSI, proposed by Hofmann [8], is a probabilistic version of LSI, where it is formalized that documents and terms co-occur through a latent variable. PLSI puts no assumptions on distributions of documents or terms, while LSI performs optimal model fitting, assuming that documents and terms are under Gaussian distribution [9]. Moreover, ad hoc weighting such as idf is not necessary for PLSI, although it is for LSI, and it is shown experimentally to outperform the former model [8].","This study applies the PLSI model to the automatic acquisition of synonyms by estimating each word’s latent meanings. First, a number of verb-noun pairs were collected from a large corpus using heuristic rules. This operation is based on the assumption that semantically similar words share similar contexts, which was also employed in Hindle’s work [7] and has been shown to be considerably plausible. Secondly, the co-occurrences obtained in this way were fit into the PLSI model, and the probability distribution of latent classes was calculated for each noun. Finally, similarity for each pair of nouns can be calculated by measuring the distances or the similarity between two probability distributions using an appropriate distance/similarity measure. We then evaluated and discussed the results using two evaluation criteria, discrimination rates and scores.","This paper also discusses basic techniques when applying PLSI to the automatic acquisition of synonyms. In particular, the following are discussed from methodological and experimental views: (1) choice of distance/similarity measures between probability distributions; (2) filtering words according to their frequencies of occurrence; and (3) multiple executions of PLSI and integration of the results.","This paper is organized as follows: in Sect. 2 a brief explanation of the PLSI model and calculation is provided, and Sect. 3 outlines our approach. Sect. 4 shows the results of comparative experiments and basic techniques. Sect. 5 concludes this paper."]},{"title":"2 The PLSI Model","paragraphs":["This section provides a brief explanation of the PLSI model in information retrieval settings. The PLSI model, which is based on the aspect model, assumes that document d and term w co-occur through latent class z, as shown in Fig. 1 (a).","The co-occurrence probability of documents and terms is given by: P (d, w)=P (d) ∑ z P (z|d)P (w|z). (1) Note that this model can be equivalently rewritten as P (d, w)=∑ z P (z)P (d|z)P (w|z), (2) 336 M. Hagiwara, Y. Ogawa, and K. Toyama d z wP(d) P(z|d) P(w|z) d z w P(z) P(d|z) P(w|z) (a) (b) Fig. 1. PLSI model asymmetric (a) and symmetric (b) parameterization corpus","co-occurrence (v, c, n) (eat, obj, lunch) (eat, obj, hamburger) (have, obj, breakfast)"," PLSI model z n P(v) P(z|v) P(n|z)","latent class noun (v,c) verb+case   lunch              breakfast ),( 21 wwsim latent class distribution similarity calculation P(z|n) P(z|n) Fig. 2. Outline of our approach whose graphical model representation is shown in Fig. 1 (b). This is a symmetric parameterization with respect to documents and terms. The latter parameterization is used in the experiment section because of its simple implementation.","Theoretically, probabilities P (d), P (z|d), P (w|z) are determined by maximum likelihood estimation, that is, by maximizing the likelihood of document term co-occurrence: L = ∑ d,w N (d, w)logP (d, w), (3) where N (d, w) is the frequency document d and term w co-occur.","While the co-occurrence of document d and term w in the corpora can be observed directly, the contribution of latent class z cannot be directly seen in this model. For the maximum likelihood estimation of this model, the EM algorithm [1], which is used for the estimation of systems with unobserved (latent) data, is used. The EM algorithm performs the estimation iteratively, similar to the steepest descent method."]},{"title":"3 Approach","paragraphs":["The original PLSI model, as described above, deals with co-occurrences of documents and terms, but it can also be applied to verbs and nouns in the corpora. In this way, latent PLSI Utilization for Automatic Thesaurus Construction 337 John gave presents to his colleagues. John gave presents to his colleagues. NNP NP VBD VP NNS TO NNS NP PRP$ PPNP S [John] gave [presents] to [his colleagues] NP S VP NPVPVBD PPVPVBD NPTO PP (a) Original sentence (b) Parsing result (c) Dependency structure (d) Co-occurrence extraction from dependencies John gave NP S VP (“give”, subj, “John”) gave presents VBDVP NP (“give”, obj, “present”) gave to his colleagues TO PP NP (“give”, “to”, “colleague”) PPVPVBD n NP S VP v (v, subj, n) (v, obj, n) (v, prep, n) but (v, obj, n) when the verb is “be” + past participle. n NP VP baseVP v n NP PP prep PP* VP baseVP v Rule 1 Rule 2 Rule 3 (e) Rules for co-occurrence identification Fig. 3. Co-occurrence extraction class distribution, which can be interpreted as latent “meaning” corresponding to each noun, is obtained. Semantically similar words are then obtained accordingly, because words with similar meaning have similar distributions. Fig. 2 outlines our approach, and the following subsections provide the details. 3.1 Extraction of Co-occurrence We adopt triples (v, c, n) extracted from the corpora as co-occurrences fit into the PLSI model, where v, c,andn represent a verb, case/preposition, and a noun, respectively. The relationships between nouns and verbs, expressed by c, include case relation (subject and object) as well as what we call here “prepositional relation,” that is, a co-occurrence through a preposition. Take the following sentence for example: John gave presents to his colleagues.","First, the phrase structure (Fig. 3(b)) is obtained by parsing the original sentence (Fig. 3(a)). The resulting tree is then used to derive the dependency structure (Fig. 3(c)), using Collins’ method [4]. Note that dependencies in baseNPs (i.e., noun phrases that do not contain NPs as their child constituents, shown as the groups of words enclosed by square brackets in Fig. 3(c)), are ignored. Also, we introduced baseVPs, that is, sequences of verbs 1",", modals (MD), or adverbs (RB), of which the last word must be a verb. BaseVPs simplify the handling of sequences of verbs such as “might not be” 1","Ones expressed as VB, VBD, VBG, VBN, VBP, and VBZ by the Penn Treebank POS tag set","[15]. 338 M. Hagiwara, Y. Ogawa, and K. Toyama and “is always complaining.” The last word of a baseVP represents the entire baseVP to which it belongs. That is, all the dependencies directed to words in a baseVP are redirected to the last verb of the baseVP.","Finally, co-occurrences are extracted and identified by matching the dependency patterns and the heuristic rules for extraction, which are all listed in Fig. 3 (e). For example, since the label of the dependency “John” →“gave” is “NP S VP”, the noun “John” is identified as the subject of the verb “gave” (Fig. 3(d)). Likewise, the dependencies “presents”→“gave” and “his colleagues”→“to”→“gave” are identified as a verb-object relation and prepositional relation through “to”.","A simple experiment was conducted to test the effectiveness of this extraction method, using the corpus and the parser mentioned in the experiment section. Co-occurrence extraction was performed for the 50 sentences randomly extracted from the corpus, and precision and recall turned out to be 88.6% and 78.1%, respectively. In this context, precision is more important than recall because of the substantial size of the corpus, and some of the extraction errors result from parsing error caused by the parser, whose precision is claimed to be around 90% [2]. Therefore, we conclude that this method and its performance are sufficient for our purpose. 3.2 Applying PLSI to Extracted Co-occurence Data While the PLSI model deals with dyadic data (d, w) of document d and term w,thecooccurrences obtained by our method are triples (v, c, n) of a verb v, a case/preposition c, and a noun n. To convert these triples into dyadic data (pairs), verb v and case/ preposition c are paired as (v, c) and considered a new “virtual” verb v. This enables it to handle the triples as the co-occurrence (v, n) of verb v and noun n to which the PLSI model becomes applicable. Pairing verb v and case/preposition c also has a benefit that such phrasal verbs as “look for” or “get to” can be naturally treated as a single verb.","After the application of PLSI, we obtain probabilities P (z),P(v|z),andP (n|z). Using Bayes theorem, we then obtain P (z|n), which corresponds to the latent class distribution for each noun. In other words, distribution P (z|n) represents the features of meaning possessed by noun n. Therefore, we can calculate the similarity between nouns n1 and n2 by measuring the distance or similarity between the two corresponding distribution, P (z|n1) and P (z|n2), using an appropriate measure. The choice of measure affects the synonym acquisition results and experiments on comparison of distance/similarity measures are detailed in Sect. 4.3."]},{"title":"4 Experiments","paragraphs":["This section includes the results of comparison experiments and those on the basic PLSI techniques. 4.1 Conditions The automatic acquisition of synonyms was conducted according to the method described in Sect. 3, using WordBank (190,000 sentences, 5 million words) [3] as a cor-PLSI Utilization for Automatic Thesaurus Construction 339 pus. Charniak’s parser [2] was used for parsing and TreeTagger [16] for stemming. A total of 702,879 co-occurrences was extracted by the method described in Sect. 3.1.","When using EM algorithm to implement PLSI, overfitting, which aggravates the performance of the resultant language model, occasionally occurs. We employed the tempered EM (TEM) [8] algorithm, instead of a naive one, to avoid this problem. TEM algorithm is closely related to the deterministic annealing EM (DAEM) algorithm [17], and helps avoid local extrema by introducing inverse temperature β. The parameter was set to β =0.86, considering the results of the preliminary experiments.","As the similarity/distance measure and frequency threshold tf , Skew Divergence (α = 0.99) and tf =15were employed in the following experiments in response to the results from the experiments described in Sects. 4.3 and 4.5. Also, because estimation by EM algorithm is started from the random parameters and consequently the PLSI results change every time it is executed, the average performance of the three executions was recorded, except in Sect. 4.6. 4.2 Measures for Performance The following two measures, discrimination rate and scores, were employed for the evaluation of automated synonym acquisition. Discrimination rate Discrimination rate, originally proposed by Kojima et al. [10], is the rate (percentage) of pairs (w1,w2) whose degree of association between two words w1,w2 is successfully discriminated by the similarity derived by a method. Kojima et al. dealt with three-level discrimination of a pair of words, that is, highly related (synonyms or nearly synonymous), moderately related (a certain degree of association), and unrelated (irrelevant). However, we omitted the moderately related level and limited the discrimination to two-level: high or none, because of the high cost of preparing a test set that consists of moderately related pairs.","The calculation of discrimination rate follows these steps: first, two test sets, one of which consists of highly related word pairs and the other of unrelated ones, were prepared, as shown in Fig. 4. The similarity between w1 and w2 is then calculated for each pair (w1,w2) in both test sets via the method under evaluation, and the pair is labeled highly related when similarity exceeds a given threshold t and unrelated when the similarity is lower than t. The number of pairs labeled highly related in the highly related test set and unrelated in the unrelated test set are denoted na and nb, respectively. The discrimination rate is then given by: 1 2 ( na Na + nb Nb ) , (4) where Na and Nb are the numbers of pairs in highly related and unrelated test sets, respectively. Since the discrimination rate changes depending on threshold t,maximum value is adopted by varying t.","We created a highly related test set using the synonyms in WordNet [6]. Pairs in a unrelated test set were prepared by first choosing two words randomly and then confirmed by hand whether the consisting two words are truly irrelevant. The numbers of pairs in the highly and unrelated test sets are 383 and 1,124, respectively. 340 M. Hagiwara, Y. Ogawa, and K. Toyama (answer, reply) (phone, telephone) (sign, signal) (concern, worry) (animal, coffee) (him, technology) (track, vote) (path, youth)","...","... highly related unrelated Fig. 4. Test-sets for discrimination rate calculation","base word: computer","rank synonym sim sim∗","rel.(p) p · sim∗ 1 equipment 0.6 0.3 B(0.5) 0.15 2 machine 0.4 0.2 A(1.0) 0.20 3 Internet 0.4 0.2 B(0.5) 0.10 4 spray 0.4 0.2 C(0.0) 0.00 5 PC 0.2 0.1 A(1.0) 0.10","total 2.0 1.0 0.55 Table 5. Procedure for score calculation Scores We propose a score which is similar to precision used for information retrieval evaluation, but different in that it considers the similarity of words. This extension is based on the notion that the more accurately the degrees of similarity are assigned to the results of synonym acquisition, the higher the score values should be.","Described in the following, along with Table 5, is the procedure for score calculation. Table 5 shows the obtained synonyms and their similarity with respect to the base word “computer.” Results are obtained by calculating the similarity between the base word and each noun, and ranking all the nouns in descending order of similarity sim. The highest five are used for calculations in this example.","The range of similarity varies based on such factors as the employed distance/ similarity measure, which unfavorably affects the score value. To avoid this, the values of similarity are normalized such that their sum equals one, as shown in the column sim∗","in Fig. 5. Next, the relevance of each synonym to the base word is checked and evaluated manually, giving them three-level grades: highly related (A), moderately related (B), and unrelated (C), and relevance scores p =1.0, 0.5, 0.0 are assigned for each grade, respectively (“rel.(p)” column in Fig. 5). Finally, each relevance score p is multiplied by corresponding similarity sim∗",", and the products (the p · sim∗","column in Fig. 5) are totaled and then multiplied by 100 to obtain a score, which is 55 in this case. In actual experiments, thirty words chosen randomly were adopted as base words, and the average of the scores of all base words was employed. Although this example considers only the top five words for simplicity, the top twenty words were used for evaluation in the following experiments. 4.3 Distance/Similarity Measures of Probability Distribution The choice of distance measure between two latent class distributions P (z|ni), P (z|nj) affects the performance of synonym acquisition. Here we focus on the following seven distance/similarity measures and compare their performance. – Kullback-Leibler (KL) divergence [12]: KL(p || q)= ∑","x p(x)log(p(x)/q(x)) – Jensen-Shannon (JS) divergence [12]: JS(p, q)={KL(p || m)+KL(q || m)}/2,","m =(p + q)/2 – Skew Divergence [11]: sα(p || q)=KL(p || αq +(1− α)p) – Euclidean distance: euc(p, q)=||p − q|| – L1 distance: L1(p, q)= ∑ x |p(x) − q(x)| PLSI Utilization for Automatic Thesaurus Construction 341 – Inner product: p · q = ∑","x p(x)q(x)","– Cosine: cos(p, q)=(p · q)/||p|| · ||q||","KL divergence is widely used for measuring the distance between two probability distributions. However, it has such disadvantages as asymmetricity and zero frequency problem, that is, if there exists x such that p(x) ̸=0,q(x)=0, the distance is not defined. JS divergence, in contrast, is considered the symmetrized KL divergence and has some favorable properties: it is bounded [12] and does not cause the zero frequency problem. Skew Divergence, which has recently been receiving attention, has also solved the zero frequency problem by introducing parameter α and mixing the two distributions. It has shown that Skew Divergence achieves better performance than the other measures [11]. The other measures commonly used for calculation of the similarity/distance of two vectors, namely Euclidean distance, L1 distance (also called Manhattan Distance), inner product, and cosine, are also included for comparison.","Notice that the first five measures are of distance (the more similar p and q,thelower value), whereas the others, inner product and cosine, are of similarity (the more similar p and q, the higher value). We converted distance measure D to a similarity measure sim by the following expression: sim(p, q)=exp{−λD(p, q)}, (5) inspired by Mochihashi and Matsumoto [13]. Parameter λ was determined in such a way that the average of sim doesn’t change with respect to D. Because KL divergence and Skew Divergence are asymmetric, the average of both directions (e.g. for KL divergence, 1","2 (KL(p||q)+KL(q||p))) is employed for the evaluation.","Figure 6 shows the performance (discrimination rate and score) for each measure. It can be seen that Skew Divergence with parameter α =0.99 shows the highest performance of the seven, with a slight difference to JS divergence. These results, along with several studies, also show the superiority of Skew Divergence. In contrast, measures for vectors such as Euclidean distance achieved relatively poor performance compared to those for probability distributions. 4.4 Word Filtering by Frequencies It may be difficult to estimate the latent class distributions for words with low frequencies because of a lack of sufficient data. These words can be noises that may degregate the results of synonym acquisition. Therefore, we consider removing such words with low frequencies before the execution of PLSI improves the performance. More specifically, we introduced threshold tf on the frequency, and removed nouns ni such that∑","j tf i","j <tf and verbs vj such that","∑","i tf i","j <tf from the extracted co-occurrences. The discrimination rate change on varying threshold tf was measured and shown in Fig. 7 for d = 100, 200, and 300. In every case, the rate increases with a moderate increase of tf , which shows the effectiveness of the removal of low frequency words. We consequently fixed tf =15in other experiments, although this value may depend on the corpus size in use. 342 M. Hagiwara, Y. Ogawa, and K. Toyama 50.0% 55.0% 60.0% 65.0% 70.0% 75.0% 80.0%","K","L J","S s ( 0 . 9 9 ) s ( 0 . 9 5 ) s ( 0 . 9 0 ) E u c .  d i s t . L 1  d i s t . c o s i n e i n n e r  p r o d .","distance/similarity measure d i s c ri m i n a t i o n ra t e (% ) 5.07.09.011.013.015.017.019.021.023.025.0 sc o r e disc. rate score Fig. 6. Performances of distance/similarity measures 70.0% 71.0% 72.0% 73.0% 74.0% 75.0% 76.0% 77.0% 78.0%","0 5 10 15 20 25 30 threshold d i s c r i m i nat i on r a t e  ( % ) d=100 d=200 d=300 Fig. 7. Discrimination rate measured by varying threshold tf 4.5 Comparison Experiments with Conventional Methods Here the performances of PLSI and the following conventional methods are compared. In the following, N and M denote the numbers of nouns and verbs, respectively.","– tf: The number of co-occurrence tfi","j of noun ni and verb vj is used directly for similarity calculation. The corresponding vector ni to noun ni is given by:","ni = t","[tf i 1 tf i","2 ... tf i","M ]. (6) – tf·idf: The vectors given by tf method are weighted by idf. That is,","n∗","i = t [tf i 1 · idf1 tf i","2 · idf2 ... tf i","M · idfM ], (7) where idfj is given by idfj =","log(N/dfj) maxk log(N/dfk) , (8)","using dfj, the number of distinct nouns that co-occur with verb vj. – tf+LSI: A co-occurrence matrix X is created using vectors ni defined by tf: X =[n1 n2 ... nN ], (9)","to which LSI is applied.","– tf·idf+LSI : A co-occurrence matrix X∗","is created using vectors n∗","i defined by tf·idf:","X∗","=[n∗ 1 n∗","2 ... n∗","N ], (10) to which LSI is applied.","– Hindle’s method: The method described in [7] is used. Whereas he deals only with subjects and objects as verb-noun co-occurrence, we used all the kinds of co-occurrence mentioned in Sect. 3.1, including prepositional relations. PLSI Utilization for Automatic Thesaurus Construction 343 60.0% 62.0% 64.0% 66.0% 68.0% 70.0% 72.0% 74.0% 76.0% 78.0% number of latent classes di s c r i mi na t i on r a t e ( % ) PLSI tf idf+LSI tf+LSI tf idf tf Hindletf tf idf Hindle 0 500 1000 5.0 7.0 9.0 11.0 13.0 15.0 17.0 19.0 21.0 23.0 number of latent classes sc o r e Hindle tf idf tf 0 500 1000 Fig. 8. Performances of PLSI and conventional methods","The values of discrimination rate and scores are calculated for PLSI as well as the methods described above, and the results are shown in Fig. 8. Because the number of latent classes d must be given beforehand for PLSI and LSI, the performances of the latent semantic models are measured varying d from50to1,000withastepof50.The cosine measure is used for the similarity calculation of tf, tf·idf, tf+LSI, and tf·idf+LSI.","The results reveal that the highest discrimination rate is achieved by PLSI, with the latent class number of approximately 100, although LSI overtakes with an increase of d. As for the scores, the performance of PLSI stays on top for almost all the values of d, strongly suggesting the superiority of PLSI over the conventional method, especially when d is small, which is often.","The performances of tf and tf+LSI, which are not weighted by idf, are consistently low regardless of the value of d. PLSI and LSI distinctly behave with respect to d, especially in the discrimination rate, whose cause require examination and discussion. 4.6 Integration of PLSI Results In maximum likelihood estimation by EM algorithm, the initial parameters are set to values chosen randomly, and likelihood is increased by an iterative process. Therefore, the results are generally local extrema, not global, and they vary every execution, which is unfavorable. To solve this problem, we propose to execute PLSI several times and integrate the results to obtain a single one.","To achieve this, PLSI is executed several times for the same co-occurrence data obtained via the method described in Sect. 3.1. This yields N values of similarity sim1(ni,nj), ..., simN (ni,nj) for each noun pair (ni,nj). These values are integrated using one of the following four schemes to obtain a single value of similarity sim(ni,nj).","– arithmetic mean: sim(ni,nj)= 1 N","∑N k=1 simk(ni,nj) – geometric mean:sim(ni,nj)= N√∏N","k=1 simk(ni,nj) – maximum: sim(ni,nj)=maxk simk(ni,nj) – minimum: sim(ni,nj)=mink simk(ni,nj) 344 M. Hagiwara, Y. Ogawa, and K. Toyama 70.0% 71.0% 72.0% 73.0% 74.0% 75.0% 76.0% 77.0% 78.0% d i s c ri m i n a t i o n ra t e (% ) 15.0 17.0 19.0 21.0 23.0 25.0 sco r e disc. rate score","1 23arith. mean geo. mean max min before integration after integration Fig. 9. Integration result for N =3 71.0% 72.0% 73.0% 74.0% 75.0% 76.0% 77.0%","12345678910 N dis c r i minatio n r a t e ( % ) 15.0 17.0 19.0 21.0 23.0 25.0 27.0 29.0 31.0 sco r e integrated (disc. score) maximum (disc. score) average (disc. rate) integrated (score) maximum (score) average (score) Fig. 10. Integration results varying N","Integration results are shown in Fig. 9, where the three sets of performance on the left are the results of single PLSI executions, i.e., before integration. On the right are the results after integration by the four schemes. It can be observed that integration improves the performance. More specifically, the results after integration are as good or better than any of the previous ones, except when using the minimum as a scheme.","An additional experiment was conducted that varied N from 1 to 10 to confirm that such performance improvement is always achieved by integration. Results are shown in Fig. 10, which includes the average and maximum of the N PLSI results (unintegrated) as well as the performance after integration using arithmetic average as the scheme. The results show that the integration consistently improves the performance for all 2 ≤ N ≤ 10. An increase of the integration performance was observed for N ≤ 5, whereas increases in the average and maximum of the unintegrated results were relatively low. It is also seen that using N>5 has less effect for integration."]},{"title":"5Conclusion","paragraphs":["In this study, automatic synonym acquisition was performed using a latent semantic model PLSI by estimating the latent class distribution for each noun. For this purpose, co-occurrences of verbs and nouns extracted from a large corpus were utilized. Discrimination rates and scores were used to evaluate the current method, and it was found that PLSI outperformed such conventional methods as tf·idf and LSI. These results make PLSI applicable for automatic thesaurus construction. Moreover, the following techniques were found effective: (1) employing Skew Divergence as the distance/similarity measure between probability distributions; (2) removal of words with low frequencies, and (3) multiple executions of PLSI and integration of the results.","As future work, the automatic extraction of the hierarchical relationship of words also plays an important role in constructing thesauri, although only synonym relationships were extracted this time. Many studies have been conducted for this purpose, but extracted hyponymy/hypernymy relations must be integrated in the synonym relations to construct a single thesaurus based on tree structure. The characteristics of the latent class distributions obtained by the current method may also be used for this purpose. PLSI Utilization for Automatic Thesaurus Construction 345","In this study, similarity was calculated only for nouns, but one for verbs can be obtained using an identical method. This can be achieved by pairing noun n and case / preposition c of co-occurrence (v, c, n), not v and c as previously done, and executing PLSI for the dyadic data (v, (c, n)). By doing this, the latent class distributions for each verb v, and consequently the similarity between them, are obtained.","Moreover, although this study only deals with verb-noun co-occurrences, other information such as adjective-noun modifications or descriptions in dictionaries may be used and integrated. This will be an effective way to improve the performance of automatically constructed thesauri."]},{"title":"References","paragraphs":["1. Bilmes, J. 1997. A gentle tutorial on the EM algorithm and its application to parameter","estimation for gaussian mixture and hidden markov models. Technical Report ICSI-TR-97-","021, International Computer Science Institute (ICSI), Berkeley, CA. 2. Charniak, E. 2000. A maximum-entropy-inspired parser. NAACL 1, 132–139. 3. Collins. 2002. Collins Cobuild Major New Edition CD-ROM. HarperCollins Publishers. 4. Collins, M. 1996. A new statistical parser based on bigram lexical dependencies. Proc. of","34th ACL, 184–191. 5. Deerwester, S., et al. 1990. Indexing by Latent Semantic Analysis. Journal of the American","Society for Information Science, 41(6):391–407. 6. Fellbaum, C. 1998. WordNet: an electronic lexical database. MIT Press. 7. Hindle, D. 1990. Noun classification from predicate-argument structures. Proc. of the 28th","Annual Meeting of the ACL, 268–275. 8. Hofmann, T. 1999. Probabilistic Latent Semantic Indexing. Proc. of the 22nd International","Conference on Research and Development in Information Retrieval (SIGIR ’99), 50–57. 9. Hofmann, T. 2001. Unsupervised Learning by Probabilistic Latent Semantic Analysis. Ma-","chine Learning, 42:177–196. 10. Kojima, K., et. al. 2004. Existence and Application of Common Threshold of the Degree of","Association. Proc. of the Forum on Information Technology (FIT2004) F-003. 11. Lee, L. 2001. On the Effectiveness of the Skew Divergence for Statistical Language Analysis.","Artificial Intelligence and Statistics 2001, 65–72. 12. Lin, J. 1991. Divergence measures based on the shannon entropy. IEEE Transactions on","Information Theory, 37(1):140–151. 13. Mochihashi, D., Matsumoto, Y. 2002. Probabilistic Representation of Meanings. IPSJ SIG-","Notes Natural Language, 2002-NL-147:77–84. 14. The National Institute of Japanese Language. 2004. Bunruigoihyo. Dainippontosho. 15. Santorini, B. 1990. Part-of-Speech Tagging Guidelines for the Penn Treebank Project.","ftp://ftp.cis.upenn.edu/pub/treebank/doc/tagguide.ps.gz 16. Schmid, H. 1994. Probabilistic Part-of-Speech Tagging Using Decision Trees. Proc. of the","First International Conference on New Methods in Natural Language Processing (NemLap-","94), 44–49. 17. Ueda, N., Nakano, R. 1998. Deterministic annealing EM algorithm. Neural Networks,","11:271–282."]}]}