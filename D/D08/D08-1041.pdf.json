{"sections":[{"title":"","paragraphs":["Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 391–399, Honolulu, October 2008. c⃝2008 Association for Computational Linguistics"]},{"title":"When Harry Met Harri, and : Cross-lingual Name Spelling Normalization  Fei Huang , Ahmad Emami and Imed Zitouni","paragraphs":["IBM T. J. Watson Research Center","1101 Kitchawan Road Yorktown Heights, NY 10598","{huangfe, emami, izitouni}@us.ibm.com "]},{"title":"Abstract","paragraphs":["Foreign name translations typically include multiple spelling variants. These variants cause data sparseness problems, increase Out-of-Vocabulary (OOV) rate, and present challenges for machine translation, information extraction and other NLP tasks. This paper aims to identify name spelling variants in the target language using the source name as an anchor. Based on word-to-word translation and transliteration probabilities, as well as the string edit distance metric, target name translations with similar spellings are clustered. With this approach tens of thousands of high precision name translation spelling variants are extracted from sentence-aligned bilingual corpora. When these name spelling variants are applied to Machine Translation and Information Extraction tasks, improvements over strong baseline systems are observed in both cases."]},{"title":"1 Introduction","paragraphs":["Foreign names typically have multiple spelling","variants after translation, as seen in the","following examples: He confirmed that \"al-Kharroub province is at the top of our priorities.\" ...for the Socialist Progressive Party in upper Shuf and the Al-Kharrub region,... ...during his tour of a number of villages in the region of Al-Kharub,... ...Beirut and its suburbs and Iqlim al-Khurub,...     Such name spelling variants also frequently appear in other languages, such as (bushi) / (bushu) / (buxi) (for Bush) in Chinese, and (sbrngfyld) / (sbryngfyld) / (sbrynjfyld) (for Springfield) in Arabic.","These spelling variants present challenges for many NLP tasks, increasing vocabulary size and OOV rate, exacerbating the data sparseness problem and reducing the readability of MT output when different spelling variants are generated for the same name in one document. We address this problem by replacing each spelling variant with its corresponding canonical form. Such text normalization could potentially benefit many NLP tasks including information retrieval, information extraction, question answering, speech recognition and machine translation.","Research on name spelling variants has been studied mostly in Information Retrieval research, especially in query expansion and cross-lingual IR. Baghat and Hovy (2007) proposed two approaches for spelling variants generation, based on the letters-to-phonemes mapping and Soundex algorithm (Knuth 1973). Raghaven and Allan (2005) proposed several techniques to group names in ASR output and evaluated their effectiveness in spoken document retrieval (SDR). Both approaches use a named entity extraction system to automatically identify names. For multi-lingual name spelling variants, Linden (2005) proposed to use a general edit distance metric with a weighted FST to find technical term translations (which were referred to as “cross-lingual spelling variants”). These 391 variants are typically translated words with similar stems in another language. Toivonen and colleagues (2005) proposed a two-step fuzzy translation technique to solve similar problems. Al-Onaizan and Knight (2002), Huang (2003) and Ji and Grishman (2007) investigated the general name entity translation problem, especially in the context of machine translation.","This paper aims to identify mono-lingual name spelling variants using cross-lingual information. Instead of using a named entity tagger to identify name spelling variants, we treat names in one language as the anchor of spelling variants in another language. From sentence-aligned bilingual corpora we collect word co-occurrence statistics and calculate word translation1","probabilities. For each source word, we group its target translations into clusters according to string edit distances, then calculate the transliteration cost between the source word and each target translation cluster. Word pairs with small transliteration costs are considered as name translations, and the target cluster contains multiple spelling variants corresponding to the source name.","We apply this approach to extract name transliteration spelling variants from bilingual corpora. We obtained tens of thousands of high precision name translation pairs. We further apply these spelling variants to Machine Translation (MT) and Information Extraction (IE) tasks, and observed statistically significant improvement on the IE task, and close to oracle improvement on the MT task.","The rest of the paper is organized as follows. In section 2 we describe the technique to identify name spelling variants from bilingual data. In section 3 and 4 we address their application to MT and IE respectively. We present our experiment results and detailed analysis in section 5. Section 6 concludes this paper with future work."]},{"title":"2 Finding Name Translation Variants ","paragraphs":["1 In this paper, the translation cost measures the semantic difference between source and target names, which are estimated from their co-occurrence statistics. The transliteration cost measures their phonetic distance and are estimated based on a character transliteration model. Starting from sentence-aligned parallel data, we run HMM alignment (Vogel et. al. 1996 & Ge 2004) to obtain a word translation model. For each source word this model generates target candidate translations as well as their translation probabilities. A typical entry is shown in Table 1. It can be observed that the Arabic name’s translations include several English words with similar spellings, all of which are correct translations. However, because the lexical translation probabilities are distributed among these variants, none of them has the highest probability. As a result, the incorrect translation, iqlim, is assigned the highest probability and often selected in MT output. To fix this problem, it is desirable to identify and group these target spelling variants, convert them into a canonical form and merge their translation probabilities. | Alxrwb iqlim [0.22]","al-kharrub [0.16] al-kharub [0.11] overflew [0.09] junbulat [0.05] al-khurub [0.05] hours [0.04]","al-kharroub [0.03]  Table 1. English translations of a Romanized Arabic name Alxrwb with translation probabilities. For each source word in the word translation model, we cluster its target translations based on string edit distances using group average agglomerative clustering algorithm (Manning and Schütze, 2000). Initially each target word is a single word cluster. We calculate the average editing distance between any two clusters, and merge them if the distance is smaller than a certain threshold. This process repeats until the minimum distance between any two clusters is above a threshold. In the above example, al-kharrub, al-kharub, al-khurub and al-kharroub are grouped into a single cluster, and each of the ungrouped words remains in its single word cluster. Note that the source word may not be a name while its translations may still have similar spellings. An example is the Arabic word which is aligned to English words brief, briefing, briefed and briefings. To detect whether a source word is a name, we calculate the transliteration cost between the source word and its target translation cluster, which is defined as the average transliteration cost between the source word and each target word in the cluster. As 392 many names are translated based on their pronunciations, the source and target names have similar phonetic features and lower transliteration costs. Word pairs whose transliteration cost is lower than an empirically selected threshold are considered as name translations. 2.1 Name Transliteration Cost The transliteration cost measures the phonetic similarity between a source word and a target word. It is calculated based on the character transliteration model, which can be trained from bilingual name translation pairs. We segment the source and target names into characters, then run monotone 2","HMM alignment on the source and target character pairs. After the training, character transliteration probabilities can be estimated from the relevant frequencies of character alignments.","Suppose the source word f contains m characters, f1, f2, ..., fm, and the target word e contains n characters, e1, e2, ..., en. For j=1, 2,..., n, letter ej is aligned to character ja"]},{"title":"f","paragraphs":["according to the HMM aligner. Under the assumption that character alignments are independent, the word transliteration probability is calculated as "]},{"title":"∏","paragraphs":["= = n j aj jfepfeP 1 )|()|( (2.1) where )|( jaj fep is the character transliteration probability. Note that in the above configuration one target character can be aligned to only one source character, and one source character can be aligned to multiple target characters.","An example of the trained A-E character transliteration model is shown in Figure 1. The Arabic character is aligned with high probabilities to English letters with similar pronunciation. Because Arabic words typically omit vowels, English vowels are also aligned to Arabic characters. Given this model, the characters within a Romanized Arabic name and its English translation are aligned as shown in Figure 1.  2 As name are typically phonetically translated, the character alignment are often monotone. There is no crosslink in character alignments. 2.2 Transliteration Unit Selection The transliteration units are typically characters. The Arabic alphabet includes 32 characters, and the English alphbet includes 56 letters 3",". However, Chinese has about 4000 frequent characters. The imbalance of Chinese and English vocabulary sizes results in suboptimal transliteration model estimation. Each Chinese character also has a pinyin, the Romanized representation of its pronunciation. Segmenting the Chinese pinyin into sequence of Roman letters, we now have comparable vocabulary sizes for both Chinese and English. We build a pinyin transliteration model using Chinese-English name translation pairs, and compare its performance with a character transliteration model in Experiment section 5.1. ","h [0.44]","K [0.29]","k [0.21]","a [0.03]","u [0.015]","i [0.004] "," Figure 1. Example of the learned A-E character transliteration model with probabilities, and its application in the alignment between an Romanized Arabic name and an English translation."]},{"title":"3 Application to Machine Translation","paragraphs":["We applied the extracted name translation spelling variants to the machine translation task. Given the name spelling variants, we updated both the translation and the language model, adding variants’ probabilities to the canonical form. Our baseline MT decoder is a phrase-based decoder as described in (Al-Onaizan and Papineni 2006). Given a source sentence, the decoder tries to find the translation hypothesis with minimum translation cost, which is defined as the log-linear combination of different feature functions, such as translation model cost, language model cost, distortion cost and  3 Uppercase and lowercase letters plus some special symbols such as ‘_’, ‘-“. 393 sentence length cost. The translation cost includes word translation probability and phrase translation probability. 3.1 Updating The Translation Model Given target name spelling variants { m"]},{"title":"ttt ,...,,","paragraphs":["21 } for a source name s, here m"]},{"title":"ttt ,...,,","paragraphs":["21 are sorted based on their lexical translation probabilities,"]},{"title":").|(...)|()|(","paragraphs":["21"]},{"title":"stpstpstp","paragraphs":["m"]},{"title":"≥≥≥","paragraphs":["We select 1"]},{"title":"t","paragraphs":["as the canonical spelling, and merge other spellings’ translation probabilities with this one:"]},{"title":"∑","paragraphs":["="]},{"title":"=","paragraphs":["m j m"]},{"title":"stpstp","paragraphs":["11"]},{"title":").|()|(","paragraphs":["Other spelling variants get zero probability. Table 2 shows the updated word translation probabilities for “|Alxwrb”. Compared with Figure 1, the translation probabilities from several spelling variants are merged with the canonical form, al-kharrub, which now has the highest probability in the new model.  Table 2. English translations of an Arabic name |Alxrwb with the updated word translation model.  The phrase translation table includes source phrases, their target phrase translations and the frequencies of the bilingual phrase pair alignment. The phrase translation probabilities are calculated based on their alignment frequencies, which are collected from word aligned parallel data. To update the phrase translation table, for each phrase pair including a source name and its spelling variant in the target phrase, we replace the target name with its canonical spelling. After the mapping, two target phrases differing only in target names may end up with the identical target phrase, and their alignment frequencies are added. Phrase translation probabilities are re-estimated with the updated frequencies. 3.2 Updating The Language Model The machine translation decoder uses a language model as a measure of a well-formedness of the output sentence. Since the updated translation model can produce only the canonical form of a group of spelling variants, the language model should be updated in that all m-grams ("]},{"title":"Nm ≤≤1","paragraphs":[") that are spelling variants of each other are merged (and their counts added), resulting in the canonical form of the m-gram. Two m-grams are considered spelling variants of each other if they contain words i"]},{"title":"t","paragraphs":["1 , i"]},{"title":"t","paragraphs":["2 ( ii"]},{"title":"tt","paragraphs":["21"]},{"title":"≠","paragraphs":[") at the same position i in the m-gram, and that i"]},{"title":"t","paragraphs":["1 and i"]},{"title":"t","paragraphs":["2 belong to the same spelling variant group. An easy way to achieve this update is to replace every spelling variant in the original language model training data with its corresponding canonical form, and then build the language model again. However, since we do not want to replace words that are not names we need to have a mechanism for detecting names. For simplicity, in our experiments we assumed a word is a name if it is capitalized, and we replaced spelling variants with their canonical forms only for words that start with a capital letter."]},{"title":"4 Applying to Information Extraction","paragraphs":["Information extraction is a crucial step toward understanding a text, as it identifies the important conceptual objects in a discourse. We address here one important and basic task of information extraction: mention detection 4",": we call instances of textual references to objects mentions, which can be either named (e.g. John Smith), nominal (the president) or pronominal (e.g. he, she). For instance, in the sentence"]},{"title":"•","paragraphs":["President John Smith said he has no comments"]},{"title":".","paragraphs":["there are two mentions: John Smith and he. Similar to many classical NLP tasks, we formulate the mention detection problem as a classification problem, by assigning to each token in the text a label, indicating whether it starts a specific mention, is inside a specific mention, or is outside any mentions. Good  4 We adopt here the ACE (NIST 2007) nomenclature. | Alxwrb","al-kharrub [0.35] iqlim [0.22]","al-kharub [0.0] overflew [0.09] junbulat [0.05]","al-khurub [0.0] hours [0.04]","al-kharroub [0.0]  394 performance in many natural language processing tasks has been shown to depend heavily on integrating many sources of information (Florian et al. 2007). We select an exponential classifier, the Maximum Entropy (MaxEnt henceforth) classifier that can integrate arbitrary types of information and make a classification decision by aggregating all information available for a given classification (Berger et al. 1996). In this paper, the MaxEnt model is trained using the sequential conditional generalized iterative scaling (SCGIS) technique (Goodman, 2002), and it uses a Gaussian prior for regularization (Chen and Rosenfeld, 2000). In ACE, there are seven possible mention types: person, organization, location, facility, geopolitical entity (GPE), weapon, and vehicle. Experiments are run on Arabic and English. Our baseline system achieved very competitive result among systems participating in the ACE 2007 evaluation. It uses a large range of features, including lexical, syntactic, and the output of other information extraction models. These features were described in (Zitouni and Florian, 2008 & Florian et al. 2007), and are not discussed here. In this paper we focus on examining the effectiveness of name spelling variants in improving mention detection systems. We add a new feature that for each token x i to process we fire its canonical form (class label) C(xi ) , representative of name spelling variants of x i . This name spelling variant feature is also used in conjunction with the lexical (e.g., words and morphs in a 3-word window, prefixes and suffixes of length up to 4, stems in a 4-word window for Arabic) and syntactic (POS tags, text chunks) features."]},{"title":"5 Experiments 5.1 Evaluating the precision of name spelling variants","paragraphs":["We extracted Arabic-English and English-Arabic name translation variants from sentence-aligned parallel corpora released by LDC. The accuracy of the extracted name translation spelling variants are judged by proficient Arabic and Chinese speakers. The Arabic-English parallel corpora include 5.6M sentence pairs, 845K unique Arabic words and 403K unique English words. We trained a word translation model by running HMM alignment on the parallel data, grouped target translation with similar spellings and computed the average transliteration cost between the Arabic word and each English word in the translation clusters according to Formula 2.1. We sorted the name translation groups according to their transliteration costs, and selected 300 samples at different ranking position for evaluation (20 samples at each ranking position). The quality of the name translation variants are judged as follows: for each candidate name translation group"]},{"title":"}|,...,,{","paragraphs":["21"]},{"title":"sttt","paragraphs":["m , if the source word s is a name and all the target spelling variants are correct translations, it gets a credit of 1. If s is not a name, the credit is 0. If s is a name but only part of the target spelling variants are correct, it gets partial credit n/m, where n is the number of correct target translations. We evaluate only the precision of the extracted spelling variants 5",". As seen in Figure 2, the precision of the top 22K A-E name translations is 96.9%. Among them 98.5% of the Arabic words are names. The precision gets lower and lower when more non-name Arabic words are included. On average, each Arabic name has 2.47 English spelling variants, although there are some names with more than 10 spelling variants. Switching the source and target languages, we obtained English-Arabic name spelling variants, i.e., one English name with multiple Arabic spellings. As seen in Figure 3, top 20K E-A name pairs are obtained with a precision above 87.9%, and each English name has 3.3 Arabic spellings on average. Table 3 shows some A-E and E-A name spelling variants, where Arabic words are represented in their Romanized form. We conduct a similar experiment on the Chinese-English language pair, extracting Chinese-English and English-Chinese name spelling variants from 8.7M Chinese-English sentence pairs. After word segmentation, the Chinese vocabulary size is 1.5M words, and English vocabulary size is 1.4M words. With the  5 Evaluating recall requires one to manually look through the space of all possible transliterations (hundreds of thousands of entries), which is impractical. 395 Chinese pinyin transliteration model, we extract 64K C-E name spelling variants with 93.6% precision. Figure 4 also shows the precision curve of the Chinese character transliteration model. On average the pinyin transliteration model has about 6% higher precision than the character transliteration model. The pinyin transliteration model is particularly better on the tail of the curve, extracting more C-E transliteration variants. Figure 5 shows the precision curve for E-C name spelling variants, where 20K name pairs are extracted using letter-to-character transliteration model, and obtaining a precision of 74.3%.","Table 4 shows some C-E and E-C name spelling variants. We observed errors due to word segmentation. For example, the last two Chinese words corresponding to “drenica” have additional Chinese characters, meaning “drenica region” and “drenica river”. Similarly for tenet, the last two Chinese words also have segmentation errors due to missing or spurious characters. Note that in the C-E spelling variants, the source word “ ” has 14 spelling variants. Judge solely from the spelling, it is hard to tell whether they are the same person name with different spellings."]},{"title":"5.2 Experiments on Machine Translation","paragraphs":["We apply the Arabic-English name spelling variants on the machine translation task. Our baseline system is trained with 5.6M Arabic-English sentence pairs, the same training data used to extract A-E spelling variants. The language model is a modified Kneser-Ney 5-gram model trained on roughly 3.5 billion words. After pruning (using count cutoffs), it contains a total of 935 million N-grams. We updated the translation models and the language model with the name spelling variant class. Table 5 shows a Romanized Arabic sentence, the translation output from the baseline system and the output from the updated models. In the baseline system output, the Arabic name “Alxrwb” was incorrectly translated into “regional”. This error was fixed in the updated model, where both translation and language models assign higher probabilities to the correct translation “al-kharroub” after spelling variant normalization.       ","               "," Figure 2. Arabic-English name spelling variants precision curve (Precision of evaluation sample at different ranking positions. The larger square indicates the cutoff point).     ","               r a n k i n g p i n y i n","c","h","a","r"," Figure 4. Chinese-English name spelling variants precision curve.     ","                Figure 3. English-Arabic name spelling variants precision curve.     ","              "," Figure 5. English-Chinese name spelling variants precision curve. 396 Source Alm&tmr AlAwl lAqlym Alxrwb AlErby AlmqAwm Reference the first conference of the Arab resistance in Iqlim Kharoub Baseline the first conference of the Arab regional resistance Updated model first conference of the Al-Kharrub the Arab resistance  Table 5. English translation output with the baseline MT system and the system with updated models   BLEU","r1n4 TER Baseline 0.2714 51.66 Baseline+ULM+UTM 0.2718 51.46 Ref. Normalization 0.2724 51.40 Table 6. MT scores with updated TM and LM We also evaluated the updated MT models on a MT test set. The test set includes 70 documents selected from GALE 2007 Development set. It contains 42 newswire documents and 28 weblog and newsgroup documents. There are 669 sentences with 16.3K Arabic words in the test data. MT results are evaluated against one reference human translation using BLEU (Papineni et. al. 2001) and TER (Snover et. al. 2006) scores. The results using the baseline decoder and the updated models are shown in Table 6. Applying the updated language model (ULM) and the translation model (UTM) lead to a small reduction in TER. After we apply similar name spelling normalization on the reference translation, we observed some additional improvements. Overall, the BLEU score is increased by 0.1 BLEU point and TER is reduced by 0.26. Although the significance of correct name translation can not be fully represented by  Table 3. Arabic-English and English-Arabic name spelling variant examples. Italic words represent different persons with similar spelling names. ","Lang. Pair Source Name Target Spelling Variants Alxmyny khomeini al-khomeini al-khomeni khomeni khomeyni khamenei khameneh'i krwby karroubi karrubi krobi karubi karoubi kroubi Arabic-English gbryAl gabriel gabrielle gabrial ghobrial ghybrial cirebon syrybwn syrbwn syrbn kyrybwn bsyrybwn bsyrwbwn mbinda mbyndA mbndA mbydA AmbyndA AmbAndA mbynydA English-Arabic","nguyen njwyn ngwyn ngwyyn ngyyn Angwyn nygwyyn nygwyn wnjwyn njwyyn nyjyn bnjwyn wngyyn ngwyAn njyn nykwyn  Table 4. Chinese-English and English-Chinese name spelling variant examples with pinyin for Chinese characters. Italic words represent errors due to word segmentation.","Lang. Pair Source Name Target Spelling Variants  (yan/duo/wei/ci/ji) endovitsky jendovitski yendovitski endovitski  (si/te/fan/ni) stefani steffani stephani stefanni stefania Chinese-English  (wei/er/man) woermann wellman welman woellmann wohrmann wormann velman wollmann wehrmann verman woehrmann wellmann welmann wermann","tenet (te/ni/te) (te/nei/te) (tai/nei/te) (te/nai/te) (te/nai/te) (te/nei/te/yu) (te/nei)","drenica (de/lei/ni/cha) (de/lei/ni/ka) (te/lei/ni/cha) (te/lei/ni/cha) (de/lei/ni/cha/qu) (de/lei/ni/cha/he) English-Chinese","ahmedabad (ai/ha/mai/da/ba/de) (ai/a/mai/da/ba/de) (ai/ha/mo/de/ba/de) (a/ha/mai/da/ba/de) 397 BLEU and TER scores 6",", we still want to understand the reason of the relatively small improvement. After some error analysis, we found that in the testset only 2.5% of Arabic words are names with English spelling variants. Among them, 73% name spelling errors can be corrected with the translation spelling variants obtained in section 5.1. Because the MT system is trained on the same bilingual data from which the name spelling variants are extracted, some of these Arabic names are already correctly translated in the baseline system. So the room of improvement is small. We did an oracle experiment, manually correcting the name translation errors in the first 10 documents (89 sentences with 2545 words). With only 6 name translation errors corrected, this reduced the TER from 48.83 to 48.65.","5.2 Experiments on Information Extraction Mention detection system experiments are conducted on the ACE 2007 data sets in Arabic and English. Since the evaluation test set is not publicly available, we have split the publicly available training corpus into an 85%/15% data split. To facilitate future comparisons with work presented here, and to simulate a realistic scenario, the splits are created based on article dates: the test data is selected as the latest 15% of the data in chronological order. This way, the documents in the training and test data sets do not overlap in time, and the content of the test data is more recent than the training data. For English we use 499 documents for training and 100 documents for testing, while for Arabic we use 323 documents for training and 56 documents for testing. English and Arabic mention detection systems are using a large range of features, including lexical (e.g., words and morphs in a 3-word window, prefixes and suffixes of length up to 4, stems in a 4-word window for Arabic), syntactic (POS tags, text chunks), and the output of other information extraction models. These features were described in (Zitouni and Florian, 2008 & Florian et al. 2007) with more details. Our goal here is to investigate the effectiveness of name  6 These scores treat information bearing words, like names, the same as any other words, like punctuations. spelling variants information in improving mention detection system performance. Baseline Baseline+NSV P R F P R F English 84.4 80.6 82.4 84.6 80.9 82.7 Arabic 84.3 79.0 81.6 84.4 79.1 81.7 Table 7: Performance of English and Arabic mention detection systems without (Baseline) and with (Baseline+NSV) the use of name spelling variants. Performance is presented in terms of Precision (P), Recall (R), and F-measure (F). Results in Table 7 show that the use of name spelling variants (NSV) improves mention detection systems performance, especially for English; an interesting improvement is obtained in recall – which is to be expected, given the method –, but also in precision, leading to systems with better performance in terms of F-measure (82.4 vs. 82.7). This improvement in performance is statistically significant according to the stratified bootstrap re-sampling approach (Noreen 1989). This approach is used in the named entity recognition shared task of CoNLL-2002 7",". However, the small improvement obtained for Arabic is not statistically significant based on the approach described earlier. One hypothesis is that Arabic name spelling variants are not rich enough and that a better tuning of the alignment score is required to improve precision."]},{"title":"6 Conclusion","paragraphs":["We proposed a cross-lingual name spelling variants extraction technique. We extracted tens of thousands of high precision bilingual name translation spelling variants. We applied the spelling variants to the IE task, observing statistically significant improvements over a strong baseline system. We also applied the spelling variants to MT task and even though the overall improvement is relatively small, it achieves performance close to the one observed in an oracle experiment.","    7 http://www.cnts.ua.ac.be/conll2002/ner/ 398"]},{"title":"Acknowledgments","paragraphs":["This work was supported by DARPA/IPTO Contract No. HR0011-06-2-0001 under the GALE program. We are grateful to Yaser Al-Onaizan, Salim Roukos and anonymous reviewers for their constructive comments."]},{"title":"References","paragraphs":["Al-Onaizan, Y. and Papineni, K. Distortion Models for Statistical Machine Translation. In Proceedings of the 44th Annual Meeting on Association For Computational Linguistics. Sydney, Australia. July 2006.","Al-Onaizan, Y. and Knight, K. Translating named entities using monolingual and bilingual resources. In Proceedings of the 40th Annual Meeting on Association For Computational Linguistics (Philadelphia, Pennsylvania, July 07 - 12, 2002). Annual Meeting of the ACL. Association for Computational Linguistics, Morristown, NJ. 2002","Berger, A., S. Della Pietra, and V. Della Pietra. A","Maximum entropy approach to natural language processing. Computational Linguistics, 22(1):39–71. 1996","Bhagat, R. and Hovy, E. \"Phonetic Models for Generating Spelling Variants\", In Proceedings International Joint Conference of Artificial Intelligence (IJCAI). Hyderabad, India. 2007.","Chen, S. and Rosenfeld R. A survey of smoothing techniquesfor ME models. IEEE Trans. On Speech and Audio Processing. 2002","Florian, R., Hassan, H., Ittycheriah, A., Jing, H., Kambhatla, N., Luo, X., Nicolov, N., and Roukos. S. A statistical model for multilingual entity detection and tracking. In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics: HLT-NAACL 2004, pages 1–8.","Ge, N. Improvements in Word Alignments. Presentation given at DARPA/TIDES NIST MT Evaluation workshop. 2004","Goodman. J. Sequential conditional generalized iterative scaling. In Proceedings of the 40th Annual Meeting on Association For Computational Linguistics (Philadelphia, Pennsylvania, July 07 - 12, 2002). Annual Meeting of the ACL. Association for Computational Linguistics, Morristown, NJ. 2002","Huang, F., Vogel, S., and Waibel, A. Automatic extraction of named entity translingual equivalence based on multi-feature cost minimization. In Proceedings of the ACL 2003 Workshop on Multilingual and Mixed-Language Named Entity Recognition - Annual Meeting of the ACL. Association for Computational Linguistics, Morristown, NJ, 2003","Ji, H. and Grishman. R. Collaborative Entity Extraction and Translation. Proc. International Conference on Recent Advances in Natural Language Processing. Borovets, Bulgaria. Sept 2007.","Knuth, D. The Art of Computer Programming – Volume 3: Sorting and Searching. Addison- Wesley Publishing Company, 1973.","Linden, K. “Multilingual Modeling of Cross-Lingual Spelling Variants”, Information Retrieval, Vol. 9, No. 3. (June 2006), pp. 295-310.","Manning, C.D., and Schutze., H. Foundations of Statistical Natural Language Processing. MIT Press, 2000","NIST. 2007. The ACE evaluation plan.","www.nist.gov/speech/tests/ace/index.htm.","Noreen, E. W. Computer-Intensive Methods for Testing Hypothesis. John Wiley Sons. 1989","Papineni, K.A., Roukos, S., Ward, T., Zhu, W.J. BLEU: a method for automatic evaluation of machine translation. Technical Report RC22176 (W0109-022), IBM Research Division, Thomas J. Watson Research Center (2001)","Raghavan, H. and Allan, J., \"Matching Inconsistently Spelled Names in Automatic Speech Recognizer Output for Information Retrieval,\" the Proceedings of HLT/EMNLP 2005, pp. 451-458.","Snover, M., Dorr, B., Schwartz, R., Micciulla, L., and Makhoul, J. \"A Study of Translation Edit Rate with Targeted Human Annotation,\" Proceedings of Association for Machine Translation in the Americas, 2006.","Toivonen, J., Pirkola, A., Keskustalo, H., Visala, K. and Järvelin, K. Translating cross-lingual spelling variants using transformation rules. Inf. Process. Manage. 41(4): 859-872 (2005)","Vogel, S., Ney, H., and Tillmann, C.. HMM-based word alignment in statistical translation. In Proceedings of the 16th Conference on Computational Linguistics - Volume 2 (Copenhagen, Denmark, August 05 - 09, 1996). International Conference On Computational Linguistics. Association for Computational Linguistics, Morristown, NJ 1996","Zitouni, I., Florian R.. Mention Detection Crossing the Language Barrier. Proceedings of Conference on Empirical Methods in Natural Language Processing. Waikiki, Honolulu, Hawaii (October, 2008)","Zitouni, I., Sorensen, J., Luo, X., and Florian, R. The impact of morphological stemming on Arabic mention detection and coreference resolution. In Proceedings of the ACL Workshop on Computational Approaches to Semitic Languages. The 43rd Annual Meeting of the Association for Computational Linguistics. Ann Arbor (June, 2005) 399"]}]}