{"sections":[{"title":"","paragraphs":["Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 523–532, Honolulu, October 2008. c⃝2008 Association for Computational Linguistics"]},{"title":"A Japanese Predicate Argument Structure Analysis using Decision Lists Hirotoshi Taira, Sanae Fujita, Masaaki Nagata NTT Communication Science Laboratories 2-4, Hikaridai, Seika-cho, Keihanna Science City, Kyoto 619-0237, Japan {{taira,sanae}@cslab.kecl, nagata.masaaki@lab}.ntt.co.jp Abstract","paragraphs":["This paper describes a new automatic method for Japanese predicate argument structure analysis. The method learns relevant features to assign case roles to the argument of the target predicate using the features of the words located closest to the target predicate under various constraints such as dependency types, words, semantic categories, parts of speech, functional words and predicate voices. We constructed decision lists in which these features were sorted by their learned weights. Using our method, we integrated the tasks of semantic role labeling and zero-pronoun identification, and achieved a 17% improvement compared with a baseline method in a sentence level performance analysis."]},{"title":"1 Introduction","paragraphs":["Recently, predicate argument structure analysis has attracted the attention of researchers because this information can increase the precision of text processing tasks, such as machine translation, information extraction (Hirschman et al., 1999), question answering (Narayanan and Harabagiu, 2004) (Shen and Lapata, 2007), and summarization (Melli et al., 2005). In English predicate argument structure analysis, large corpora such as FrameNet (Fillmore et al., 2001), PropBank (Palmer et al., 2005) and NomBank (Meyers et al., 2004) have been created and utilized. Recently, the GDA Corpus (Hashida, 2005), Kyoto Text Corpus Ver.4.0 (Kawahara et al., 2002) and NAIST Text Corpus (Iida et al., 2007) were constructed in Japanese, and these corpora have become the target of an automatic Japanese predicate argument structure analysis system. We conducted Japanese predicate argument structure (PAS) analysis for the NAIST Text Corpus, which is the largest of these three corpora, and, as far as we know, this is the first time PAS analysis has been conducted for whole articles of the corpus.","The NAIST Text Corpus has the following characteristics, i) semantic roles for both predicates and event nouns are annotated in the corpus, ii) three major case roles,1","namely the ga, wo and ni-cases in Japanese are annotated for the base form of predicates and event nouns, iii) both the case roles in sentences containing the target predicates and those outside the sentences (zero-pronouns) are annotated, and iv) coreference relations are also annotated.","As regards i), recently there has been an increase in the number of papers dealing with nominalized predicates (Pradhan et al., 2004) (Jiang and Ng, 2006) (Xue, 2006) (Liu and Ng, 2007). For example, ‘trip’ in the sentence “During my trip to Italy, I met him.” refers not only to the event “I met him” but also to the event “I traveled to Italy.” As in this example, nouns sometimes have argument structures referring to an event. Such nouns are called event nouns (Komachi et al., 2007) in the NAIST Text Corpus. At the same time, the problems related to compound nouns are also important. In Japanese, a compound noun sometimes simultaneously contains both an event noun and its arguments. For example, the compound noun, ‘ (corporate buyout)’ contains an event noun ‘ (buyout)’ and its accusative, ‘ (corporate).’ However, compound","1","Kyoto Text Corpus has about 15 case roles. 523 nouns provide no information about syntactic dependency or about case markers, so it is difficult to specify the predicate-argument structure. Komachi et al. investigated the argument structure of event nouns using the co-occurrence of target nouns and their case roles in the same sentence (Komachi et al., 2007). In these approaches, predicates and event nouns are dealt with separately. Here, we try to unify these different argument structures using decision lists.","As regards ii), for example, in the causative sentence, ‘ (Mary makes Tom fix dinner),’ the basic form of the causative verb, ‘ (make fix)’ is ‘ (fix),’ and its nominative is ‘ (Tom)’ and the accusative case role (wo-case) is ‘ (dinner),’ although the surface case particle is ni (dative). We must deal with syntactic transformations in passive, causative, and benefactive constructions when analyzing the corpus.","As regards iii) and iv), in Japanese, zero pronouns often occur, especially when the argument has already been mentioned in previous sentences. There have been many studies of zero-pronoun identification (Walker et al., 1994) (Nakaiwa, 1997) (Iida et al., 2006).","In this paper, we present a general procedure for handling both the case role assignment of predicates and event nouns, and zero-pronoun identification. We use the decision list learning of rules to find the closest words with various constraints, because with decision lists the readability of learned lists is high and the learning is fast.","The rest of this paper is organized as follows. We describe the NAIST Text Corpus, which is our target corpus in Section 2. We describe our proposed method in Section 3. The result of experiments using the NAIST Text Corpus and our method are reported in Section 4 and our conclusions are provided in Section 5."]},{"title":"2 NAIST Text Corpus","paragraphs":["In the NAIST Text Corpus, three major obligatory Japanese case roles are annotated, namely the ga-case (nominative or subjective case), the wo-case (accusative or direct object) and the ni-case (dative or in-direct object). The NAIST Text Corpus is based on the Kyoto Text Corpus Ver. 3.0, which contains 38,384 sentences in 2,929 texts taken from news articles and editorials in a Japanese newspaper, the ‘Mainichi Shinbun’.","We divided these case roles into four types by location in the article as in (Iida et al., 2006), i) the case role depends on the predicate or the predicate depends on the case role in the intra-sentence (‘dependency relations’), ii) the case role does not depend on the predicate and the predicate does not depend on the case role in the intra-sentence (‘zero-anaphoric (intra-sentential)’), iii) the case role is not in the sentence containing the predicate (‘zero-anaphoric (inter-sentential)’), and iv) the case role and the predicate are in the same phrase (‘in same phrase’). Here, we do not deal with exophora.","We show the distribution of the above four types in test samples in our split of the NAIST Text Corpus in Tables 1 and 2. In predicates, the ‘dependency relations’ type in the wo-case and the ni-case occur frequently. In event nouns, the ‘zero-anaphoric (intra-sentential)’ and ‘zero-anaphoric (inter-sentential)’ types in the ga-case occur frequently. With respect to the ‘in same phrase’ type, the wo-case occurs frequently."]},{"title":"3 Predicate Argument Structure Analysis using Features of Closest Words","paragraphs":["In this section, we describe our algorithm. In the algorithm, we used various constraints when search-ing for the words located closest to the target predicate. We described these constraints as features with the direct products of dependency types (ic, oc, ga c, wo c, ni c, sc, nc, fw and bw), generalization levels (words, semantic categories, parts of speech), functional words and voices. 3.1 Dependency Types In Japanese, the functional words in a phrase (Bunsetsu in Japanese) and the interdependency of bunsetsu phrases are important for determining the predicate argument structure. In accordance with the character of the dependency between the case roles and the predicates or event nouns, we divided Japanese word dependency into the following seven types that cover all dependency types in Japanese. Additionally, we use two optional dependency types. 524 Table 1: Distribution of case roles for predicates (Test Data)","predicate","ga (Nominative) wo (Accusative) ni (Dative)","all 15,996 (100.00%) 8,348 (100.00%) 4,871 (100.00%)","dependency relations 9,591 ( 59.96%) 7,184 ( 86.06%) 4,276 ( 87.78%)","zero-anaphoric (intra-sentential) 3,856 ( 24.11%) 870 ( 10.42%) 360 ( 7.39%)","zero-anaphoric (inter-sentential) 2,496 ( 15.60%) 225 ( 2.70%) 132 ( 2.71%) in same phrase 53 ( 0.33%) 69 ( 0.83%) 103 ( 2.11%) Table 2: Distribution of case roles for event nouns (Test Data)","event noun","ga (Nominative) wo (Accusative) ni (Dative)","all 4,099 (100.00%) 2,314 (100.00%) 423 (100.00%)","dependency relations 977 (23.84%) 648 (28.00%) 105 (24.82%)","zero-anaphoric (intra-sentential) 1,672 (40.79%) 348 (15.04%) 135 (31.91%)","zero-anaphoric (inter-sentential) 1,040 (25.37%) 165 (7.13%) 44 (10.40%) in same phrase 410 (10.00%) 1,153 (49.83%) 139 (32.86%) Figure 1: Type ic 3.1.1 Incoming Connection Type (ic)","With this type, the target case role is the headword of a bunsetsu phrase and the case role phrase depends on the target predicate phrase (Figure 1). 3.1.2 Outgoing Connection Type (oc)","With this type, the target case role is the headword of a phrase and a phrase containing a target predicate or event noun depends on the case role phrase (Figure 2). Figure 2: Type oc 525 Figure 3: Type sc Figure 4: Type ga c, wo c, ni c 3.1.3 ‘Within the Same Phrase’ Type (sc)","With this type, the target case role and the target predicate or event noun are in the same phrase (Figure 3).","3.1.4 ‘Connection into Other Case role Types (ga c, wo c, ni c)","With these types, a phrase containing the target case role depends on a phrase containing another predetermined case role (Figure 4). We use the terms ‘ga c’, ‘wo c’ and ‘ni c’ when the predetermined case roles are the ga-case, wo-case and ni-case, respectively. Figure 5: Type nc 3.1.5 Non-connection Type (nc)","With this type, a phrase containing the target case role and a phrase containing the target predicate or event noun are in the same article, but these phrases do not depend on each other (Figure 5). 3.1.6 Optional Type (fw and bw)","Type fw and bw stand for ‘forward’ and ‘backward’ types, respectively. Type fw means the word located closest to the target predicate or event noun without considering functional words or voices. With fw, the word is located between the top of the article containing the target predicate and the target predicate or event noun. Similarly, type bw means the word located closest to the target predicate or noun, which is located between the targeted predicate or event noun, and the tail of the article containing the predicate. 3.2 Generalization Levels We used three levels of generalization for every case role candidate, that is, word, semantic category, and part of speech. Every word is annotated with a part of speech in the Kyoto Text Corpus, and we used these annotations. With regard to semantic categories, we annotated every word with a semantic category based on a Japanese thesaurus, Nihongo Goi Taikei. The thesaurus consists of a hierarchy of 2,710 semantic classes, defined for over 264,312 nouns, with a maximum depth of twelve (Ikehara et al., 1997). We mainly used the semantic classes of 526 Figure 6: Top 3 levels of the Japanese thesaurus, ‘Nihongo Goi Taikei’ the third level, and partly the fourth level, which are similar to semantic roles. We show the top three levels of the Nihongo Goi Taikei common noun thesaurus in Figure 6. We annotated the words with their semantic category by hand. 3.3 Functional Word and Voice We used a functional word in the phrase containing the target case role and active and passive voices for the predicate as base features. 3.4 Training Algorithm The training algorithm used for our method is shown in Figure 7. First, the algorithm constructs features that search for the words located closest to the target predicate under various constraints. Next, the algorithm learns by using linear Support Vector Machines (SVMs) (Vapnik, 1995). SVMs learn effective features by the one vs. rest method for every case role. We used TinySVM 2","as an SVM implementation. Moreover, we construct decision lists sorted by weight from linear SVMs. Finally, the algorithm calculates the existing probabilities of case roles for every predicate or event noun. This step","2","http://chasen.org/t̃aku/software/TinySVM/ produces the criterion that decides whether or not we will determine the case roles when there is no interdependency between the case role candidate and the predicate.","Our split of the NAIST Text Corpus has only 62,264 training samples for 2,874 predicates, and we predict that there will be a shortage of training samples when adopting traditional learning algorithms, such as learning algorithms using entropy. So, we used SVMs with a high generalization capability to learn the decision lists. 3.5 Test Algorithm The test algorithm of our method is shown in Figure 8. In the test phase, we analyzed test samples using decision lists and the existing probabilities of case roles learned in the training phase. In step 1, we determined case roles using a decision list consisting of features exhibiting case role and predicate interdependency, that is, ic, oc, ga c, wo c, and ni c. This is because there are many cases in Japanese where the syntactic constraint is stronger than the semantic constraint when we determine the case roles. In step 2, we determined case roles using a decision list of sc (‘in same phrase’) for the case roles that were not determined in step 1. This step was mainly for event nouns. Japanese event nouns frequently form compound nouns that contain case roles. In step 3, we decided whether or not to proceed to the next step by using the existing probabilities of case roles. If the probability was less than a certain threshold (50%), then the algorithm stopped. In step 4, we determined case roles using a decision list of the features that have no interdependency, that is, nc, fw and bw. This step will be executed when the target case role is syntactically necessary and determined by the co-occurrence of the case roles and predicate or event noun without syntactic clues, such as dependency, functional words and voices."]},{"title":"4 Experimental Results 4.1 Experimental Setting","paragraphs":["We performed our experiments using the NAIST Text Corpus 1.4β (Iida et al., 2007). We used 49,527 predicates and 12,737 event nouns from articles published from January 1st to January 11th and the editorials from January to August as training ex-527","for each predicate pi in all predicates appeared in the training corpus do","f eature list(pi)={} ; n ← 0","clear (x, y)","for each instance pij of pi, in the training corpus do Clear order() for all features aij ← the article including pij Wij ← the number of words in aij pred index ← the word index of pij in aij for (m = pred index − 1; m ≥ 1; m −−) do","n ++","dep type = get dependency type(wm,pij)","if dep type == ‘ic’, ‘nc’, ‘ga c’, ‘wo c’ or ‘ni c’ then inc order(n, dep type, wm, pij)","else if dep type == ‘sc’ then inc order(n, dep type, ‘’, ‘’)","endif","inc order(n, ‘fw’, ‘’, ‘’)","if wm is the ga-case role then yn,ga ← 1 else yn,ga ← 0","if wm is the wo-case role then yn,wo ← 1 else yn,wo ← 0","if wm is the ni-case role then yn,ni ← 1 else yn,ni ← 0 end for for (m = pred index +1; m ≤ Wij; m ++) do","n ++","dep type = get dependency type(wm,pij)","if dep type == ‘oc’, ‘nc’, ‘ga c’, ‘wo c’ or ‘ni c’ then inc order(n, dep type, wm,pij)","else if dep type == ‘sc’ then inc order(n, dep type, ‘’, ‘’)","endif","inc order(n, ‘bw’, ‘’, ‘’)","if wm is the ga-case role then yn,ga ← 1 else yn,ga ← 0","if wm is the wo-case role then yn,wo ← 1 else yn,wo ← 0","if wm is the ni-case role then yn,ni ← 1 else yn,ni ← 0 end for","end for","Learn linear SVMs using (x1, y1,ga), ..., (xn, yn,ga)","Learn linear SVMs using (x1, y1,wo), ..., (xn, yn,wo)","Learn linear SVMs using (x1, y1,ni), ..., (xn, yn,ni)","Make the decision list for pi, sorting features by weight.","Calculate the existing probabilities of case roles for pi.","end for","procedure get dependency type(wm, pij)","if phrase(wm) depends on phrase(pij) then return ‘ic’","else if phrase(pij) depends on phrase(wm) then return ‘oc’","else if phrase(wm) depends on phrase(pga) then return ‘ga c’","else if phrase(wm) depends on phrase(pwo) then return ‘wo c’","else if phrase(wm) depends on phrase(pni) then return ‘ni c’","else if phrase(wm) equals phrase(pij) then return ‘sc’","else return ‘nc’","end procedure","procedure inc order(n, dep type, f unc, voice)","Set a feature fw =(wm, dep type, f unc, voice) ; order(fw)++ ; if order(fw) == 1 then xn,fw ← 1","Set a feature fs =(sem(wm), dep type, f unc, voice) ; order(fs)++ ; if order(fs) == 1 then xn,fs ← 1","Set a feature fp =(pos(wm), dep type, f unc, voice) ; order(fp)++ ; if order(fp) == 1 then xn,fp ← 1","f eature list(pi) ← f eature list(pi) ⋃ {fw,fs,fp} end procedure Figure 7: Training algorithm 528 Step 1. Determine case roles using a decision list concerning ic, oc, ga c, wo c and ni c. Step 2. Determine case roles using a decision list concerning sc for undetermined case roles in Step.1. Step 3. If the existing probability of case roles < 50 % then the program ends. Step 4. Determine case roles using a decision list concerning nc, fw and bw types. Figure 8: Test algorithm amples. We used 11,023 predicates and 3,161 event nouns from articles published on January 12th and 13th and the September editorials as development examples. And we used 19,501 predicate and 5,276 event nouns from articles dated January 14th to 17th and editorials dated October to December as test examples. This is a typical way to split the data.","We used the annotations in the Kyoto Text Corpus as the interdependency of bunsetsu phrases. We used both individual and multiple words as case roles. We used the phrase boundaries annotated in the NAIST Text Corpus in the training phase, and used those annotated automatically by our system using POSs and simple rules in the test phase. The accuracy of the automatic annotation is about 90%. 4.2 Baseline Method To evaluate our algorithm, we conducted experiments using a baseline method. With the method, we used only nouns that depended on predicates or event nouns as case role candidates. If the functional word (post-positional case) in the phrase is ‘ga’,‘wo’ and ‘ni’, we determined the ga-case, wo-case, or ni-case for the candidates. Next, as regards event nouns in compound nouns, if there was another word in a compound noun containing an event noun and it cooccurred with the event noun as a case role with a higher probability in the training samples, then the word was selected for the case role. 4.3 Entropy Method The conventional approach for making decision lists utilizes the entropy of samples selected by the rules (Yarowsky, 1994) (Goodman, 2002). We performed comparative experiments using Yarowsky’s entropy algorithm (Yarowsky, 1994). Table 3: Existing probabilities of case roles for predicates and event nouns","Predicate Existing Probability or Event Noun ga (NOM) wo (ACC) ni (DAT)","(use) 44.72% 82.92% 5.33% (negotiation) 77.41% 30.70% 0.00% (participation) 87.09% 0.00% 72.46% (based on) 81.89% 0.00% 100.00% 4.4 Overall Results The overall results are shown in Table 7. Here, ‘entropy’ indicates Yarowsky’s algorithm, which uses entropy (Yarowsky, 1994). Throughout the test data, the F-measure (%) of our method exceeded that of the baseline system and the ‘entropy’ system. With the ga-case (nominative) in particular, the F-measure increased 9 points.","Table 3 shows some examples of the existing probabilities of case roles for predicates or event nouns. When the probabilities are extreme values such as the ni-case (dative) of (negotiation), the wo-case (accusative) of (participation), and the wo-case and ni-base of (based on), we can decide to fill the targeted case role or not with high precision. However, it is difficult to decide to fill the targeted case role or not when the probability is close to 50 percent as in the ga-case of (use).","We show the learned decision list of the ic type (the case role depends on the predicate or event noun), sc type (in the same phrase) and the other types for event noun (negotiation) in Tables 4, 5 and 6, respectively. Here, ‘word’ in the ‘level’ column means ‘base form of predicate’ and ‘sem’ means ’semantic category of predicate.’ In the ic and sc type decision lists, features with semantic categories, such as ‘REGION’, ’LOCATION’ and ‘EVENT’, occupy a higher order. In contrast, in the list of the other types, the features that occupy the higher order are the features of the word base 529 Table 4: Decision list for ic type of event noun (negotiation) order case dep type level head word functional voice weight","word 1 ga ic word (North Korea) (of) active 0.9820 2 ga ic sem (REGION) (of) active 0.6381 3 ga ic word (both Japan and U.S.) (of) active 0.5502 4 wo ic word (establishment of joint ventures) (of) active 0.5288 5 wo ic word (telecommunications) (of) active 0.4142 6 wo ic word (North Korea) (for) active 0.3168 7 wo ic word (ACTION) (of) active 0.3083 8 ga ic sem (OOV NOUN) (of) active 0.2939 9 wo ic word (car and auto parts sector) (of) active 0.2775 10 wo ic sem (LOCATION) (of) active 0.2471 Table 5: Decision list for sc type of event noun (negotiation)","order case dep type level head word weight 1 wo sc sem (EVENT) 1.1738 2 wo sc word (arrangement) 1.0000 3 ga sc word (airline of Japan and China) 0.9392 4 wo sc sem (MENTAL STATE) 0.8958 5 ga sc word (financial services of Japan and U.S.) 0.8371 6 wo sc word (contract extension) 0.7870 7 wo sc word (joint venture) 0.7865 8 wo sc word (intellectual property rights) 0.7224 9 wo sc word (car and auto parts) 0.7196 10 ga sc word (Japan and North Korea) 0.6771 Table 6: Decision list for other types of event noun (negotiation)","order case dep type level head word functional word voice weight 1 ga fw word (Japan and U.S.) 1.9954 2 ga fw word (Taiwan) 1.9952 3 ga fw word (U.S. and North Korea) 1.4979 4 ga fw word (U.K. and China) 1.1773 5 ga nc word (both nations) (TOP) active 1.1379 6 wo fw word (diplomatic normalization) 1.0000 7 ga bw word (U.S. and North Korea) 1.0000 8 ga fw word (capital and labor) 1.0000 9 wo fw word (automotive area) 1.0000 10 ga nc word (both sides) (TOP) active 1.0000 Table 7: Overall results for NAIST Text Corpus (F-measure(%))","training data test data","sentence ga (NOM) wo (ACC) ni (DAT) sentence ga (NOM) wo (ACC) ni (DAT) baseline 25.32 32.58 74.51 82.70 21.34 30.08 69.48 76.62 entropy 73.46 89.53 92.72 91.09 33.10 45.67 73.28 77.77 our method 64.81 86.76 92.52 92.20 38.06 55.07 75.82 80.45 530 Table 8: Results for predicates in test sets (F-measure(%))","baseline / our method","ga (Nominative) wo (Accusative) ni (Dative)","all 34.44 / 57.40 77.00 / 79.50 79.83 / 83.15","dependency relations 51.96 / 75.53 85.42 / 88.20 81.83 / 89.51","zero-anaphoric (intra-sentential) 0.00 / 30.15 0.00 / 11.41 0.00 / 3.66","zero-anaphoric (inter-sentential) 1.85 / 23.45 3.00 / 9.32 0.00 / 11.76 in same phrase 0.00 / 75.00 0.00 / 51.78 0.00 / 84.65 Table 9: Results for event nouns (F-measure(%))","baseline / our method","ga (Nominative) wo (Accusative) ni (Dative)","all 11.05 / 45.64 32.30 / 61.80 20.85 / 38.88","dependency relations 12.98 / 68.01 25.00 / 62.46 40.00 / 56.05","zero-anaphoric (intra-sentential) 0.00 / 36.19 0.00 / 20.46 0.00 / 6.62","zero-anaphoric (inter-sentential) 1.40 / 23.25 1.06 / 10.37 0.00 / 3.51 in same phrase 58.76 / 78.93 47.44 / 77.96 28.91 / 58.13 form. This means local knowledge of relations between case roles and predicates or event nouns in the word level is more important than semantic level knowledge. 4.5 Results for Predicates in Test Sets We show the results we obtained for predicates in Table 8. The results reveal that our method is superior to the baseline system. Our algorithm is particularly effective in the ga-case. 4.6 Results for Event Nouns in Test Sets We show the results we obtained for event nouns in Table 9. This also shows that our method is superior to the baseline system. The precision with sc type is high and our method is effective as regards event nouns."]},{"title":"5 Conclusion","paragraphs":["We presented a new method for Japanese automatic predicate argument structure analysis using decision lists based on the features of the words located closest to the target predicate under various constraints. The method learns the relative weights of these different features for case roles and ranks them using decision lists. Using our method, we integrated the knowledge of case role determination and zero-pronoun identification, and generally achieved a high precision in Japanese PAS analysis. In particular, we can extract knowledge at various levels from the corpus for event nouns. In future, we will use richer constraints and research better ways of distinguishing whether or not cases are obligatory."]},{"title":"Acknowledgments","paragraphs":["We thank Ryu Iida and Yuji Matsumoto of NAIST for the definitions of the case roles in the NAIST Text Corpus and functional words, and Franklin Chang for valuable comments."]},{"title":"References","paragraphs":["Charles J. Fillmore, Charles Wooters, and Collin F. Baker. 2001. Building a large lexical databank which provides deep semantics. In Proc. of the Pacific Asian Conference on Language, Information and Computa-tion (PACLING).","Joshua Goodman. 2002. An incremental decision list learner. In Proc. of the ACL-02 Conference on Empirical Methods in Natural Language Processing(EMNLP02), pages 17–24.","Kouichi Hashida. 2005. Global document annotation (GDA) manual. http://i-content.org/GDA/.","Lynette Hirschman, Patricia Robinson, Lisa Ferro, Nancy Chinchor, Erica Brown, Ralph Grishman, and Beth Sundheim. 1999. Hub-4 Event’99 general guidelines.","Ryu Iida, Kentaro Inui, and Yuji Matsumoto. 2006. Exploiting syntactic patterns as clues in zero-anaphora resolution. In Proc. of the 21st International Confer-531 ence on Computational Linguistics and 44th Annual Meeting of the ACL, pages 625–632.","Ryu Iida, Mamoru Komachi, Kentaro Inui, and Yuji Matsumoto. 2007. Annotating a Japanese text corpus with predicate-argument and coreference relations. In Proc. of ACL 2007 Workshop on Linguistic Annota-tion, pages 132–139.","Satoru Ikehara, Masahiro Miyazaki, Satoshi Shirai, Akio Yokoo, Hiromi Nakaiwa, Kentaro Ogura, Yoshifumi Ooyama, and Yoshihiko Hayashi. 1997. Nihongo Goi Taikei, A Japanese Lexicon. Iwanami Shoten, Tokyo.","Zheng Ping Jiang and Hwee Tou Ng. 2006. Semantic role labeling of NomBank: A maximum entropy approach. In Proc. of the Conference on Empirical Methods in Natural Language Processing.","Daisuke Kawahara, Sadao Kurohashi, and Koichi Hashida. 2002. Construction of a Japanese relevancetagged corpus (in Japanese). Proc. of the 8th Annual Meeting of the Association for Natural Language Processing, pages 495–498.","Mamoru Komachi, Ryu Iida, Kentaro Inui, and Yuji Matsumoto. 2007. Learning-based argument structure analysis of event-nouns in Japanese. In Proc. of the Conference of the Pacific Association for Computational Linguistics (PACLING), pages 120–128.","Chang Liu and Hwee Tou Ng. 2007. Learning predictive structures for semantic role labeling of NomBank. In Proc. of the 45th Annual Meeting of the Association for Computational Linguistics (ACL), pages 208–215.","Gabor Melli, Yang Wang, Yudong Liu, Mehdi M. Kashani, Zhongmin Shi, Baohua Gu, Anoop Sarkar, and Fred Popowich. 2005. Description of SQUASH, the SFU question answering summary handler for the DUC-2005 summarization task. In Proc. of DUC 2005.","Adam Meyers, Ruth Reeves, Catherine Macleod, Rachel Szekely, Veronika Zielinska, Brian Young, and Ralph Grishman. 2004. The NomBank project: An interim report. In Proc. of HLT-NAACL 2004 Workshop on Frontiers in Corpus Annotation.","Hiromi Nakaiwa. 1997. Automatic identification of zero pronouns and their antecedents within aligned sentence pairs. In Proc. of the 3rd Annual Meeting of the Association for Natural Language Processing (in Japanese).","Srini Narayanan and Sanda Harabagiu. 2004. Question answering based on semantic structures. In Proc. of the 20th International Conference on Computational Linguistics (COLING).","M. Palmer, P. Kingsbury, and D. Gildea. 2005. The proposition bank: An annotated corpus of semantic roles. Computational Linguistics, 31(1):71–106.","Sameer Pradhan, Waybe Ward, Kadri Hacioglu, James Martin, and Dan Jurafsky. 2004. Shallow semantic parsing using support vector machines. In Proc. of the Human Language Technology Conference/North American Chapter of the Association of Computational Linguistics HLT/NAACL 2004.","Dan Shen and Mirella Lapata. 2007. Using semantic roles to improve question answering. In Proc. of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP/CoNLL), pages 12–21.","V. Vapnik. 1995. The Nature of Statistical Learning The-ory. Springer-Verlag, New York.","M. Walker, M. Iida, and S. Cote. 1994. Japanese discourse and the process of centering. Computational Linguistics, 20(2):193–233.","Nianwen Xue. 2006. Semantic role labeling of nominalized predicates in Chinese. In Proc. of the HLT-NAACL, pages 431–438.","David Yarowsky. 1994. Decision lists for lexical ambiguity resolution: Application to accent restoration in Spanish and French. In Proc. of the 32nd Annual Meeting of the Association for Computational Linguistics (ACL), pages 88–95. 532"]}]}