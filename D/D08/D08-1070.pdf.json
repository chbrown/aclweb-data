{"sections":[{"title":"","paragraphs":["Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 670–679, Honolulu, October 2008. c⃝2008 Association for Computational Linguistics"]},{"title":"Learning with Probabilistic Features for Improved Pipeline ModelsRazvan C. BunescuSchool of EECSOhio UniversityAthens, OH 45701bunescu@ohio.eduAbstract","paragraphs":["We present a novel learning framework for pipeline models aimed at improving the communication between consecutive stages in a pipeline. Our method exploits the confidence scores associated with outputs at any given stage in a pipeline in order to compute probabilistic features used at other stages downstream. We describe a simple method of integrating probabilistic features into the linear scoring functions used by state of the art machine learning algorithms. Experimental evaluation on dependency parsing and named entity recognition demonstrate the superiority of our approach over the baseline pipeline models, especially when upstream stages in the pipeline exhibit low accuracy."]},{"title":"1 Introduction","paragraphs":["Machine learning algorithms are used extensively in natural language processing. Applications range from fundamental language tasks such as part of speech (POS) tagging or syntactic parsing, to higher level applications such as information extraction (IE), semantic role labeling (SRL), or question an-swering (QA). Learning a model for a particular language processing problem often requires the output from other natural language tasks. Syntactic parsing and dependency parsing usually start with a textual input that is tokenized, split in sentences and POS tagged. In information extraction, named entity recognition (NER), coreference resolution, and relation extraction (RE) have been shown to benefit from features that use POS tags and syntactic dependencies. Similarly, most SRL approaches assume a parse tree representation of the input sentences. The common practice in modeling such dependencies is to use a pipeline organization, in which the output of one task is fed as input to the next task in the sequence. One advantage of this model is that it is very simple to implement; it also allows for a modular approach to natural language processing. The key disadvantage is that errors propagate between stages in the pipeline, significantly affect-ing the quality of the final results. One solution is to solve the tasks jointly, using the principled framework of probabilistic graphical models. Sutton et al. (2004) use factorial Conditional Random Fields (CRFs) (Lafferty et al., 2001) to jointly predict POS tags and segment noun phrases, improving on the cascaded models that perform the two tasks in sequence. Wellner et al. (2004) describe a CRF model that integrates the tasks of citation segmentation and citation matching. Their empirical results show the superiority of the integrated model over the pipeline approach. While more accurate than their pipeline analogues, probabilistic graphical models that jointly solve multiple natural language tasks are generally more demanding in terms of finding the right representations, the associated inference algorithms and their computational complexity. Recent negative results on the integration of syntactic parsing with SRL (Sutton and McCallum, 2005) provide additional evidence for the difficulty of this general approach. When dependencies between the tasks can be formulated in terms of constraints between their outputs, a simpler approach is to solve the tasks separately and integrate the constraints in a linear programming formulation, as proposed by Roth and 670 Yih (2004) for the simultaneous learning of named entities and relations between them. More recently, Finkel et al. (2006) model the linguistic pipelines as Bayesian networks on which they perform Monte Carlo inference in order to find the most likely output for the final stage in the pipeline.","In this paper, we present a new learning method for pipeline models that mitigates the problem of error propagation between the tasks. Our method exploits the probabilities output by any given stage in the pipeline as weights for the features used at other stages downstream. We show a simple method of integrating probabilistic features into linear scoring functions, which makes our approach applicable to state of the art machine learning algorithms such as CRFs and Support Vector Machines (Vapnik, 1998; Schölkopf and Smola, 2002). Experimental results on dependency parsing and named entity recognition show useful improvements over the baseline pipeline models, especially when the basic pipeline components exhibit low accuracy."]},{"title":"2 Learning with Probabilistic Features","paragraphs":["We consider that the task is to learn a mapping from inputs x ∈ X to outputs y ∈ Y(x). Each input x is also associated with a different set of outputs z ∈ Z(x) for which we are given a probabilistic confidence measure p(z|x). In a pipeline model, z would correspond to the annotations performed on the input x by all stages in the pipeline other than the stage that produces y. For example, in the case of dependency parsing, x is a sequence of words, y is a set of word-word dependencies, z is a sequence of POS tags, and p(z|x) is a measure of the confidence that the POS tagger has in the output z. Let φ be a representation function that maps an example (x, y, z) to a feature vector φ(x, y, z) ∈ Rd",", and w ∈ Rd","a parameter vector. Equations (1) and (2) below show the traditional method for computing the optimal output ŷ in a pipeline model, assuming a linear scoring function defined by w and φ.","ŷ(x) = argmax y∈Y(x) w · φ(x, y, ẑ(x)) (1)","ẑ(x) = argmax z∈Z(x) p(z|x) (2) The weight vector w is learned by optimizing a predefined objective function on a training dataset.","In the model above, only the best annotation ẑ produced by upstream stages is used for determining the optimal output ŷ. However, ẑ may be an incorrect annotation, while the correct annotation may be ignored because it was assigned a lower confidence value. We propose exploiting all possible annotations and their probabilities as illustrated in the new model below:","ŷ(x) = argmax y∈Y(x) w · ψ(x, y) (3) ψ(x, y) = ∑ z∈Z(x) p(z|x) · φ(x, y, z) (4) In most cases, directly computing ψ(x, y) is unfeasible, due to a large number of annotations in Z(x). In our dependency parsing example, Z(x) contains all possible POS taggings of sentence x; consequently its cardinality is exponential in the length of the sentence. A more efficient way of computing ψ(x, y) can be designed based on the observation that most components φi of the original feature vector φ utilize only a limited amount of evidence from the example (x, y, z). We define (x̃, ỹ, z̃) ∈ Fi(x, y, z) to cap-ture the actual evidence from (x, y, z) that is used by one instance of feature function φi. We call (x̃, ỹ, z̃) a feature instance of φi in the example (x, y, z). Correspondingly, Fi(x, y, z) is the set of all feature instances of φi in example (x, y, z). Usually, φi(x, y, z) is set to be equal with the number of instances of φi in example (x, y, z), i.e. φi(x, y, z) = |Fi(x, y, z)|. Table 1 illustrates three feature instances (x̃, ỹ, z̃) generated by three typical dependency parsing features in the example from Figure 1. Because the same feature may be instantiated multi- φ1 : DT → NN φ2 : NNS → thought φ3 : be ← in ỹ 10 →11 2 →4 7 ←9 z̃ DT10 NN11 NNS2 x̃ thought4 be7 in9 |Fi|","O(|x|2 ) O(|x|) O(1) Table 1: Feature instances. ple times in the same example, the components of each feature instance are annotated with their positions relative to the example. Given these definitions, the feature vector ψ(x, y) from (4) can be 671 Figure 1: Dependency Parsing Example. rewritten in a component-wise manner as follows: ψ(x, y) = [ψ1(x, y) . . . ψd(x, y)] (5) ψi(x, y) = ∑ z∈Z(x) p(z|x) · φi(x, y, z) = ∑ z∈Z(x) p(z|x) · |Fi(x, y, z)| = ∑ z∈Z(x) p(z|x) ∑ (x̃,ỹ,z̃)∈Fi(x,y,z)1 = ∑ z∈Z(x) ∑ (x̃,ỹ,z̃)∈Fi(x,y,z)p(z|x) = ∑ (x̃,ỹ,z̃)∈Fi(x,y,Z(x)) ∑ z∈Z(x),z⊇z̃p(z|x) where Fi(x, y, Z(x)) stands for: Fi(x, y, Z(x)) = ⋃ z∈Z(x) Fi(x, y, z) We introduce p(z̃|x) to denote the expectation: p(z̃|x) = ∑ z∈Z(x),z⊇z̃ p(z|x) Then ψi(x, y) can be written compactly as: ψi(x, y) = ∑ (x̃,ỹ,z̃)∈Fi(x,y,Z(x)) p(z̃|x) (6) The total number of terms in (6) is equal with the number of instantiations of feature φi in the example (x, y) across all possible annotations z ∈ Z(x), i.e. |Fi(x, y, Z(x))|. Usually this is significantly smaller than the exponential number of terms in (4). The actual number of terms depends on the particular feature used to generate them, as illustrated in the last row of Table 1 for the three features used in dependency parsing. The overall time complexity for calculating ψ(x, y) also depends on the time complexity needed to compute the expectations p(z̃|x). When z is a sequence, p(z̃|x) can be computed efficiently using a constrained version of the forward-backward algorithm (to be described in Section 3). When z is a tree, p(z̃|x) will be computed using a constrained version of the CYK algorithm (to be described in Section 4).","The time complexity can be further reduced if instead of ψ(x, y) we use its subcomponent ψ̂(x, y) that is calculated based only on instances that appear in the optimal annotation ẑ: ψ̂(x, y) = [ ψ̂1(x, y) . . . ψ̂d(x, y)] (7) ψ̂i(x, y) = ∑ (x̃,ỹ,z̃)∈Fi(x,y,ẑ) p(z̃|x) (8) The three models are summarized in Table 2 below. In the next two sections we illustrate their applica- ŷ(x) = argmax y∈Y(x) w · φ(x, y) M1 φ(x, y) = φ(x, y, ẑ(x)) ẑ(x) = argmax z∈Z(x) p(z|x) ŷ(x) = argmax y∈Y(x) w · ψ(x, y) M2 ψ(x, y) = [ψ1(x, y) . . . ψd(x, y)] ψi(x, y) = ∑ (x̃,ỹ,z̃)∈Fi(x,y,Z(x)) p(z̃|x) ŷ(x) = argmax y∈Y(x) w · ψ̂(x, y) M3 ψ̂(x, y) = [ ψ̂1(x, y) . . . ψ̂d(x, y)] ψ̂i(x, y) = ∑ (x̃,ỹ,z̃)∈Fi(x,y,ẑ) p(z̃|x) Table 2: Three Pipeline Models. tion to two common tasks in language processing: dependency parsing and named entity recognition."]},{"title":"3 Dependency Parsing Pipeline","paragraphs":["In a traditional dependency parsing pipeline (model M1 in Table 2), an input sentence x is first aug-672 mented with a POS tagging ẑ(x), and then processed by a dependency parser in order to obtain a dependency structure ŷ(x). To evaluate the new pipeline models we use MSTPARSER1",", a linearly scored dependency parser developed by McDonald et al. (2005). Following the edge based factorization method of Eisner (1996), the score of a dependency tree in the first order version is defined as the sum of the scores of all edges in the tree. Equivalently, the feature vector of a dependency tree is defined as the sum of the feature vectors of all edges in the tree:","M1: φ(x, y) = ∑ u→v∈y φ(x, u → v, ẑ(x))","M2: ψ(x, y) = ∑ u→v∈y ψ(x, u → v)","M3: ψ̂(x, y) = ∑ u→v∈y ψ̂(x, u → v) For each edge u → v ∈ y, MSTPARSER generates features based on a set of feature templates that take into account the words and POS tags at positions u, v, and their left and right neighbors u ± 1, v ± 1. For example, a particular feature template T used inside MSTPARSER generates the following POS bigram features:","φi(x, u → v, z) = { 1, if ⟨zu, zv⟩ = ⟨t1, t2⟩ 0, otherwise where t1, t2 ∈ P are the two POS tags associated with feature index i. By replacing y with u → v in the feature expressions from Table 2, we obtain the following formulations:","M1: φi(x, u → v) = { 1, if ⟨ẑu, ẑv⟩ = ⟨t1, t2⟩ 0, otherwise M2: ψi(x, u → v) = p(z̃ = ⟨t1, t2⟩|x)","M3: ψ̂i(x, u → v) = { p(z̃ = ⟨t1, t2⟩|x), if ⟨ẑu, ẑv⟩ = ⟨t1, t2⟩","0, otherwise where, following the notation from Section 2, z̃ = ⟨zu, zv⟩ is the actual evidence from z that is used by feature i, and ẑ is the top scoring annotation produced by the POS tagger. The implementation in MSTPARSER corresponds to the traditional pipeline model M1. Given a method for computing feature","1","URL: http://sourceforge.net/projects/mstparser probabilities p(z̃ = ⟨t1, t2⟩|x), it is straightforward to modify MSTPARSER to implement models M2 and M3 – we simply replace the feature vectors φ with ψ and ψ̂ respectively. As mentioned in Section 2, the time complexity of computing the feature vectors ψ in model M2 depends on the complexity of the actual evidence z̃ used by the features. For example, the feature template T used above is based on the POS tags at both ends of a dependency edge, consequently it would generate |P|2 features in model M2 for any given edge u → v. There are however feature templates used in MSTPARSER that are based on the POS tags of up to 4 tokens in the input sentence, which means that for each edge they would generate |P|4","≈ 4.5M features. Whether using all these probabilistic features is computationally feasible or not also depends on the time complexity of computing the confidence measure p(z̃|x) associated with each feature. 3.1 Probabilistic POS features The new pipeline models M2 and M3 require an annotation model that, at a minimum, facilitates the computation of probabilistic confidence values for each output. We chose to use linear chain CRFs (Lafferty et al., 2001) since CRFs can be easily modified to compute expectations of the type p(z̃|x), as needed by M2 and M3.","The CRF tagger was implemented in MALLET (McCallum, 2002) using the original feature templates from (Ratnaparkhi, 1996). The model was trained on sections 2–21 from the English Penn Treebank (Marcus et al., 1993). When tested on section 23, the CRF tagger obtains 96.25% accuracy, which is competitive with more finely tuned systems such as Ratnaparkhi’s MaxEnt tagger.","We have also implemented in MALLET a constrained version of the forward-backward procedure that allows computing feature probabilities p(z̃|x). If z̃ = ⟨ti1 ti2 ...tik ⟩ specifies the tags at k positions in the sentence, then the procedure recomputes the α parameters for all positions between i1 and ik by constraining the state transitions to pass through the specified tags at the k positions. A similar approach was used by Culotta et al. in (2004) in order to associate confidence values with sequences of contiguous tokens identified by a CRF model as fields in an information extraction task. The constrained proce-673 dure requires (ik − i1)|P|2","= O(N |P|2",") multiplications in an order 1 Markov model, where N is the length of the sentence. Because MSTPARSER uses an edge based factorization of the scoring function, the constrained forward procedure will need to be run for each feature template, for each pair of tokens in the input sentence x. If the evidence z̃ required by the feature template T constrains the tags at k positions, then the total time complexity for computing the probabilistic features p(z̃|x) generated by T is:","O(N 3","|P|k+2",") = O(N |P|2",") · O(N 2",") · O(|P|k",") (9) As mentioned earlier, some feature templates used in the dependency parser constrain the POS tags at 4 positions, leading to a O(N 3","|P|6",") time complexity for a length N sentence. Experimental runs on the same machine that was used for CRF training show that such a time complexity is not yet feasible, especially because of the large size of P (46 POS tags). In order to speed up the computation of probabilistic features, we made the following two approximations:","1. Instead of using the constrained forward-backward procedure, we enforce an independence assumption between tags at different positions and rewrite p(z̃ = ⟨ti1 ti2 ...tik ⟩|x) as: p(ti1 ti2 ...tik |x) ≈ k ∏ j=1 p(tij |x) The marginal probabilities p(tij |x) are easily computed using the original forward and backward parameters as:","p(tij |x) = αij (tij |x)βij (tij |x) Z(x) This approximation eliminates the factor O(N |P|2",") from the time complexity in (9).","2. If any of the marginal probabilities p(tij |x) is less than a predefined threshold (τ |P|)−1",", we set p(z̃|x) to 0. When τ ≥ 1, the method is guaranteed to consider at least the most probable state when computing the probabilistic features. Looking back at Equation (4), this is equivalent with summing feature vectors only over the most probable annotations z ∈ Z(x). The approximation effectively replaces the factor O(|P|k",") in (9) with a quasi-constant factor. The two approximations lead to an overall time complexity of O(N 2",") for computing the probabilistic features associated with any feature template T , plus O(N |P|2",") for the unconstrained forward-backward procedure. We will use M ′","2 to refer to the model M2 that incorporates the two approximations. The independence assumption from the first approximation can be relaxed without increasing the asymptotic time complexity by considering as independent only chunks of contiguous POS tags that are at least a certain number of tokens apart. Consequently, the probability of the tag sequence will be approximated with the product of the probabilities of the tag chunks, where the exact probability of each chunk is computed in constant time with the constrained forward-backward procedure. We will use M ′′","2 to refer to the resulting model. 3.2 Experimental Results MSTPARSER was trained on sections 2–21 from the WSJ Penn Treebank, using the gold standard POS tagging. The parser was then evaluated on section 23, using the POS tagging output by the CRF tagger. For model M1 we need only the best output from the POS tagger. For models M ′","2 and M ′′","2 we compute the probability associated with each feature using the corresponding approximations, as described in the previous section. In model M ′′","2 we consider as independent only chunks of POS tags that are 4 tokens or more apart. If the distance between the chunks is less than 4 tokens, the probability for the entire tag sequence in the feature is computed exactly using the constrained forward-backward procedure. Table 3 shows the accuracy obtained by models M1, M ′","2(τ ) and M ′′","2 (τ ) for various values of the threshold parameter τ . The accuracy is com-M1","M ′ 2(1)","M ′ 2(2)","M ′ 2(4)","M ′′ 2 (4) 88.51 88.66 88.67 88.67 88.70 Table 3: Dependency parsing results. puted over unlabeled dependencies i.e. the percentage of words for which the parser has correctly identified the parent in the dependency tree. The pipeline 674 Figure 2: Named Entity Recognition Example. model M ′","2 that uses probabilistic features outperforms the traditional pipeline model M1. As expected, M ′′","2 performs slightly better than M ′","2, due to a more exact computation of feature probabilities. Overall, only by using the probabilities associated with the POS features, we achieve an absolute error reduction of 0.19%, in a context where the POS stage in the pipeline already has a very high accuracy of 96.25%. We expect probabilistic features to yield a more substantial improvement in cases where the pipeline model contains less accurate upstream stages. Such a case is that of NER based on a combination of POS and dependency parsing features."]},{"title":"4 Named Entity Recognition Pipeline","paragraphs":["In Named Entity Recognition (NER), the task is to identify textual mentions of predefined types of entities. Traditionally, NER is modeled as a sequence classification problem: each token in the input sentence is tagged as being either inside (I) or outside (O) of an entity mention. Most sequence tagging approaches use the words and the POS tags in a limited neighborhood of the current sentence position in order to compute the corresponding features. We augment these flat features with a set of tree features that are computed based on the words and POS tags found in the proximity of the current token in the dependency tree of the sentence. We argue that such dependency tree features are better at capturing predicate-argument relationships, especially when they span long stretches of text. Figure 2 shows a sentence x together with its POS tagging z1, dependency links z2, and an output tagging y. As-suming the task is to recognize mentions of people, the word sailors needs to be tagged as inside. If we extracted only flat features using a symmetric window of size 3, the relationship between sailors and thought would be missed. This relationship is useful, since an agent of the predicate thought is likely to be a person entity. On the other hand, the nodes sailors and thought are adjacent in the dependency tree of the sentence. Therefore, their relationship can be easily captured as a dependency tree feature using the same window size.","For every token position, we generate flat features by considering all unigrams, bigrams and trigrams that start with the current token and extend either to the left or to the right. Similarly, we generate tree features by considering all unigrams, bigrams and trigrams that start with the current token and extend in any direction in the undirected version of the dependency tree. The tree features are also augmented with the actual direction of the dependency arcs between the tokens. If we use only words to create n-gram features, the token sailors will be associated with the following features: • Flat: sailors, the sailors, ⟨S⟩ the sailors, sailors mistakenly, sailors mistakenly thought. • Tree: sailors, sailors ← the, sailors → thought, sailors → thought ← must, sailors → thought ← mistakenly. We also allow n-grams to use word classes such as POS tags and any of the following five categories: ⟨1C⟩ for tokens consisting of one capital letter, ⟨AC⟩ for tokens containing only capital letters, ⟨FC⟩ for tokens that start with a capital letter, followed by small letters, ⟨CD⟩ for tokens containing at least one digit, and ⟨CRT⟩ for the current token.","The set of features can then be defined as a Cartesian product over word classes, as illustrated in Figure 3 for the original tree feature sailors → thought ← mistakenly. In this case, instead of one completely lexicalized feature, the model will consider 12 different features such as sailors → VBD ← RB, NNS → thought ← RB, or NNS → VBD ← RB. 675 "," ⟨CRT⟩","NNS","sailors  ×[→]×[","VBD","thought ]","×[←]×[","RB mistakenly ] Figure 3: Dependency tree features.","The pipeline model M2 uses features that appear in all possible annotations z = ⟨z1, z2⟩, where z1 and z2 are the POS tagging and the dependency parse respectively. If the corresponding evidence is z̃ = ⟨z̃1, z̃2⟩, then: p(z̃|x) = p(z̃2|z̃1, x)p(z̃1|x) For example, NNS2 → thought4 ← RB3 is a feature instance for the token sailors in the annotations from Figure 2. This can be construed as having been generated by a feature template T that outputs the POS tag ti at the current position, the word xj that is the parent of xi in the dependency tree, and the POS tag tk of another dependent of xj (i.e. ti → xj ← tk). The probability p(z̃|x) for this type of features can then be written as: p(z̃|x) = p(i → j ← k|ti, tk, x) · p(ti, tk|x) The two probability factors can be computed exactly as follows:","1. The M2 model for dependency parsing from Section 3 is used to compute the probabilistic features ψ(x, u → v|ti, tk) by constraining the POS annotations to pass through tags ti and tk at positions i and k. The total time complexity for this step is O(N 3","|P|k+2",").","2. Having access to ψ(x, u → v|ti, tk), the factor p(i→j←k|ti, tk, x) can be computed in O(N 3",") time using a constrained version of Eisner’s algorithm, as will be explained in Section 4.1.","3. As described in Section 3.1, computing the expectation p(ti, tk|x) takes O(N |P2","|) time using the constrained forward-backward algorithm. The current token position i can have a total of N values, while j and k can be any positions other than i. Also, ti and tk can be any POS tag from","P. Consequently, the feature template T induces","O(N 3","|P|2",") feature instances. Overall, the time","complexity for computing the feature instances gen-","erated by T is O(N 6","|P|k+4","), as results from:","O(N 3","|P|2",") · (O(N 3 |P|k+2",") + O(N 3",") + O(N |P|2",")) While still polynomial, this time complexity is feasible only for small values of N . In general, the time complexity for computing probabilistic features in the full model M2 increases with both the number of stages in the pipeline and the complexity of the features.","Motivated by efficiency, we decided to use the pipeline model M3 in which probabilities are computed only over features that appear in the top scoring annotation ẑ = ⟨ẑ1, ẑ2⟩, where ẑ1 and ẑ2 represent the best POS tagging, and the best dependency parse respectively. In order to further speed up the computation of probabilistic features, we made the following approximations:","1. We consider the POS tagging and the dependency parse independent and rewrite p(z̃|x) as: p(z̃|x) = p(z̃1, z̃2|x) ≈ p(z̃1|x)p(z̃2|x)","2. We enforce an independence assumption between POS tags. Thus, if z̃1 = ⟨ti1 ti2 ...tik ⟩ specifies the tags at k positions in the sentence, then p(z̃1|x) is rewritten as: p(ti1 ti2 ...tik |x) ≈ k ∏ j=1 p(tij |x)","3. We also enforce a similar independence assumption between dependency links. Thus, if z̃2 = ⟨u1 → v1...uk → vk⟩ specifies k dependency links, then p(z̃2|x) is rewritten as: p(u1 → v1...uk → vk|x) ≈ k ∏ l=1 p(ul → vl|x) For example, the probability p(z̃|x) of the feature instance NNS2 → thought4 ← RB3 is approximated as: p(z̃|x) ≈ p(z̃1|x) · p(z̃2|x) p(z̃1|x) ≈ p(t2 = NNS|x) · p(t3 = RB|x) p(z̃2|x) ≈ p(2 → 4|x) · p(3 → 4|x)","We will use M ′ 3 to refer to the resulting model. 676 4.1 Probabilistic Dependency Features The probabilistic POS features p(ti|x) are computed using the forward-backward procedure in CRFs, as described in Section 3.1. To completely specify the pipeline model for NER, we also need an efficient method for computing the probabilistic dependency features p(u → v|x), where u → v is a dependency edge between positions u and v in the sentence x. MSTPARSER is a large-margin method that computes an unbounded score s(x, y) for any given sentence x and dependency structure y ∈ Y(x) using the following edge-based factorization:","s(x, y) = ∑ u→v∈ys(x, u → v) = w ∑","u→v∈yφ(x, u → v) The following three steps describe a general method for associating probabilities with output substructures. The method can be applied whenever a structured output is associated a score value that is unbounded in R, assuming that the score of the entire output structure can be computed efficiently based on a factorization into smaller substructures.","S1. Map the unbounded score s(x, y) from R into [0, 1] using the softmax function (Bishop, 1995): n(x, y) =","es(x,y)","∑ y∈Y(x) es(x,y) The normalized score n(x, y) preserves the ranking given by the original score s(x, y). The normalization constant at the denominator can be computed in O(N 3",") time by replacing the max operator with the sum operator inside Eisner’s chart parsing algorithm.","S2. Compute a normalized score for the sub-structure by summing up the normalized scores of all the complete structures that contain it. In our model, dependency edges are substructures, while dependency trees are complete structures. The normalized score will then be computed as: n(x, u → v) = ∑ y∈Y(x),u→v∈y n(x, y) The sum can be computed in O(N 3",") time using a constrained version of the algorithm that computes the normalization constant in step S1. This constrained version of Eisner’s algorithm works in a similar manner with the constrained forward backward algorithm by restricting the dependency structures to contain a predefined edge or set of edges.","S3. Use the isotonic regression method of Zadrozny and Elkan (2002) to map the normalized scores n(x, u → v) into probabilities p(u → v|x). A potential problem with the softmax function is that, depending on the distribution of scores, the exponential transform could dramatically overinflate the higher scores. Isotonic regression, by redistributing the normalized scores inside [0, 1], can alleviate this problem. 4.2 Experimental Results We test the pipeline model M ′","3 versus the traditional model M1 on the task of detecting mentions of person entities in the ACE dataset2",". We use the standard training – testing split of the ACE 2002 dataset in which the training dataset is also augmented with the documents from the ACE 2003 dataset. The combined dataset contains 674 documents for training and 97 for testing. We implemented the CRF model in MALLET using three different sets of features: Tree, Flat, and Full corresponding to the union of all flat and tree features. The POS tagger and the dependency parser were trained on sections 2-21 of the Penn Treebank, followed by an isotonic regression step on section 23 for the dependency parser. We compute precision recall (PR) graphs by varying a threshold on the token level confidence output by the CRF tagger, and summarize the tagger performance using the area under the curve. Table 4 shows the results obtained by the two models under the three feature settings. The model based on probabilistic fea-Model Tree Flat Full","M ′ 3 76.78 77.02 77.96 M1 74.38 76.53 77.02 Table 4: Mention detection results. tures consistently outperforms the traditional model, especially when only tree features are used. Dependency parsing is significantly less accurate than POS tagging. Consequently, the improvement for the tree based model is more substantial than for the flat 2 URL: http://www.nist.gov/speech/tests/ace 677 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Precision Recall Probabilistic Traditional Figure 4: PR graphs for tree features. model, confirming our expectation that probabilistic features are more useful when upstream stages in the pipeline are less accurate. Figure 4 shows the PR curves obtained for the tree-based models, on which we see a significant 5% improvement in precision over a wide range of recall values."]},{"title":"5 Related Work","paragraphs":["In terms of the target task – improving the performance of linguistic pipelines – our research is most related to the work of Finkel et al. (2006). In their approach, output samples are drawn at each stage in the pipeline conditioned on the samples drawn at previous stages, and the final output is determined by a majority vote over the samples from the final stage. The method needs very few samples for tasks such as textual entailment, where the final outcome is binary, in agreement with a theoretical result on the rate of convergence of the vot-ing Gibbs classifier due to Ng and Jordan (2001). While their sampling method is inherently approximate, our full pipeline model M2 is exact in the sense that feature expectations are computed exactly in polynomial time whenever the inference step at each stage can be done in polynomial time, irrespective of the cardinality of the final output space. Also, the pipeline models M2 and M3 and their more efficient alternatives propagate uncertainty during both training and testing through the vector of probabilistic features, whereas the sampling method takes advantage of the probabilistic nature of the outputs only during testing. Overall, the two approaches can be seen as complementary. In order to be applicable with minimal engineering effort, the sampling method needs NLP researchers to write packages that can generate samples from the posterior. Similarly, the new pipeline models could be easily applied in a diverse range of applications, assuming researchers develop packages that can efficiently compute marginals over output substructures."]},{"title":"6 Conclusions and Future Work","paragraphs":["We have presented a new, general method for improving the communication between consecutive stages in pipeline models. The method relies on the computation of probabilities for count features, which translates in adding a polynomial factor to the overall time complexity of the pipeline whenever the inference step at each stage is done in polynomial time, which is the case for the vast majority of inference algorithms used in practical NLP applications. We have also shown that additional independence assumptions can make the approach more practical by significantly reducing the time complexity. Existing learning based models can implement the new method by replacing the original feature vector with a more dense vector of probabilistic features3",". It is essential that every stage in the pipeline produces probabilistic features, and to this end we have described an effective method for associating probabilities with output substructures.","We have shown for NER that simply using the probabilities associated with features that appear only in the top annotation can lead to useful improvements in performance, with minimal engineering effort. In future work we plan to empirically evaluate NER with an approximate version of the full model M2 which, while more demanding in terms of time complexity, could lead to even more significant gains in accuracy. We also intend to comprehensively evaluate the proposed scheme for computing probabilities by experimenting with alternative normalization functions."]},{"title":"Acknowledgements","paragraphs":["We would like to thank Rada Mihalcea and the anonymous reviewers for their insightful comments and suggestions.","3","The Java source code will be released on my web page. 678"]},{"title":"References","paragraphs":["Christopher M. Bishop. 1995. Neural Networks for Pattern Recogntion. Oxford University Press.","Aron Culotta and Andrew McCallum. 2004. Confidence estimation for information extraction. In Proceedings of Human Language Technology Conference and North American Chapter of the Association for Computational Linguistics (HLT-NAACL), Boston, MA.","Jason M. Eisner. 1996. Three new probabilistic models for dependency parsing: An exploration. In Proceedings of the 16th Conference on Computational linguistics, pages 340–345, Copenhagen, Denmark.","Jenny R. Finkel, Christopher D. Manning, and Andrew Y. Ng. 2006. Solving the problem of cascading errors: Approximate Bayesian inference for linguistic annotation pipelines. In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, pages 618–626, Sydney, Australia.","John Lafferty, Andrew McCallum, and Fernando Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of 18th International Conference on Machine Learning (ICML-2001), pages 282–289, Williamstown, MA.","M. Marcus, B. Santorini, and M. A. Marcinkiewicz. 1993. Building a large annotated corpus of English: The Penn treebank. Computational Linguistics, 19(2):313–330.","Andrew Kachites McCallum. 2002. Mallet: A machine learning for language toolkit. http://mallet.cs.umass.edu.","Ryan McDonald, Koby Crammer, and Fernando Pereira. 2005. Online large-margin training of dependency parsers. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics (ACL-05), pages 91–98, Ann Arbor, Michigan.","Andrew Y. Ng and Michael I. Jordan. 2001. Convergence rates of the Voting Gibbs classifier, with application to bayesian feature selection. In Proceedings of 18th International Conference on Machine Learning (ICML-2001), pages 377–384, Williamstown, MA.","Adwait Ratnaparkhi. 1996. A maximum entropy model for part of speech tagging. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP-96), pages 133–141, Philadelphia, PA.","D. Roth and W. Yih. 2004. A linear programming formulation for global inference in natural language tasks. In Proceedings of the Eighth Conference on Computational Natural Language Learning (CoNLL-2004), pages 1–8, Boston, MA. Bernhard Schölkopf and Alexander J. Smola. 2002. Learning with kernels - support vector machines, regularization, optimization and beyond. MIT Press, Cambridge, MA.","Charles Sutton and Andrew McCallum. 2005. Joint parsing and semantic role labeling. In CoNLL-05 Shared Task.","Charles Sutton, Khashayar Rohanimanesh, and Andrew McCallum. 2004. Dynamic conditional random fields: Factorized probabilistic models for labeling and segmenting sequence data. In Proceedings of 21st International Conference on Machine Learning (ICML-2004), pages 783–790, Banff, Canada, July.","Vladimir N. Vapnik. 1998. Statistical Learning Theory. John Wiley & Sons.","Ben Wellner, Andrew McCallum, Fuchun Peng, and Michael Hay. 2004. An integrated, conditional model of information extraction and coreference with application to citation matching. In Proceedings of 20th Conference on Uncertainty in Artificial Intelligence (UAI-2004), Banff, Canada, July.","Bianca Zadrozny and Charles Elkan. 2002. Transforming classifier scores into accurate multiclass probability estimates. In Proceedings of the Eighth ACM SIGKDD International Conference on Knowl-edge Discovery and Data Mining (KDD-2002), Edmonton, Alberta. 679"]}]}
