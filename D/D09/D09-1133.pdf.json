{"sections":[{"title":"","paragraphs":["Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1280–1288, Singapore, 6-7 August 2009. c⃝2009 ACL and AFNLP "]},{"title":"Improving Nominal SRL in Chinese Language with Verbal SRL In-formation and Automatic Predicate Recognition","paragraphs":[""]},{"title":"Junhui Li","paragraphs":["†"]},{"title":"Guodong Zhou","paragraphs":["†∗"]},{"title":"Hai Zhao","paragraphs":["†‡"]},{"title":"Qiaoming Zhu","paragraphs":["†"]},{"title":"Peide Qian","paragraphs":["†"]},{"title":"","paragraphs":["†"]},{"title":"Jiangsu Provincial Key Lab for Computer Information Processing TechnologiesSchool of Computer Science and Technology Soochow University, Suzhou, China 215006","paragraphs":["‡"]},{"title":"Department of Chinese, Translation and Linguistics City University of HongKong, China Email: {lijunhui,gdzhou,hzhao,qmzhu,pdqian}@suda.edu.cn","paragraphs":["    ∗ Corresponding author"]},{"title":"Abstract","paragraphs":["This paper explores Chinese semantic role labeling (SRL) for nominal predicates. Besides those widely used features in verbal SRL, various nominal SRL-specific features are first included. Then, we improve the performance of nominal SRL by integrating useful features derived from a state-of-the-art verbal SRL system. Finally, we address the issue of automatic predicate recognition, which is essential for a nominal SRL system. Evaluation on Chinese NomBank shows that our research in integrating various features derived from verbal SRL significantly improves the performance. It also shows that our nominal SRL system much outperforms the state-of-the-art ones."]},{"title":"1. Introduction","paragraphs":["Semantic parsing maps a natural language sentence into a formal representation of its meaning. Due to the difficulty in deep semantic parsing, most of previous work focuses on shallow semantic parsing, which assigns a simple structure (such as WHO did WHAT to WHOM, WHEN, WHERE, WHY, HOW) to each predicate in a sentence. In particular, the well-defined semantic role labeling (SRL) task has been drawing more and more attention in recent years due to its importance in deep NLP applications, such as question answering (Narayanan and Harabagiu, 2004), information extraction (Surdeanu et al., 2003), and co-reference resolution (Ponzetto and Strube, 2006). Given a sentence and a predicate (either a verb or a noun) in it, SRL recognizes and maps all the constituents in the sentence into their corresponding semantic arguments (roles) of the predicate. According to the predicate types, SRL could be divided into SRL for verbal predicates (verbal SRL, in short) and SRL for nominal predicates (nominal SRL, in short).","During the past few years, verbal SRL has dominated the research on SRL with the availability of FrameNet (Baker et al., 1998), PropBank (Palmer et al., 2005), and the consecutive CoNLL shared tasks (Carreras and Màrquez, 2004 & 2005) in English language. As a complement to PropBank on verbal predicates, NomBank (Meyers et al., 2004) annotates nominal predicates and their corresponding semantic roles using similar semantic framework as PropBank. As a representative, Jiang and Ng (2006) pioneered the exploration of various nominal SRL-specific features besides the traditional verbal SRL-related features on NomBank. They achieved the performance of 72.73 and 69.14 in F1-measure on golden and automatic syntactic parse trees, respectively, given golden nominal predicates.","For SRL in Chinese, Sun and Jurafsky (2004) and Pradhan et al. (2004) pioneered the research on Chinese verbal and nominal SRLs, respectively, on small private datasets. Taking the advantage of recent release of Chinese PropBank (Xue and Palmer, 2003) and Chinese NomBank (Xue, 2006a), Xue and his colleagues (Xue and Palmer 2005; Xue 2006b; Xue, 2008) pioneered the exploration of Chinese verbal and nominal SRLs, given golden predicates. Among them, Xue and Palmer (2005) studied Chinese verbal SRL using Chinese PropBank and achieved the performance of 91.3 and 61.3 in F1-measure on golden and automatic syntactic parse trees, respectively. Xue (2006b) extended their study on Chinese nominal SRL and attempted to improve the performance of nominal SRL by simply in-1280  cluding the Chinese PropBank training instances into the training data for nominal SRL on Chinese NomBank. However, such integration was empirically proven unsuccessful due to the different nature of certain features for verbal and nominal SRLs. Xue (2008) further improved the performance on both verbal and nominal SRLs with a better syntactic parser and new features. Ding and Chang (2008) focused on argument classification for Chinese verbal predicates with hierarchical feature selection strategy. They achieved the classification precision of 94.68% on golden parse trees on Chinese PropBank.","This paper focuses on Chinese nominal SRL. This is done by adopting a traditional verbal SRL architecture to handle Chinese nominal predicates with additional nominal SRL-specific features. Moreover, we significantly enhance the performance of nominal SRL by properly integrating various features derived from verbal SRL. Finally, this paper investigates the effect of automatic nominal predicate recognition on the performance of Chinese nominal SRL. Although previous research (e.g. CoNLL’2008) in English nominal SRL reveals the importance of automatic predicate recognition, there has no reported research on automatic predicate recognition in Chinese nominal SRL.","The rest of this paper is organized as follows: Section 2 introduces Chinese NomBank while the baseline nominal SRL system is described in Section 3 with traditional and nominal SRL-specific features. Then, the baseline nominal SRL system is improved by integrating useful features derived from verbal SRL (Section 4) and extended with automatic recognition of nominal predicates (Section 5). Section 6 gives experimental results and discussion. Finally, Section 7 concludes the paper."]},{"title":"2. Chinese NomBank","paragraphs":["Chinese NomBank (Xue, 2006a) adopts similar semantic framework as NomBank, and focuses on Chinese nominal predicates with their arguments in Chinese TreeBank. The semantic arguments include: 1) Core arguments: Arg0 to Arg5. Generally,","Arg0 and Arg1 denotes the agent and the","patient, respectively, while arguments from","Arg2 to Arg5 are predicate-specific. 2) Adjunct arguments, which are universal to","all predicates, e.g. ArgM-LOC for locative,","and ArgM-TMP for temporal. ","All the arguments are annotated on parse tree nodes with their boundaries aligning with the spans of tree nodes. Figure 1 gives an example with two nominal predicates and their respective arguments, while the nominal predicate “投资 /investment” has two core arguments, “NN(外商 /foreign businessman)” as Arg0 and “NN(银行 /bank)” as Arg1, and the other nominal predicate “ 贷款/loan” also has two core arguments, “NP(中国银行/Bank of China)” as Arg1 and Figure 1: Two nominal predicates and their arguments in the style of NomBank. 向 外商 投资 银行 提供 四十亿 P NN NN NN VV NN NN Arg0/Rel1 Rel1 Arg1/Rel1 NP PP Arg0/Rel2 ArgM-MNR/Rel2 Rel2 NP CD QP NP VP VP 人民币 贷款 。 NN NN PU NP Arg1/Rel2 IP 中国 银行 Sup/Rel2 Bank of China to Foreign Investment Bank provide 4 billion RMB loan . Bank of China provides 4 billion RMB loan to Foreign Investment Bank. 1281  “PP( 向外商投资银行/to Foreign Investment Bank)” as Arg0, and 1 adjunct argument, “NN(人民币/RMB)” as ArgM-MNR, denoting the manner of loan. It is worth noticing that there is a (Chinese) NomBank-specific label in Figure 1, Sup (support verb) (Xue, 2006a), in helping introduce the arguments, which occur outside the nominal predicate-headed noun phrase. This is illustrated by the nominal predicate “贷款/loan”, whose Arg0 and Arg1 are both realized outside the nominal predicate-headed noun phrase, NP(四十亿人民币贷款/4 billion RMB loan). Normally, a verb is marked as a support verb only when it shares some arguments with the nominal predicate."]},{"title":"3. Baseline: Chinese Nominal SRL","paragraphs":["Popular SRL systems usually formulate SRL as a classification problem, which annotates each constituent in a parse tree with a semantic role label or with the non-argument label NULL. Besides, we divide the system into three consecutive phases so as to overcome the imbalance between the training instances of the NULL class and those of any other argument classes.","Argument pruning. Here, several heuristic rules are adopted to filter out constituents, which are most likely non-arguments. According to the argument structures of nominal predicates, we categorize arguments into two types: arguments inside NP (called inside arguments) and arguments introduced via a support verb (called outside arguments), and handle them separately. For the inside arguments, the following three heuristic rules are applied to find inside argument candidates: z All the sisters of the predicate are candi-","dates. z If a CP or DNP node is a candidate, its chil-","dren are candidates too. z For any node X, if its parent is an ancestral","node of the predicate, and the internal","nodes along the path between X and the","predicate are all NPs, then X is a candidate.","For outside arguments, we look for the support verb of the focus nominal predicate, and then adopt the rules as proposed in Xue and Palmer (2005) to find the candidates for the support verb, since outside argument candidates are introduced via this support verb. That to say, the argument candidates of the support verb are regarded as outside argument candidates of the nominal predicate. However, as support verbs are not annotated explicitly in the testing phase, we identify intervening verbs as alternatives to support verbs in both training and testing phases with the path between the nominal predicate and intervening verb in the form of “VV<VP>[NP>]+","NN”, where “[NP>]+","” denotes one or more NPs. Our statistics on Chinese NomBank shows that 51.96% of nominal predicates have no intervening verb while 48.04% of nominal predicates have only one intervening verb.","Taken the nominal predicate “贷款/loan” in Figure 1 as an example, NN(人民币/RMB) and QP( 四十亿/4 billion) are identified as inside argument candidates, while PP(向外商投资银 行/to Foreign Investment Bank) and NP(中国银 行/Bank of China) are identified as outside argument candidates via the support verb VV(提 供/provide).","Argument identification. A binary classifier is applied to determine the candidates as either valid arguments or non-arguments. It is worth pointing out that we only mark those candidates that are most likely to be NULL (with probability > 0.90) as non-arguments. Our empirical study shows that this little trick much benefits nominal SRL, since argument identification for nominal predicates is much more difficult than that for verbal predicates and thus many arguments would have been falsely marked as non-arguments if the threshold is set as 0.5.","Argument classification. A multi-class classifier is employed to label identified arguments with specific argument labels (including the NULL class for non-argument).","In the following, we first adapt some traditional features, which have been proven effective in verbal SRL, to nominal SRL, and then introduce several nominal SRL-specific features."]},{"title":"3.1. Traditional Features","paragraphs":["Using the feature naming convention as adopted in Jiang and Ng (2006), Table 1 lists the traditional features, where “I” and “C” indicate the features for argument identification and classification, respectively. Among them, the predicate class (b2) feature was first introduced in Xue and Palmer (2005) to overcome the imbalance of the predicate distribution in that some predicates can be only found in the training data while some predicates in the testing data are absent from the training data. In particular, the verb class is classified along three dimensions: the number of arguments, the number of framesets and selected syntactic alternations. For example, 1282  the verb class of “C1C2a” means that it has two framesets, with the first frameset having one argument and the second having two arguments. The symbol “a” in the second frameset represents a type of syntactic alternation.  Feature Remarks: b1-b5(C, I), b6-b7(C) b1 Predicate: the nominal predicate itself. (贷款","/loan) b2 Predicate class: the verb class that the predi-","cate belongs to. (C4a) b3 Head word (b3H) and its POS (b3P). (银行","/bank, NN) b4 Phrase type: the syntactic category of the","constituent. (NP) b5 Path: the path from the constituent to the","nominal predicate.","(NP<IP>VP>VP>NP>NP>NN) b6 Position: the positional relationship of the","constituent with the predicate. “left” or","“right”. (left) b7 First word (b7F) and last word (b7L) of the","focus constituent. (中国/China, 银行/bank) Combined features: b11-b14(C, I), b15(C) b11: b1&b4; b12: b1&b3H; b13: b2&b4; b14: b2&b3H; b15: b5&b6 Table 1: Traditional features and their instantiations for argument identification and classification, with NP(中国银行/Bank of China) as the focus constituent and NN(贷款/loan) as the nominal predicate, regarding Figure 1."]},{"title":"3.2. Nominal SRL-specific Features","paragraphs":["To capture more useful information in the predicate-argument structure, we also study additional features which provide extra information. Statistics on Chinese NomBank show that about 40% of pruned inside candidates are arguments. Since inside arguments usually locate near to the nominal predicate, its surroundings are expected to be helpful in SRL. Table 2 shows the features in better capturing the details between inside arguments and nominal predicates. Specially, features ai6 and ai7 are sister-related features, inspired by the features related with the neighboring arguments in Jiang and Ng (2006).","Statistics on NomBank and Chinese NomBank show that about 20% and 22% of arguments are introduced via a support verb, respectively. Since a support verb pivots outside arguments and the nominal predicate on its two sides, support verbs play an important role in labeling these arguments. Here, we also identify intervening verbs as alternatives to support verbs since support verbs are not explicitly in the testing phase. Table 3 lists the intervening verb-related features (ao1-ao4, ao11-ao14) employed in this paper.  Feature Remarks ai1 Whether the focus constituent is adjacent to the predicate. Yes or No. (Yes) ai2","The headword (ai2H) and pos (ai2P) of the","predicate’s nearest right sister. (银行/bank,","NN) ai3 Whether the predicate has right sisters. Yes","or No. (Yes) ai4 Compressed path of b5: compressing se-","quences of identical labels into one.","(NN<NP>NN) ai5 Whether the predicate has sisters. Yes or","No. (Yes) ai6 For each sister of the focus constituent,","combine b3H&b4&b5&b6. ( 银行","/bank&NN & NN<NP>NN&right) ai7 Coarse version of ai6, b4&b6. (NN&right) Table 2: Additional features and their instantiations for inside argument candidates, with “NN( 外商 /foreign businessman)” as the focus constituent and “NN( 投资/investment)” as the nominal predicate, regarding Figure1.  Feature Remarks ao1 Intervening verb itself. (提供/provide) ao2 The verb class that the intervening verb belongs to. (C3b) ao3 The path from the focus constituent to the intervening verb. (NP<IP>VP>VP>VV) ao4 The compressed path of ao3: compressing sequences of identical labels into one. (NP<IP>VP>VV) Combined features: ao11-ao14 ao11: ao1&ao3; ao12: ao1&ao4; ao13: ao2&ao3; ao14: ao2&ao4. Table 3: Additional features and their instantiations for outside argument candidates, with “NP(中国银行 /Bank of China)” as the focus constituent and “贷款 /loan” as the nominal predicate, regarding Figure1.","Feature selection. Some Features proposed above may not be effective in tasks of identification and classification. We adopt the greedy feature selection algorithm as described in Jiang and Ng (2006) to pick up positive features empirically and incrementally according to their contributions on the development data. The algorithm repeatedly selects one feature each time which contributes most, and stops when adding any of the remaining features fails to improve the performance. As far as the SRL task concerned, the whole feature selection process could be done as follows: 1). Feature selection for argument identification: run the selection algo-1283  rithm with the basic set of features (b1-b5, b11b14) to pick up effective features from (ai1-ai7, ao1-ao4, ao11-ao14); 2). Feature selection for argument classification: fix the output returned in step1 as the feature set of argument identification, and run the selection algorithm with the basic set of features (b1-b7, b11-b15) to select positive features from (ai1-ai7, ao1-ao4, ao11-ao14) for argument classification."]},{"title":"4. Integrating Features derived from Verbal SRL","paragraphs":["Since Chinese PropBank and NomBank are annotated on the same data set with the same lexical guidelines (e.g. frame files), it may be interesting to investigate the contribution of Chinese verbal SRL on the performance of Chinese nominal SRL. In the frame files, argument labels are defined with regard to their semantic roles to the predicate, either a verbal or nominal predicate. For example, in the frame file of predicate “贷款/loan”, the borrower is always labeled with Arg0 and the lender labeled with Arg1. This can be demonstrated by the following two sentences: “贷款/loan” is annotated as a nominal and a verbal predicate in S1 and S2, respectively. S1 [Arg1 中国银行/Bank of China] [Arg0 向外商","投资银行/to Foreign Investment Bank] 提供","/provide [Rel 贷款/loan] S2 [Arg0 中国银行/Bank of China] [Arg1 向外商","投资银行/from Foreign Investment Bank] [Rel","贷款/loan]","Therefore, it is straightforward to augment nominal training instances with verbal ones. However, Xue (2006b) found that simply adding the training instances for verbal SRL to the training data for nominal SRL and indiscriminately extracting the same features in both verbal and nominal SRLs hurt the performance. This may be due to that certain features (e.g. the path feature) are much different for verbal and nominal SRLs. This can be illustrated in sentences S1 and S2: the verbal instances in S2 are negative for semantic role labeling of the nominal predicate “贷款/loan” in S1, since “中国银 行/Bank of China” takes opposite roles in S1 and S2. So does “向外商投资银行/(from/to) Foreign Investment Bank”.","Although several support verb-related features (ao1-ao4, ao11-ao14) have been proposed, one may still ask how large the role support verbs can play in nominal SRL. It is interesting to note that outside arguments and the highest NP phrase headed by the nominal predicate are also annotated as arguments of the support verb in Chinese PropBank. For example, Chinese PropBank marks “中国银行/Bank of China” as Arg0 and “四十亿人民币贷款/4 billion RMB loan” as Arg1 for verb “提供/provide” in Figure1. Let OA be the outside argument, VV be the support verb, and NP be the highest NP phrase headed by the nominal predicate NN, then there exists a pattern “OA VV NN” in the sentence, where the support verb VV plays a certain role in transferring roles between OA and NN. For example, if OA is the agent of VV, then OA is also the agent of phrase VP(VV NN). Like the example in Figure1, supposing a NP is the agent of support verb “提供/provide” as well as VP phrase (“ 提供四十亿人民币贷款/provide 4 billion RMB loan”), we can infer that the NP is the lender of the nominal predicate “贷款/loan” in-dependently on any other information, such as the NP content and the path from the NP to the nominal predicate “贷款/loan”.","Let C be the focus constituent, V be the intervening verb, and NP be the highest NP headed by the nominal predicate. Table 4 shows the features (ao5-ao8, p1-p7) derived from verbal SRL. In this paper, we develop a state-of-the-art Chinese verbal SRL system, similar to the one as shown in Xue (2008), to achieve the goal. Based on golden parse trees on Chinese PropBank, our Chinese verbal SRL system achieves the performance of 92.38 in F1-measure, comparable to Xue (2008) which achieved the performance of 92.0 in F1-measure.  Feature Remarks ao5 Whether C is an argument for V. Yes or No ao6 The semantic role of C for V. ao7 Whether NP is an argument for V. Yes or No ao8 The semantic role of NP for V. Combined features: p1-p7 p1: ao1&ao5; p2: ao1&ao6; p3: ao1&ao5&b1; p4: ao1&ao6&b1; p5: ao1&apo7; p6: ao1&ao8; p7: ao5&ao7. Table 4: Features derived from verbal SRL."]},{"title":"5. Automatic Predicate Recognition","paragraphs":["Unlike Chinese PropBank where almost all the verbs are annotated as predicates, Chinese NomBank only marks those nouns having arguments as predicates. Statistics on Chinese NomBank show that only 17.5% of nouns are marked as predicates. It is possible that a noun is a predi-1284  cate in some cases but not in others. Previous Chinese nominal SRL systems (Xue, 2006b; Xue, 2008) assume that nominal predicates have already been manually annotated and thus are available. To our best knowledge, there is no report on addressing automatic recognition of nominal predicates on Chinese nominal SRL.","Automatic recognition of nominal predicates can be cast as a binary classification (e.g., Predicate vs. Non-Predicate) problem. This paper employs the convolution tree kernel, as proposed in Collins and Duffy (2001), on automatic recognition of nominal predicates.","Given the convolution tree kernel, the key problem is how to extract a parse tree structure from the parse tree for a nominal predicate candidate. In this paper, the parse tree structure is constructed as follows: 1) starting from the predicate candidate’s POS node, collect all of its sister nodes (with their headwords); 2). recursively move one level up and collect all of its sister nodes (with their headwords) till reaching a non-NP node. Specially, in order to explicitly mark the positional relation between a node and the predicate candidate, all nodes on the left side of the candidate are augmented with tags 1 and 2 for nodes on the right side. Figure 2 shows an example of the parse tree structure with regard to the predicate candidate “贷款/loan” as shown in Figure 1.","In our extra experiments we found global statistic features (e.g. g1-g5) about the predicate candidate are helpful in a feature vector-based method for predicate recognition. Figure 2 makes an attempt to utilize those features in kernel-based method. We have explored other ways to include those global features. However, the way in Figure 2 works best.  ","Let the predicate candidate be w0, and its left and right neighbor words be w-1 and w1, respectively. The five global features are defined as follows. g1 Whether w0 is ever tagged as a verb in the","training data? Yes or No. g2 Whether w0 is ever annotated as a nominal predicate in the training data? Yes or No. g3 The most likely label for w0 when it occurs together with w-1 and w1. g4 The most likely label for w0 when it occurs together with w-1. g5 The most likely label for w0 when it occurs together with w1."]},{"title":"6. Experiment Results and Discussion","paragraphs":["We have evaluated our Chinese nominal SRL system on Chinese NomBank with Chinese PropBank 2.0 as its counterpart."]},{"title":"6.1. Experimental Settings","paragraphs":["This version of Chinese NomBank consists of standoff annotations on the files (chtb_001 to 1151.fid) of Chinese Penn TreeBank 5.1. Following the experimental setting in Xue (2008), 648 files (chtb_081 to 899.fid) are selected as the training data, 72 files (chtb_001 to 040.fid and chtb_900 to 931.fid) are held out as the test data, and 40 files (chtb_041 to 080.fid) as the development data, with 8642, 1124, and 731 propositions, respectively.","As Chinese words are not naturally segmented in raw sentences, two Chinese automatic parsers are constructed: word-based parser (assuming golden word segmentation) and character-based parser (with automatic word segmentation). Here, Berkeley parser (Petrov and Klein, 2007)1"," is chosen as the Chinese automatic parser. With regard to character-based parsing, we employ a Chinese word segmenter, similar to Ng and Low (2004), to obtain the best automatic segmenta-tion result for a given sentence, which is then fed into Berkeley parser for further syntactic parsing. Both the word segmenter and Berkeley parser are developed with the same training and development datasets as our SRL experiments. The word segmenter achieves the performance of 96.1 in F1-measure while the Berkeley parser gives a performance of 82.5 and 85.5 in F1-measure on golden and automatic word segmentation, respectively2",". 人民币 1","In addition, SVMLight with the tree kernel function (Moschitti, 2004) 3","is selected as our classifier. In order to handle multi-classification  1 Berkeley Parser. http://code.google.com/p/berkeleyparser/ 2 POSs are not counted in evaluating the performance of word-based syntactic parser, but they are counted in evaluating the performance of character-based parser. Therefore the F1-measure for the later is higher than that for the for-mer. 3 SVM-LIGHT-TK. http://dit.unitn.it/~moschitt/","Figure 2: Semantic sub-tree for nominal predicateRMB 贷款 loan","提供 1","provide","四十亿 1","4 billion VV1 NN1 NN NP QP1 NP VP g1 .... g5 1285  problem in argument classification, we apply the one vs. others strategy, which builds K classifiers so as to separate one class from all others. For argument identification and classification, we adopt the linear kernel and the training parameter C is fine-tuned to 0.220. For automatic recognition of nominal predicates, the training parameter C and the decay factor"]},{"title":"λ","paragraphs":["in the convolution tree kernel are fine-tuned to 2.0 and 0.2, respectively."]},{"title":"6.2. Results with Golden Parse Trees and Golden Nominal Predicates Effect of nominal SRL-specific features","paragraphs":["  Rec.(%) Pre.(%) F1 traditional features 62.83 73.58 67.78 +nominal SRL-specific features 69.90 75.11 72.55 Table 5: The performance of nominal SRL on the development data with golden parse trees and golden nominal predicates After performing the greedy feature selection algorithm on the development data, features {ao1, ai6, ai2P, ai5, ao2, ao12, ao14}, as proposed in Section 3.2, are selected consecutively for argument identification, while features {ai7, ao1, ai1, ao2, ai5, ao4} are selected for argument classification. Table 5 presents the SRL results on the development data. It shows that nominal SRL-specific features significantly improve the performance from 67.78 to 72.55 ( ) in F1-measure."]},{"title":"05.0;","paragraphs":["2"]},{"title":"<pχEffect of features derived from verbal SRL","paragraphs":["  Features Rec.(%) Pre.(%) F1 baseline 67.86 73.63 70.63 +ao5 68.15 73.60 70.77 (+0.14) +ao6 67.66 72.80 70.14 (-0.49) +ao7 68.20 75.41 71.62 (+0.99) +ao8 68.30 75.39 71.67 (+1.04) +p1 67.91 74.40 71.00 (+0.37) +p2 67.76 74.20 70.83 (+0.20) +p3 67.96 74.69 71.16 (+0.53) +p4 68.01 74.18 70.96 (+0.33) +p5 68.01 75.01 71.39 (+0.76) +p6 68.20 75.12 71.49 (+0.86) +p7 68.40 75.70 71.87 (+1.24)","Table 6: Effect of features derived from verbal SRL","on the performance of nominal SRL on the test data","with golden parse trees and golden nominal predi-","cates. The first row presents the performance using","traditional and nominal SRL-specific features.","   Rec.(%) Pre.(%) F1 baseline 67.86 73.63 70.63","+features derived","from verbal SRL 68.40 77.51 72.67","Xue (2008) 66.1 73.4 69.6 Table 7: The performance of nominal SRL on the test data with golden parse trees and golden nominal predicates  Table 6 shows the effect of features derived from verbal SRL in an incremental way. It shows that only the feature ao6 has negative effect due to its strong relevance with intervening verbs and thus not included thereafter. Table 7 shows the performance on the test data with or without using the features derived from the verbal SRL system. It shows these features significantly improve the performance ( ) on nominal SRL. Table 7 also shows our system outperforms Xue (2008) by 3.1 in F1-measure."]},{"title":"05.0;","paragraphs":["2"]},{"title":"<pχ6.3. Results with Automatic Parse Trees and Golden Nominal Predicates","paragraphs":["In previous section we have assumed the availability of golden parse trees during the testing process. Here we conduct experiments on automatic parse trees, using the Berkeley parser. Since arguments come from constituents in parse trees, those arguments, which do not align with any syntactic constituents, are simply discarded. Moreover, for any nominal predicate segmented incorrectly by the word segmenter, all its arguments are unable to be labeled neither. Table 8 presents the SRL performance on the test data by using automatic parse trees. It shows that the performance drops from 72.67 to 60.87 in F1-measure when replacing golden parse trees with word-based automatic ones, partly due to the absence of 6.9% arguments in automatic trees, and wrong POS tagging of nominal predicates. Table 8 also compares our system with Xue (2008). It shows that our system also outperforms Xue (2008) on Chinese NomBank. Rec. (%) Pre. (%) F1 This paper 56.95(53.55) 66.74(66.69) 60.87(59.40) Xue (2008) 53.1 (52.9) 62.9 (62.3) 57.6 (57.3) Table 8: The performance of nominal SRL on the test data with automatic parse trees and golden predicates. Here, the numbers outside the parentheses indicate the performance using a word-based parser, while the numbers inside indicate the performance using a character-based parser4",".    4 About 1.6% nominal predicates are mistakenly segmented by the character-based parser, thus their arguments are missed directly. 1286 "]},{"title":"6.4. Results with Automatic Nominal Predi-cates","paragraphs":["So far nominal predicates are assumed to be manually annotated and available. Here we turn to a more realistic scenario in which both the parse tree and nominal predicates are automatically obtained. In the following, we first report the results of automatic nominal predicate recognition and then the results of nominal SRL on automatic recognition of nominal predicates. Results of nominal predicate recognition Parses g1-g5 Rec.(%) Pre.(%) F1","no 91.46 88.93 90.18 golden","yes 92.62 89.36 90.96 word-based yes 86.39 81.80 84.03 character-based yes 84.79 81.94 83.34 Table 9: The performance of automatic nominal predicate recognition on the test data  Table 9 lists the predicate recognition results, using the parse tree structure, as shown in Section 5, and the convolution tree kernel, as proposed in Collins and Duffy (2001). The second column (g1-g5) indicates whether the global features (g1-g5) are included in the parse tree structure. We have also defined a simple rule that treats a noun which is ever a verb or a nominal predicate in the training data as a nominal predicate. Based on golden parse trees, the rule receives the performance of 81.40 in F1-measure. This suggests that our method significantly outperforms the simple rule-based one. Table 9 also shows that: z As a complement to local structural informa-","tion, global features improve the performance","of automatic nominal predicate recognition","by 0.78 in F1-measure. z The word-based syntactic parser decreases","the F1-measure from 90.96 to 84.03, mostly","due to the POSTagging errors between NN","and VV, while the character-based syntactic","parser further drops the F1-measure by 0.69,","due to automatic word segmentation. Results with automatic predicates  Parses Predicates Rec.(%) Pre.(%) F1","golden 68.40 77.51 72.67 golden","automatic 65.07 74.65 69.53","golden 55.95 66.74 60.87 word-based automatic 52.67 59.56 55.90","golden 53.55 66.69 59.40 character-based automatic 50.66 59.60 54.77 Table 10: The performance of nominal SRL on the test data with the choices of golden/automatic parse trees and golden/automatic predicates In order to have a clear performance comparison among nominal SRL on golden/automatic parse trees and golden/automatic predicates, Table 10 lists all the results in those scenarios."]},{"title":"6.5. Comparison Chinese nominal SRL vs. Chinese verbal SRL","paragraphs":[" Comparison with Xue (2008) shows that the performance of Chinese nominal SRL is about 20 lower (e.g. 72.67 vs. 92.38 in F1-measure) than that of Chinese verbal SRL, partly due to the smaller amount of annotated data (about 1/5) in Chinese NomBank than that in Chinese PropBank. Moreover, according to Chinese NomBank annotation criteria (Xue 2006a), even when a noun is a true deverbal noun, not all of its modifiers are legitimate arguments or adjuncts of this predicate. Only arguments that can co-occur with both the nominal and verbal forms of the predicate are considered in the NomBank annotation. This means that the judgment of arguments is semantic rather than syntactic. These facts may also partly explain the lower nominal SRL performance, especially the performance of argument identification. This can be illustrated by the statistics on the development data that 96% (40%) of verbal (nominal) predicates’ sisters are annotated as arguments. Finally, the predicate-argument structure of nominal predicates is more flexible and complicated than that of verbal predicates as illustrated in Xue (2006a). Chinese nominal SRL vs. English nominal SRL Liu and Ng (2007) reported the performance of 77.04 and 72.83 in F1-measure on English NomBank when golden and automatic parse trees are used, respectively. Taking into account that Chinese verbal SRL achieves comparable performance with English verbal SRL on golden parse trees, the performance gap between Chinese and English nominal SRL (e.g. 72.67 vs. 77.04 in F1-measure) presents great challenge for Chinese nominal SRL. Moreover, while automatic parse trees only decrease the performance of English nominal SRL by about 4.2 in F1-measure, automatic parse trees significantly decrease the performance of Chinese nominal SRL by more than 12 in F1-measure due to the much lower performance of Chinese syntactic parsing."]},{"title":"7. Conclusion","paragraphs":["In this paper we investigate nominal SRL in Chinese language. In particular, some nominal SRL-specific features are included to improve 1287  the performance. Moreover, various features derived from verbal SRL are properly integrated into nominal SRL. Finally, a convolution tree kernel is adopted to address the issue of automatic nominal predicates recognition, which is essential in a nominal SRL system.","To our best knowledge, this is the first research on 1) Exploring Chinese nominal SRL on auto-","matic parse trees with automatic predicate","recognition; 2) Successfully integrating features derived","from Chinese verbal SRL into Chinese nomi-","nal SRL with much performance improve-","ment."]},{"title":"Acknowledgement","paragraphs":["This research was supported by Project 60673041 and 60873150 under the National Natural Science Foundation of China, Project 2006AA01Z147 under the “863” National High-Tech Research and Development of China, and Project BK2008160 under the Natural Science Foundation of the Jiangsu province of China. We also want to thank Dr. Nianwen Xue for share of the verb class file. We also want to thank the reviewers for insightful comments."]},{"title":"References","paragraphs":["Collin F. Baker, Charles J. Fillmore, and John B. Lowe. 1998. The Berkeley FrameNet Project. In Proceedings of COLING-ACL 1998.","Xavier Carreras and Lluis Màrquez. 2004. Introduc-tion to the CoNLL-2004 Shared Task: Semantic Role Labeling. In Proceedings of CoNLL 2004.","Xavier Carreras and Lluis Màrquez. 2005. Introduc-tion to the CoNLL-2005 Shared Task: Semantic Role Labeling. In Proceedings of CoNLL 2005.","Michael Collins and Nigel Duffy. 2001. Convolution Kernels for Natural Language. In Proceedings of NIPS 2001.","Weiwei Ding and Baobao Chang. 2008. Improving Chinese Semantic Role Classification with Hierarchical Feature Selection Strategy. In Proceedings of EMNLP 2008.","Zheng Ping Jiang and Hwee Tou Ng. 2006. Semantic role Labeling of NomBank: a Maximum Entropy Approach. In Proceedings of EMNLP 2006.","Chang Liu and Hwee Tou Ng. 2007. Learning Predic-tive Structures for Semantic Role Labeling of NomBank. In Proceedings of ACL 2007.","A. Meyers, R. Reeves, C. Macleod, R. Szekely, V. Zielinska, B. Yong, and R. Grishman. 2004. Annotating Noun Argument Structure for NomBank. In Proceedings of LREC 2004.","Alessandro Moschitti. 2004. A Study on Convolution Kernels for Shallow Semantic Parsing. In Proceedings of ACL 2004.","Srini Narayanan and Sanda Harabagiu. 2004. Ques-tion Answering based on Semantic Structures. In Proceedings of COLING 2004.","Hwee Tou Ng and Jin Kiat Low. 2004. Chinese Part-of-speech Tagging: One-at-a-time or All-at-once? Word-based or Character-based? In Proceedings of EMNLP 2004.","Martha Palmer, Daniel Gildea, and Paul Kingsbury. 2005. The Proposition Bank: An Annotated Corpus of Semantic Roles. Computational Linguistics.","Slav Petrov. and Dan Klein. 2007. Improved Inference for Unlexicalized Parsing. In Proceesings of NAACL 2007.","Simone Paolo Ponzetto and Michael Strube. 2006. Semantic Role Labeling for Coreference Resolu-tion. In Proceedings of EACL 2006.","Sameer Pradhan, Honglin Sun, Wayne Ward, James H. Martin, and Dan Jurafsky. 2004. Parsing Ar-guments of Nominalizations in English and Chinese. In Proceedings of NAACL-HLT 2004.","Honglin Sun and Daniel Jurafsky. 2004. Shallow Semantic Parsing of Chinese. In Proceedings of NAACL 2004.","Mihai Surdeanu, Sanda Harabagiu, John Williams and Paul Aarseth. 2003. Using Predicate-argument Structures for Information Extraction. In Proceedings of ACL 2003.","Mihai Surdeanu, Richard Johansson, Adam Meyers, Lluis Màrquez, and Joakim Nivre. 2008. The CoNLL-2008 Shared Task on Joint Parsing of Syntactic and Semantic Dependencies. In Proceedings of CoNLL 2008.","Nianwen Xue and Martha Palmer. 2003. Annotating the Propositions in the Penn Chinese TreeBank. In Proceedings of 2nd","SIGHAN Workshop on Chinese Language Processing.","Nianwen Xue and Martha Palmer. 2005. Automatic Semantic Role Labeling for Chinese verbs. In Proceedings of IJCAI 2005.","Nianwen Xue. 2006a. Annotating the Predicate-Argument Structure of Chinese Nominalizations. In Proceedings of the LREC 2006.","Nianwen Xue. 2006b. Semantic Role Labeling of Nominalized Predicates in Chinese. In Proceedings of HLT-NAACL 2006.","Nianwen Xue. 2008. Labeling Chinese Predicates with Semantic Roles. Computational Linguistics, 34(2):225-255. 1288"]}]}
