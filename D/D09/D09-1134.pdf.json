{"sections":[{"title":"","paragraphs":["Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1289–1297, Singapore, 6-7 August 2009. c⃝2009 ACL and AFNLP"]},{"title":"On the Use of Virtual Evidence in Conditional Random FieldsXiao LiMicrosoft ResearchOne Microsoft WayRedmond, WA 98052 USAxiaol@microsoft.comAbstract","paragraphs":["Virtual evidence (VE), first introduced by (Pearl, 1988), provides a convenient way of incorporating prior knowledge into Bayesian networks. This work generalizes the use of VE to undirected graphical models and, in particular, to conditional random fields (CRFs). We show that VE can be naturally encoded into a CRF model as potential functions. More importantly, we propose a novel semi-supervised machine learning objective for estimating a CRF model integrated with VE. The objective can be optimized using the Expectation-Maximization algorithm while maintaining the discriminative nature of CRFs. When evaluated on the CLASSIFIEDS data, our approach significantly outperforms the best known solutions reported on this task."]},{"title":"1 Introduction","paragraphs":["Statistical approaches to sequential labeling problems rely on necessary training data to model the uncertainty of a sequence of events. Human’s prior knowledge about the task, on the other hand, often requires minimum cognitive load to specify, and yet can provide information often complementary to that offered by a limited amount of training data. Whenever prior knowledge becomes available, it is desired that such information is integrated to a probabilistic model to improve learning.","Virtual evidence (VE), first introduced by Pearl (1988), offers a principled and convenient way of incorporating external knowledge into Bayesian networks. In contrast to standard evidence (also known as observed variables), VE expresses a prior belief over values of random variables. It has been shown that VE can significantly extend the modeling power of Bayesian networks without complicating the fundamental inference method-ology (Bilmes, 2004; Reynolds and Bilmes, 2005).","This work extends the use of VE to undirected graphical models and, in particular, to conditional random fields (CRFs). We show that VE can be naturally encoded into an undirected graphical model as potential functions. More importantly, we discuss a semi-supervised machine learning setting for estimating CRFs with the presence of VE. As the conditional likelihood objective of CRFs is not directly maximizable with respect to unlabeled data, we propose a novel semi-supervised learning objective that can be optimized using the Expectation-Maximization (EM) algorithm while maintaining the discriminative nature of CRFs.","We apply our model to the CLASSIFIEDS data (Grenager et al., 2005). Specifically, we use VE to incorporate into a CRF model two types of prior knowledge specified in previous works. The first is defined based on the notion of prototypes, i.e., example words for a given label; and the other as-sumes that adjacent tokens tend to have the same label. When unlabeled data becomes available, we further extend the sparse prototype information to other words based on distributional similarity. This results in so-called collocation lists, each consisting of a relatively large number of noisy “prototypes” for a label. Given the fact that these noisy prototypes are often located close to each other in an input sequence, we create a new type of VE based on word collocation to reduce ambiguity. 1289","We compare our CRF model integrated with VE with two state-of-the-art models, i.e., constraint-driven learning (Chang et al., 2007) and generalized expectation criteria (Mann and McCallum, 2008). Experiments show that our approach leads to sequential labeling accuracies superior to the best results reported on this task in both supervised and semi-supervised learning."]},{"title":"2 Related work","paragraphs":["There have been various works that make use of prior knowledge in sequential labeling tasks. Grenager et al. (2005) explicitly constrain the transition matrix of a hidden Markov model (HMM) to favor self transitions, assuming that fields tend to consist of consecutive runs of the same label.","Prototype-drive learning (Haghighi and Klein, 2006) specifies prior knowledge by providing a few prototypes (i.e., canonical example words) for each label. This sparse prototype information is then propagated to other words based on distributional similarity. The relation between words and their prototypes are then used as features in a Markov random field (MRF) model. Since an MRF model aims to optimize the joint probability p(x, y) of input and state sequences, it is possible to apply the EM algorithm for unsupervised/semi-supervised learning.","Constraint-driven learning (Chang et al., 2007) expresses several kinds of constraints in a unified form. In inference, a new decision function is proposed to penalize the violation of the desired constraints as follows,","argmax y λ · F (x, y) − ∑","k ρkd(y, 1Ck (x)) (1) Here λ · F (x, y) is a linear decision function applicable to a number of sequential models, such as HMMs, MRFs and CRFs. Function d is implemented as the Hamming distance (or its approximation) between a hypothesis sequence and the space of state sequences that satisfy the constraint Ci. Due to the nature of the distance function, their work approximates EM training by finding the top K hypothesis sequences and using them as newly labeled instances to update the model. This process is repeated for a number of iterations in a self-training fashion (Yarowsky, 1995).","Generalized expectation criteria (Mann and McCallum, 2008) represent prior knowledge as labeled features, and use such information to regularize semi-supervised learning for CRFs. For-mally, their learning objective consists of the standard CRF training objective, plus a Gaussian prior on model parameters and an additional regularization term:1 ∑","i log pλ(y(i)","|x(i) )− 1","2σ2 ∥λ∥2","−ρD(p̂||p̃λ) (2) In the last term, p̂ and p̃λ both refer to conditional distributions of labels given a feature. While the former is specified by prior knowledge, and the latter is estimated from unlabeled data.","Our approach incorporates prior knowledge as virtual evidence to express preferences over the values of a set of random variables. The notion of VE was first introduced by Pearl (1998) and further developed by Bilmes (2004), both in the context of Bayesian networks. Different from constraint-driven learning, VE can be formally encoded as part of a graphical model. The fundamental inference methodology, therefore, does not need to be altered. Moreover, VE has the flexibility of representing various kinds of prior knowledge. For example, Reynolds and Bilmes (2005) use VE that explicitly favors self transitions in dynamic Bayesian networks.","This work extends the use of VE to CRFs. In essence, VE herein can be viewed as probabilistic constraints in an undirected graph that allow exact inference. One of the biggest challenges of such a model lies in the semi-supervised machine learning setting. Since the entire state sequence of an unlabeled instance remains hidden, the conditional likelihood objective of CRFs is not directly optimizable. There have been a number of works that address this problem for conditional models. For example, minimum entropy regularization (Grandvalet and Bengio, 2004; Jiao et al., 2006), aims to maximize the conditional likelihood of labeled data while minimizing the conditional entropy of unlabeled data: ∑","i log pλ(y(i)","|x(i) ) − 1","2σ2 ∥λ∥2","− ρH(y|x) (3) This approach generally would result in “sharper” models which can be data-sensitive in practice.","Another approach (Suzuki and Isozaki, 2008) embeds a joint probability model (HMM in their","1","We slightly modify the notation here to be consistent with the rest of the paper. 1290 case) into a CRF model as a new potential function. Semi-supervised learning is then conducted by iteratively (1) fixing the HMM and updating CRF parameters on labeled data and (2) fixing the CRF model and updating the HMM on unlabeled data.","Additionally, when unlabeled instances have partial labeling information, it is possible to optimize a marginal distribution of the conditional likelihood, i.e., pλ(y(i)","o |x), on unlabeled data. Here y(i)","o is a subvector of y(i)","that denotes the set of observed state variables. The optimization can be done in a similar fashion as training a hidden-state CRF model (Quattoni et al., 2007)."]},{"title":"3 Task","paragraphs":["We consider the problem of extracting fields from free-text advertisements. We use the CLASSIFIEDS data (Grenager et al., 2005) which consists of 8767 ads for apartment rental. 302 of the ads in the CLASSIFIEDS data have been manually-labeled with 12 fields, including size, rent, neighborhood and so on. The labeled data has been divided into train/dev/test sets with 102/100/100 ads respectively. The evaluation metric is the tokenlevel accuracy where tokens include both words and punctuations.","Our goal in this work is two folds: (1) leverage both the training data and the prior knowledge specified for this task for supervised learning, and (2) additionally use the unlabeled data for semi-supervised learning. We exploit two types of prior knowledge: • K1: label consistency with prototypes; • K2: label consistency within a sentence. K1 involves a set of prototype lists. Each list is attached with a label and consists of a set of example words for that label. In this work, we use the prototype lists originally defined by Haghighi and Klein (2006) (HK06) and subsequently used by Chang et al. (2005) (CRR07) and Mann and McCallum (2008) (MM08). The labels as well as their prototypes are shown in the first two columns of Table 1. Our model is desired to be consistent with such prototype information. Secondly, K2 means that tokens tend to have consistent labels within a sentence. A similar type of prior knowledge is implemented by CRR07 as a constraint in inference."]},{"title":"4 Conditional Random Fields","paragraphs":["Conditional random fields are a probabilistic model that directly optimizes the conditional probability of a state (label) sequence given an input sequence (Lafferty et al., 2001). Formally, we let x = (x1, x2, . . . , xT ) denote an input sequence of T tokens, and y = (y1, y2, . . . , yT ) the corresponding state sequence. We further augment y with two special states, Start and End,2","represented by y0 and yT +1 respectively. A linear-chain CRF model is an undirected graphical model as depicted in Figure 1(a), with the conditional probability given by pλ(y|x) =","1","Zλ(x) ∏","t ψ(t)","λ (x, yt−1, yt) (4) The partition function Zλ(x) normalizes the exponential form to be a probability distribution. ψ(t)","λ are a set of potential functions defined on the maximum cliques of the graph, i.e., (x, yt−1, yt) in the case of a linear-chain CRF model. The potential functions are typically in the form of ψ(t) λ (x, yt−1, yt) = exp(","λ · f (x, yt−1, yt, t)) (5) where λ is a weight vector and f is a feature vector of arbitrary functions of the corresponding clique.","Given a set of labeled examples {x(i)",", y(i)",")}m","i=1, we can estimate model parameters in a supervised machine learning setting. The objective is to estimate λ that maximizes the conditional likelihood while regularizing the model size: L1 = m ∑","i=1 log pλ(y(i)","|x(i)",") − 1","2σ2 ∥λ∥2 (6) In this work, we optimize L1 using stochastic gradient descent and use the accuracy on the development set as the stopping criterion."]},{"title":"5 CRFs with Virtual Evidence","paragraphs":["A canonical way of using virtual evidence (VE) in Bayesian networks is to have a directed edge from a hidden variable h to a VE variable v. The variable v will always be observed with a particular value, e.g., v = 1, but the actual value itself does not matter. The prior knowledge about h is","2","Start and End are with regard to a document, which are different from start and end of a sentence. 1291 x yT End Start y1 y2 (a) (b) x yT End Start y1 y2 vT=1 v1=1 v2=1 vT+1=1 Figure 1: Graphical model representations of (a) a CRF model and (b) a CRF model integrated with virtual evidence. Solid and empty nodes denote observed and hidden variables respectively. expressed via the conditional probability p(v = 1|h). For example, by setting p(v = 1|h = a) > p(v = 1|h = b), we know that h = a is more likely a event than h = b. This conditional distribution is not learned from data, Instead, it is pre-defined in such a way that reflects a prior belief over the value of h.","VE can be encoded in an undirected graphical model in a similar fashion. For our task, we modify the structure of a linear-chain CRF model as depicted in Figure 1(b) — we create a sequence of VE variables, denoted by v1, v2, . . . , vT +1, in parallel to the state variables. Each vt is assigned a constant 1 (one), and is connected with yt−1 and yt, forming a new set of maximum cliques (yt−1, yt, vt), t = 1, . . . , T + 1. We create cliques of size 3 because it is the minimum size required to represent the prior knowledge used in our task, as will be discussed shortly. However, it is possible to have a different graph structure to incorporate other types of prior knowledge, e.g., using large cliques to represent constraints that involve more variables. Next, in analogy to Equation (5), we define the corresponding potential functions as follows,","φ(t) (yt−1, yt, vt) = exp(","ω · s(yt−1, yt, vt, t)) (7) s is a vector of VE feature functions and ω is the corresponding weight vector with pre-defined values. Given the new graphical model in Figure 1(b). It is natural to model the conditional probability of the state sequence given both the standard evidence and the VE as follows, pλ(y|x, v) =","1 Zλ(x, v) ∏","t ψ(t)","λ (x, yt−1, yt)φ(t)","(yt−1, yt, vt)","(8) Analogous to using p(v = 1|h) in Bayesian net-","works, we can utilize φ(t)","(yt−1, yt, v = 1) to ex-","press preferences over state hypotheses in a CRF","model. In general, the function form of φ(t)","may","or may not depend on the input x. Even when φ(t)","does depend on x, the relation is completely deter-","mined by external knowledge/systems (as opposed","to by data). Thus we do not explicitly connect v","with x in the graph. 5.1 Incorporating prior knowledge Now we show how to represent the prior knowledge introduced in Section 3 using the VE feature functions. Unless otherwise stated, we as-sume vt = 1 for all t = 1, . . . , T and simply use vt instead of vt = 1 in all equations. First, we define a VE function s1 that represents K1: label consistency with prototypes. We let Pl denote a prototype list associated with the label l. If xt belongs to Pl, we should prefer yt = l as opposed to other values. To this end, for cases where xt ∈ Pl, we set s1 as","s1(yt, vt, t) = { 1 if yt = l 0 otherwise (9) On the other hand, if xt is not a prototype, we will always have s1(yt, vt, t) = 0 for all hypotheses of yt. The impact of this prior knowledge is controlled by the weight of s1, denote by ω1. At one extreme where ω1 = 0, the prior knowledge is completely ignored in training. At the other extreme where ω1 → +∞, we constrain the values of state variables to agree with the prior knowledge. Note that although s1 is implicitly related to x, we do not write s1 as a function of x for consistency with the general definition of VE. 1292","To represent K2: label consistency within a sentence, we define a second VE feature function s2 with weight ω2. Assume that we have an external system that detects sentence boundaries. If it is determined that xt is not the start of a sentence, we set s2 as","s2(yt−1, yt, vt, t) = { 1 if yt−1 = yt 0 otherwise (10) It is easy to see that this would penalize state transitions within a sentence. On the other hand, if xt is a sentence start, we set s2(yt−1, yt, vt, t) = 0 for all possible (yt−1, yt) pairs. In this work, we use a simple heuristics to detect sentence boundaries: we determine that xt is the start of a sentence if its previous token xt−1 is a period (.), a semi-colon (;) or an acclamation mark (!), and if xt is not a punctuation. 5.2 Semi-supervised learning When a large amount of unlabeled data is available, it is often helpful to leverage such data to improve learning. However, we cannot directly optimize p(y|x, v) since the correct state sequences of the unlabeled data are hidden. One heuristic approach is to adapt the self-training algorithm (Yarowsky, 1995) to our model. More specifically, for each input in the unlabeled dataset {x(i)","}n","i=m+1, we decode the best state sequence,","ŷ(i)","= argmax","y(i) p(y(i)","|x(i)",", v(i)",") (11) Then we use {(x(i)",", ŷ(i)",")}n","i=m+1 in addition to the labeled data to train a supervised CRF model. This approach, however, does not have a theoretical guarantee on optimality unless certain nontrivial conditions are satisfied (Abney, 2004).","On the other hand, it is well known that unlabeled data can be naturally incorporated using a generative approach that models a joint probability (Nigam et al., 2000). This is achieved by maximizing a marginal distribution of the joint probability over hidden variables. Inspired by the generative approach, we propose to explicitly model p(y, v|x). In contrast to Equation (8), here we jointly model y and v but the probability is still conditioned on x. This “joint” distribution should be chosen such that it results in the same conditional distribution p(y|x, v) as defined in Equation (8). To this end, we define pλ(y, v|x) as pλ(y, v|x) =","1","Z′","λ(x) ∏ t ψ(t)","λ (x, yt−1, yt)φ(t)","(yt−1, yt, vt)","(12) Here Z′","λ(x) is a normalization function obtained by summing the numerator over both y and v. By applying the Bayes rule, it is easy to see that p(y|x, v) is exactly equal to Equation (8).","Given unlabeled data {x(i)","}n","i=m+1, we aim to optimize the following objective,3 L2 = m+n ∑","i=1 log pλ(v(i)","|x(i)",") − 1","2σ2 ∥λ∥2 (13) This is essentially the marginal distribution of p(y, v|x) over hidden variables y. Here we ignore the labels of the dataset {(x(i)",", y(i)",")}m","i=1, but we do use the label information in initializing the model which will described in Section 6. To optimize such an objective, we apply the EM algorithm in the same fashion as is used in a generative approach. In other words, we iteratively optimize Q(λ) = ∑","y pλg (y|x, v) log pλ(y, v|x) where λg denotes the model estimated from the previous iteration. The gradient of the Q function is straightforward to compute with the result given by ∂Q(λ) ∂λk = ∑","t ∑","yt−1,yt fk(yt−1, yt, x, t)· ( pλ(yt−1, yt|x, v) − pλ(yt−1, yt|x)) (14) We keep two sets of accumulators in running the Forward-Backward algorithm, one for computing pλ(yt−1, yt|x, v) and the other for computing pλ(yt−1, yt|x). Loosely speaking, the model will converge to a local optimum if the difference between these two posterior probabilities becomes trivial. 5.3 Collocation based virtual evidence Prior knowledge represented by prototypes is typically sparse. This sparse information, however, can be propagated across all data based on distributional similarity (Haghighi and Klein, 2006). Following the same idea, we extend the prototype lists as follows. (1) We merge all prototypes in Pl into a single word type wl. (2) For each word","3","In Equation (13), the fact that v is assigned a constant 1 does not mean p(v = 1|x) = 1 (Bilmes, 2004) 1293 Label Prototype lists of HK06 Collocation lists (top examples) ADDRESS address carlmont [4-digit] street [3-digit] streets AVAILABLE immediately begin cheaper available CONTACT [phone] call [time] [email] appointment email see today ... FEATURES kitchen laundry parking room new covered building garage ... NEIGHBORHOOD close near shopping transportation center located restaurants ... PHOTOS pictures image link [url] click view photos RENT $ month [amount] lease deposit security year agreement ... RESTRICTIONS pets smoking dog ok sorry please allowed negotiable ... ROOMMATES roommate respectful drama SIZE [1-digit] br sq [4-digit] [3-digit] ft bath ba ... UTILITIES utilities pays electricity water included owner garbage paid Table 1: Field labels (except other) for the CLASSIFIEDS task, their respective prototype lists specified by prior knowledge, and collocation lists mined from unlabeled data. in the corpus, we collect a context vector of the counts of all words (excluding stop words) that occur within a window of size k in either direc-tion, where the window is applied only within sentence boundaries. (3) Latent semantic analysis (Deerwester et al., 1990) is performed on the constructed context vectors. (4) In the resulting latent semantic space, all words (except stop words) that have a high enough dot product with wl will be grouped to form a new set, denoted as Cl, which is a superset of Pl. In this regard, Cl can be viewed as lists of noisy “prototypes”. As observed in HK06, another consequence of this method is that many neighboring tokens will share the same prototypes.","Differently from previous works, we use Cl directly as virtual evidence. We could apply s1 in Equation (9) when xt ∈ Cl (as opposed to when xt ∈ Pl). This, however, would contaminate our model since Cl are often noisy. For example, “water” is found to be distributionally similar to the prototypes of utilities. Although in most cases “water” indeed means utilities, it can mean features in the context of “water front view”. To maximally reduce ambiguity, we propose to apply s1 in Equation (9) if both of the following conditions hold, (1) xt ∈ Cl (2) There exists τ s.t. |τ − t| < k, and xτ ∈ Cl In other words, we will impose a non-uniform prior on yt if xt ∈ Cl “collocates”, within k tokens, with another word that belongs to Cl. Based on K2, it is reasonable to believe that neighboring tokens tend to share the same label. Therefore, knowing that two tokens close to each other both belong to Cl would strengthen our belief that either word is likely to have label l. We thus refer to this type of virtual evidence as collocation-based VE, and refer to Cl as collocation lists."]},{"title":"6 Evaluation","paragraphs":["We use the CLASSIFIEDS data provided by Grenager et al. (2005) and compare with results reported by CRR07 (Chang et al., 2007) and MM08 (Mann and McCallum, 2008) for both supervised and semi-supervised learning. Following all previous works conducted on this task, we to-kenized both words and punctuations, and created a number of regular expression tokens for phone numbers, email addresses, URLs, dates, money amounts and so on. However, we did not tokenize newline breaks, as CRR07 did, which might be useful in determining sentence boundaries. Based on such tokenization, we extract n-grams, n = 1, 2, 3, from the corpus as features for CRFs.","As described in Section 3, we integrate the prior knowledge K1 and K2 in our CRF model. The prototypes that represent K1 are given in Table 1. CRR07 used the same two kinds of prior knowledge in the form of constraints, and they implemented another constraint on the minimum number of words in a field chunk. MM08 used almost the same set of prototypes as labeled features, but they exploited two sets of 33 additional features for some experiments. In this regard, the comparison between CRR07, MM08 and the method presented here cannot be exact. However, we show that while our prior knowledge is no more than that used in previous works, our approach is able 1294 # labeled examples Supervised model 10 25 100 CRR07: HMM 61.6 70.0 76.3 + Constr in decoding 66.1 73.7 80.4 MM08: CRF 64.6 72.9 79.4 CRF 62.3 71.4 79.1 + VE in decoding 68.9 74.6 81.1 CRF + VE (auto weights) 48.0 54.8 59.8 + VE in decoding 66.0 72.5 80.9 Table 2: Token-level accuracy of supervised learning methods; “+ VE” refers to the cases where both kinds of prior knowledge, K1 and K2, are incorporated as VE in the CRF model. to achieve the state-of-art performance. 6.1 Decoding settings Depending on whether VE is used at test time, we explore two decoding settings in all experiments:","1. Find y that maximizes pλ(y|x) as in standard CRF decoding, ignoring virtual evidence.","2. Find y that maximizes p(y|x, v). We use “+ VE in decoding” to represent this setting. These two scenarios are analogous to those in CRR07 which conducted HMM decoding without/with constraints applied. We use “+ constr. in decoding” to represent the latter scenario of their work. MM08, on the other hand, found no accuracy improvement when adding constraints at test time.","Note that in our second decoding setting, the weights for the VE feature functions, i.e., ω1 and ω2, are tuned on the development set. This is done by a greedy search that first finds the best ω1, and then finds the best ω2 while fixing the value of ω1, both with a step size 0.5. 6.2 Supervised learning results First, we experimented with a standard CRF model with VE applied neither in training nor in decoding. As shown in Table 2, our CRF implementation performed slightly worse than the implementation by MM08, probably due to slight difference in tokenization. Secondly, we used the same CRF model but additionally applied VE in decoding, corresponding to the second setting in Section 6.1. This method gave a significant boost to the tagging performance, yielding the best supervised learning results (shown as bolded in the # labeled examples Semi-supervised models 10 25 100 CRR07: HMM + Constr 70.9 74.8 78.6 + Constr in decoding 74.7 78.5 81.7 MM08: CRF + GE 72.6 76.3 80.1 CRF + VE (Self-train) 69.0 74.2 81.4 + VE in decoding 69.1 75.2 81.2 CRF + Col-VE (Self-train) 73.1 76.4 81.8 + Col-VE in decoding 75.7 77.6 82.9 CRF + Col-VE (EM) 78.3 79.1 82.7 + Col-VE in decoding 78.8 79.5 82.9 Table 3: Token-level accuracy of semi-supervised learning methods. “+ Col-VE” refers to cases where collocation-based VE is integrated in the CRF model in addition to the VE representing K1 and K2. table). This proves that the prior knowledge is indeed complementary to the information offered by the training data.","Similar to the second decoding setting that incorporates VE, we can have a counterpart setting at training time. In other words, we can optimize pλ(y|x, v) instead of pλ(y|x) during learning. In deciding ω = (ω1, ω2), it is possible to learn ω from data in the same way as how we learn λ. This, however, might undermine the role of other useful features since we do not always have sufficient training data to reliably estimate the weight of prior knowledge. As shown in Table 2, we experimented with learning ω automatically (shown as “auto weights”). While applying VE with such weights in both training and decoding worked reasonably well, applying VE only in training but not in decoding yielded very poor performance (probably due to excessively large estimates of ω1 and ω2). Additionally, we repeated the above experi-ment with manually specified weights, but did not find further accuracy improvement over the best supervised learning results. 6.3 Semi-supervised learning results One natural way of leveraging the unlabeled data (more than 8K examples) is to perform semi-supervised learning in a self-training fashion. To this end, we used our best supervised model in Table 2 to decode the unlabeled examples as well as the test-set examples (by treating them as unlabeled). Note that by doing this our comparison with CRR07 and MM08 cannot be exact as they 1295 sampled the unlabeled examples, with different rates, for semi-supervised learning, while we used as much data as possible. We applied the same ω that was used for the supervised model, and then combined the newly labeled examples, in addition to the manually labeled ones, as training data to learn a supervised CRF model. On this particular dataset, we did not find it helpful by selecting automatically labeled data based on a confidence threshold. We simply used all data available in self-training. This paradigm is referred to as “CRF + VE (self-train)” in Table 3. When no VE is applied at test time, this semi-supervised CRF model significantly outperformed the best model in Table 2. When applying VE at test time, however, the improvement over its supervised counterpart became trivial.","Next, following Section 5.3, we collected context vectors on the unlabeled data using a window size k = 3, and extracted the top 50 singular vectors therefrom.4","We created collocation lists that contain words close to the merged prototype words in the latent semantic space. Some examples are given in the last column of Table 1. We then augmented the prototype-based VE based on the following rules: If xt belongs to any prototype list Pl, we directly apply s1 in Equation (9); otherwise, we apply s1 if xt and at least one neighbor (within 3 tokens from xt) belong to the same collocation list Cl. In our experiments, we let “Col-VE” represent such collocation-based VE. We conducted self-training using a CRF model integrated with Col-VE, where ω was tuned a priori by testing the same model on the development set. As shown in the table, “CRF + Col-VE (self-train)” gave significant accuracy improvement over “CRF + VE”, while adding Col-VE at test time further boosted the performance. The accuracies were already on par with the best results previously reported on this task.","Finally, we implemented the EM algorithm proposed in Section 5.2 that iteratively optimizes p(v|x) on all data. The model was initial-ized by the one obtained from “CRF + Col-VE (self-train)”. After the model was initialized, we performed the EM algorithm until the model reached a maximum accuracy on the development set. Note that in some cases, we observed a development-set accuracy degradation after the first iteration of the EM, but the accuracy quickly 4 The same configuration is used in HK06. recovered from the second iteration and kept in-creasing until a maximum accuracy was reached.5 As shown in the last two rows in Table 3, this method is clearly advantageous over self-training, leading to the best tagging accuracies in both decoding settings. Our model achieved 2.6% − 5.7% absolute accuracy increases in the three training settings compared with MM08 which had the best results without using any constraints in decoding. When applying VE at test time, our model was 1.2% − 4.1% better than CRR07 which had the best overall results. Additionally, when compared with supervised learning results, our best semi-supervised model trained on only 10 labeled examples performed almost as well as a standard supervised CRF model trained on 100 labeled examples."]},{"title":"7 Conclusions","paragraphs":["We have presented the use of virtual evidence as a principled way of incorporating prior knowledge into conditional random fields. A key contribu-tion of our work is the introduction of a novel semi-supervised learning objective for training a CRF model integrated with VE. We also found it useful to create so-called collocation-based VE, assuming that tokens close to each other tend to have consistent labels. Our evaluation on the CLASSIFIEDS data showed that the learning objective presented here, combined with the use of collocation-based VE, yielded remarkably good accuracy performance. In the future, we would like to see the application of our approach to other tasks such as (Li et al., 2009)."]},{"title":"References","paragraphs":["Steven Abney. 2004. Understanding the Yarowsky algorithm. Association for Computational Linguistics, 30(3):365–395. Jeff Bilmes. 2004. On soft evidence in Bayesian networks. Technical Report UWEETR-2004-0016, University of Washington, Dept. of Electrical Engineering.","Ming-Wei Chang, Lev Ratinov, and Dan Roth. 2007. Guiding semi-supervision with constraint-driven learning. In Proceedings of ACL.","5","The initial degradation is probably due to the fact that self-training can result in an initial model with decent accuracy but low p(v|x); thus the EM algorithm that maximizes p(v|x) may temporarily decrease the accuracy. 1296","Scott Deerwester, Susan Dumais, Thomas Landauer, George Furnas, and Richard Harshman. 1990. Indexing by latent semantic analysis. Journal of the American Society of Information Science, 41(6):391–407.","Yves Grandvalet and Yoshua Bengio. 2004. Semi-supervised learning by entropy minimization. In Advances in Neural Information Processing Systems.","Trond Grenager, Dan Klein, and Christopher D. Manning. 2005. Unsupervised learning for field segmentation models for information extraction. In Proceedings of Association of Computational Linguistics.","Aria Haghighi and Dan Klein. 2006. Prototype-driven learning for sequence models. In Proceedings of HLT-NAACL.","Feng Jiao, Shaojun Wang, Chi-Hoon Lee, Russell Greiner, and Dale Schuurmans. 2006. Semi-supervised conditional random fields for improved sequence segmentation and labeling. In Proceedings of ACL.","John Lafferty, Andrew McCallum, and Fernando Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of the International Conference on Machine Learning, pages 282–289.","Xiao Li, Ye-Yi Wang, and Alex Acero. 2009. Extract-ing structured information from user queries with semi-supervised conditional random fields. In Proceedings of SIGIR.","Gideon Mann and Andrew McCallum. 2008. Generalized expectation criteria for semi-supervised learning of conditional random fields. In Proceedings of ACL.","Kamal Nigam, Andrew Kachites Mccallum, Sebastian Thrun, and Tom Mitchell. 2000. Text classification from labeled and unlabeled documents using EM. Machine Learning, 39:103–134.","Judea Pearl. 1988. Probabilistic Reasoning in In-telligent Systems: Networks of Plausible Inference. Morgan Kaufmann, 2nd printing edition edition.","A. Quattoni, S. Wang, L.-P. Morency, M. Collins, and T. Darrell. 2007. Hidden conditional random fields. IEEE Transaction on Pattern Analysis and Machine Intellegence, 29(10):1848–1852.","Sheila Reynolds and Jeff Bilmes. 2005. Part-of-speech tagging using virtual evidence and negative training. In Proceedings of HLT/EMNLP.","Jun Suzuki and Hideki Isozaki. 2008. Semi-supervised sequential labeling and segmentation using giga-word scale unlabeled data. In Proceedings of ACL/HLT.","David Yarowsky. 1995. Unsupervised word sense disambiguation rivaling supervised methods. In Proceedings of ACL, pages 189–196. 1297"]}]}
